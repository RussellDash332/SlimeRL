{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evostra import EvolutionStrategy\n",
    "from evostra.models import FeedForwardNetwork\n",
    "\n",
    "# A feed forward neural network with input size of 5, two hidden layers of size 4 and output of size 3\n",
    "model = FeedForwardNetwork(layer_sizes=[5, 4, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = np.array([0.1, -0.4, 0.5])\n",
    "inp = np.asarray([1, 2, 3, 4, 5])\n",
    "\n",
    "def get_reward(weights):\n",
    "    global solution, model, inp\n",
    "    model.set_weights(weights)\n",
    "    prediction = model.predict(inp)\n",
    "    # here our best reward is zero\n",
    "    reward = -np.sum(np.square(solution - prediction))\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100. reward: -0.008695\n",
      "iter 200. reward: -0.006042\n",
      "iter 300. reward: -0.000917\n",
      "iter 400. reward: -0.001901\n",
      "iter 500. reward: -0.000459\n",
      "iter 600. reward: -0.000287\n",
      "iter 700. reward: -0.000939\n",
      "iter 800. reward: -0.000504\n",
      "iter 900. reward: -0.000522\n",
      "iter 1000. reward: -0.000178\n"
     ]
    }
   ],
   "source": [
    "# if your task is computationally expensive, you can use num_threads > 1 to use multiple processes;\n",
    "# if you set num_threads=-1, it will use number of cores available on the machine; Here we use 1 process as the\n",
    "#  task is not computationally expensive and using more processes would decrease the performance due to the IPC overhead.\n",
    "es = EvolutionStrategy(model.get_weights(), get_reward, population_size=20, sigma=0.1, learning_rate=0.03, decay=0.995, num_threads=1)\n",
    "es.run(1000, print_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_weights = es.get_weights()\n",
    "model.set_weights(optimized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_ppo1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[721]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "from slimevolleygym import SurvivalRewardEnv\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "NUM_TIMESTEPS = int(5e6)\n",
    "SEED = 721\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "LOGDIR = \"bnn_ppo1\" # moved to zoo afterwards.\n",
    "\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FeedForwardNetwork' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7c4be929f7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = PPO1(model, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m                  optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOGDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOGDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVAL_FREQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVAL_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, gamma, timesteps_per_actorbatch, clip_param, entcoeff, optim_epochs, optim_stepsize, optim_batchsize, lam, adam_epsilon, schedule, verbose, tensorboard_log, _init_setup_model, policy_kwargs, full_tensorboard_log, seed, n_cpu_tf_sess)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pretrain_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# Construct network for new policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 self.policy_pi = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n\u001b[0;32m--> 108\u001b[0;31m                                              None, reuse=False, **self.policy_kwargs)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# Network for old policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FeedForwardNetwork' object is not callable"
     ]
    }
   ],
   "source": [
    "model = PPO1(model, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slime-rl",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
