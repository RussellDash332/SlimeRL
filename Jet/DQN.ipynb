{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_dqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 20.0   memory length: 21   epsilon: 0.9792086759647052\n",
      "episode: 1   score: 12.0   memory length: 34   epsilon: 0.9665550620990835\n",
      "episode: 2   score: 20.0   memory length: 55   epsilon: 0.946459102605027\n",
      "episode: 3   score: 41.0   memory length: 97   epsilon: 0.9075119613694457\n",
      "episode: 4   score: 9.0   memory length: 107   epsilon: 0.8984775710829266\n",
      "episode: 5   score: 10.0   memory length: 118   epsilon: 0.8886435861147077\n",
      "episode: 6   score: 14.0   memory length: 133   epsilon: 0.8754068367770318\n",
      "episode: 7   score: 12.0   memory length: 146   epsilon: 0.8640945798906344\n",
      "episode: 8   score: 11.0   memory length: 158   epsilon: 0.8537822855004553\n",
      "episode: 9   score: 19.0   memory length: 178   epsilon: 0.8368678892362568\n",
      "episode: 10   score: 13.0   memory length: 192   epsilon: 0.8252275899809898\n",
      "episode: 11   score: 26.0   memory length: 219   epsilon: 0.8032337005612527\n",
      "episode: 12   score: 27.0   memory length: 247   epsilon: 0.7810441642583167\n",
      "episode: 13   score: 9.0   memory length: 257   epsilon: 0.7732687760416481\n",
      "episode: 14   score: 21.0   memory length: 279   epsilon: 0.7564343028582378\n",
      "episode: 15   score: 32.0   memory length: 312   epsilon: 0.7318672718448656\n",
      "episode: 16   score: 9.0   memory length: 322   epsilon: 0.724581445483085\n",
      "episode: 17   score: 11.0   memory length: 334   epsilon: 0.7159341314628666\n",
      "episode: 18   score: 13.0   memory length: 348   epsilon: 0.7059759437435444\n",
      "episode: 19   score: 9.0   memory length: 358   epsilon: 0.6989478686545415\n",
      "episode: 20   score: 13.0   memory length: 372   epsilon: 0.689225949030651\n",
      "episode: 21   score: 11.0   memory length: 384   epsilon: 0.6810005752658319\n",
      "episode: 22   score: 19.0   memory length: 404   epsilon: 0.6675091808180759\n",
      "episode: 23   score: 9.0   memory length: 414   epsilon: 0.660864046961939\n",
      "episode: 24   score: 11.0   memory length: 426   epsilon: 0.65297715036201\n",
      "episode: 25   score: 14.0   memory length: 441   epsilon: 0.6432507594921204\n",
      "episode: 26   score: 17.0   memory length: 459   epsilon: 0.6317701402576925\n",
      "episode: 27   score: 10.0   memory length: 470   epsilon: 0.6248553120386914\n",
      "episode: 28   score: 8.0   memory length: 479   epsilon: 0.6192540566123834\n",
      "episode: 29   score: 15.0   memory length: 495   epsilon: 0.6094199565354498\n",
      "episode: 30   score: 9.0   memory length: 505   epsilon: 0.6033531078655693\n",
      "episode: 31   score: 13.0   memory length: 519   epsilon: 0.5949608504704863\n",
      "episode: 32   score: 12.0   memory length: 532   epsilon: 0.5872725966265356\n",
      "episode: 33   score: 15.0   memory length: 548   epsilon: 0.5779463799857277\n",
      "episode: 34   score: 18.0   memory length: 567   epsilon: 0.5670636698003494\n",
      "episode: 35   score: 11.0   memory length: 579   epsilon: 0.5602962074911925\n",
      "episode: 36   score: 10.0   memory length: 590   epsilon: 0.5541636732359665\n",
      "episode: 37   score: 10.0   memory length: 601   epsilon: 0.548098260578011\n",
      "episode: 38   score: 11.0   memory length: 613   epsilon: 0.5415571356255309\n",
      "episode: 39   score: 11.0   memory length: 625   epsilon: 0.5350940738940482\n",
      "episode: 40   score: 9.0   memory length: 635   epsilon: 0.5297671482893791\n",
      "episode: 41   score: 11.0   memory length: 647   epsilon: 0.5234447908547367\n",
      "episode: 42   score: 10.0   memory length: 658   epsilon: 0.5177156014229363\n",
      "episode: 43   score: 9.0   memory length: 668   epsilon: 0.5125616805934888\n",
      "episode: 44   score: 10.0   memory length: 679   epsilon: 0.5069516085956246\n",
      "episode: 45   score: 8.0   memory length: 688   epsilon: 0.5024072518560504\n",
      "episode: 46   score: 11.0   memory length: 700   epsilon: 0.4964114134310989\n",
      "episode: 47   score: 12.0   memory length: 713   epsilon: 0.4899966435273741\n",
      "episode: 48   score: 9.0   memory length: 723   epsilon: 0.48511866824423777\n",
      "episode: 49   score: 30.0   memory length: 754   epsilon: 0.4703034042831738\n",
      "episode: 50   score: 12.0   memory length: 767   epsilon: 0.4642260095219155\n",
      "episode: 51   score: 10.0   memory length: 778   epsilon: 0.45914497940338683\n",
      "episode: 52   score: 13.0   memory length: 792   epsilon: 0.452758565214779\n",
      "episode: 53   score: 8.0   memory length: 801   epsilon: 0.44869999946146477\n",
      "episode: 54   score: 8.0   memory length: 810   epsilon: 0.44467781503197196\n",
      "episode: 55   score: 8.0   memory length: 819   epsilon: 0.4406916858010624\n",
      "episode: 56   score: 9.0   memory length: 829   epsilon: 0.4363045472783448\n",
      "episode: 57   score: 9.0   memory length: 839   epsilon: 0.4319610832451573\n",
      "episode: 58   score: 16.0   memory length: 856   epsilon: 0.424676198829174\n",
      "episode: 59   score: 10.0   memory length: 867   epsilon: 0.42002804790136294\n",
      "episode: 60   score: 9.0   memory length: 877   epsilon: 0.41584661836923925\n",
      "episode: 61   score: 10.0   memory length: 888   epsilon: 0.41129510865353336\n",
      "episode: 62   score: 10.0   memory length: 899   epsilon: 0.4067934159611651\n",
      "episode: 63   score: 9.0   memory length: 909   epsilon: 0.402743738775386\n",
      "episode: 64   score: 13.0   memory length: 923   epsilon: 0.3971418299163796\n",
      "episode: 65   score: 7.0   memory length: 931   epsilon: 0.3939757930361214\n",
      "episode: 66   score: 14.0   memory length: 946   epsilon: 0.3881073448764583\n",
      "episode: 67   score: 12.0   memory length: 959   epsilon: 0.38309211104426205\n",
      "episode: 68   score: 10.0   memory length: 970   epsilon: 0.3788991048049279\n",
      "episode: 69   score: 8.0   memory length: 979   epsilon: 0.3755026214496253\n",
      "episode: 70   score: 9.0   memory length: 989   epsilon: 0.3717644478715407\n",
      "episode: 71   score: 11.0   memory length: 1001   epsilon: 0.36732772934619257\n",
      "episode: 72   score: 9.0   memory length: 1011   epsilon: 0.3636709377982701\n",
      "episode: 73   score: 11.0   memory length: 1023   epsilon: 0.3593308089987087\n",
      "episode: 74   score: 12.0   memory length: 1036   epsilon: 0.3546874337726758\n",
      "episode: 75   score: 11.0   memory length: 1048   epsilon: 0.35045451608208705\n",
      "episode: 76   score: 10.0   memory length: 1059   epsilon: 0.3466187336940617\n",
      "episode: 77   score: 9.0   memory length: 1069   epsilon: 0.34316810267859194\n",
      "episode: 78   score: 12.0   memory length: 1082   epsilon: 0.3387335865546259\n",
      "episode: 79   score: 10.0   memory length: 1093   epsilon: 0.3350260916703695\n",
      "episode: 80   score: 10.0   memory length: 1104   epsilon: 0.33135917592813624\n",
      "episode: 81   score: 9.0   memory length: 1114   epsilon: 0.3280604556381725\n",
      "episode: 82   score: 14.0   memory length: 1129   epsilon: 0.3231738463307536\n",
      "episode: 83   score: 11.0   memory length: 1141   epsilon: 0.31931701871011153\n",
      "episode: 84   score: 9.0   memory length: 1151   epsilon: 0.3161381795377863\n",
      "episode: 85   score: 8.0   memory length: 1160   epsilon: 0.31330429038059615\n",
      "episode: 86   score: 10.0   memory length: 1171   epsilon: 0.30987512333041833\n",
      "episode: 87   score: 110.0   memory length: 1282   epsilon: 0.27730385413804465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 88   score: 109.0   memory length: 1392   epsilon: 0.24840458664265946\n",
      "episode: 89   score: 156.0   memory length: 1549   epsilon: 0.21229573144400934\n",
      "episode: 90   score: 76.0   memory length: 1626   epsilon: 0.1965548911501048\n",
      "episode: 91   score: 130.0   memory length: 1757   epsilon: 0.17241014339140628\n",
      "episode: 92   score: 139.0   memory length: 1897   epsilon: 0.1498756793589105\n",
      "episode: 93   score: 62.0   memory length: 1960   epsilon: 0.14072035528032842\n",
      "episode: 94   score: 74.0   memory length: 2000   epsilon: 0.13054749411608307\n",
      "episode: 95   score: 88.0   memory length: 2000   epsilon: 0.11942547903549745\n",
      "episode: 96   score: 125.0   memory length: 2000   epsilon: 0.10528063808739813\n",
      "episode: 97   score: 114.0   memory length: 2000   epsilon: 0.093838196948523\n",
      "episode: 98   score: 126.0   memory length: 2000   epsilon: 0.0826412084436871\n",
      "episode: 99   score: 203.0   memory length: 2000   epsilon: 0.06738391769911038\n",
      "episode: 100   score: 61.0   memory length: 2000   epsilon: 0.06333102648821683\n",
      "episode: 101   score: 74.0   memory length: 2000   epsilon: 0.05875274256781066\n",
      "episode: 102   score: 137.0   memory length: 2000   epsilon: 0.05117590357210289\n",
      "episode: 103   score: 104.0   memory length: 2000   epsilon: 0.04607250046787233\n",
      "episode: 104   score: 89.0   memory length: 2000   epsilon: 0.04210519892026722\n",
      "episode: 105   score: 69.0   memory length: 2000   epsilon: 0.03925725231456919\n",
      "episode: 106   score: 134.0   memory length: 2000   epsilon: 0.03429736930118335\n",
      "episode: 107   score: 56.0   memory length: 2000   epsilon: 0.032396167716827545\n",
      "episode: 108   score: 164.0   memory length: 2000   epsilon: 0.027466239073575264\n",
      "episode: 109   score: 411.0   memory length: 2000   epsilon: 0.01818780733585145\n",
      "episode: 110   score: 397.0   memory length: 2000   epsilon: 0.012213627178383235\n",
      "episode: 111   score: 96.0   memory length: 2000   epsilon: 0.011084012756089733\n",
      "episode: 112   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 113   score: 407.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 114   score: 370.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 115   score: 233.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 116   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 117   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 118   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 119   score: 300.0   memory length: 2000   epsilon: 0.009998671593271896\n",
      "episode: 120   score: 500.0   memory length: 2000   epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9fcc37d097df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# get action for the current state and go one step in environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2180cc54cfe0>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mq_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scores, episodes = [], []\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        reward = reward if not done or score == 499 else -100\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            score = score if score == 500 else score + 100\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "            if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
