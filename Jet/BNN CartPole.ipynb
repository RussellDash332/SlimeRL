{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseFlipout(layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseFlipout(pi_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseFlipout(vf_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import ActorCriticPolicy, nature_cnn\n",
    "\n",
    "class FeedForwardPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy\n",
    "        (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor\n",
    "        documentation for details).\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, net_arch=None,\n",
    "                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            if feature_extraction == \"cnn\":\n",
    "                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "            elif feature_extraction == \"bnn\":\n",
    "                pi_latent, vf_latent = bnn_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "            else:\n",
    "                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import zip_longest\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.distributions import make_proba_dist_type, CategoricalProbabilityDistribution, \\\n",
    "    MultiCategoricalProbabilityDistribution, DiagGaussianProbabilityDistribution, BernoulliProbabilityDistribution\n",
    "from stable_baselines.common.input import observation_input\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a Bayesian neural net (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to dnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"dnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002B5E3718E08>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00969 |       0.00000 |      94.98589 |       0.00422 |       0.68881\n",
      "     -0.01565 |       0.00000 |      80.90788 |       0.00764 |       0.68535\n",
      "     -0.01750 |       0.00000 |      51.83912 |       0.00793 |       0.68505\n",
      "     -0.02026 |       0.00000 |      26.18728 |       0.00861 |       0.68437\n",
      "     -0.02339 |       0.00000 |      17.64771 |       0.01008 |       0.68290\n",
      "     -0.02584 |       0.00000 |      15.85339 |       0.01176 |       0.68122\n",
      "     -0.02714 |       0.00000 |      14.70422 |       0.01268 |       0.68031\n",
      "     -0.02843 |       0.00000 |      13.50612 |       0.01320 |       0.67980\n",
      "     -0.02917 |       0.00000 |      12.44080 |       0.01368 |       0.67933\n",
      "     -0.02963 |       0.00000 |      11.63809 |       0.01379 |       0.67923\n",
      "Evaluating losses...\n",
      "     -0.03005 |       0.00000 |      11.31306 |       0.01308 |       0.67993\n",
      "----------------------------------\n",
      "| EpLenMean       | 23.7         |\n",
      "| EpRewMean       | 23.7         |\n",
      "| EpThisIter      | 169          |\n",
      "| EpisodesSoFar   | 169          |\n",
      "| TimeElapsed     | 4.2          |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | 0.0004       |\n",
      "| loss_ent        | 0.67993355   |\n",
      "| loss_kl         | 0.01307779   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.030052606 |\n",
      "| loss_vf_loss    | 11.313062    |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.01023 |       0.00000 |      54.02348 |       0.00268 |       0.66773\n",
      "     -0.01524 |       0.00000 |      40.25584 |       0.00304 |       0.66559\n",
      "     -0.01664 |       0.00000 |      37.14255 |       0.00347 |       0.66422\n",
      "     -0.01724 |       0.00000 |      35.37858 |       0.00378 |       0.66343\n",
      "     -0.01769 |       0.00000 |      34.07252 |       0.00396 |       0.66308\n",
      "     -0.01822 |       0.00000 |      33.06485 |       0.00422 |       0.66239\n",
      "     -0.01851 |       0.00000 |      32.24352 |       0.00435 |       0.66221\n",
      "     -0.01893 |       0.00000 |      31.52555 |       0.00458 |       0.66169\n",
      "     -0.01915 |       0.00000 |      30.87495 |       0.00476 |       0.66136\n",
      "     -0.01940 |       0.00000 |      30.26494 |       0.00493 |       0.66103\n",
      "Evaluating losses...\n",
      "     -0.01952 |       0.00000 |      29.92483 |       0.00444 |       0.66236\n",
      "---------------------------------\n",
      "| EpLenMean       | 32.2        |\n",
      "| EpRewMean       | 32.2        |\n",
      "| EpThisIter      | 126         |\n",
      "| EpisodesSoFar   | 295         |\n",
      "| TimeElapsed     | 8.48        |\n",
      "| TimestepsSoFar  | 8192        |\n",
      "| ev_tdlam_before | 0.21        |\n",
      "| loss_ent        | 0.66236424  |\n",
      "| loss_kl         | 0.004436978 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | -0.01951667 |\n",
      "| loss_vf_loss    | 29.924828   |\n",
      "---------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00340 |       0.00000 |      77.06859 |       0.00027 |       0.65883\n",
      "     -0.00453 |       0.00000 |      69.38636 |       0.00028 |       0.65829\n",
      "     -0.00494 |       0.00000 |      65.65818 |       0.00027 |       0.65840\n",
      "     -0.00509 |       0.00000 |      63.42146 |       0.00028 |       0.65822\n",
      "     -0.00516 |       0.00000 |      61.85703 |       0.00031 |       0.65793\n",
      "     -0.00527 |       0.00000 |      60.61993 |       0.00032 |       0.65789\n",
      "     -0.00529 |       0.00000 |      59.56841 |       0.00034 |       0.65761\n",
      "     -0.00538 |       0.00000 |      58.64883 |       0.00034 |       0.65769\n",
      "     -0.00535 |       0.00000 |      57.79991 |       0.00036 |       0.65750\n",
      "     -0.00542 |       0.00000 |      57.02058 |       0.00037 |       0.65749\n",
      "Evaluating losses...\n",
      "     -0.00549 |       0.00000 |      56.63147 |       0.00038 |       0.65733\n",
      "-----------------------------------\n",
      "| EpLenMean       | 42.1          |\n",
      "| EpRewMean       | 42.1          |\n",
      "| EpThisIter      | 96            |\n",
      "| EpisodesSoFar   | 391           |\n",
      "| TimeElapsed     | 12.7          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | 0.275         |\n",
      "| loss_ent        | 0.657331      |\n",
      "| loss_kl         | 0.00038263167 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0054938477 |\n",
      "| loss_vf_loss    | 56.631466     |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "dnn = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "dnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "dnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"bnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002B5E38FACC8>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00146 |       0.00000 |      89.24778 |      8.30e-05 |       0.69307\n",
      "     -0.00557 |       0.00000 |      86.26679 |       0.00073 |       0.69243\n",
      "     -0.01185 |       0.00000 |      82.83413 |       0.00353 |       0.68967\n",
      "     -0.01629 |       0.00000 |      78.88023 |       0.00735 |       0.68591\n",
      "     -0.01717 |       0.00000 |      74.26678 |       0.00875 |       0.68453\n",
      "     -0.01806 |       0.00000 |      69.10488 |       0.00883 |       0.68445\n",
      "     -0.01902 |       0.00000 |      63.59353 |       0.00890 |       0.68438\n",
      "     -0.02016 |       0.00000 |      57.98149 |       0.00952 |       0.68376\n",
      "     -0.02159 |       0.00000 |      52.46237 |       0.00961 |       0.68367\n",
      "     -0.02315 |       0.00000 |      47.11676 |       0.01053 |       0.68277\n",
      "Evaluating losses...\n",
      "     -0.02390 |       0.00000 |      44.51559 |       0.01051 |       0.68279\n",
      "---------------------------------\n",
      "| EpLenMean       | 22.8        |\n",
      "| EpRewMean       | 22.8        |\n",
      "| EpThisIter      | 180         |\n",
      "| EpisodesSoFar   | 180         |\n",
      "| TimeElapsed     | 6.97        |\n",
      "| TimestepsSoFar  | 4096        |\n",
      "| ev_tdlam_before | -0.00123    |\n",
      "| loss_ent        | 0.68278795  |\n",
      "| loss_kl         | 0.010505633 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | -0.02389669 |\n",
      "| loss_vf_loss    | 44.51559    |\n",
      "---------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00174 |       0.00000 |      93.18971 |       0.00021 |       0.68260\n",
      "     -0.00470 |       0.00000 |      86.35285 |       0.00103 |       0.67841\n",
      "     -0.00600 |       0.00000 |      80.11701 |       0.00157 |       0.67610\n",
      "     -0.00685 |       0.00000 |      74.39065 |       0.00199 |       0.67458\n",
      "     -0.00748 |       0.00000 |      69.04443 |       0.00209 |       0.67422\n",
      "     -0.00825 |       0.00000 |      64.14536 |       0.00230 |       0.67369\n",
      "     -0.00882 |       0.00000 |      59.58319 |       0.00228 |       0.67381\n",
      "     -0.00916 |       0.00000 |      55.31921 |       0.00245 |       0.67342\n",
      "     -0.00981 |       0.00000 |      51.40889 |       0.00247 |       0.67346\n",
      "     -0.00952 |       0.00000 |      47.78319 |       0.00244 |       0.67362\n",
      "Evaluating losses...\n",
      "     -0.01022 |       0.00000 |      46.10051 |       0.00274 |       0.67279\n",
      "----------------------------------\n",
      "| EpLenMean       | 31           |\n",
      "| EpRewMean       | 31           |\n",
      "| EpThisIter      | 134          |\n",
      "| EpisodesSoFar   | 314          |\n",
      "| TimeElapsed     | 12.2         |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | -0.0462      |\n",
      "| loss_ent        | 0.6727895    |\n",
      "| loss_kl         | 0.002742334  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.010215976 |\n",
      "| loss_vf_loss    | 46.100506    |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00017 |       0.00000 |      90.17539 |       0.00019 |       0.67320\n",
      "     -0.00021 |       0.00000 |      87.77088 |       0.00029 |       0.67131\n",
      "     -0.00128 |       0.00000 |      85.50111 |       0.00034 |       0.67061\n",
      "     -0.00128 |       0.00000 |      83.39241 |       0.00033 |       0.67070\n",
      "     -0.00158 |       0.00000 |      81.43102 |       0.00034 |       0.67072\n",
      "     -0.00146 |       0.00000 |      79.51936 |       0.00038 |       0.67023\n",
      "     -0.00216 |       0.00000 |      77.70733 |       0.00035 |       0.67051\n",
      "     -0.00159 |       0.00000 |      76.01561 |       0.00036 |       0.67037\n",
      "     -0.00172 |       0.00000 |      74.33300 |       0.00041 |       0.67008\n",
      "     -0.00148 |       0.00000 |      72.67845 |       0.00042 |       0.66993\n",
      "Evaluating losses...\n",
      "     -0.00184 |       0.00000 |      71.88917 |       0.00042 |       0.66997\n",
      "-----------------------------------\n",
      "| EpLenMean       | 33.7          |\n",
      "| EpRewMean       | 33.7          |\n",
      "| EpThisIter      | 116           |\n",
      "| EpisodesSoFar   | 430           |\n",
      "| TimeElapsed     | 17.6          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | -0.00693      |\n",
      "| loss_ent        | 0.66997373    |\n",
      "| loss_kl         | 0.00041666607 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018373987 |\n",
      "| loss_vf_loss    | 71.88917      |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "bnn = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "bnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "bnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 175.349, Std reward: 38.768868941458685\n",
      "BNN - Mean reward: 179.614, Std reward: 34.566848337677534\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 175.151, Std reward: 38.99818199608797\n",
      "BNN - Mean reward: 164.046, Std reward: 45.66565321989822\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 156.935, Std reward: 48.66798511341928\n",
      "BNN - Mean reward: 189.304, Std reward: 18.416991719605022\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 161.503, Std reward: 47.323144348193935\n",
      "BNN - Mean reward: 178.989, Std reward: 32.992982268961384\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
