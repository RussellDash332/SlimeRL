{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO-CMA\n",
    "\n",
    "Proximal Policy Optimisation with Covariance Matrix Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating toy data\n",
      "Initializing matplotlib\n",
      "Creating model\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\losses\\losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Initializing model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing\n",
      "Iter 0/8000, loss 0.6020018458366394\n",
      "Iter 100/8000, loss 0.3761787712574005\n",
      "Iter 200/8000, loss 0.09893457591533661\n",
      "Iter 300/8000, loss 0.05290144681930542\n",
      "Iter 400/8000, loss 0.03515776991844177\n",
      "Iter 500/8000, loss 0.027424192056059837\n",
      "Iter 600/8000, loss 0.02095777541399002\n",
      "Iter 700/8000, loss 0.017665307968854904\n",
      "Iter 800/8000, loss 0.015292865224182606\n",
      "Iter 900/8000, loss 0.013497601263225079\n",
      "Iter 1000/8000, loss 0.012439178302884102\n",
      "Iter 1100/8000, loss 0.012007561512291431\n",
      "Iter 1200/8000, loss 0.01711842603981495\n",
      "Iter 1300/8000, loss 0.010233844630420208\n",
      "Iter 1400/8000, loss 0.010073010809719563\n",
      "Iter 1500/8000, loss 0.007701020687818527\n",
      "Iter 1600/8000, loss 0.007348811253905296\n",
      "Iter 1700/8000, loss 0.007462271489202976\n",
      "Iter 1800/8000, loss 0.008086278103291988\n",
      "Iter 1900/8000, loss 0.006054494995623827\n",
      "Iter 2000/8000, loss 0.005794447846710682\n",
      "Iter 2100/8000, loss 0.005570920184254646\n",
      "Iter 2200/8000, loss 0.006008478347212076\n",
      "Iter 2300/8000, loss 0.0055296714417636395\n",
      "Iter 2400/8000, loss 0.012955953367054462\n",
      "Iter 2500/8000, loss 0.005202867556363344\n",
      "Iter 2600/8000, loss 0.00471936771646142\n",
      "Iter 2700/8000, loss 0.00486229732632637\n",
      "Iter 2800/8000, loss 0.004388500936329365\n",
      "Iter 2900/8000, loss 0.004240524023771286\n",
      "Iter 3000/8000, loss 0.0041213370859622955\n",
      "Iter 3100/8000, loss 0.005011857021600008\n",
      "Iter 3200/8000, loss 0.00405107531696558\n",
      "Iter 3300/8000, loss 0.003843763843178749\n",
      "Iter 3400/8000, loss 0.003771386109292507\n",
      "Iter 3500/8000, loss 0.007029023952782154\n",
      "Iter 3600/8000, loss 0.0036817227955907583\n",
      "Iter 3700/8000, loss 0.006169364787638187\n",
      "Iter 3800/8000, loss 0.003888140432536602\n",
      "Iter 3900/8000, loss 0.00369309913367033\n",
      "Iter 4000/8000, loss 0.003556774230673909\n",
      "Iter 4100/8000, loss 0.003833589144051075\n",
      "Iter 4200/8000, loss 0.0048394049517810345\n",
      "Iter 4300/8000, loss 0.003281152341514826\n",
      "Iter 4400/8000, loss 0.003222222439944744\n",
      "Iter 4500/8000, loss 0.003438291372731328\n",
      "Iter 4600/8000, loss 0.0030501510482281446\n",
      "Iter 4700/8000, loss 0.003051972482353449\n",
      "Iter 4800/8000, loss 0.007450224831700325\n",
      "Iter 4900/8000, loss 0.003251483663916588\n",
      "Iter 5000/8000, loss 0.0030478702392429113\n",
      "Iter 5100/8000, loss 0.0029114368371665478\n",
      "Iter 5200/8000, loss 0.0027878466062247753\n",
      "Iter 5300/8000, loss 0.0026968985330313444\n",
      "Iter 5400/8000, loss 0.0037656379863619804\n",
      "Iter 5500/8000, loss 0.0043384418822824955\n",
      "Iter 5600/8000, loss 0.0028599959332495928\n",
      "Iter 5700/8000, loss 0.005617504473775625\n",
      "Iter 5800/8000, loss 0.005219942424446344\n",
      "Iter 5900/8000, loss 0.0027050264179706573\n",
      "Iter 6000/8000, loss 0.0027318825013935566\n",
      "Iter 6100/8000, loss 0.009518003091216087\n",
      "Iter 6200/8000, loss 0.004263939335942268\n",
      "Iter 6300/8000, loss 0.002202410250902176\n",
      "Iter 6400/8000, loss 0.0033705360256135464\n",
      "Iter 6500/8000, loss 0.002236449858173728\n",
      "Iter 6600/8000, loss 0.005767979193478823\n",
      "Iter 6700/8000, loss 0.002539035165682435\n",
      "Iter 6800/8000, loss 0.0032628774642944336\n",
      "Iter 6900/8000, loss 0.002151117892935872\n",
      "Iter 7000/8000, loss 0.0029648374766111374\n",
      "Iter 7100/8000, loss 0.002757906448096037\n",
      "Iter 7200/8000, loss 0.003302693832665682\n",
      "Iter 7300/8000, loss 0.0024504801258444786\n",
      "Iter 7400/8000, loss 0.008014345541596413\n",
      "Iter 7500/8000, loss 0.01145381759852171\n",
      "Iter 7600/8000, loss 0.001999382860958576\n",
      "Iter 7700/8000, loss 0.010449670255184174\n",
      "Iter 7800/8000, loss 0.003144577844068408\n",
      "Iter 7900/8000, loss 0.0028097552713006735\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "useUnitNormInit=True\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,input:tf.Tensor,initInput:tf.Tensor,nUnits:int,useSkips=True,activation=None):\n",
    "        X=input\n",
    "        xDim=X.shape[1].value\n",
    "\n",
    "        #Default forward pass ops\n",
    "        #Initial weight matrix rows are random and unit-norm.\n",
    "        #in other words, if input is unit-variance, output variables are also unit-variance \n",
    "        if useUnitNormInit:\n",
    "            initialW=np.random.normal(0,1,size=[nUnits,xDim])\n",
    "            initialW=initialW/np.linalg.norm(initialW,axis=1,keepdims=True)\n",
    "        else:\n",
    "            #MSRA initialization (https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer)\n",
    "            initialW=tf.truncated_normal([nUnits,xDim], 0.0, stddev=tf.sqrt(2.0 / xDim))\n",
    "        self.W=tf.Variable(initial_value=initialW,dtype=tf.float32,name='W')\n",
    "        self.b=tf.Variable(initial_value=np.zeros([1,nUnits]),dtype=tf.float32,name='b')\n",
    "        h=tf.matmul(X,tf.transpose(self.W))+self.b\n",
    "        if activation==\"relu\":\n",
    "            h=tf.nn.relu(h)\n",
    "        elif activation==\"selu\":\n",
    "            h=tf.nn.selu(h)\n",
    "        elif activation==\"lrelu\":\n",
    "            h=tf.nn.leaky_relu(h,alpha=0.1)\n",
    "        elif activation==\"tanh\":\n",
    "            h=tf.nn.tanh(h)\n",
    "        elif activation==\"swish\":\n",
    "            h=tf.nn.swish(h)\n",
    "        elif activation is not None:\n",
    "            raise NameError(\"Invalid activation type ({}) for Layer\".format(activation))\n",
    "        if useSkips:\n",
    "            self.output=tf.concat([X,h],axis=1)\n",
    "        else:\n",
    "            self.output=h\n",
    "\n",
    "        #Init ops for data-dependent initialization pass.\n",
    "        #Same functionality as above, but we init biases such that the input distribution's mean maps to zero output.\n",
    "        X=initInput\n",
    "        xMean=tf.reduce_mean(X,axis=0,keepdims=True)\n",
    "        b=tf.assign(self.b,-tf.matmul(xMean,tf.transpose(self.W)))\n",
    "        h=tf.matmul(X,tf.transpose(self.W))+b\n",
    "        if activation==\"relu\":\n",
    "            h=tf.nn.relu(h)\n",
    "        elif activation==\"selu\":\n",
    "            h=tf.nn.selu(h)\n",
    "        elif activation==\"lrelu\":\n",
    "            h=tf.nn.leaky_relu(h,alpha=0.1)\n",
    "        elif activation==\"tanh\":\n",
    "            h=tf.nn.tanh(h)\n",
    "        elif activation==\"swish\":\n",
    "            h=tf.nn.swish(h)\n",
    "        elif activation is not None:\n",
    "            raise NameError(\"Invalid activation type ({}) for Layer\".format(activation))\n",
    "        if useSkips:\n",
    "            self.initOutput=tf.concat([X,h],axis=1)\n",
    "        else:\n",
    "            self.initOutput=h\n",
    "\n",
    "\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self,input:tf.Tensor,nLayers:int,nUnitsPerLayer:int, nOutputUnits:int, activation=\"lrelu\", firstLinearLayerUnits:int=0, useSkips:bool=True):\n",
    "        self.layers=[]\n",
    "        X=input\n",
    "        initX=input\n",
    "\n",
    "        #add optional first linear layer (useful, e.g., for reducing the dimensionality of high-dimensional outputs\n",
    "        #which reduces the parameter count of all subsequent layers)\n",
    "        if firstLinearLayerUnits!=0:\n",
    "            layer=Layer(X,initX,firstLinearLayerUnits,useSkips=False,activation=None)\n",
    "            self.layers.append(layer)\n",
    "            X,initX=layer.output,layer.initOutput\n",
    "\n",
    "        #add hidden layers\n",
    "        for layerIdx in range(nLayers):\n",
    "            layer=Layer(X,initX,nUnitsPerLayer,useSkips=useSkips,activation=activation)\n",
    "            self.layers.append(layer)\n",
    "            X,initX=layer.output,layer.initOutput\n",
    "\n",
    "        #add output layer\n",
    "        layer=Layer(X,initX,nOutputUnits,useSkips=False,activation=None)\n",
    "        self.layers.append(layer)\n",
    "        self.output,self.initOutput=layer.output,layer.initOutput\n",
    "\n",
    "\n",
    "\n",
    "    #This method returns a list of assign ops that can be used with a sess.run() call to copy\n",
    "    #all network parameters from a source network. This is useful, e.g., for implementing a slowly updated\n",
    "    #target network in Reinforcement Learning\n",
    "    def copyFromOps(self,src):\n",
    "        result=[]\n",
    "        for layerIdx in range(len(self.layers)):\n",
    "            result.append(tf.assign(self.layers[layerIdx].W,src.layers[layerIdx].W))\n",
    "            result.append(tf.assign(self.layers[layerIdx].b,src.layers[layerIdx].b))\n",
    "        return result\n",
    "\n",
    "#functional interface\n",
    "def mlp(input:tf.Tensor,nLayers:int,nUnitsPerLayer:int, nOutputUnits:int, activation=\"selu\", firstLinearLayerUnits:int=0,useSkips:bool=True):\n",
    "    instance=MLP(input,nLayers,nUnitsPerLayer,nOutputUnits,activation,firstLinearLayerUnits)\n",
    "    return instance.output,instance.initOutput\n",
    "\n",
    "#simple test: \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating toy data\")\n",
    "    x=[]\n",
    "    y=[]\n",
    "    maxAngle=5*np.pi\n",
    "    discontinuousTest=True\n",
    "    if discontinuousTest:\n",
    "        maxAngle=np.pi\n",
    "        for angle in np.arange(0,maxAngle,0.01):\n",
    "            x.append(angle)\n",
    "            if angle>maxAngle*0.8:\n",
    "                y.append(0.0)\n",
    "            else:\n",
    "                y.append(np.sin(angle)*np.sign(np.sin(angle*10)))\n",
    "    else:\n",
    "        for angle in np.arange(0,maxAngle,0.1):\n",
    "            r=angle*0.15\n",
    "            x.append(angle)\n",
    "            if angle>maxAngle*0.8:\n",
    "                y.append(0.0)\n",
    "            else:\n",
    "                y.append(r*np.sin(angle))\n",
    "\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    x=np.reshape(x,[x.shape[0],1])\n",
    "    y=np.reshape(y,[y.shape[0],1])\n",
    "    interpRange=0.2\n",
    "    xtest=np.arange(-interpRange+np.min(x),np.max(x)+interpRange,0.001)\n",
    "    xtest=np.reshape(xtest,[xtest.shape[0],1])\n",
    "    \n",
    "    print(\"Initializing matplotlib\")\n",
    "    pp.figure(1)\n",
    "    pp.subplot(3,1,1)\n",
    "    pp.scatter(x[:,0],y[:,0])\n",
    "    pp.ylabel(\"data\")\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    sess=tf.InteractiveSession()\n",
    "    tfX=tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "    tfY=tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "    #IMPORTANT: deep networks benefit immensely from data-dependent initialization.\n",
    "    #This is why the constructor returns the initial predictions separately - to initialize, fetch this tensor in a sess.run with \n",
    "    #the first minibatch. See the sess.run below\n",
    "    predictions,initialPredictions=mlp(input=tfX,nLayers=8,nUnitsPerLayer=8,nOutputUnits=1,activation=\"lrelu\")\n",
    "    optimizer=tf.train.AdamOptimizer()\n",
    "    loss=tf.losses.mean_squared_error(tfY,predictions)\n",
    "    optimize=optimizer.minimize(loss)\n",
    "  \n",
    "    print(\"Initializing model\")\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    #This sess.run() initializes the network biases based on x, and also returns the initial predictions.\n",
    "    #It is noteworthy that with this initialization, even a deep network has zero-mean output with variance similar to input.\n",
    "    networkOut=sess.run(initialPredictions,feed_dict={tfX:x})\n",
    "    pp.subplot(3,1,2)\n",
    "    pp.scatter(x[:,0],networkOut[:,0])\n",
    "    pp.ylabel(\"initialization\")\n",
    "    pp.draw()\n",
    "    pp.pause(0.001)\n",
    "\n",
    "\n",
    "    print(\"Optimizing\")\n",
    "    nIter=8000\n",
    "    for iter in range(nIter):\n",
    "        temp,currLoss=sess.run([optimize,loss],feed_dict={tfX:x,tfY:y})\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Iter {}/{}, loss {}\".format(iter,nIter,currLoss))\n",
    "    networkOut=sess.run(predictions,feed_dict={tfX:x})\n",
    "    pp.subplot(3,1,3)\n",
    "    pp.scatter(x[:,0],networkOut[:,0])\n",
    "    pp.ylabel(\"trained\")\n",
    "\n",
    "    pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def softClip(x, minVal, maxVal):\n",
    "    #return minVal+(maxVal-minVal)*(1.0+0.5*tf.tanh(x))\n",
    "    return minVal+(maxVal-minVal)*tf.sigmoid(x)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, stateDim:int, actionDim:int, actionMinLimit:np.array, actionMaxLimit:np.array, mode=\"PPO-CMA\"\n",
    "                 , entropyLossWeight=0, networkDepth=2, networkUnits=64, networkActivation=\"lrelu\"\n",
    "                 , networkSkips=False, networkUnitNormInit=True, usePPOLoss=False, separateVarAdapt=False\n",
    "                 , learningRate=0.001, minSigma=0.01, useSigmaSoftClip=True, PPOepsilon=0.2, piEpsilon=0, nHistory=1\n",
    "                 , globalVariance=False, trainableGlobalVariance=True, useGradientClipping=False\n",
    "                 , maxGradientNorm=0.5, negativeAdvantageAvoidanceSigma=0):\n",
    "        self.networkDepth = networkDepth\n",
    "        self.networkUnits = networkUnits\n",
    "        self.networkActivation = networkActivation\n",
    "        self.networkSkips = networkSkips\n",
    "        self.networkUnitNormInit = networkUnitNormInit\n",
    "        self.usePPOLoss = usePPOLoss\n",
    "        self.separateVarAdapt = separateVarAdapt\n",
    "        self.learningRate = learningRate\n",
    "        self.minSigma = minSigma\n",
    "        self.useSigmaSoftClip=useSigmaSoftClip\n",
    "        self.PPOepsilon = PPOepsilon\n",
    "        self.piEpsilon = piEpsilon\n",
    "        self.nHistory = nHistory\n",
    "        self.globalVariance = globalVariance\n",
    "        self.trainableGlobalVariance = trainableGlobalVariance\n",
    "        self.useGradientClipping = useGradientClipping\n",
    "        self.maxGradientNorm = maxGradientNorm\n",
    "        self.negativeAdvantageAvoidanceSigma = negativeAdvantageAvoidanceSigma\n",
    "\n",
    "        maxSigma=1.0*(actionMaxLimit-actionMinLimit)\n",
    "        self.mode=mode\n",
    "\n",
    "        #to be able to benchmark with Schulman's original network architecture, we may have to disable the data-dependent init of the DenseNet module\n",
    "        useUnitNormInit=self.networkUnitNormInit\n",
    "\n",
    "        #some bookkeeping\n",
    "        self.usedSigmaSum=0\n",
    "        self.usedSigmaSumCounter=0\n",
    "\n",
    "        #inputs\n",
    "        stateIn=tf.placeholder(dtype=tf.float32,shape=[None,stateDim],name=\"stateIn\")\n",
    "        actionIn=tf.placeholder(dtype=tf.float32,shape=[None,actionDim],name=\"actionIn\")    #training targets for policy network\n",
    "        oldPolicyMean=tf.placeholder(dtype=tf.float32,shape=[None,actionDim],name=\"oldPolicyMeanIn\")    #training targets for policy network\n",
    "        self.oldPolicyMean=oldPolicyMean\n",
    "        advantagesIn=tf.placeholder(dtype=tf.float32,shape=[None],name=\"advantagesIn\")     #weights, computed based on action advantages\n",
    "        logPiOldIn=tf.placeholder(dtype=tf.float32,shape=[None],name=\"logPiOldIn\")             #pi_old(a | s), used for PPO\n",
    "        initSigmaIn=tf.placeholder(dtype=tf.float32,shape=[1,actionDim],name=\"initSigmaIn\")\n",
    "        \n",
    "        #by default, we won't use a linear layer at the beginning of the network to reduce dimensionality\n",
    "        firstLinearLayerUnits=0\n",
    "\n",
    "        #First, define the mean and log var tensors, depending on configuration. \n",
    "        #Depending on the network architecture, we may also need initialization tensors, fetching which causes a data-dependent initialization of the graph\n",
    "        policyInit=[]\n",
    "        if stateDim==0:\n",
    "            #We don't have state at all => policyMean and variance are simply TensorFlow variables\n",
    "            policyMean=tf.Variable(initial_value=np.zeros([actionDim]),dtype=tf.float32)\n",
    "            policyLogVar=tf.Variable(initial_value=np.log(np.square(0.5*(actionMaxLimit-actionMinLimit)))*np.ones([actionDim]),dtype=tf.float32,trainable=self.trainableGlobalVariance)\n",
    "            self.globalLogVarVariable=policyLogVar\n",
    "        else:\n",
    "            #We have state, i.e., need neural networks that output a state-dependent mean and variance\n",
    "            if self.separateVarAdapt or self.globalVariance:\n",
    "                #Need separate networks for mean and variance\n",
    "                policyMean,policyMeanInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                policyInit.append(policyMeanInit)\n",
    "                if self.globalVariance:\n",
    "                    policyLogVar=tf.Variable(initial_value=np.log(np.square(0.5*(actionMaxLimit-actionMinLimit)))*np.ones([actionDim]),dtype=tf.float32,trainable=self.trainableGlobalVariance)\n",
    "                    self.globalLogVarVariable=policyLogVar\n",
    "                else:\n",
    "                    policyLogVar,policyLogVarInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                    policyInit.append(policyLogVarInit)\n",
    "            else:\n",
    "                #Single network that outputs both mean and variance\n",
    "                policyMeanAndLogVar,policyMeanAndLogVarInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim*2,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                policyMean=policyMeanAndLogVar[:,:actionDim]\n",
    "                policyLogVar=policyMeanAndLogVar[:,actionDim:]\n",
    "                policyInit.append(policyMeanAndLogVarInit)\n",
    "\n",
    "        #sigmoid-clipping of mean to ensure stability\n",
    "        policyMean=softClip(policyMean, actionMinLimit,actionMaxLimit)\n",
    "        if self.useSigmaSoftClip:\n",
    "            #sigmoid-clipping of log var to ensure stability\n",
    "            #Note: tanh or hard clipping doesn't work as well due to higher chance of zero or almost zero gradients \n",
    "            maxLogVar=np.log(maxSigma*maxSigma)\n",
    "            minLogVar=np.log(self.minSigma*self.minSigma)\n",
    "            policyLogVar=softClip(policyLogVar,minLogVar,maxLogVar)\n",
    "        policyVar=tf.exp(policyLogVar)  \n",
    "        policySigma=tf.sqrt(policyVar)\n",
    "\n",
    "\n",
    "        #loss functions\n",
    "        if self.usePPOLoss:\n",
    "            def loss(policyMean,policyVar,policyLogVar):\n",
    "                #1/sqrt(var)=exp(log(1/sqrt(var)))=exp(log(1)-log(var^0.5))=exp(-0.5*log(var))=exp(-log(std))\n",
    "                logPi=tf.reduce_sum(-0.5*tf.square(actionIn-policyMean)/policyVar-0.5*policyLogVar,axis=1)\n",
    "                #Some PPO implementations use r=tf.exp(logPi-logPiOldIn). However, we've noticed this to cause NaNs especially\n",
    "                #with non-saturating policy network activation functions like lrelu and the MuJoCo humanoid env.\n",
    "                #Thus, we also support using the epsilon below to regularize. \n",
    "                if self.piEpsilon==0:\n",
    "                    r=tf.exp(logPi-logPiOldIn)\n",
    "                else:\n",
    "                    r=tf.exp(logPi)/(self.piEpsilon+tf.exp(logPiOldIn))\n",
    "                perSampleLoss=tf.minimum(r*advantagesIn,tf.clip_by_value(r,1-self.PPOepsilon,1+self.PPOepsilon)*advantagesIn)\n",
    "                return -tf.reduce_mean(perSampleLoss) #because we want to minimize instead of maximize...\n",
    "            print(\"Using PPO clipped surrogate loss with epsilon {}\".format(self.PPOepsilon))\n",
    "            policyLoss=loss(policyMean,policyVar,policyLogVar)\n",
    "            if entropyLossWeight>0:\n",
    "                #Entropy of a diagonal Gaussian=0.5*log(det(Cov))=0.5*log(trace(Cov))=0.5*sum(log(diag(Cov)))\n",
    "                policyLoss-=entropyLossWeight*0.5*tf.reduce_mean(tf.reduce_sum(policyLogVar,axis=1))\n",
    "            assert(self.separateVarAdapt==False)\n",
    "            #just to be on the safe side, if some batch has an occasional NaN, set the loss to zero\n",
    "            policyLoss=tf.where(tf.is_nan(policyLoss), tf.zeros_like(policyLoss),policyLoss)\n",
    "            policyMeanLoss=policyLoss\n",
    "            policySigmaLoss=policyLoss\n",
    "        else:\n",
    "            #Separate mean and sigma adaptation losses\n",
    "            policyNoGrad=tf.stop_gradient(policyMean)\n",
    "            policyVarNoGrad=tf.stop_gradient(policyVar)\n",
    "            policyLogVarNoGrad=tf.stop_gradient(policyLogVar)\n",
    "            logpNoMeanGrad=-tf.reduce_sum(0.5*tf.square(actionIn-policyNoGrad)/policyVar+0.5*policyLogVar,axis=1)\n",
    "            logpNoVarGrad=-tf.reduce_sum(0.5*tf.square(actionIn-policyMean)/policyVarNoGrad+0.5*policyLogVarNoGrad,axis=1) \n",
    "            posAdvantages=tf.nn.relu(advantagesIn)\n",
    "            policySigmaLoss=-tf.reduce_mean(posAdvantages*logpNoMeanGrad)\n",
    "            policyMeanLoss=-tf.reduce_mean(posAdvantages*logpNoVarGrad)\n",
    "            if self.negativeAdvantageAvoidanceSigma>0:\n",
    "                negAdvantages=tf.nn.relu(-advantagesIn)\n",
    "                mirroredAction=oldPolicyMean-(actionIn-oldPolicyMean)  #mirror negative advantage actions around old policy mean (convert them to positive advantage actions assuming linearity) \n",
    "                logpNoVarGradMirrored=-tf.reduce_sum(0.5*tf.square(mirroredAction-policyMean)/policyVarNoGrad+0.5*policyLogVarNoGrad,axis=1) \n",
    "                effectiveKernelSqWidth=self.negativeAdvantageAvoidanceSigma*self.negativeAdvantageAvoidanceSigma*policyVarNoGrad\n",
    "                avoidanceKernel=tf.reduce_mean(tf.exp(-0.5*tf.square(actionIn-oldPolicyMean)/effectiveKernelSqWidth),axis=1)\n",
    "                policyMeanLoss-=tf.reduce_mean((negAdvantages*avoidanceKernel)*logpNoVarGradMirrored)\n",
    "\n",
    "            #just to be on the safe side, if some batch has an occasional NaN, set the loss to zero\n",
    "            policySigmaLoss=tf.where(tf.is_nan(policySigmaLoss), tf.zeros_like(policySigmaLoss),policySigmaLoss)\n",
    "            policyMeanLoss=tf.where(tf.is_nan(policyMeanLoss), tf.zeros_like(policyMeanLoss),policyMeanLoss)\n",
    "\n",
    "            #Vanilla Policy Gradient loss\n",
    "            logp=-tf.reduce_sum(0.5*tf.square(actionIn-policyMean)/policyVar+0.5*policyLogVar,axis=1)\n",
    "            policyLoss=tf.reduce_mean(-advantagesIn*logp)  \n",
    "\n",
    "        #loss functions for initialization (pretraining)\n",
    "        policyInitLoss=tf.reduce_mean(tf.square(actionIn-policyMean))\n",
    "        policyInitLoss+=tf.reduce_mean(tf.square(initSigmaIn-policySigma))\n",
    "\n",
    "        #optimizers\n",
    "        def optimize(loss):\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=self.learningRate)\n",
    "            if not self.useGradientClipping:\n",
    "                return optimizer.minimize(loss)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, self.maxGradientNorm)\n",
    "            return optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.optimizePolicy=optimize(policyLoss)\n",
    "        self.optimizePolicySigma=optimize(policySigmaLoss)\n",
    "        self.optimizePolicyMean=optimize(policyMeanLoss)\n",
    "        self.optimizePolicyInit=optimize(policyInitLoss)\n",
    "\n",
    "        #cache stuff needed elsewhere\n",
    "        self.actionMinLimit=actionMinLimit\n",
    "        self.actionMaxLimit=actionMaxLimit\n",
    "        self.stateDim=stateDim\n",
    "        self.actionDim=actionDim\n",
    "        self.policyMean=policyMean\n",
    "        self.stateIn=stateIn\n",
    "        self.actionIn=actionIn\n",
    "        self.policyInit=policyInit\n",
    "        self.policyInitLoss=policyInitLoss\n",
    "        self.advantagesIn=advantagesIn\n",
    "        self.policyLoss=policyLoss\n",
    "        self.logPiOldIn=logPiOldIn\n",
    "        self.initSigmaIn=initSigmaIn\n",
    "        self.history=deque()\n",
    "        self.policyVar=policyVar\n",
    "        self.policyLogVar=policyLogVar\n",
    "        self.policySigma=policySigma\n",
    "        self.initialized=False  #remember that one has to call init() before training (can't call it here as TF globals might not have been initialized yet)\n",
    "\n",
    "    #init the policy with random Gaussian state samples, such that the network outputs the desired mean and sd\n",
    "    def init(self,sess:tf.Session,stateMean:np.array,stateSd:np.array,actionMean:np.array,actionSd:np.array,nMinibatch:int=64,nBatches:int=4000,verbose=True):\n",
    "        for batchIdx in range(nBatches):\n",
    "            states=np.random.normal(stateMean,stateSd,size=[nMinibatch,self.stateDim])\n",
    "            if batchIdx==0 and len(self.policyInit)>0:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.policyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            else:\n",
    "                #drive output towards the desired mean and sd\n",
    "                temp,currLoss=sess.run([self.optimizePolicyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            if verbose and (batchIdx % 100 ==0):\n",
    "                print(\"Initializing policy with random Gaussian data, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "        self.initialized=True\n",
    "    #init the policy with uniform random state samples, such that the network outputs the desired mean and sd\n",
    "    def initUniform(self,sess:tf.Session,stateMin:np.array,stateMax:np.array,actionMean:np.array,actionSd:np.array,nMinibatch:int=64,nBatches:int=4000):\n",
    "        for batchIdx in range(nBatches):\n",
    "            states=np.random.uniform(stateMin,stateMax,size=[nMinibatch,self.stateDim])\n",
    "            if batchIdx==0 and len(self.policyInit)>0:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.policyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            else:\n",
    "                #drive output towards the desired mean and sd\n",
    "                temp,currLoss=sess.run([self.optimizePolicyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            if batchIdx % 100 ==0:\n",
    "                print(\"Initializing policy with random Gaussian data, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "        self.initialized=True\n",
    "    #if nBatches==0, nEpochs will be used\n",
    "    def train(self,sess:tf.Session,states:np.array,actions:np.array,advantages:np.array,nMinibatch:int,nEpochs:int,nBatches:int=0,stateOffset=0,stateScale=1,verbose=True):\n",
    "        assert(np.all(np.isfinite(states)))\n",
    "        assert(np.all(np.isfinite(actions)))\n",
    "        assert(np.all(np.isfinite(advantages)))\n",
    "        assert(self.initialized)\n",
    "        nData=actions.shape[0]\n",
    "\n",
    "        #reset bookkeeping for next iter\n",
    "        self.usedSigmaSum=0\n",
    "        self.usedSigmaSumCounter=0\n",
    "\n",
    "        #manage history\n",
    "        self.history.append([states.copy(),actions.copy(),advantages.copy()])\n",
    "        if len(self.history)>self.nHistory:\n",
    "            self.history.popleft()\n",
    "\n",
    "        #safety-check that the observed state distribution is at least roughly zero-mean unit sd\n",
    "        if self.stateDim>0:\n",
    "            scaledStates=(states-stateOffset)*stateScale\n",
    "            stateAbsMax=np.max(np.absolute(scaledStates))\n",
    "            if stateAbsMax>10:\n",
    "                print(\"Warning: states deviate up to {} sd:s from expected!\".format(stateAbsMax))\n",
    "        else:\n",
    "            scaledStates=states\n",
    "        #train\n",
    "        assert(len(advantages.shape)==1)  #to prevent nasty silent broadcasting bugs\n",
    "        nMinibatch=min([nData,nMinibatch])\n",
    "        if nBatches==0:\n",
    "            nBatches=max([1,int(nData*nEpochs/nMinibatch)])\n",
    "        #nBatches=1000\n",
    "        nVarAdaptBatches=nBatches\n",
    "        mbStates=np.zeros([nMinibatch,self.stateDim])\n",
    "        mbActions=np.zeros([nMinibatch,self.actionDim])\n",
    "        mbOldMean=np.zeros([nMinibatch,self.actionDim])\n",
    "        mbAdvantages=np.zeros([nMinibatch])\n",
    "        logPiOld=np.ones([nData])\n",
    "        mbLogPiOld=np.ones([nMinibatch])\n",
    "        if self.usePPOLoss:\n",
    "            policyMean,policyVar,policyLogVar=sess.run([self.policyMean,self.policyVar,self.policyLogVar],feed_dict={self.stateIn:scaledStates})\n",
    "            #for i in range(nData):\n",
    "            #    logPiOld[i]=np.sum(-0.5*np.square(actions[i,:]-policyMean[i,:])/policyVar[i,:]-0.5*policyLogVar[i,:])\n",
    "            logPiOld=np.sum(-0.5*np.square(actions-policyMean)/policyVar-0.5*policyLogVar,axis=1)\n",
    "        if self.separateVarAdapt:\n",
    "            assert(self.usePPOLoss==False)\n",
    "            #if negativeAdvantageAvoidanceSigma>0:\n",
    "            oldMeans=sess.run(self.policyMean,{self.stateIn:scaledStates})\n",
    "            for batchIdx in range(nBatches + nVarAdaptBatches if self.separateVarAdapt else nBatches):\n",
    "                if batchIdx<nVarAdaptBatches:\n",
    "                    historyLen=len(self.history)\n",
    "                    for i in range(nMinibatch):\n",
    "                        histIdx=np.random.randint(0,historyLen)\n",
    "                        h=self.history[histIdx]\n",
    "                        nData=h[1].shape[0]\n",
    "                        dataIdx=np.random.randint(0,nData)\n",
    "                        mbStates[i,:]=h[0][dataIdx,:]\n",
    "                        mbActions[i,:]=h[1][dataIdx,:]\n",
    "                        mbAdvantages[i]=h[2][dataIdx]\n",
    "                    advantageMean=np.mean(mbAdvantages)\n",
    "                    mbStates=(mbStates-stateOffset)*stateScale  #here, we must scale per batch because using the history\n",
    "                    temp,currLoss=sess.run([self.optimizePolicySigma,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages})\n",
    "                    if verbose and (batchIdx % 100 == 0):\n",
    "                        print(\"Adapting policy variance, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx,nVarAdaptBatches,advantageMean,currLoss))\n",
    "                #temp,currLoss=sess.run([self.optimizePolicyMean,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages})\n",
    "                else:\n",
    "                    nData=actions.shape[0]\n",
    "                    for i in range(nMinibatch):\n",
    "                        dataIdx=np.random.randint(0,nData)\n",
    "                        mbStates[i,:]=scaledStates[dataIdx,:]  \n",
    "                        mbActions[i,:]=actions[dataIdx,:]\n",
    "                        if self.stateDim>0:\n",
    "                            mbOldMean[i,:]=oldMeans[dataIdx,:]\n",
    "                        mbAdvantages[i]=advantages[dataIdx]\n",
    "                    advantageMean=np.mean(mbAdvantages)\n",
    "                    temp,currLoss=sess.run([self.optimizePolicyMean,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages,self.logPiOldIn:mbLogPiOld, self.oldPolicyMean:mbOldMean})\n",
    "                    if verbose and (batchIdx % 100 == 0):\n",
    "                        print(\"Adapting policy mean, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx-nVarAdaptBatches,nBatches,advantageMean,currLoss))\n",
    "\n",
    "        else:\n",
    "            for batchIdx in range(nBatches + nVarAdaptBatches if self.separateVarAdapt else nBatches):\n",
    "                for i in range(nMinibatch):\n",
    "                    dataIdx=np.random.randint(0,nData)\n",
    "                    if self.stateDim!=0:\n",
    "                        mbStates[i,:]=scaledStates[dataIdx,:]\n",
    "                    mbActions[i,:]=actions[dataIdx,:]\n",
    "                    mbAdvantages[i]=advantages[dataIdx]\n",
    "                    mbLogPiOld[i]=logPiOld[dataIdx]\n",
    "                advantageMean=np.mean(mbAdvantages)\n",
    "                temp,currLoss=sess.run([self.optimizePolicy,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages,self.logPiOldIn:mbLogPiOld})\n",
    "                if verbose and (batchIdx % 100 == 0):\n",
    "                    print(\"Training policy, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx,nBatches,advantageMean,currLoss))\n",
    "    def setGlobalStdev(self,relStdev:float, sess:tf.Session):\n",
    "        assert(self.globalVariance and (not self.trainableGlobalVariance))\n",
    "        stdev=relStdev*(self.actionMaxLimit-self.actionMinLimit)\n",
    "        var=np.square(stdev)\n",
    "        logVar=np.log(var)\n",
    "        self.globalLogVarVariable.load(logVar,sess)\n",
    "        \n",
    "    def sample(self,sess:tf.Session,observations:np.array,enforcedRelSigma:float=None):\n",
    "        obs=observations\n",
    "        nObs=obs.shape[0]\n",
    "        result=np.zeros([nObs,self.actionDim])\n",
    "        assert(self.initialized)\n",
    "        policyMean,policySigma=sess.run([self.policyMean,self.policySigma],feed_dict={self.stateIn:obs})\n",
    "        if np.any(np.isnan(policyMean)):\n",
    "            raise Exception(\"Policy mean is NaN\")\n",
    "        if np.any(np.isnan(policySigma)):\n",
    "            raise Exception(\"Policy sigma is NaN\")\n",
    "        #if np.any(policySigma<minSigma):\n",
    "        #    raise Exception(\"Policy sigma violates limits\")\n",
    "\n",
    "        for i in range(nObs):\n",
    "            if self.stateDim==0:\n",
    "                result[i,:]=np.random.normal(policyMean,policySigma,[self.actionDim])\n",
    "            else:\n",
    "                if self.globalVariance:\n",
    "                    result[i,:]=np.random.normal(policyMean[i,:],policySigma,[self.actionDim])\n",
    "                else:\n",
    "                    result[i,:]=np.random.normal(policyMean[i,:],policySigma[i,:],[self.actionDim])\n",
    "\n",
    "        #bookkeeping for logging\n",
    "        self.usedSigmaSum+=np.mean(policySigma)\n",
    "        self.usedSigmaSumCounter+=1\n",
    "        return result\n",
    "    def getExpectation(self,sess:tf.Session,observations:np.array):\n",
    "        return sess.run(self.policyMean,feed_dict={self.stateIn:observations})\n",
    "    def getSd(self,sess:tf.Session,observations:np.array):\n",
    "        return sess.run(self.policySigma,feed_dict={self.stateIn:observations})\n",
    "    #def get2dEllipse(self,observations:np.array):\n",
    "    #    def logProb(self,state:np.array,action:np.array):\n",
    "    #    def adapt(self,states:np.array,actions:np.array,advantages:np.array,batchSize:int,nEpochs:int):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "useGradientClipping=False\n",
    "maxGradientNorm=1\n",
    "\n",
    "#A function that behaves like abs, but has zero derivative at the origin, which should improve final convergence if this is \n",
    "#used in computing a loss \n",
    "def softAbs(x:tf.Tensor):\n",
    "    x=tf.abs(x)\n",
    "    return tf.where(x > 0.5, x-0.25, x*x)\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self,stateDim:int,nHidden:int,networkUnits:int,networkActivation,useSkips=False,learningRate:float=1e-3,nHistory:int=1,lossType=\"L2\"):\n",
    "        stateIn=tf.placeholder(dtype=tf.float32,shape=[None,stateDim])\n",
    "        valueIn=tf.placeholder(dtype=tf.float32,shape=[None,1])             #training targets for value network\n",
    "        critic,criticInit=mlp(stateIn,nHidden,networkUnits,1,networkActivation,firstLinearLayerUnits=0,useSkips=useSkips)  #need a handle for the DenseNet instance for network switching\n",
    "        diff=valueIn-critic\n",
    "        if lossType==\"L2\":\n",
    "            loss=tf.reduce_mean(tf.square(diff))    \n",
    "        elif lossType==\"L1\":\n",
    "            loss=tf.reduce_mean(tf.abs(diff))       #L1 loss, can be more stable\n",
    "        elif lossType==\"SoftL1\":\n",
    "            loss=tf.reduce_mean(softAbs(diff))       #L1 loss with zero gradient at optimum\n",
    "        else:\n",
    "            raise Exception(\"Loss type not recognized!\")\n",
    "        def optimize(loss):\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=learningRate)\n",
    "            if not useGradientClipping:\n",
    "                return optimizer.minimize(loss)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, maxGradientNorm)\n",
    "            return optimizer.apply_gradients(zip(gradients, variables))\n",
    "        optimizeCritic=optimize(loss)\n",
    "        #remember some of the tensors for later\n",
    "        self.loss=loss\n",
    "        self.nHistory=nHistory\n",
    "        self.history=deque()\n",
    "        self.criticInit=criticInit\n",
    "        self.stateIn=stateIn\n",
    "        self.valueIn=valueIn\n",
    "        self.initialized=False\n",
    "        self.stateDim=stateDim\n",
    "        self.critic=critic\n",
    "        self.optimize=optimizeCritic\n",
    "\n",
    "    def train(self,sess,states:np.array,values:np.array,nMinibatch:int,nEpochs:int,nBatches:int=0,verbose=True):\n",
    "        assert(np.all(np.isfinite(states)))\n",
    "        assert(np.all(np.isfinite(values)))\n",
    "        nData=states.shape[0]\n",
    "\n",
    "        #manage history\n",
    "        self.history.append([states.copy(),values.copy()])\n",
    "        if len(self.history)>self.nHistory:\n",
    "            self.history.popleft()\n",
    "\n",
    "        #train\n",
    "        nMinibatch=min([nData,nMinibatch])\n",
    "        if nBatches==0:\n",
    "            nBatches=max([1,int(nData*nEpochs/nMinibatch)])\n",
    "        mbState=np.zeros([nMinibatch,self.stateDim])\n",
    "        mbValue=np.zeros([nMinibatch,1])\n",
    "        for batchIdx in range(nBatches):\n",
    "            historyLen=len(self.history)\n",
    "            for i in range(nMinibatch):\n",
    "                histIdx=np.random.randint(0,historyLen)\n",
    "                h=self.history[histIdx]\n",
    "                nData=h[0].shape[0]\n",
    "                dataIdx=np.random.randint(0,nData)\n",
    "                mbState[i,:]=h[0][dataIdx,:]\n",
    "                mbValue[i]=h[1][dataIdx]\n",
    "            if batchIdx==0 and not self.initialized:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.criticInit,self.loss],feed_dict={self.stateIn:mbState,self.valueIn:mbValue})\n",
    "                self.initialized=True\n",
    "            else:\n",
    "                temp,currLoss=sess.run([self.optimize,self.loss],feed_dict={self.stateIn:mbState,self.valueIn:mbValue})\n",
    "            if verbose and (batchIdx % 100 == 0):\n",
    "                print(\"Training critic, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "    def predict(self,sess,states):\n",
    "        return sess.run(self.critic,feed_dict={self.stateIn:states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Scaling Utilities\n",
    "Modified from code by Patrick Coady (pat-coady.github.io)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class Scaler(object):\n",
    "    \"\"\" Generate scale and offset based on running mean and stddev along axis=0\n",
    "        offset = running mean\n",
    "        scale = 1 / (2.0*stddev + epsilon) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, epsilon=0.001, clip=1e10, useOffset=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: dimension of axis=1\n",
    "        \"\"\"\n",
    "        self.xMean=np.zeros(obs_dim)\n",
    "        self.xSqMean=np.zeros(obs_dim)\n",
    "        self.scale=np.ones(obs_dim)\n",
    "        self.offset=np.zeros(obs_dim)\n",
    "        self.nUpdates=0\n",
    "        self.epsilon=epsilon\n",
    "        self.clip=clip\n",
    "        self.useOffset=useOffset\n",
    "\n",
    "    def update(self, x):\n",
    "        self.nUpdates+=1\n",
    "        newWeight=1/self.nUpdates\n",
    "        self.xSqMean=(1-newWeight)*self.xSqMean+newWeight*np.mean(np.square(x),axis=0) \n",
    "        self.xMean=(1-newWeight)*self.xMean+newWeight*np.mean(x,axis=0) \n",
    "        if self.useOffset:\n",
    "            mean=self.xMean\n",
    "        else:\n",
    "            mean=0\n",
    "        #var(x)=E((x-mean(x))^2)=E(x^2-2*x*E(x)+mean(x)^2)=E(x^2)-2*E(x)*E(x)+E(x)^2=E(x^2)-E(x)^2\n",
    "        var=self.xSqMean-np.square(mean)\n",
    "        var=np.maximum(0.0,var)\n",
    "        self.scale=np.minimum(self.scale,1.0/(2.0*np.sqrt(var)+self.epsilon))  \n",
    "        self.offset=mean\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\" returns 2-tuple: (scale, offset) \"\"\"\n",
    "        return self.scale,self.offset\n",
    "\n",
    "    def process(self,x:np.array):\n",
    "        return np.clip((x-self.offset)*self.scale,-self.clip,self.clip)\n",
    "    def unscale(self,x:np.array):\n",
    "        return x/self.scale+self.offset\n",
    "    \n",
    "\n",
    "    \n",
    "class MinMaxScaler(object):\n",
    "    \"\"\" Generate scale and offset based on low-pass filtered max and min vals\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, filter=0.9, epsilon=0.001, useOffset=False, scalarMode=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: dimension of axis=1\n",
    "        \"\"\"\n",
    "        self.minVals=-np.ones(obs_dim)\n",
    "        self.maxVals=np.zeros(obs_dim)\n",
    "        self.filteredMinVals=self.minVals.copy()\n",
    "        self.filteredMaxVals=self.maxVals.copy()\n",
    "        self.first_pass = True\n",
    "        self.filter=filter\n",
    "        self.epsilon=epsilon\n",
    "        self.scalarMode=scalarMode\n",
    "        self.useOffset=useOffset\n",
    "        self.scale=1 if scalarMode else np.ones(obs_dim)\n",
    "        self.offset=0 if scalarMode else np.zeros(obs_dim)\n",
    "\n",
    "    def update(self, x):\n",
    "        if self.first_pass:\n",
    "            self.minVals=np.min(x,axis=0)\n",
    "            self.maxVals=np.max(x,axis=0)\n",
    "            self.filteredMinVals=self.minVals.copy()\n",
    "            self.filteredMaxVals=self.maxVals.copy()\n",
    "        else:\n",
    "            self.minVals=np.minimum([self.minVals,np.min(x,axis=0)])\n",
    "            self.maxVals=np.maximum([self.maxVals,np.max(x,axis=0)])\n",
    "            self.filteredMinVals=self.filter*self.filteredMinVals+(1-self.filter)*self.minVals\n",
    "            self.filteredMaxVals=self.filter*self.filteredMaxVals+(1-self.filter)*self.maxVals\n",
    "        if self.scalarMode:\n",
    "            self.scale=np.minimum(self.scale,2.0/np.max((self.filteredMaxVals-self.filteredMinVals)+self.epsilon))\n",
    "        else:\n",
    "            self.scale=2.0/((self.filteredMaxVals-self.filteredMinVals)+self.epsilon)\n",
    "            if self.useOffset:\n",
    "                self.offset=0.5*(self.filteredMaxVals+self.filteredMinVals)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\" returns 2-tuple: (scale, offset) \"\"\"\n",
    "        return self.scale,self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Known issues (TODO): \n",
    "- policy.train() takes as input the scaler scale and offset, and uses them internally. However, the scaler also has a clip parameter, which is\n",
    "  by default set to a large value, but if one wants to use it, scaler.process() will then produce different results than the scaling in policy.train()\n",
    "  - fix: policy should take the Scaler instance as an argument, although this creates a dependency\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Data structure for holding experience\n",
    "class Experience:\n",
    "    def __init__(self,s:np.array,a:np.array,r:float,s_next:np.array,terminated:bool,timeStep=0):\n",
    "        self.s=s.copy()\n",
    "        self.a=a\n",
    "        self.r=r\n",
    "        self.s_next=s_next.copy()\n",
    "        self.terminated=terminated\n",
    "        self.timeStep=timeStep     \n",
    "        self.V=r            #value, will be updated by the agent or the client. note: this is the on-policy value, i.e., averaged over children\n",
    "        self.Vselect=r      #value used for the UCB\n",
    "        self.advantage=0    #will be updated by the agent\n",
    "        self.nonDiscountedRewardSum=0   #used in some applications\n",
    "        self.fullState=None #used in tree search\n",
    "        #the following only used when building trajectory trees\n",
    "        self.parent=None\n",
    "        self.children=[]\n",
    "        self.depth=1\n",
    "        #visitation count used by UCB tree search\n",
    "        self.n=1\n",
    "\n",
    "    #Updates value upwards in the tree, called after adding a new trajectory to the tree.\n",
    "    #Also keeps track of tree depth\n",
    "    def propagateUpwards(self,gamma:float,bestGamma:float=1):\n",
    "        node=self\n",
    "        node.V=node.r\n",
    "        node.Vselect=node.r\n",
    "        while node.parent is not None:\n",
    "            node=node.parent\n",
    "            node.V=node.r\n",
    "            node.Vselect=-np.inf\n",
    "            nChildren=len(node.children)\n",
    "            node.depth=0\n",
    "            for child in node.children:\n",
    "                node.V+=gamma*child.V/nChildren\n",
    "                node.Vselect=max([node.Vselect,node.r+bestGamma*child.Vselect])\n",
    "                node.depth=max([node.depth,child.depth+1])\n",
    "            #node.Vselect=node.V\n",
    "\n",
    "    #adds a child to this node, updating tree linkage\n",
    "    def addChild(self,child):\n",
    "        self.children.append(child)\n",
    "        child.parent=self\n",
    "\n",
    "    #selects a child node at specific depth, using the UCB formula to visit the children\n",
    "    def selectChildAtDepth(self,depth,C_ucb):\n",
    "        node=self\n",
    "        self.n+=1       #keep track of visitation count\n",
    "        while depth>0:\n",
    "            nChildren=len(node.children)\n",
    "            if nChildren==1:\n",
    "                #if only one child, just move forward\n",
    "                node=node.children[0]\n",
    "            else:\n",
    "                #if multiple children, select the best-scoring one based on the UCB-1 formula\n",
    "                maxScore=-np.inf\n",
    "                maxScoringChild=None\n",
    "                for child in node.children:\n",
    "                    if child.depth>=depth:  #we only accept branches that will take us up to the desired depth\n",
    "                        score=node.Vselect+C_ucb*math.sqrt(math.log(node.n)/child.n)\n",
    "                        if score>maxScore:\n",
    "                            maxScoringChild=child\n",
    "                            maxScore=score\n",
    "                node=maxScoringChild\n",
    "            depth-=1        #keep track of depth\n",
    "            node.n+=1       #keep track of visitation count\n",
    "        return node\n",
    "\n",
    "class Agent:\n",
    "    #In most cases, one only needs to specify stateDim, actionDim, actionMin, and actionMax.\n",
    "    #The mode parameter defines the algorithm. PPO-CMA-m is the default, i.e., PPO-CMA using the sample mirroring trick.\n",
    "    #Other choices are \"PPO-CMA\", \"PPO\", \"PG\" and \"PG-pos\". The two last modes denote vanilla policy gradient, and the \"-pos\"\n",
    "    #means that only positive advantage actions are used. See DidacticExample.py for visualization of different modes in a simple quadratic problem.\n",
    "    def __init__(self, stateDim:int, actionDim:int, actionMin:np.array, actionMax:np.array, learningRate=0.0005\n",
    "                 , gamma=0.99, GAElambda=0.95, PPOepsilon=0.2, PPOentropyLossWeight=0, nHidden:int=2\n",
    "                 , nUnitsPerLayer:int=128, mode=\"PPO-CMA-m\", activation=\"lrelu\", H:int=9, entropyLossWeight:float=0\n",
    "                 , sdLowLimit=0.01, useScaler:bool=True, criticTimestepScale=0.001,initialMean:np.array=None,initialSd:np.array=None):\n",
    "        #Create policy network \n",
    "        print(\"Creating policy\")\n",
    "        self.actionMin=actionMin\n",
    "        self.actionMax=actionMax\n",
    "        self.actionDim=actionDim\n",
    "        self.stateDim=stateDim\n",
    "        self.useScaler=useScaler\n",
    "        if useScaler:\n",
    "            self.scaler=Scaler(stateDim)\n",
    "        self.scalerInitialized=False\n",
    "        self.normalizeAdvantages=True\n",
    "        self.gamma=gamma\n",
    "        self.GAElambda=GAElambda\n",
    "        self.criticTimestepScale=0 if gamma==0 else criticTimestepScale     #with gamma==0, no need for this\n",
    "        piEpsilon = None\n",
    "        nHistory = 1\n",
    "        negativeAdvantageAvoidanceSigma = 0\n",
    "        if mode==\"PPO-CMA\" or mode==\"PPO-CMA-m\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt=True\n",
    "            self.reluAdvantages=True if mode==\"PPO-CMA\" else False\n",
    "            nHistory=H             #policy mean adapts immediately, policy covariance as an aggreagate of this many past iterations\n",
    "            useSigmaSoftClip=True\n",
    "            negativeAdvantageAvoidanceSigma=1 if mode==\"PPO-CMA-m\" else 0\n",
    "        elif mode==\"PPO\":\n",
    "            usePPOLoss=True           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=False\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        elif mode==\"PG\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=False\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        elif mode==\"PG-pos\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=True\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        else:\n",
    "            raise(\"Unknown mode {}\".format(mode))\n",
    "        self.policy=Policy(stateDim, actionDim, actionMin, actionMax, entropyLossWeight=PPOentropyLossWeight\n",
    "                           , networkActivation=activation, networkDepth=nHidden, networkUnits=nUnitsPerLayer\n",
    "                           , networkSkips=False, learningRate=learningRate, minSigma=sdLowLimit, PPOepsilon=PPOepsilon\n",
    "                           , usePPOLoss=usePPOLoss, separateVarAdapt=separateVarAdapt, nHistory=nHistory\n",
    "                           , useSigmaSoftClip=useSigmaSoftClip, piEpsilon=piEpsilon\n",
    "                           , negativeAdvantageAvoidanceSigma=negativeAdvantageAvoidanceSigma)\n",
    "\n",
    "        #Create critic network, +1 stateDim because at least in OpenAI gym, episodes are time-limited and the value estimates thus depend on simulation time.\n",
    "        #Thus, we use time step as an additional feature for the critic.\n",
    "        #Note that this does not mess up generalization, as the feature is not used for the policy during training or at runtime\n",
    "        print(\"Creating critic network\")\n",
    "        self.critic=Critic(stateDim=stateDim+1,learningRate=learningRate,nHidden=nHidden,networkUnits=nUnitsPerLayer,networkActivation=activation,useSkips=False,lossType=\"L1\")\n",
    "\n",
    "        #Experience trajectory buffers for the memorize() and updateWithMemorized() methods\n",
    "        self.experienceTrajectories=[]\n",
    "        self.currentTrajectory=[]\n",
    "\n",
    "        #Init may take as argument a desired initial action mean and sd. These need to be remembered for the first iteration's act,\n",
    "        #which samples the initial mean and sd directly instead of utilizing the policy network.\n",
    "        if initialMean is not None:\n",
    "            self.initialMean=initialMean.copy()\n",
    "        else:\n",
    "            self.initialMean=0.5*(self.actionMin+self.actionMax)*np.ones(self.actionDim)\n",
    "        if initialSd is not None:\n",
    "            self.initialSd=initialSd.copy()\n",
    "        else:\n",
    "            self.initialSd=0.5*(self.actionMax-self.actionMin)*np.ones(self.actionDim)\n",
    "\n",
    "    #call this after tensorflow's global variables initializer\n",
    "    def init(self,sess:tf.Session,verbose=False):\n",
    "        #Pretrain the policy to output the initial Gaussian for all states\n",
    "        self.policy.init(sess,0,1,self.initialMean,self.initialSd,256,2000,verbose)\n",
    "    \n",
    "    #stateObs is an n-by-m tensor, where n = number of observations, m = number of observation variables\n",
    "    def act(self,sess:tf.Session,stateObs:np.array,deterministic=False,clipActionToLimits=True):\n",
    "        #Expand a single 1d-observation into a batch of 1 vectors\n",
    "        if len(stateObs.shape)==1:\n",
    "            stateObs=np.reshape(stateObs,[1,stateObs.shape[0]])\n",
    "        #Query the policy for the action, except for the first iteration where we sample directly from the initial exploration Gaussian\n",
    "        #that covers the whole action space.\n",
    "        #This is done because we don't know the scale of state observations a priori; thus, we can only init the state scaler in update(), \n",
    "        #after we have collected some experience.\n",
    "        if self.useScaler and (not self.scalerInitialized):\n",
    "            actions=np.random.normal(self.initialMean,self.initialSd,size=[stateObs.shape[0],self.actionDim])\n",
    "            if clipActionToLimits:\n",
    "                actions=np.clip(actions,np.reshape(self.actionMin,[1,self.actionDim]),np.reshape(self.actionMax,[1,self.actionDim]))\n",
    "            return actions\n",
    "        else:\n",
    "            if self.useScaler:\n",
    "                scaledObs=self.scaler.process(stateObs)\n",
    "            else:\n",
    "                scaledObs=stateObs\n",
    "            if deterministic:\n",
    "                actions=self.policy.getExpectation(sess,scaledObs)\n",
    "            else:\n",
    "                actions=self.policy.sample(sess,scaledObs)\n",
    "            if clipActionToLimits:\n",
    "                actions=np.clip(actions,self.actionMin,self.actionMax)\n",
    "            return actions\n",
    "    def memorize(self,observation:np.array,action:np.array,reward:float,nextObservation:np.array,done:bool):\n",
    "        e = Experience(observation, action, reward, nextObservation, done)\n",
    "        self.currentTrajectory.append(e)\n",
    "        if done:\n",
    "            self.experienceTrajectories.append(self.currentTrajectory)\n",
    "            self.currentTrajectory=[]\n",
    "\n",
    "    def getAverageActionStdev(self):\n",
    "        if self.useScaler and (not self.scalerInitialized):\n",
    "            return np.mean(0.5*(self.actionMax-self.actionMin))\n",
    "        else:\n",
    "            return self.policy.usedSigmaSum/(1e-20+self.policy.usedSigmaSumCounter)\n",
    "\n",
    "    #If you call memorize() after each action, you can update the agent with this method. \n",
    "    #If you handle the experience buffers yourself, e.g., due to a multithreaded implementation, use the update() method instead.\n",
    "    def updateWithMemorized(self,sess:tf.Session,batchSize:int=512,nBatches:int=100,verbose=True,valuesValid=False,timestepsValid=False):\n",
    "        self.update(sess,experienceTrajectories=self.experienceTrajectories,batchSize=batchSize,nBatches=nBatches,verbose=verbose,valuesValid=valuesValid,timestepsValid=timestepsValid)\n",
    "        averageEpisodeReturn=0\n",
    "        for t in self.experienceTrajectories:\n",
    "            episodeReturn=0\n",
    "            for e in t:\n",
    "                episodeReturn+=e.r\n",
    "            averageEpisodeReturn+=episodeReturn\n",
    "        averageEpisodeReturn/=len(self.experienceTrajectories)\n",
    "        self.experienceTrajectories=[]\n",
    "        self.currentTrajectory=[]\n",
    "        return averageEpisodeReturn\n",
    "\n",
    "    #experienceTrajectories is a list of lists of Experience instances such that each of the contained lists corresponds to an episode simulation trajectory\n",
    "    def update(self,sess:tf.Session,experienceTrajectories,batchSize:int=512,nBatches:int=100,verbose=True,valuesValid=False,timestepsValid=False):\n",
    "        trajectories=experienceTrajectories   #shorthand\n",
    "\n",
    "        #Collect all data into linear arrays for training. \n",
    "        nTrajectories=len(trajectories)\n",
    "        nData=0\n",
    "        for trajectory in trajectories:\n",
    "            nData+=len(trajectory)\n",
    "            #propagate values backwards along trajectory if not already done\n",
    "            if not valuesValid:\n",
    "                for i in reversed(range(len(trajectory)-1)):\n",
    "                    #value estimates, used for training the critic and estimating advantages\n",
    "                    trajectory[i].V=trajectory[i].r+self.gamma*trajectory[i+1].V\n",
    "            #update time steps if not updated\n",
    "            if not timestepsValid:\n",
    "                for i in range(len(trajectory)):\n",
    "                    trajectory[i].timeStep=i\n",
    "        allStates=np.zeros([nData,self.stateDim])\n",
    "        allActions=np.zeros([nData,self.actionDim])\n",
    "        allValues=np.zeros([nData])\n",
    "        allTimes=np.zeros([nData,1])\n",
    "        k=0\n",
    "        for trajectory in trajectories:\n",
    "            for e in trajectory:\n",
    "                allStates[k,:]=e.s\n",
    "                allValues[k]=e.V  \n",
    "                allActions[k,:]=e.a\n",
    "                allTimes[k,0]=e.timeStep*self.criticTimestepScale \n",
    "                k+=1\n",
    "\n",
    "\n",
    "        #Update scalers\n",
    "        if self.useScaler:\n",
    "            self.scaler.update(allStates)\n",
    "            scale, offset = self.scaler.get()\n",
    "            self.scalerInitialized=True\n",
    "        else:\n",
    "            offset=0\n",
    "            scale=1\n",
    " \n",
    "        #Scale the observations for training the critic\n",
    "        scaledStates=self.scaler.process(allStates)\n",
    "\n",
    "        #Train critic\n",
    "        def augmentCriticObs(obs:np.array,timeSteps:np.array):\n",
    "            return np.concatenate([obs,timeSteps],axis=1)\n",
    "        self.critic.train(sess,augmentCriticObs(scaledStates,allTimes),allValues,batchSize,nEpochs=0,nBatches=nBatches,verbose=verbose)\n",
    "\n",
    "        #Policy training needs advantages, which depend on the critic we just trained.\n",
    "        #We use Generalized Advantage Estimation by Schulman et al.\n",
    "        if verbose:\n",
    "            print(\"Estimating advantages...\".format(len(trajectories)))\n",
    "        for t in trajectories:\n",
    "            #query the critic values of all states of this trajectory in one big batch\n",
    "            nSteps=len(t)\n",
    "            states=np.zeros([nSteps+1,self.stateDim])\n",
    "            timeSteps=np.zeros([nSteps+1,1])\n",
    "            for i in range(nSteps):\n",
    "                states[i,:]=t[i].s\n",
    "                timeSteps[i,0]=t[i].timeStep*self.criticTimestepScale\n",
    "            states[nSteps,:]=t[nSteps-1].s_next\n",
    "            states=(states-offset)*scale\n",
    "            values=self.critic.predict(sess,augmentCriticObs(states,timeSteps))\n",
    "\n",
    "            #GAE loop, i.e., take the instantaneous advantage (how much value a single action brings, assuming that the\n",
    "            #values given by the critic are unbiased), and smooth those along the trajectory using 1st-order IIR filter.\n",
    "            advantage=0\n",
    "            for step in reversed(range(nSteps)):\n",
    "                delta_t=t[step].r+self.gamma*values[step+1] - values[step]\n",
    "                advantage=delta_t+self.GAElambda*self.gamma*advantage\n",
    "                t[step].advantage=advantage\n",
    "\n",
    "        #Gather the advantages to linear array and apply ReLU and normalization if needed\n",
    "        allAdvantages=np.zeros([nData])\n",
    "        k=0\n",
    "        for trajectory in trajectories:\n",
    "            for e in trajectory:\n",
    "                allAdvantages[k]=e.advantage  \n",
    "                k+=1\n",
    "\n",
    "        if self.reluAdvantages:\n",
    "            allAdvantages=np.clip(allAdvantages,0,np.inf)\n",
    "        if self.normalizeAdvantages:\n",
    "            aMean=np.mean(allAdvantages)\n",
    "            aSd=np.std(allAdvantages)\n",
    "            if verbose:\n",
    "                print(\"Advantage mean {}, sd{}\".format(aMean,aSd))\n",
    "            allAdvantages/=1e-10+aSd\n",
    "            #Clamp the normalized advantages to 3 sd:s, in case of outliers. \n",
    "            #Commented out for now to allow computing additional ICML results, as this was not yet implemented in the ICML version\n",
    "            #advantageLimit=3\n",
    "            #allAdvantages=np.clip(allAdvantages,-advantageLimit,advantageLimit)\n",
    "\n",
    "        #Train policy. Note that this uses original unscaled states, because the PPO-CMA variance training needs a history of\n",
    "        #states in the same scale\n",
    "        self.policy.train(sess,allStates,allActions,allAdvantages,batchSize,nEpochs=0,nBatches=nBatches,stateOffset=offset,stateScale=scale,verbose=verbose)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simulation environment\n",
      "Creating policy\n",
      "Creating critic network\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "import tensorflow as tf\n",
    "\n",
    "#Simulation budget (steps) per iteration. This is the main parameter to tune.\n",
    "#8k works for relatively simple environments like the OpenAI Gym Roboschool 2D Hopper.\n",
    "#For more complex problems such as 3D humanoid locomotion, try 32k or even 64k.\n",
    "#Larger values are slower but more robust.\n",
    "N=4096\n",
    "\n",
    "# Stop training after this many steps\n",
    "max_steps=50000000\n",
    "\n",
    "# Init tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create environment (replace this with your own simulator)\n",
    "print(\"Creating simulation environment\")\n",
    "sim = gym.make(\"SlimeVolley-v0\")\n",
    "\n",
    "# Create the agent using the default parameters for the neural network architecture\n",
    "agent=Agent(\n",
    "    stateDim=sim.observation_space.low.shape[0]\n",
    "    , actionDim=3\n",
    "    , actionMin=np.array([0,0,0])\n",
    "    , actionMax=np.array([1,1,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodesSoFar 8, totalSimSteps 4144, AvgEpReturn -5.0, AvgEpLen 518.0\n",
      "EpisodesSoFar 16, totalSimSteps 8721, AvgEpReturn -4.875, AvgEpLen 572.125\n",
      "EpisodesSoFar 24, totalSimSteps 13517, AvgEpReturn -4.875, AvgEpLen 599.5\n",
      "EpisodesSoFar 32, totalSimSteps 18058, AvgEpReturn -4.75, AvgEpLen 567.625\n",
      "EpisodesSoFar 40, totalSimSteps 22682, AvgEpReturn -4.75, AvgEpLen 578.0\n",
      "EpisodesSoFar 47, totalSimSteps 26796, AvgEpReturn -5.0, AvgEpLen 587.7142857142857\n",
      "EpisodesSoFar 55, totalSimSteps 31397, AvgEpReturn -5.0, AvgEpLen 575.125\n",
      "EpisodesSoFar 63, totalSimSteps 35708, AvgEpReturn -5.0, AvgEpLen 538.875\n",
      "EpisodesSoFar 70, totalSimSteps 40287, AvgEpReturn -4.714285714285714, AvgEpLen 654.1428571428571\n",
      "EpisodesSoFar 78, totalSimSteps 44813, AvgEpReturn -4.875, AvgEpLen 565.75\n",
      "EpisodesSoFar 86, totalSimSteps 49121, AvgEpReturn -4.75, AvgEpLen 538.5\n",
      "EpisodesSoFar 94, totalSimSteps 53776, AvgEpReturn -4.75, AvgEpLen 581.875\n",
      "EpisodesSoFar 102, totalSimSteps 58210, AvgEpReturn -4.875, AvgEpLen 554.25\n",
      "EpisodesSoFar 109, totalSimSteps 62487, AvgEpReturn -4.857142857142857, AvgEpLen 611.0\n",
      "EpisodesSoFar 116, totalSimSteps 66788, AvgEpReturn -5.0, AvgEpLen 614.4285714285714\n",
      "EpisodesSoFar 123, totalSimSteps 71159, AvgEpReturn -4.857142857142857, AvgEpLen 624.4285714285714\n",
      "EpisodesSoFar 129, totalSimSteps 75399, AvgEpReturn -4.833333333333333, AvgEpLen 706.6666666666666\n",
      "EpisodesSoFar 137, totalSimSteps 80109, AvgEpReturn -5.0, AvgEpLen 588.75\n",
      "EpisodesSoFar 145, totalSimSteps 84344, AvgEpReturn -5.0, AvgEpLen 529.375\n",
      "EpisodesSoFar 151, totalSimSteps 88696, AvgEpReturn -4.666666666666667, AvgEpLen 725.3333333333334\n",
      "EpisodesSoFar 158, totalSimSteps 92950, AvgEpReturn -5.0, AvgEpLen 607.7142857142857\n",
      "EpisodesSoFar 165, totalSimSteps 97748, AvgEpReturn -5.0, AvgEpLen 685.4285714285714\n",
      "EpisodesSoFar 172, totalSimSteps 102492, AvgEpReturn -5.0, AvgEpLen 677.7142857142857\n",
      "EpisodesSoFar 179, totalSimSteps 106867, AvgEpReturn -5.0, AvgEpLen 625.0\n",
      "EpisodesSoFar 186, totalSimSteps 110992, AvgEpReturn -5.0, AvgEpLen 589.2857142857143\n",
      "EpisodesSoFar 193, totalSimSteps 115723, AvgEpReturn -4.857142857142857, AvgEpLen 675.8571428571429\n",
      "EpisodesSoFar 200, totalSimSteps 120266, AvgEpReturn -5.0, AvgEpLen 649.0\n",
      "EpisodesSoFar 208, totalSimSteps 124622, AvgEpReturn -5.0, AvgEpLen 544.5\n",
      "EpisodesSoFar 215, totalSimSteps 129289, AvgEpReturn -5.0, AvgEpLen 666.7142857142857\n",
      "EpisodesSoFar 222, totalSimSteps 133853, AvgEpReturn -5.0, AvgEpLen 652.0\n",
      "EpisodesSoFar 230, totalSimSteps 138503, AvgEpReturn -4.875, AvgEpLen 581.25\n",
      "EpisodesSoFar 236, totalSimSteps 142838, AvgEpReturn -4.833333333333333, AvgEpLen 722.5\n",
      "EpisodesSoFar 243, totalSimSteps 147243, AvgEpReturn -4.571428571428571, AvgEpLen 629.2857142857143\n",
      "EpisodesSoFar 251, totalSimSteps 151558, AvgEpReturn -4.875, AvgEpLen 539.375\n",
      "EpisodesSoFar 258, totalSimSteps 155904, AvgEpReturn -5.0, AvgEpLen 620.8571428571429\n",
      "EpisodesSoFar 265, totalSimSteps 160539, AvgEpReturn -4.714285714285714, AvgEpLen 662.1428571428571\n",
      "EpisodesSoFar 272, totalSimSteps 164750, AvgEpReturn -4.857142857142857, AvgEpLen 601.5714285714286\n",
      "EpisodesSoFar 279, totalSimSteps 168916, AvgEpReturn -4.857142857142857, AvgEpLen 595.1428571428571\n",
      "EpisodesSoFar 286, totalSimSteps 173147, AvgEpReturn -4.714285714285714, AvgEpLen 604.4285714285714\n",
      "EpisodesSoFar 293, totalSimSteps 177304, AvgEpReturn -5.0, AvgEpLen 593.8571428571429\n",
      "EpisodesSoFar 301, totalSimSteps 181601, AvgEpReturn -4.875, AvgEpLen 537.125\n",
      "EpisodesSoFar 307, totalSimSteps 185850, AvgEpReturn -4.333333333333333, AvgEpLen 708.1666666666666\n",
      "EpisodesSoFar 314, totalSimSteps 190072, AvgEpReturn -4.857142857142857, AvgEpLen 603.1428571428571\n",
      "EpisodesSoFar 323, totalSimSteps 194788, AvgEpReturn -5.0, AvgEpLen 524.0\n",
      "EpisodesSoFar 329, totalSimSteps 198956, AvgEpReturn -4.833333333333333, AvgEpLen 694.6666666666666\n",
      "EpisodesSoFar 336, totalSimSteps 203186, AvgEpReturn -4.714285714285714, AvgEpLen 604.2857142857143\n",
      "EpisodesSoFar 343, totalSimSteps 207783, AvgEpReturn -4.857142857142857, AvgEpLen 656.7142857142857\n",
      "EpisodesSoFar 351, totalSimSteps 212173, AvgEpReturn -5.0, AvgEpLen 548.75\n",
      "EpisodesSoFar 358, totalSimSteps 216533, AvgEpReturn -4.571428571428571, AvgEpLen 622.8571428571429\n",
      "EpisodesSoFar 366, totalSimSteps 220816, AvgEpReturn -5.0, AvgEpLen 535.375\n",
      "EpisodesSoFar 375, totalSimSteps 225290, AvgEpReturn -4.888888888888889, AvgEpLen 497.1111111111111\n",
      "EpisodesSoFar 382, totalSimSteps 229837, AvgEpReturn -4.857142857142857, AvgEpLen 649.5714285714286\n",
      "EpisodesSoFar 390, totalSimSteps 233970, AvgEpReturn -5.0, AvgEpLen 516.625\n",
      "EpisodesSoFar 396, totalSimSteps 238114, AvgEpReturn -4.833333333333333, AvgEpLen 690.6666666666666\n",
      "EpisodesSoFar 403, totalSimSteps 242398, AvgEpReturn -5.0, AvgEpLen 612.0\n",
      "EpisodesSoFar 410, totalSimSteps 246792, AvgEpReturn -5.0, AvgEpLen 627.7142857142857\n",
      "EpisodesSoFar 417, totalSimSteps 251113, AvgEpReturn -4.857142857142857, AvgEpLen 617.2857142857143\n",
      "EpisodesSoFar 425, totalSimSteps 255623, AvgEpReturn -4.875, AvgEpLen 563.75\n",
      "EpisodesSoFar 432, totalSimSteps 260572, AvgEpReturn -4.857142857142857, AvgEpLen 707.0\n",
      "EpisodesSoFar 439, totalSimSteps 264751, AvgEpReturn -4.857142857142857, AvgEpLen 597.0\n",
      "EpisodesSoFar 447, totalSimSteps 268914, AvgEpReturn -5.0, AvgEpLen 520.375\n",
      "EpisodesSoFar 455, totalSimSteps 273616, AvgEpReturn -5.0, AvgEpLen 587.75\n",
      "EpisodesSoFar 462, totalSimSteps 277948, AvgEpReturn -4.857142857142857, AvgEpLen 618.8571428571429\n",
      "EpisodesSoFar 470, totalSimSteps 282347, AvgEpReturn -4.875, AvgEpLen 549.875\n",
      "EpisodesSoFar 477, totalSimSteps 286531, AvgEpReturn -4.857142857142857, AvgEpLen 597.7142857142857\n",
      "EpisodesSoFar 484, totalSimSteps 290799, AvgEpReturn -5.0, AvgEpLen 609.7142857142857\n",
      "EpisodesSoFar 491, totalSimSteps 295177, AvgEpReturn -5.0, AvgEpLen 625.4285714285714\n",
      "EpisodesSoFar 498, totalSimSteps 299820, AvgEpReturn -4.857142857142857, AvgEpLen 663.2857142857143\n",
      "EpisodesSoFar 505, totalSimSteps 304633, AvgEpReturn -4.857142857142857, AvgEpLen 687.5714285714286\n",
      "EpisodesSoFar 512, totalSimSteps 309139, AvgEpReturn -5.0, AvgEpLen 643.7142857142857\n",
      "EpisodesSoFar 518, totalSimSteps 313420, AvgEpReturn -4.666666666666667, AvgEpLen 713.5\n",
      "EpisodesSoFar 524, totalSimSteps 317606, AvgEpReturn -4.666666666666667, AvgEpLen 697.6666666666666\n",
      "EpisodesSoFar 531, totalSimSteps 322209, AvgEpReturn -4.857142857142857, AvgEpLen 657.5714285714286\n",
      "EpisodesSoFar 538, totalSimSteps 327049, AvgEpReturn -4.714285714285714, AvgEpLen 691.4285714285714\n",
      "EpisodesSoFar 545, totalSimSteps 331504, AvgEpReturn -4.857142857142857, AvgEpLen 636.4285714285714\n",
      "EpisodesSoFar 552, totalSimSteps 336081, AvgEpReturn -4.714285714285714, AvgEpLen 653.8571428571429\n",
      "EpisodesSoFar 559, totalSimSteps 340911, AvgEpReturn -4.857142857142857, AvgEpLen 690.0\n",
      "EpisodesSoFar 566, totalSimSteps 345658, AvgEpReturn -5.0, AvgEpLen 678.1428571428571\n",
      "EpisodesSoFar 572, totalSimSteps 350300, AvgEpReturn -4.333333333333333, AvgEpLen 773.6666666666666\n",
      "EpisodesSoFar 579, totalSimSteps 354622, AvgEpReturn -5.0, AvgEpLen 617.4285714285714\n",
      "EpisodesSoFar 586, totalSimSteps 359063, AvgEpReturn -4.857142857142857, AvgEpLen 634.4285714285714\n",
      "EpisodesSoFar 593, totalSimSteps 363463, AvgEpReturn -5.0, AvgEpLen 628.5714285714286\n",
      "EpisodesSoFar 599, totalSimSteps 367593, AvgEpReturn -5.0, AvgEpLen 688.3333333333334\n",
      "EpisodesSoFar 606, totalSimSteps 371833, AvgEpReturn -5.0, AvgEpLen 605.7142857142857\n",
      "EpisodesSoFar 613, totalSimSteps 376273, AvgEpReturn -4.571428571428571, AvgEpLen 634.2857142857143\n",
      "EpisodesSoFar 621, totalSimSteps 380553, AvgEpReturn -5.0, AvgEpLen 535.0\n",
      "EpisodesSoFar 629, totalSimSteps 385324, AvgEpReturn -5.0, AvgEpLen 596.375\n",
      "EpisodesSoFar 637, totalSimSteps 389538, AvgEpReturn -5.0, AvgEpLen 526.75\n",
      "EpisodesSoFar 643, totalSimSteps 393736, AvgEpReturn -5.0, AvgEpLen 699.6666666666666\n",
      "EpisodesSoFar 650, totalSimSteps 398217, AvgEpReturn -4.857142857142857, AvgEpLen 640.1428571428571\n",
      "EpisodesSoFar 657, totalSimSteps 402910, AvgEpReturn -5.0, AvgEpLen 670.4285714285714\n",
      "EpisodesSoFar 663, totalSimSteps 407144, AvgEpReturn -4.333333333333333, AvgEpLen 705.6666666666666\n",
      "EpisodesSoFar 670, totalSimSteps 411318, AvgEpReturn -5.0, AvgEpLen 596.2857142857143\n",
      "EpisodesSoFar 677, totalSimSteps 415561, AvgEpReturn -4.571428571428571, AvgEpLen 606.1428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodesSoFar 683, totalSimSteps 419757, AvgEpReturn -4.833333333333333, AvgEpLen 699.3333333333334\n",
      "EpisodesSoFar 691, totalSimSteps 424242, AvgEpReturn -5.0, AvgEpLen 560.625\n",
      "EpisodesSoFar 698, totalSimSteps 428475, AvgEpReturn -4.857142857142857, AvgEpLen 604.7142857142857\n",
      "EpisodesSoFar 705, totalSimSteps 432902, AvgEpReturn -4.857142857142857, AvgEpLen 632.4285714285714\n",
      "EpisodesSoFar 712, totalSimSteps 437428, AvgEpReturn -5.0, AvgEpLen 646.5714285714286\n",
      "EpisodesSoFar 719, totalSimSteps 442002, AvgEpReturn -5.0, AvgEpLen 653.4285714285714\n",
      "EpisodesSoFar 726, totalSimSteps 446199, AvgEpReturn -5.0, AvgEpLen 599.5714285714286\n",
      "EpisodesSoFar 733, totalSimSteps 450571, AvgEpReturn -5.0, AvgEpLen 624.5714285714286\n",
      "EpisodesSoFar 740, totalSimSteps 454922, AvgEpReturn -5.0, AvgEpLen 621.5714285714286\n",
      "EpisodesSoFar 746, totalSimSteps 459444, AvgEpReturn -4.833333333333333, AvgEpLen 753.6666666666666\n",
      "EpisodesSoFar 752, totalSimSteps 463638, AvgEpReturn -5.0, AvgEpLen 699.0\n",
      "EpisodesSoFar 759, totalSimSteps 467831, AvgEpReturn -4.857142857142857, AvgEpLen 599.0\n",
      "EpisodesSoFar 766, totalSimSteps 472192, AvgEpReturn -5.0, AvgEpLen 623.0\n",
      "EpisodesSoFar 773, totalSimSteps 476524, AvgEpReturn -4.857142857142857, AvgEpLen 618.8571428571429\n",
      "EpisodesSoFar 780, totalSimSteps 481310, AvgEpReturn -4.857142857142857, AvgEpLen 683.7142857142857\n",
      "EpisodesSoFar 787, totalSimSteps 485714, AvgEpReturn -4.714285714285714, AvgEpLen 629.1428571428571\n",
      "EpisodesSoFar 794, totalSimSteps 489969, AvgEpReturn -4.857142857142857, AvgEpLen 607.8571428571429\n",
      "EpisodesSoFar 801, totalSimSteps 494290, AvgEpReturn -4.857142857142857, AvgEpLen 617.2857142857143\n",
      "EpisodesSoFar 809, totalSimSteps 498991, AvgEpReturn -4.875, AvgEpLen 587.625\n",
      "EpisodesSoFar 815, totalSimSteps 503367, AvgEpReturn -4.666666666666667, AvgEpLen 729.3333333333334\n",
      "EpisodesSoFar 823, totalSimSteps 507830, AvgEpReturn -5.0, AvgEpLen 557.875\n",
      "EpisodesSoFar 830, totalSimSteps 512434, AvgEpReturn -4.714285714285714, AvgEpLen 657.7142857142857\n",
      "EpisodesSoFar 837, totalSimSteps 516736, AvgEpReturn -5.0, AvgEpLen 614.5714285714286\n",
      "EpisodesSoFar 845, totalSimSteps 520930, AvgEpReturn -5.0, AvgEpLen 524.25\n",
      "EpisodesSoFar 852, totalSimSteps 525448, AvgEpReturn -5.0, AvgEpLen 645.4285714285714\n",
      "EpisodesSoFar 860, totalSimSteps 530092, AvgEpReturn -5.0, AvgEpLen 580.5\n",
      "EpisodesSoFar 867, totalSimSteps 534395, AvgEpReturn -5.0, AvgEpLen 614.7142857142857\n",
      "EpisodesSoFar 874, totalSimSteps 538844, AvgEpReturn -4.571428571428571, AvgEpLen 635.5714285714286\n",
      "EpisodesSoFar 882, totalSimSteps 543182, AvgEpReturn -5.0, AvgEpLen 542.25\n",
      "EpisodesSoFar 888, totalSimSteps 547358, AvgEpReturn -4.833333333333333, AvgEpLen 696.0\n",
      "EpisodesSoFar 896, totalSimSteps 551775, AvgEpReturn -5.0, AvgEpLen 552.125\n",
      "EpisodesSoFar 902, totalSimSteps 556342, AvgEpReturn -5.0, AvgEpLen 761.1666666666666\n",
      "EpisodesSoFar 910, totalSimSteps 560983, AvgEpReturn -4.875, AvgEpLen 580.125\n",
      "EpisodesSoFar 917, totalSimSteps 565217, AvgEpReturn -5.0, AvgEpLen 604.8571428571429\n",
      "EpisodesSoFar 924, totalSimSteps 569427, AvgEpReturn -4.857142857142857, AvgEpLen 601.4285714285714\n",
      "EpisodesSoFar 931, totalSimSteps 573950, AvgEpReturn -5.0, AvgEpLen 646.1428571428571\n",
      "EpisodesSoFar 938, totalSimSteps 578576, AvgEpReturn -4.857142857142857, AvgEpLen 660.8571428571429\n",
      "EpisodesSoFar 944, totalSimSteps 582868, AvgEpReturn -5.0, AvgEpLen 715.3333333333334\n",
      "EpisodesSoFar 951, totalSimSteps 587343, AvgEpReturn -5.0, AvgEpLen 639.2857142857143\n",
      "EpisodesSoFar 958, totalSimSteps 591829, AvgEpReturn -5.0, AvgEpLen 640.8571428571429\n",
      "EpisodesSoFar 966, totalSimSteps 596621, AvgEpReturn -4.875, AvgEpLen 599.0\n",
      "EpisodesSoFar 974, totalSimSteps 601023, AvgEpReturn -5.0, AvgEpLen 550.25\n",
      "EpisodesSoFar 980, totalSimSteps 605256, AvgEpReturn -4.5, AvgEpLen 705.5\n",
      "EpisodesSoFar 988, totalSimSteps 609831, AvgEpReturn -4.875, AvgEpLen 571.875\n",
      "EpisodesSoFar 995, totalSimSteps 613999, AvgEpReturn -4.714285714285714, AvgEpLen 595.4285714285714\n",
      "EpisodesSoFar 1001, totalSimSteps 618351, AvgEpReturn -5.0, AvgEpLen 725.3333333333334\n",
      "EpisodesSoFar 1009, totalSimSteps 622721, AvgEpReturn -5.0, AvgEpLen 546.25\n",
      "EpisodesSoFar 1016, totalSimSteps 626920, AvgEpReturn -5.0, AvgEpLen 599.8571428571429\n",
      "EpisodesSoFar 1023, totalSimSteps 631714, AvgEpReturn -4.857142857142857, AvgEpLen 684.8571428571429\n",
      "EpisodesSoFar 1031, totalSimSteps 636126, AvgEpReturn -5.0, AvgEpLen 551.5\n",
      "EpisodesSoFar 1037, totalSimSteps 640231, AvgEpReturn -4.833333333333333, AvgEpLen 684.1666666666666\n",
      "EpisodesSoFar 1043, totalSimSteps 644358, AvgEpReturn -4.833333333333333, AvgEpLen 687.8333333333334\n",
      "EpisodesSoFar 1050, totalSimSteps 649049, AvgEpReturn -4.857142857142857, AvgEpLen 670.1428571428571\n",
      "EpisodesSoFar 1057, totalSimSteps 653170, AvgEpReturn -5.0, AvgEpLen 588.7142857142857\n",
      "EpisodesSoFar 1063, totalSimSteps 657906, AvgEpReturn -4.833333333333333, AvgEpLen 789.3333333333334\n",
      "EpisodesSoFar 1070, totalSimSteps 662169, AvgEpReturn -4.714285714285714, AvgEpLen 609.0\n",
      "EpisodesSoFar 1078, totalSimSteps 666691, AvgEpReturn -4.875, AvgEpLen 565.25\n",
      "EpisodesSoFar 1085, totalSimSteps 671114, AvgEpReturn -4.857142857142857, AvgEpLen 631.8571428571429\n",
      "EpisodesSoFar 1092, totalSimSteps 675351, AvgEpReturn -5.0, AvgEpLen 605.2857142857143\n",
      "EpisodesSoFar 1100, totalSimSteps 680458, AvgEpReturn -4.75, AvgEpLen 638.375\n",
      "EpisodesSoFar 1107, totalSimSteps 685291, AvgEpReturn -4.571428571428571, AvgEpLen 690.4285714285714\n",
      "EpisodesSoFar 1113, totalSimSteps 689804, AvgEpReturn -5.0, AvgEpLen 752.1666666666666\n",
      "EpisodesSoFar 1120, totalSimSteps 694141, AvgEpReturn -5.0, AvgEpLen 619.5714285714286\n",
      "EpisodesSoFar 1127, totalSimSteps 698370, AvgEpReturn -5.0, AvgEpLen 604.1428571428571\n",
      "EpisodesSoFar 1134, totalSimSteps 702958, AvgEpReturn -4.857142857142857, AvgEpLen 655.4285714285714\n",
      "EpisodesSoFar 1141, totalSimSteps 707695, AvgEpReturn -4.714285714285714, AvgEpLen 676.7142857142857\n",
      "EpisodesSoFar 1148, totalSimSteps 711810, AvgEpReturn -5.0, AvgEpLen 587.8571428571429\n",
      "EpisodesSoFar 1156, totalSimSteps 716241, AvgEpReturn -5.0, AvgEpLen 553.875\n",
      "EpisodesSoFar 1163, totalSimSteps 720644, AvgEpReturn -4.857142857142857, AvgEpLen 629.0\n",
      "EpisodesSoFar 1170, totalSimSteps 724985, AvgEpReturn -4.571428571428571, AvgEpLen 620.1428571428571\n",
      "EpisodesSoFar 1177, totalSimSteps 729460, AvgEpReturn -4.857142857142857, AvgEpLen 639.2857142857143\n",
      "EpisodesSoFar 1185, totalSimSteps 733922, AvgEpReturn -4.75, AvgEpLen 557.75\n",
      "EpisodesSoFar 1192, totalSimSteps 738027, AvgEpReturn -4.857142857142857, AvgEpLen 586.4285714285714\n",
      "EpisodesSoFar 1199, totalSimSteps 742124, AvgEpReturn -4.857142857142857, AvgEpLen 585.2857142857143\n",
      "EpisodesSoFar 1207, totalSimSteps 746578, AvgEpReturn -4.875, AvgEpLen 556.75\n",
      "EpisodesSoFar 1214, totalSimSteps 750879, AvgEpReturn -5.0, AvgEpLen 614.4285714285714\n",
      "EpisodesSoFar 1222, totalSimSteps 755265, AvgEpReturn -5.0, AvgEpLen 548.25\n",
      "EpisodesSoFar 1229, totalSimSteps 759628, AvgEpReturn -4.571428571428571, AvgEpLen 623.2857142857143\n",
      "EpisodesSoFar 1236, totalSimSteps 764192, AvgEpReturn -5.0, AvgEpLen 652.0\n",
      "EpisodesSoFar 1244, totalSimSteps 768741, AvgEpReturn -5.0, AvgEpLen 568.625\n",
      "EpisodesSoFar 1251, totalSimSteps 773257, AvgEpReturn -5.0, AvgEpLen 645.1428571428571\n",
      "EpisodesSoFar 1258, totalSimSteps 777843, AvgEpReturn -4.714285714285714, AvgEpLen 655.1428571428571\n",
      "EpisodesSoFar 1266, totalSimSteps 782316, AvgEpReturn -4.875, AvgEpLen 559.125\n",
      "EpisodesSoFar 1274, totalSimSteps 787058, AvgEpReturn -4.75, AvgEpLen 592.75\n",
      "EpisodesSoFar 1282, totalSimSteps 791418, AvgEpReturn -5.0, AvgEpLen 545.0\n",
      "EpisodesSoFar 1290, totalSimSteps 795771, AvgEpReturn -4.875, AvgEpLen 544.125\n",
      "EpisodesSoFar 1298, totalSimSteps 800162, AvgEpReturn -4.875, AvgEpLen 548.875\n",
      "EpisodesSoFar 1306, totalSimSteps 804360, AvgEpReturn -5.0, AvgEpLen 524.75\n",
      "EpisodesSoFar 1313, totalSimSteps 808516, AvgEpReturn -5.0, AvgEpLen 593.7142857142857\n",
      "EpisodesSoFar 1320, totalSimSteps 813014, AvgEpReturn -4.571428571428571, AvgEpLen 642.5714285714286\n",
      "EpisodesSoFar 1328, totalSimSteps 817514, AvgEpReturn -5.0, AvgEpLen 562.5\n",
      "EpisodesSoFar 1335, totalSimSteps 821679, AvgEpReturn -5.0, AvgEpLen 595.0\n",
      "EpisodesSoFar 1343, totalSimSteps 825930, AvgEpReturn -4.875, AvgEpLen 531.375\n",
      "EpisodesSoFar 1351, totalSimSteps 830369, AvgEpReturn -4.75, AvgEpLen 554.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodesSoFar 1359, totalSimSteps 834979, AvgEpReturn -5.0, AvgEpLen 576.25\n",
      "EpisodesSoFar 1367, totalSimSteps 839152, AvgEpReturn -4.875, AvgEpLen 521.625\n",
      "EpisodesSoFar 1374, totalSimSteps 843568, AvgEpReturn -4.714285714285714, AvgEpLen 630.8571428571429\n",
      "EpisodesSoFar 1381, totalSimSteps 847765, AvgEpReturn -4.857142857142857, AvgEpLen 599.5714285714286\n",
      "EpisodesSoFar 1389, totalSimSteps 852407, AvgEpReturn -5.0, AvgEpLen 580.25\n",
      "EpisodesSoFar 1396, totalSimSteps 856874, AvgEpReturn -5.0, AvgEpLen 638.1428571428571\n",
      "EpisodesSoFar 1403, totalSimSteps 861158, AvgEpReturn -4.857142857142857, AvgEpLen 612.0\n",
      "EpisodesSoFar 1411, totalSimSteps 865821, AvgEpReturn -4.875, AvgEpLen 582.875\n",
      "EpisodesSoFar 1418, totalSimSteps 870413, AvgEpReturn -4.857142857142857, AvgEpLen 656.0\n",
      "EpisodesSoFar 1425, totalSimSteps 874623, AvgEpReturn -5.0, AvgEpLen 601.4285714285714\n",
      "EpisodesSoFar 1433, totalSimSteps 878847, AvgEpReturn -4.875, AvgEpLen 528.0\n",
      "EpisodesSoFar 1441, totalSimSteps 883558, AvgEpReturn -5.0, AvgEpLen 588.875\n",
      "EpisodesSoFar 1447, totalSimSteps 887792, AvgEpReturn -4.333333333333333, AvgEpLen 705.6666666666666\n",
      "EpisodesSoFar 1454, totalSimSteps 892226, AvgEpReturn -4.857142857142857, AvgEpLen 633.4285714285714\n",
      "EpisodesSoFar 1461, totalSimSteps 896543, AvgEpReturn -5.0, AvgEpLen 616.7142857142857\n",
      "EpisodesSoFar 1468, totalSimSteps 901147, AvgEpReturn -4.714285714285714, AvgEpLen 657.7142857142857\n",
      "EpisodesSoFar 1475, totalSimSteps 905304, AvgEpReturn -4.714285714285714, AvgEpLen 593.8571428571429\n",
      "EpisodesSoFar 1483, totalSimSteps 909891, AvgEpReturn -5.0, AvgEpLen 573.375\n",
      "EpisodesSoFar 1491, totalSimSteps 914427, AvgEpReturn -5.0, AvgEpLen 567.0\n",
      "EpisodesSoFar 1498, totalSimSteps 918731, AvgEpReturn -4.857142857142857, AvgEpLen 614.8571428571429\n",
      "EpisodesSoFar 1505, totalSimSteps 922890, AvgEpReturn -4.714285714285714, AvgEpLen 594.1428571428571\n",
      "EpisodesSoFar 1513, totalSimSteps 927281, AvgEpReturn -4.875, AvgEpLen 548.875\n",
      "EpisodesSoFar 1521, totalSimSteps 932041, AvgEpReturn -5.0, AvgEpLen 595.0\n",
      "EpisodesSoFar 1528, totalSimSteps 936185, AvgEpReturn -5.0, AvgEpLen 592.0\n",
      "EpisodesSoFar 1535, totalSimSteps 940467, AvgEpReturn -4.714285714285714, AvgEpLen 611.7142857142857\n",
      "EpisodesSoFar 1542, totalSimSteps 944861, AvgEpReturn -4.714285714285714, AvgEpLen 627.7142857142857\n",
      "EpisodesSoFar 1548, totalSimSteps 949126, AvgEpReturn -4.666666666666667, AvgEpLen 710.8333333333334\n",
      "EpisodesSoFar 1556, totalSimSteps 953766, AvgEpReturn -4.875, AvgEpLen 580.0\n",
      "EpisodesSoFar 1563, totalSimSteps 957994, AvgEpReturn -4.857142857142857, AvgEpLen 604.0\n",
      "EpisodesSoFar 1570, totalSimSteps 962439, AvgEpReturn -4.714285714285714, AvgEpLen 635.0\n",
      "EpisodesSoFar 1577, totalSimSteps 966601, AvgEpReturn -4.714285714285714, AvgEpLen 594.5714285714286\n",
      "EpisodesSoFar 1584, totalSimSteps 970706, AvgEpReturn -5.0, AvgEpLen 586.4285714285714\n",
      "EpisodesSoFar 1591, totalSimSteps 975050, AvgEpReturn -4.857142857142857, AvgEpLen 620.5714285714286\n",
      "EpisodesSoFar 1599, totalSimSteps 979591, AvgEpReturn -4.75, AvgEpLen 567.625\n",
      "EpisodesSoFar 1606, totalSimSteps 983755, AvgEpReturn -5.0, AvgEpLen 594.8571428571429\n",
      "EpisodesSoFar 1614, totalSimSteps 988347, AvgEpReturn -4.875, AvgEpLen 574.0\n",
      "EpisodesSoFar 1622, totalSimSteps 992894, AvgEpReturn -5.0, AvgEpLen 568.375\n",
      "EpisodesSoFar 1629, totalSimSteps 997059, AvgEpReturn -4.857142857142857, AvgEpLen 595.0\n",
      "EpisodesSoFar 1638, totalSimSteps 1001946, AvgEpReturn -4.777777777777778, AvgEpLen 543.0\n",
      "EpisodesSoFar 1645, totalSimSteps 1006202, AvgEpReturn -4.857142857142857, AvgEpLen 608.0\n",
      "EpisodesSoFar 1652, totalSimSteps 1010805, AvgEpReturn -4.571428571428571, AvgEpLen 657.5714285714286\n",
      "EpisodesSoFar 1659, totalSimSteps 1014933, AvgEpReturn -5.0, AvgEpLen 589.7142857142857\n",
      "EpisodesSoFar 1666, totalSimSteps 1019404, AvgEpReturn -4.857142857142857, AvgEpLen 638.7142857142857\n",
      "EpisodesSoFar 1673, totalSimSteps 1023529, AvgEpReturn -4.857142857142857, AvgEpLen 589.2857142857143\n",
      "EpisodesSoFar 1681, totalSimSteps 1027946, AvgEpReturn -4.625, AvgEpLen 552.125\n",
      "EpisodesSoFar 1688, totalSimSteps 1032573, AvgEpReturn -4.714285714285714, AvgEpLen 661.0\n",
      "EpisodesSoFar 1695, totalSimSteps 1036820, AvgEpReturn -4.714285714285714, AvgEpLen 606.7142857142857\n",
      "EpisodesSoFar 1702, totalSimSteps 1041127, AvgEpReturn -4.571428571428571, AvgEpLen 615.2857142857143\n",
      "EpisodesSoFar 1709, totalSimSteps 1045471, AvgEpReturn -4.857142857142857, AvgEpLen 620.5714285714286\n",
      "EpisodesSoFar 1716, totalSimSteps 1049810, AvgEpReturn -4.857142857142857, AvgEpLen 619.8571428571429\n",
      "EpisodesSoFar 1723, totalSimSteps 1054007, AvgEpReturn -5.0, AvgEpLen 599.5714285714286\n",
      "EpisodesSoFar 1731, totalSimSteps 1058549, AvgEpReturn -4.875, AvgEpLen 567.75\n",
      "EpisodesSoFar 1739, totalSimSteps 1063122, AvgEpReturn -4.625, AvgEpLen 571.625\n",
      "EpisodesSoFar 1746, totalSimSteps 1067541, AvgEpReturn -4.714285714285714, AvgEpLen 631.2857142857143\n",
      "EpisodesSoFar 1753, totalSimSteps 1071902, AvgEpReturn -5.0, AvgEpLen 623.0\n",
      "EpisodesSoFar 1760, totalSimSteps 1076342, AvgEpReturn -4.714285714285714, AvgEpLen 634.2857142857143\n",
      "EpisodesSoFar 1767, totalSimSteps 1080712, AvgEpReturn -4.714285714285714, AvgEpLen 624.2857142857143\n",
      "EpisodesSoFar 1773, totalSimSteps 1085099, AvgEpReturn -4.5, AvgEpLen 731.1666666666666\n",
      "EpisodesSoFar 1780, totalSimSteps 1089492, AvgEpReturn -5.0, AvgEpLen 627.5714285714286\n",
      "EpisodesSoFar 1787, totalSimSteps 1093811, AvgEpReturn -4.857142857142857, AvgEpLen 617.0\n",
      "EpisodesSoFar 1795, totalSimSteps 1098370, AvgEpReturn -4.875, AvgEpLen 569.875\n",
      "EpisodesSoFar 1803, totalSimSteps 1103305, AvgEpReturn -4.75, AvgEpLen 616.875\n",
      "EpisodesSoFar 1810, totalSimSteps 1107668, AvgEpReturn -4.857142857142857, AvgEpLen 623.2857142857143\n",
      "EpisodesSoFar 1817, totalSimSteps 1112213, AvgEpReturn -4.857142857142857, AvgEpLen 649.2857142857143\n",
      "EpisodesSoFar 1825, totalSimSteps 1117028, AvgEpReturn -4.75, AvgEpLen 601.875\n",
      "EpisodesSoFar 1832, totalSimSteps 1121647, AvgEpReturn -4.857142857142857, AvgEpLen 659.8571428571429\n",
      "EpisodesSoFar 1840, totalSimSteps 1126498, AvgEpReturn -4.75, AvgEpLen 606.375\n",
      "EpisodesSoFar 1847, totalSimSteps 1131172, AvgEpReturn -4.857142857142857, AvgEpLen 667.7142857142857\n",
      "EpisodesSoFar 1855, totalSimSteps 1135673, AvgEpReturn -5.0, AvgEpLen 562.625\n",
      "EpisodesSoFar 1863, totalSimSteps 1139950, AvgEpReturn -4.875, AvgEpLen 534.625\n",
      "EpisodesSoFar 1870, totalSimSteps 1144104, AvgEpReturn -4.857142857142857, AvgEpLen 593.4285714285714\n",
      "EpisodesSoFar 1877, totalSimSteps 1148390, AvgEpReturn -5.0, AvgEpLen 612.2857142857143\n",
      "EpisodesSoFar 1884, totalSimSteps 1152729, AvgEpReturn -4.857142857142857, AvgEpLen 619.8571428571429\n",
      "EpisodesSoFar 1892, totalSimSteps 1157248, AvgEpReturn -4.75, AvgEpLen 564.875\n",
      "EpisodesSoFar 1900, totalSimSteps 1161847, AvgEpReturn -4.875, AvgEpLen 574.875\n",
      "EpisodesSoFar 1907, totalSimSteps 1166123, AvgEpReturn -5.0, AvgEpLen 610.8571428571429\n",
      "EpisodesSoFar 1914, totalSimSteps 1170293, AvgEpReturn -4.714285714285714, AvgEpLen 595.7142857142857\n",
      "EpisodesSoFar 1922, totalSimSteps 1174668, AvgEpReturn -5.0, AvgEpLen 546.875\n",
      "EpisodesSoFar 1929, totalSimSteps 1178980, AvgEpReturn -4.571428571428571, AvgEpLen 616.0\n",
      "EpisodesSoFar 1936, totalSimSteps 1183126, AvgEpReturn -4.857142857142857, AvgEpLen 592.2857142857143\n",
      "EpisodesSoFar 1943, totalSimSteps 1187801, AvgEpReturn -4.857142857142857, AvgEpLen 667.8571428571429\n",
      "EpisodesSoFar 1950, totalSimSteps 1192164, AvgEpReturn -5.0, AvgEpLen 623.2857142857143\n",
      "EpisodesSoFar 1957, totalSimSteps 1196453, AvgEpReturn -5.0, AvgEpLen 612.7142857142857\n",
      "EpisodesSoFar 1964, totalSimSteps 1200616, AvgEpReturn -4.857142857142857, AvgEpLen 594.7142857142857\n",
      "EpisodesSoFar 1972, totalSimSteps 1204865, AvgEpReturn -5.0, AvgEpLen 531.125\n",
      "EpisodesSoFar 1979, totalSimSteps 1209102, AvgEpReturn -5.0, AvgEpLen 605.2857142857143\n",
      "EpisodesSoFar 1986, totalSimSteps 1213320, AvgEpReturn -4.714285714285714, AvgEpLen 602.5714285714286\n",
      "EpisodesSoFar 1993, totalSimSteps 1217815, AvgEpReturn -4.428571428571429, AvgEpLen 642.1428571428571\n",
      "EpisodesSoFar 2000, totalSimSteps 1222291, AvgEpReturn -4.714285714285714, AvgEpLen 639.4285714285714\n",
      "EpisodesSoFar 2007, totalSimSteps 1226471, AvgEpReturn -4.714285714285714, AvgEpLen 597.1428571428571\n",
      "EpisodesSoFar 2014, totalSimSteps 1230838, AvgEpReturn -4.428571428571429, AvgEpLen 623.8571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodesSoFar 2022, totalSimSteps 1235298, AvgEpReturn -5.0, AvgEpLen 557.5\n",
      "EpisodesSoFar 2029, totalSimSteps 1239462, AvgEpReturn -4.857142857142857, AvgEpLen 594.8571428571429\n",
      "EpisodesSoFar 2036, totalSimSteps 1243663, AvgEpReturn -4.857142857142857, AvgEpLen 600.1428571428571\n",
      "EpisodesSoFar 2044, totalSimSteps 1247986, AvgEpReturn -5.0, AvgEpLen 540.375\n",
      "EpisodesSoFar 2051, totalSimSteps 1252584, AvgEpReturn -4.714285714285714, AvgEpLen 656.8571428571429\n",
      "EpisodesSoFar 2058, totalSimSteps 1257036, AvgEpReturn -5.0, AvgEpLen 636.0\n",
      "EpisodesSoFar 2065, totalSimSteps 1261330, AvgEpReturn -4.714285714285714, AvgEpLen 613.4285714285714\n",
      "EpisodesSoFar 2073, totalSimSteps 1265699, AvgEpReturn -4.875, AvgEpLen 546.125\n",
      "EpisodesSoFar 2081, totalSimSteps 1270248, AvgEpReturn -5.0, AvgEpLen 568.625\n",
      "EpisodesSoFar 2088, totalSimSteps 1274476, AvgEpReturn -4.714285714285714, AvgEpLen 604.0\n",
      "EpisodesSoFar 2096, totalSimSteps 1278910, AvgEpReturn -4.875, AvgEpLen 554.25\n",
      "EpisodesSoFar 2103, totalSimSteps 1283447, AvgEpReturn -4.714285714285714, AvgEpLen 648.1428571428571\n",
      "EpisodesSoFar 2110, totalSimSteps 1287985, AvgEpReturn -4.714285714285714, AvgEpLen 648.2857142857143\n",
      "EpisodesSoFar 2117, totalSimSteps 1292143, AvgEpReturn -4.714285714285714, AvgEpLen 594.0\n",
      "EpisodesSoFar 2124, totalSimSteps 1296339, AvgEpReturn -5.0, AvgEpLen 599.4285714285714\n",
      "EpisodesSoFar 2130, totalSimSteps 1300666, AvgEpReturn -4.666666666666667, AvgEpLen 721.1666666666666\n",
      "EpisodesSoFar 2138, totalSimSteps 1305127, AvgEpReturn -5.0, AvgEpLen 557.625\n",
      "EpisodesSoFar 2145, totalSimSteps 1309224, AvgEpReturn -4.857142857142857, AvgEpLen 585.2857142857143\n",
      "EpisodesSoFar 2152, totalSimSteps 1313673, AvgEpReturn -4.571428571428571, AvgEpLen 635.5714285714286\n",
      "EpisodesSoFar 2160, totalSimSteps 1318045, AvgEpReturn -4.875, AvgEpLen 546.5\n",
      "EpisodesSoFar 2167, totalSimSteps 1322146, AvgEpReturn -5.0, AvgEpLen 585.8571428571429\n",
      "EpisodesSoFar 2175, totalSimSteps 1326888, AvgEpReturn -5.0, AvgEpLen 592.75\n",
      "EpisodesSoFar 2183, totalSimSteps 1331269, AvgEpReturn -5.0, AvgEpLen 547.625\n",
      "EpisodesSoFar 2190, totalSimSteps 1335496, AvgEpReturn -4.857142857142857, AvgEpLen 603.8571428571429\n",
      "EpisodesSoFar 2197, totalSimSteps 1339865, AvgEpReturn -4.857142857142857, AvgEpLen 624.1428571428571\n",
      "EpisodesSoFar 2204, totalSimSteps 1343968, AvgEpReturn -5.0, AvgEpLen 586.1428571428571\n",
      "EpisodesSoFar 2212, totalSimSteps 1348492, AvgEpReturn -4.75, AvgEpLen 565.5\n",
      "EpisodesSoFar 2220, totalSimSteps 1353042, AvgEpReturn -5.0, AvgEpLen 568.75\n",
      "EpisodesSoFar 2227, totalSimSteps 1357656, AvgEpReturn -4.857142857142857, AvgEpLen 659.1428571428571\n",
      "EpisodesSoFar 2234, totalSimSteps 1362142, AvgEpReturn -5.0, AvgEpLen 640.8571428571429\n",
      "EpisodesSoFar 2241, totalSimSteps 1366375, AvgEpReturn -4.714285714285714, AvgEpLen 604.7142857142857\n",
      "EpisodesSoFar 2248, totalSimSteps 1370992, AvgEpReturn -4.857142857142857, AvgEpLen 659.5714285714286\n",
      "EpisodesSoFar 2255, totalSimSteps 1375735, AvgEpReturn -5.0, AvgEpLen 677.5714285714286\n",
      "EpisodesSoFar 2261, totalSimSteps 1379906, AvgEpReturn -5.0, AvgEpLen 695.1666666666666\n",
      "EpisodesSoFar 2268, totalSimSteps 1384018, AvgEpReturn -4.571428571428571, AvgEpLen 587.4285714285714\n",
      "EpisodesSoFar 2276, totalSimSteps 1388478, AvgEpReturn -4.875, AvgEpLen 557.5\n",
      "EpisodesSoFar 2283, totalSimSteps 1392951, AvgEpReturn -5.0, AvgEpLen 639.0\n",
      "EpisodesSoFar 2290, totalSimSteps 1397120, AvgEpReturn -4.857142857142857, AvgEpLen 595.5714285714286\n",
      "EpisodesSoFar 2298, totalSimSteps 1401719, AvgEpReturn -4.625, AvgEpLen 574.875\n",
      "EpisodesSoFar 2305, totalSimSteps 1406130, AvgEpReturn -5.0, AvgEpLen 630.1428571428571\n",
      "EpisodesSoFar 2313, totalSimSteps 1410672, AvgEpReturn -4.875, AvgEpLen 567.75\n",
      "EpisodesSoFar 2321, totalSimSteps 1415352, AvgEpReturn -4.75, AvgEpLen 585.0\n",
      "EpisodesSoFar 2328, totalSimSteps 1419704, AvgEpReturn -4.857142857142857, AvgEpLen 621.7142857142857\n",
      "EpisodesSoFar 2335, totalSimSteps 1423841, AvgEpReturn -4.857142857142857, AvgEpLen 591.0\n",
      "EpisodesSoFar 2343, totalSimSteps 1428076, AvgEpReturn -5.0, AvgEpLen 529.375\n",
      "EpisodesSoFar 2351, totalSimSteps 1432554, AvgEpReturn -4.875, AvgEpLen 559.75\n",
      "EpisodesSoFar 2358, totalSimSteps 1436883, AvgEpReturn -4.714285714285714, AvgEpLen 618.4285714285714\n",
      "EpisodesSoFar 2365, totalSimSteps 1441147, AvgEpReturn -5.0, AvgEpLen 609.1428571428571\n",
      "EpisodesSoFar 2373, totalSimSteps 1445577, AvgEpReturn -5.0, AvgEpLen 553.75\n",
      "EpisodesSoFar 2380, totalSimSteps 1450199, AvgEpReturn -4.857142857142857, AvgEpLen 660.2857142857143\n",
      "EpisodesSoFar 2387, totalSimSteps 1454364, AvgEpReturn -5.0, AvgEpLen 595.0\n",
      "EpisodesSoFar 2395, totalSimSteps 1458817, AvgEpReturn -4.875, AvgEpLen 556.625\n",
      "EpisodesSoFar 2402, totalSimSteps 1462999, AvgEpReturn -4.857142857142857, AvgEpLen 597.4285714285714\n",
      "EpisodesSoFar 2410, totalSimSteps 1467431, AvgEpReturn -4.875, AvgEpLen 554.0\n",
      "EpisodesSoFar 2417, totalSimSteps 1471644, AvgEpReturn -5.0, AvgEpLen 601.8571428571429\n",
      "EpisodesSoFar 2425, totalSimSteps 1476136, AvgEpReturn -5.0, AvgEpLen 561.5\n",
      "EpisodesSoFar 2433, totalSimSteps 1480588, AvgEpReturn -4.875, AvgEpLen 556.5\n",
      "EpisodesSoFar 2442, totalSimSteps 1485207, AvgEpReturn -5.0, AvgEpLen 513.2222222222222\n",
      "EpisodesSoFar 2450, totalSimSteps 1489415, AvgEpReturn -5.0, AvgEpLen 526.0\n",
      "EpisodesSoFar 2458, totalSimSteps 1494103, AvgEpReturn -5.0, AvgEpLen 586.0\n",
      "EpisodesSoFar 2465, totalSimSteps 1498420, AvgEpReturn -5.0, AvgEpLen 616.7142857142857\n",
      "EpisodesSoFar 2472, totalSimSteps 1502708, AvgEpReturn -4.714285714285714, AvgEpLen 612.5714285714286\n",
      "EpisodesSoFar 2480, totalSimSteps 1507015, AvgEpReturn -4.875, AvgEpLen 538.375\n",
      "EpisodesSoFar 2488, totalSimSteps 1511689, AvgEpReturn -4.75, AvgEpLen 584.25\n",
      "EpisodesSoFar 2495, totalSimSteps 1515786, AvgEpReturn -4.857142857142857, AvgEpLen 585.2857142857143\n",
      "EpisodesSoFar 2502, totalSimSteps 1520079, AvgEpReturn -4.714285714285714, AvgEpLen 613.2857142857143\n",
      "EpisodesSoFar 2509, totalSimSteps 1524392, AvgEpReturn -4.857142857142857, AvgEpLen 616.1428571428571\n",
      "EpisodesSoFar 2515, totalSimSteps 1528538, AvgEpReturn -4.833333333333333, AvgEpLen 691.0\n",
      "EpisodesSoFar 2522, totalSimSteps 1532713, AvgEpReturn -5.0, AvgEpLen 596.4285714285714\n",
      "EpisodesSoFar 2529, totalSimSteps 1537084, AvgEpReturn -4.857142857142857, AvgEpLen 624.4285714285714\n",
      "EpisodesSoFar 2537, totalSimSteps 1541631, AvgEpReturn -5.0, AvgEpLen 568.375\n",
      "EpisodesSoFar 2544, totalSimSteps 1545808, AvgEpReturn -4.571428571428571, AvgEpLen 596.7142857142857\n",
      "EpisodesSoFar 2552, totalSimSteps 1550133, AvgEpReturn -5.0, AvgEpLen 540.625\n",
      "EpisodesSoFar 2560, totalSimSteps 1554372, AvgEpReturn -5.0, AvgEpLen 529.875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b7fbf6398fbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Query the agent for action given the state observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-8a2ab543411f>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, sess, stateObs, deterministic, clipActionToLimits)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetExpectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaledObs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaledObs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclipActionToLimits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionMin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionMax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-220678415c49>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sess, observations, enforcedRelSigma)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnObs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionDim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mpolicyMean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicySigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicyMean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicySigma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicyMean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Policy mean is NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finalize initialization\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "agent.init(sess)  # must be called after TensorFlow global variables init\n",
    "\n",
    "# Main training loop\n",
    "totalSimSteps = 0\n",
    "EpisodesSoFar = 0\n",
    "history = {\n",
    "    'EpisodesSoFar': [],\n",
    "    'TotalSimSteps': [],\n",
    "    'AvgEpReturn': [],\n",
    "    'AvgEpLen': [],\n",
    "}\n",
    "while totalSimSteps < max_steps:\n",
    "\n",
    "    #Run episodes until the iteration simulation budget runs out\n",
    "    iterSimSteps = 0\n",
    "    AvgEpLen = 0\n",
    "    numIter = 0\n",
    "    while iterSimSteps < N:\n",
    "\n",
    "        # Reset the simulation\n",
    "        observation = sim.reset()\n",
    "        t = 0\n",
    "\n",
    "        # Simulate this episode until done (e.g., due to time limit or failure)\n",
    "        done=False\n",
    "        while not done:\n",
    "            # Query the agent for action given the state observation\n",
    "            action = agent.act(sess,observation)[0]\n",
    "            action[action > 0.5] = 1\n",
    "            action[action <= 0.5] = 0\n",
    "\n",
    "            # Simulate using the action\n",
    "            # Note: this tutorial does not repeat the same action for multiple steps,\n",
    "            # unlike the Run.py script used for the paper results.\n",
    "            # Repeating the action for multiple steps seems to yield better exploration\n",
    "            # in most cases, possibly because it reduces high-frequency action noise.\n",
    "            nextObservation, reward, done, info = sim.step(action)\n",
    "\n",
    "            # Save the experience point\n",
    "            agent.memorize(observation,action,reward,nextObservation,done)\n",
    "            observation=nextObservation\n",
    "\n",
    "            # Bookkeeping\n",
    "            iterSimSteps += 1\n",
    "            t += 1\n",
    "        \n",
    "        numIter += 1\n",
    "        AvgEpLen += t\n",
    "        \n",
    "    AvgEpLen /= numIter\n",
    "    EpisodesSoFar += numIter\n",
    "\n",
    "    #All episodes of this iteration done, update the agent and print results\n",
    "    averageEpisodeReturn=agent.updateWithMemorized(sess,verbose=False)\n",
    "    totalSimSteps += iterSimSteps\n",
    "    print(f\"EpisodesSoFar {EpisodesSoFar}, totalSimSteps {totalSimSteps}, AvgEpReturn {averageEpisodeReturn}, AvgEpLen {AvgEpLen}\")\n",
    "    history['EpisodesSoFar'].append(EpisodesSoFar)\n",
    "    history['TotalSimSteps'].append(totalSimSteps)\n",
    "    history['AvgEpReturn'].append(averageEpisodeReturn)\n",
    "    history['AvgEpLen'].append(AvgEpLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
