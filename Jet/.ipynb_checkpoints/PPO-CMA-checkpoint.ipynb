{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating toy data\n",
      "Initializing matplotlib\n",
      "Creating model\n",
      "Initializing model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing\n",
      "Iter 0/8000, loss 0.5425289273262024\n",
      "Iter 100/8000, loss 0.313284307718277\n",
      "Iter 200/8000, loss 0.07331258058547974\n",
      "Iter 300/8000, loss 0.04261814057826996\n",
      "Iter 400/8000, loss 0.03155049309134483\n",
      "Iter 500/8000, loss 0.02567838318645954\n",
      "Iter 600/8000, loss 0.02142924629151821\n",
      "Iter 700/8000, loss 0.018512936308979988\n",
      "Iter 800/8000, loss 0.017459187656641006\n",
      "Iter 900/8000, loss 0.013317030854523182\n",
      "Iter 1000/8000, loss 0.011760233901441097\n",
      "Iter 1100/8000, loss 0.011722041293978691\n",
      "Iter 1200/8000, loss 0.013641536235809326\n",
      "Iter 1300/8000, loss 0.008636864833533764\n",
      "Iter 1400/8000, loss 0.007601458113640547\n",
      "Iter 1500/8000, loss 0.007181674707680941\n",
      "Iter 1600/8000, loss 0.007032033521682024\n",
      "Iter 1700/8000, loss 0.007888782769441605\n",
      "Iter 1800/8000, loss 0.006519978400319815\n",
      "Iter 1900/8000, loss 0.010867409408092499\n",
      "Iter 2000/8000, loss 0.005177628714591265\n",
      "Iter 2100/8000, loss 0.004982576239854097\n",
      "Iter 2200/8000, loss 0.0048825847916305065\n",
      "Iter 2300/8000, loss 0.00578698143362999\n",
      "Iter 2400/8000, loss 0.006320700515061617\n",
      "Iter 2500/8000, loss 0.004415755160152912\n",
      "Iter 2600/8000, loss 0.004538515117019415\n",
      "Iter 2700/8000, loss 0.014415491372346878\n",
      "Iter 2800/8000, loss 0.004250491037964821\n",
      "Iter 2900/8000, loss 0.004043246619403362\n",
      "Iter 3000/8000, loss 0.003778949147090316\n",
      "Iter 3100/8000, loss 0.003863696474581957\n",
      "Iter 3200/8000, loss 0.0036520245485007763\n",
      "Iter 3300/8000, loss 0.0035655952524393797\n",
      "Iter 3400/8000, loss 0.0034680559765547514\n",
      "Iter 3500/8000, loss 0.0034354727249592543\n",
      "Iter 3600/8000, loss 0.0038258428685367107\n",
      "Iter 3700/8000, loss 0.007127892691642046\n",
      "Iter 3800/8000, loss 0.0033614651765674353\n",
      "Iter 3900/8000, loss 0.003430780954658985\n",
      "Iter 4000/8000, loss 0.0032504082191735506\n",
      "Iter 4100/8000, loss 0.006277330685406923\n",
      "Iter 4200/8000, loss 0.0034568021073937416\n",
      "Iter 4300/8000, loss 0.003346838289871812\n",
      "Iter 4400/8000, loss 0.036137521266937256\n",
      "Iter 4500/8000, loss 0.003060121089220047\n",
      "Iter 4600/8000, loss 0.0029957604128867388\n",
      "Iter 4700/8000, loss 0.0029125914443284273\n",
      "Iter 4800/8000, loss 0.002992026275023818\n",
      "Iter 4900/8000, loss 0.0028428894001990557\n",
      "Iter 5000/8000, loss 0.0027444830629974604\n",
      "Iter 5100/8000, loss 0.0027473936788737774\n",
      "Iter 5200/8000, loss 0.0028539132326841354\n",
      "Iter 5300/8000, loss 0.002663768595084548\n",
      "Iter 5400/8000, loss 0.0027534363325685263\n",
      "Iter 5500/8000, loss 0.012649317272007465\n",
      "Iter 5600/8000, loss 0.004153485409915447\n",
      "Iter 5700/8000, loss 0.0024649922270327806\n",
      "Iter 5800/8000, loss 0.002625701017677784\n",
      "Iter 5900/8000, loss 0.0024418304674327374\n",
      "Iter 6000/8000, loss 0.0023823422379791737\n",
      "Iter 6100/8000, loss 0.0026147211901843548\n",
      "Iter 6200/8000, loss 0.003145774593576789\n",
      "Iter 6300/8000, loss 0.0024241171777248383\n",
      "Iter 6400/8000, loss 0.002428291831165552\n",
      "Iter 6500/8000, loss 0.00222343229688704\n",
      "Iter 6600/8000, loss 0.003927676938474178\n",
      "Iter 6700/8000, loss 0.0021773672197014093\n",
      "Iter 6800/8000, loss 0.0031116297468543053\n",
      "Iter 6900/8000, loss 0.004731438122689724\n",
      "Iter 7000/8000, loss 0.002106043277308345\n",
      "Iter 7100/8000, loss 0.0021436905954033136\n",
      "Iter 7200/8000, loss 0.0020313893910497427\n",
      "Iter 7300/8000, loss 0.0020311661064624786\n",
      "Iter 7400/8000, loss 0.002041041385382414\n",
      "Iter 7500/8000, loss 0.002280891640111804\n",
      "Iter 7600/8000, loss 0.012465735897421837\n",
      "Iter 7700/8000, loss 0.001943580457009375\n",
      "Iter 7800/8000, loss 0.008892909623682499\n",
      "Iter 7900/8000, loss 0.003391299629583955\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "useUnitNormInit=True\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,input:tf.Tensor,initInput:tf.Tensor,nUnits:int,useSkips=True,activation=None):\n",
    "        X=input\n",
    "        xDim=X.shape[1].value\n",
    "\n",
    "        #Default forward pass ops\n",
    "        #Initial weight matrix rows are random and unit-norm.\n",
    "        #in other words, if input is unit-variance, output variables are also unit-variance \n",
    "        if useUnitNormInit:\n",
    "            initialW=np.random.normal(0,1,size=[nUnits,xDim])\n",
    "            initialW=initialW/np.linalg.norm(initialW,axis=1,keepdims=True)\n",
    "        else:\n",
    "            #MSRA initialization (https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer)\n",
    "            initialW=tf.truncated_normal([nUnits,xDim], 0.0, stddev=tf.sqrt(2.0 / xDim))\n",
    "        self.W=tf.Variable(initial_value=initialW,dtype=tf.float32,name='W')\n",
    "        self.b=tf.Variable(initial_value=np.zeros([1,nUnits]),dtype=tf.float32,name='b')\n",
    "        h=tf.matmul(X,tf.transpose(self.W))+self.b\n",
    "        if activation==\"relu\":\n",
    "            h=tf.nn.relu(h)\n",
    "        elif activation==\"selu\":\n",
    "            h=tf.nn.selu(h)\n",
    "        elif activation==\"lrelu\":\n",
    "            h=tf.nn.leaky_relu(h,alpha=0.1)\n",
    "        elif activation==\"tanh\":\n",
    "            h=tf.nn.tanh(h)\n",
    "        elif activation==\"swish\":\n",
    "            h=tf.nn.swish(h)\n",
    "        elif activation is not None:\n",
    "            raise NameError(\"Invalid activation type ({}) for Layer\".format(activation))\n",
    "        if useSkips:\n",
    "            self.output=tf.concat([X,h],axis=1)\n",
    "        else:\n",
    "            self.output=h\n",
    "\n",
    "        #Init ops for data-dependent initialization pass.\n",
    "        #Same functionality as above, but we init biases such that the input distribution's mean maps to zero output.\n",
    "        X=initInput\n",
    "        xMean=tf.reduce_mean(X,axis=0,keepdims=True)\n",
    "        b=tf.assign(self.b,-tf.matmul(xMean,tf.transpose(self.W)))\n",
    "        h=tf.matmul(X,tf.transpose(self.W))+b\n",
    "        if activation==\"relu\":\n",
    "            h=tf.nn.relu(h)\n",
    "        elif activation==\"selu\":\n",
    "            h=tf.nn.selu(h)\n",
    "        elif activation==\"lrelu\":\n",
    "            h=tf.nn.leaky_relu(h,alpha=0.1)\n",
    "        elif activation==\"tanh\":\n",
    "            h=tf.nn.tanh(h)\n",
    "        elif activation==\"swish\":\n",
    "            h=tf.nn.swish(h)\n",
    "        elif activation is not None:\n",
    "            raise NameError(\"Invalid activation type ({}) for Layer\".format(activation))\n",
    "        if useSkips:\n",
    "            self.initOutput=tf.concat([X,h],axis=1)\n",
    "        else:\n",
    "            self.initOutput=h\n",
    "\n",
    "\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self,input:tf.Tensor,nLayers:int,nUnitsPerLayer:int, nOutputUnits:int, activation=\"lrelu\", firstLinearLayerUnits:int=0, useSkips:bool=True):\n",
    "        self.layers=[]\n",
    "        X=input\n",
    "        initX=input\n",
    "\n",
    "        #add optional first linear layer (useful, e.g., for reducing the dimensionality of high-dimensional outputs\n",
    "        #which reduces the parameter count of all subsequent layers)\n",
    "        if firstLinearLayerUnits!=0:\n",
    "            layer=Layer(X,initX,firstLinearLayerUnits,useSkips=False,activation=None)\n",
    "            self.layers.append(layer)\n",
    "            X,initX=layer.output,layer.initOutput\n",
    "\n",
    "        #add hidden layers\n",
    "        for layerIdx in range(nLayers):\n",
    "            layer=Layer(X,initX,nUnitsPerLayer,useSkips=useSkips,activation=activation)\n",
    "            self.layers.append(layer)\n",
    "            X,initX=layer.output,layer.initOutput\n",
    "\n",
    "        #add output layer\n",
    "        layer=Layer(X,initX,nOutputUnits,useSkips=False,activation=None)\n",
    "        self.layers.append(layer)\n",
    "        self.output,self.initOutput=layer.output,layer.initOutput\n",
    "\n",
    "\n",
    "\n",
    "    #This method returns a list of assign ops that can be used with a sess.run() call to copy\n",
    "    #all network parameters from a source network. This is useful, e.g., for implementing a slowly updated\n",
    "    #target network in Reinforcement Learning\n",
    "    def copyFromOps(self,src):\n",
    "        result=[]\n",
    "        for layerIdx in range(len(self.layers)):\n",
    "            result.append(tf.assign(self.layers[layerIdx].W,src.layers[layerIdx].W))\n",
    "            result.append(tf.assign(self.layers[layerIdx].b,src.layers[layerIdx].b))\n",
    "        return result\n",
    "\n",
    "#functional interface\n",
    "def mlp(input:tf.Tensor,nLayers:int,nUnitsPerLayer:int, nOutputUnits:int, activation=\"selu\", firstLinearLayerUnits:int=0,useSkips:bool=True):\n",
    "    instance=MLP(input,nLayers,nUnitsPerLayer,nOutputUnits,activation,firstLinearLayerUnits)\n",
    "    return instance.output,instance.initOutput\n",
    "\n",
    "#simple test: \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating toy data\")\n",
    "    x=[]\n",
    "    y=[]\n",
    "    maxAngle=5*np.pi\n",
    "    discontinuousTest=True\n",
    "    if discontinuousTest:\n",
    "        maxAngle=np.pi\n",
    "        for angle in np.arange(0,maxAngle,0.01):\n",
    "            x.append(angle)\n",
    "            if angle>maxAngle*0.8:\n",
    "                y.append(0.0)\n",
    "            else:\n",
    "                y.append(np.sin(angle)*np.sign(np.sin(angle*10)))\n",
    "    else:\n",
    "        for angle in np.arange(0,maxAngle,0.1):\n",
    "            r=angle*0.15\n",
    "            x.append(angle)\n",
    "            if angle>maxAngle*0.8:\n",
    "                y.append(0.0)\n",
    "            else:\n",
    "                y.append(r*np.sin(angle))\n",
    "\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    x=np.reshape(x,[x.shape[0],1])\n",
    "    y=np.reshape(y,[y.shape[0],1])\n",
    "    interpRange=0.2\n",
    "    xtest=np.arange(-interpRange+np.min(x),np.max(x)+interpRange,0.001)\n",
    "    xtest=np.reshape(xtest,[xtest.shape[0],1])\n",
    "    \n",
    "    print(\"Initializing matplotlib\")\n",
    "    pp.figure(1)\n",
    "    pp.subplot(3,1,1)\n",
    "    pp.scatter(x[:,0],y[:,0])\n",
    "    pp.ylabel(\"data\")\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    sess=tf.InteractiveSession()\n",
    "    tfX=tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "    tfY=tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "    #IMPORTANT: deep networks benefit immensely from data-dependent initialization.\n",
    "    #This is why the constructor returns the initial predictions separately - to initialize, fetch this tensor in a sess.run with \n",
    "    #the first minibatch. See the sess.run below\n",
    "    predictions,initialPredictions=mlp(input=tfX,nLayers=8,nUnitsPerLayer=8,nOutputUnits=1,activation=\"lrelu\")\n",
    "    optimizer=tf.train.AdamOptimizer()\n",
    "    loss=tf.losses.mean_squared_error(tfY,predictions)\n",
    "    optimize=optimizer.minimize(loss)\n",
    "  \n",
    "    print(\"Initializing model\")\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    #This sess.run() initializes the network biases based on x, and also returns the initial predictions.\n",
    "    #It is noteworthy that with this initialization, even a deep network has zero-mean output with variance similar to input.\n",
    "    networkOut=sess.run(initialPredictions,feed_dict={tfX:x})\n",
    "    pp.subplot(3,1,2)\n",
    "    pp.scatter(x[:,0],networkOut[:,0])\n",
    "    pp.ylabel(\"initialization\")\n",
    "    pp.draw()\n",
    "    pp.pause(0.001)\n",
    "\n",
    "\n",
    "    print(\"Optimizing\")\n",
    "    nIter=8000\n",
    "    for iter in range(nIter):\n",
    "        temp,currLoss=sess.run([optimize,loss],feed_dict={tfX:x,tfY:y})\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Iter {}/{}, loss {}\".format(iter,nIter,currLoss))\n",
    "    networkOut=sess.run(predictions,feed_dict={tfX:x})\n",
    "    pp.subplot(3,1,3)\n",
    "    pp.scatter(x[:,0],networkOut[:,0])\n",
    "    pp.ylabel(\"trained\")\n",
    "\n",
    "    pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def softClip(x, minVal, maxVal):\n",
    "    #return minVal+(maxVal-minVal)*(1.0+0.5*tf.tanh(x))\n",
    "    return minVal+(maxVal-minVal)*tf.sigmoid(x)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, stateDim:int, actionDim:int, actionMinLimit:np.array, actionMaxLimit:np.array, mode=\"PPO-CMA\"\n",
    "                 , entropyLossWeight=0, networkDepth=2, networkUnits=64, networkActivation=\"lrelu\"\n",
    "                 , networkSkips=False, networkUnitNormInit=True, usePPOLoss=False, separateVarAdapt=False\n",
    "                 , learningRate=0.001, minSigma=0.01, useSigmaSoftClip=True, PPOepsilon=0.2, piEpsilon=0, nHistory=1\n",
    "                 , globalVariance=False, trainableGlobalVariance=True, useGradientClipping=False\n",
    "                 , maxGradientNorm=0.5, negativeAdvantageAvoidanceSigma=0):\n",
    "        self.networkDepth = networkDepth\n",
    "        self.networkUnits = networkUnits\n",
    "        self.networkActivation = networkActivation\n",
    "        self.networkSkips = networkSkips\n",
    "        self.networkUnitNormInit = networkUnitNormInit\n",
    "        self.usePPOLoss = usePPOLoss\n",
    "        self.separateVarAdapt = separateVarAdapt\n",
    "        self.learningRate = learningRate\n",
    "        self.minSigma = minSigma\n",
    "        self.useSigmaSoftClip=useSigmaSoftClip\n",
    "        self.PPOepsilon = PPOepsilon\n",
    "        self.piEpsilon = piEpsilon\n",
    "        self.nHistory = nHistory\n",
    "        self.globalVariance = globalVariance\n",
    "        self.trainableGlobalVariance = trainableGlobalVariance\n",
    "        self.useGradientClipping = useGradientClipping\n",
    "        self.maxGradientNorm = maxGradientNorm\n",
    "        self.negativeAdvantageAvoidanceSigma = negativeAdvantageAvoidanceSigma\n",
    "\n",
    "        maxSigma=1.0*(actionMaxLimit-actionMinLimit)\n",
    "        self.mode=mode\n",
    "\n",
    "        #to be able to benchmark with Schulman's original network architecture, we may have to disable the data-dependent init of the DenseNet module\n",
    "        useUnitNormInit=self.networkUnitNormInit\n",
    "\n",
    "        #some bookkeeping\n",
    "        self.usedSigmaSum=0\n",
    "        self.usedSigmaSumCounter=0\n",
    "\n",
    "        #inputs\n",
    "        stateIn=tf.placeholder(dtype=tf.float32,shape=[None,stateDim],name=\"stateIn\")\n",
    "        actionIn=tf.placeholder(dtype=tf.float32,shape=[None,actionDim],name=\"actionIn\")    #training targets for policy network\n",
    "        oldPolicyMean=tf.placeholder(dtype=tf.float32,shape=[None,actionDim],name=\"oldPolicyMeanIn\")    #training targets for policy network\n",
    "        self.oldPolicyMean=oldPolicyMean\n",
    "        advantagesIn=tf.placeholder(dtype=tf.float32,shape=[None],name=\"advantagesIn\")     #weights, computed based on action advantages\n",
    "        logPiOldIn=tf.placeholder(dtype=tf.float32,shape=[None],name=\"logPiOldIn\")             #pi_old(a | s), used for PPO\n",
    "        initSigmaIn=tf.placeholder(dtype=tf.float32,shape=[1,actionDim],name=\"initSigmaIn\")\n",
    "        \n",
    "        #by default, we won't use a linear layer at the beginning of the network to reduce dimensionality\n",
    "        firstLinearLayerUnits=0\n",
    "\n",
    "        #First, define the mean and log var tensors, depending on configuration. \n",
    "        #Depending on the network architecture, we may also need initialization tensors, fetching which causes a data-dependent initialization of the graph\n",
    "        policyInit=[]\n",
    "        if stateDim==0:\n",
    "            #We don't have state at all => policyMean and variance are simply TensorFlow variables\n",
    "            policyMean=tf.Variable(initial_value=np.zeros([actionDim]),dtype=tf.float32)\n",
    "            policyLogVar=tf.Variable(initial_value=np.log(np.square(0.5*(actionMaxLimit-actionMinLimit)))*np.ones([actionDim]),dtype=tf.float32,trainable=self.trainableGlobalVariance)\n",
    "            self.globalLogVarVariable=policyLogVar\n",
    "        else:\n",
    "            #We have state, i.e., need neural networks that output a state-dependent mean and variance\n",
    "            if self.separateVarAdapt or self.globalVariance:\n",
    "                #Need separate networks for mean and variance\n",
    "                policyMean,policyMeanInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                policyInit.append(policyMeanInit)\n",
    "                if self.globalVariance:\n",
    "                    policyLogVar=tf.Variable(initial_value=np.log(np.square(0.5*(actionMaxLimit-actionMinLimit)))*np.ones([actionDim]),dtype=tf.float32,trainable=self.trainableGlobalVariance)\n",
    "                    self.globalLogVarVariable=policyLogVar\n",
    "                else:\n",
    "                    policyLogVar,policyLogVarInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                    policyInit.append(policyLogVarInit)\n",
    "            else:\n",
    "                #Single network that outputs both mean and variance\n",
    "                policyMeanAndLogVar,policyMeanAndLogVarInit=mlp(stateIn,self.networkDepth,self.networkUnits,actionDim*2,self.networkActivation,firstLinearLayerUnits,self.networkSkips)\n",
    "                policyMean=policyMeanAndLogVar[:,:actionDim]\n",
    "                policyLogVar=policyMeanAndLogVar[:,actionDim:]\n",
    "                policyInit.append(policyMeanAndLogVarInit)\n",
    "\n",
    "        #sigmoid-clipping of mean to ensure stability\n",
    "        policyMean=softClip(policyMean, actionMinLimit,actionMaxLimit)\n",
    "        if self.useSigmaSoftClip:\n",
    "            #sigmoid-clipping of log var to ensure stability\n",
    "            #Note: tanh or hard clipping doesn't work as well due to higher chance of zero or almost zero gradients \n",
    "            maxLogVar=np.log(maxSigma*maxSigma)\n",
    "            minLogVar=np.log(self.minSigma*self.minSigma)\n",
    "            policyLogVar=softClip(policyLogVar,minLogVar,maxLogVar)\n",
    "        policyVar=tf.exp(policyLogVar)  \n",
    "        policySigma=tf.sqrt(policyVar)\n",
    "\n",
    "\n",
    "        #loss functions\n",
    "        if self.usePPOLoss:\n",
    "            def loss(policyMean,policyVar,policyLogVar):\n",
    "                #1/sqrt(var)=exp(log(1/sqrt(var)))=exp(log(1)-log(var^0.5))=exp(-0.5*log(var))=exp(-log(std))\n",
    "                logPi=tf.reduce_sum(-0.5*tf.square(actionIn-policyMean)/policyVar-0.5*policyLogVar,axis=1)\n",
    "                #Some PPO implementations use r=tf.exp(logPi-logPiOldIn). However, we've noticed this to cause NaNs especially\n",
    "                #with non-saturating policy network activation functions like lrelu and the MuJoCo humanoid env.\n",
    "                #Thus, we also support using the epsilon below to regularize. \n",
    "                if self.piEpsilon==0:\n",
    "                    r=tf.exp(logPi-logPiOldIn)\n",
    "                else:\n",
    "                    r=tf.exp(logPi)/(self.piEpsilon+tf.exp(logPiOldIn))\n",
    "                perSampleLoss=tf.minimum(r*advantagesIn,tf.clip_by_value(r,1-self.PPOepsilon,1+self.PPOepsilon)*advantagesIn)\n",
    "                return -tf.reduce_mean(perSampleLoss) #because we want to minimize instead of maximize...\n",
    "            print(\"Using PPO clipped surrogate loss with epsilon {}\".format(self.PPOepsilon))\n",
    "            policyLoss=loss(policyMean,policyVar,policyLogVar)\n",
    "            if entropyLossWeight>0:\n",
    "                #Entropy of a diagonal Gaussian=0.5*log(det(Cov))=0.5*log(trace(Cov))=0.5*sum(log(diag(Cov)))\n",
    "                policyLoss-=entropyLossWeight*0.5*tf.reduce_mean(tf.reduce_sum(policyLogVar,axis=1))\n",
    "            assert(self.separateVarAdapt==False)\n",
    "            #just to be on the safe side, if some batch has an occasional NaN, set the loss to zero\n",
    "            policyLoss=tf.where(tf.is_nan(policyLoss), tf.zeros_like(policyLoss),policyLoss)\n",
    "            policyMeanLoss=policyLoss\n",
    "            policySigmaLoss=policyLoss\n",
    "        else:\n",
    "            #Separate mean and sigma adaptation losses\n",
    "            policyNoGrad=tf.stop_gradient(policyMean)\n",
    "            policyVarNoGrad=tf.stop_gradient(policyVar)\n",
    "            policyLogVarNoGrad=tf.stop_gradient(policyLogVar)\n",
    "            logpNoMeanGrad=-tf.reduce_sum(0.5*tf.square(actionIn-policyNoGrad)/policyVar+0.5*policyLogVar,axis=1)\n",
    "            logpNoVarGrad=-tf.reduce_sum(0.5*tf.square(actionIn-policyMean)/policyVarNoGrad+0.5*policyLogVarNoGrad,axis=1) \n",
    "            posAdvantages=tf.nn.relu(advantagesIn)\n",
    "            policySigmaLoss=-tf.reduce_mean(posAdvantages*logpNoMeanGrad)\n",
    "            policyMeanLoss=-tf.reduce_mean(posAdvantages*logpNoVarGrad)\n",
    "            if self.negativeAdvantageAvoidanceSigma>0:\n",
    "                negAdvantages=tf.nn.relu(-advantagesIn)\n",
    "                mirroredAction=oldPolicyMean-(actionIn-oldPolicyMean)  #mirror negative advantage actions around old policy mean (convert them to positive advantage actions assuming linearity) \n",
    "                logpNoVarGradMirrored=-tf.reduce_sum(0.5*tf.square(mirroredAction-policyMean)/policyVarNoGrad+0.5*policyLogVarNoGrad,axis=1) \n",
    "                effectiveKernelSqWidth=self.negativeAdvantageAvoidanceSigma*self.negativeAdvantageAvoidanceSigma*policyVarNoGrad\n",
    "                avoidanceKernel=tf.reduce_mean(tf.exp(-0.5*tf.square(actionIn-oldPolicyMean)/effectiveKernelSqWidth),axis=1)\n",
    "                policyMeanLoss-=tf.reduce_mean((negAdvantages*avoidanceKernel)*logpNoVarGradMirrored)\n",
    "\n",
    "            #just to be on the safe side, if some batch has an occasional NaN, set the loss to zero\n",
    "            policySigmaLoss=tf.where(tf.is_nan(policySigmaLoss), tf.zeros_like(policySigmaLoss),policySigmaLoss)\n",
    "            policyMeanLoss=tf.where(tf.is_nan(policyMeanLoss), tf.zeros_like(policyMeanLoss),policyMeanLoss)\n",
    "\n",
    "            #Vanilla Policy Gradient loss\n",
    "            logp=-tf.reduce_sum(0.5*tf.square(actionIn-policyMean)/policyVar+0.5*policyLogVar,axis=1)\n",
    "            policyLoss=tf.reduce_mean(-advantagesIn*logp)  \n",
    "\n",
    "        #loss functions for initialization (pretraining)\n",
    "        policyInitLoss=tf.reduce_mean(tf.square(actionIn-policyMean))\n",
    "        policyInitLoss+=tf.reduce_mean(tf.square(initSigmaIn-policySigma))\n",
    "\n",
    "        #optimizers\n",
    "        def optimize(loss):\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=self.learningRate)\n",
    "            if not self.useGradientClipping:\n",
    "                return optimizer.minimize(loss)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, self.maxGradientNorm)\n",
    "            return optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.optimizePolicy=optimize(policyLoss)\n",
    "        self.optimizePolicySigma=optimize(policySigmaLoss)\n",
    "        self.optimizePolicyMean=optimize(policyMeanLoss)\n",
    "        self.optimizePolicyInit=optimize(policyInitLoss)\n",
    "\n",
    "        #cache stuff needed elsewhere\n",
    "        self.actionMinLimit=actionMinLimit\n",
    "        self.actionMaxLimit=actionMaxLimit\n",
    "        self.stateDim=stateDim\n",
    "        self.actionDim=actionDim\n",
    "        self.policyMean=policyMean\n",
    "        self.stateIn=stateIn\n",
    "        self.actionIn=actionIn\n",
    "        self.policyInit=policyInit\n",
    "        self.policyInitLoss=policyInitLoss\n",
    "        self.advantagesIn=advantagesIn\n",
    "        self.policyLoss=policyLoss\n",
    "        self.logPiOldIn=logPiOldIn\n",
    "        self.initSigmaIn=initSigmaIn\n",
    "        self.history=deque()\n",
    "        self.policyVar=policyVar\n",
    "        self.policyLogVar=policyLogVar\n",
    "        self.policySigma=policySigma\n",
    "        self.initialized=False  #remember that one has to call init() before training (can't call it here as TF globals might not have been initialized yet)\n",
    "\n",
    "    #init the policy with random Gaussian state samples, such that the network outputs the desired mean and sd\n",
    "    def init(self,sess:tf.Session,stateMean:np.array,stateSd:np.array,actionMean:np.array,actionSd:np.array,nMinibatch:int=64,nBatches:int=4000,verbose=True):\n",
    "        for batchIdx in range(nBatches):\n",
    "            states=np.random.normal(stateMean,stateSd,size=[nMinibatch,self.stateDim])\n",
    "            if batchIdx==0 and len(self.policyInit)>0:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.policyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            else:\n",
    "                #drive output towards the desired mean and sd\n",
    "                temp,currLoss=sess.run([self.optimizePolicyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            if verbose and (batchIdx % 100 ==0):\n",
    "                print(\"Initializing policy with random Gaussian data, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "        self.initialized=True\n",
    "    #init the policy with uniform random state samples, such that the network outputs the desired mean and sd\n",
    "    def initUniform(self,sess:tf.Session,stateMin:np.array,stateMax:np.array,actionMean:np.array,actionSd:np.array,nMinibatch:int=64,nBatches:int=4000):\n",
    "        for batchIdx in range(nBatches):\n",
    "            states=np.random.uniform(stateMin,stateMax,size=[nMinibatch,self.stateDim])\n",
    "            if batchIdx==0 and len(self.policyInit)>0:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.policyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            else:\n",
    "                #drive output towards the desired mean and sd\n",
    "                temp,currLoss=sess.run([self.optimizePolicyInit,self.policyInitLoss],feed_dict={self.stateIn:states,self.actionIn:np.reshape(actionMean,[1,self.actionDim]),self.initSigmaIn:np.reshape(actionSd,[1,self.actionDim])})\n",
    "            if batchIdx % 100 ==0:\n",
    "                print(\"Initializing policy with random Gaussian data, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "        self.initialized=True\n",
    "    #if nBatches==0, nEpochs will be used\n",
    "    def train(self,sess:tf.Session,states:np.array,actions:np.array,advantages:np.array,nMinibatch:int,nEpochs:int,nBatches:int=0,stateOffset=0,stateScale=1,verbose=True):\n",
    "        assert(np.all(np.isfinite(states)))\n",
    "        assert(np.all(np.isfinite(actions)))\n",
    "        assert(np.all(np.isfinite(advantages)))\n",
    "        assert(self.initialized)\n",
    "        nData=actions.shape[0]\n",
    "\n",
    "        #reset bookkeeping for next iter\n",
    "        self.usedSigmaSum=0\n",
    "        self.usedSigmaSumCounter=0\n",
    "\n",
    "        #manage history\n",
    "        self.history.append([states.copy(),actions.copy(),advantages.copy()])\n",
    "        if len(self.history)>self.nHistory:\n",
    "            self.history.popleft()\n",
    "\n",
    "        #safety-check that the observed state distribution is at least roughly zero-mean unit sd\n",
    "        if self.stateDim>0:\n",
    "            scaledStates=(states-stateOffset)*stateScale\n",
    "            stateAbsMax=np.max(np.absolute(scaledStates))\n",
    "            if stateAbsMax>10:\n",
    "                print(\"Warning: states deviate up to {} sd:s from expected!\".format(stateAbsMax))\n",
    "        else:\n",
    "            scaledStates=states\n",
    "        #train\n",
    "        assert(len(advantages.shape)==1)  #to prevent nasty silent broadcasting bugs\n",
    "        nMinibatch=min([nData,nMinibatch])\n",
    "        if nBatches==0:\n",
    "            nBatches=max([1,int(nData*nEpochs/nMinibatch)])\n",
    "        #nBatches=1000\n",
    "        nVarAdaptBatches=nBatches\n",
    "        mbStates=np.zeros([nMinibatch,self.stateDim])\n",
    "        mbActions=np.zeros([nMinibatch,self.actionDim])\n",
    "        mbOldMean=np.zeros([nMinibatch,self.actionDim])\n",
    "        mbAdvantages=np.zeros([nMinibatch])\n",
    "        logPiOld=np.ones([nData])\n",
    "        mbLogPiOld=np.ones([nMinibatch])\n",
    "        if self.usePPOLoss:\n",
    "            policyMean,policyVar,policyLogVar=sess.run([self.policyMean,self.policyVar,self.policyLogVar],feed_dict={self.stateIn:scaledStates})\n",
    "            #for i in range(nData):\n",
    "            #    logPiOld[i]=np.sum(-0.5*np.square(actions[i,:]-policyMean[i,:])/policyVar[i,:]-0.5*policyLogVar[i,:])\n",
    "            logPiOld=np.sum(-0.5*np.square(actions-policyMean)/policyVar-0.5*policyLogVar,axis=1)\n",
    "        if self.separateVarAdapt:\n",
    "            assert(self.usePPOLoss==False)\n",
    "            #if negativeAdvantageAvoidanceSigma>0:\n",
    "            oldMeans=sess.run(self.policyMean,{self.stateIn:scaledStates})\n",
    "            for batchIdx in range(nBatches + nVarAdaptBatches if self.separateVarAdapt else nBatches):\n",
    "                if batchIdx<nVarAdaptBatches:\n",
    "                    historyLen=len(self.history)\n",
    "                    for i in range(nMinibatch):\n",
    "                        histIdx=np.random.randint(0,historyLen)\n",
    "                        h=self.history[histIdx]\n",
    "                        nData=h[1].shape[0]\n",
    "                        dataIdx=np.random.randint(0,nData)\n",
    "                        mbStates[i,:]=h[0][dataIdx,:]\n",
    "                        mbActions[i,:]=h[1][dataIdx,:]\n",
    "                        mbAdvantages[i]=h[2][dataIdx]\n",
    "                    advantageMean=np.mean(mbAdvantages)\n",
    "                    mbStates=(mbStates-stateOffset)*stateScale  #here, we must scale per batch because using the history\n",
    "                    temp,currLoss=sess.run([self.optimizePolicySigma,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages})\n",
    "                    if verbose and (batchIdx % 100 == 0):\n",
    "                        print(\"Adapting policy variance, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx,nVarAdaptBatches,advantageMean,currLoss))\n",
    "                #temp,currLoss=sess.run([self.optimizePolicyMean,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages})\n",
    "                else:\n",
    "                    nData=actions.shape[0]\n",
    "                    for i in range(nMinibatch):\n",
    "                        dataIdx=np.random.randint(0,nData)\n",
    "                        mbStates[i,:]=scaledStates[dataIdx,:]  \n",
    "                        mbActions[i,:]=actions[dataIdx,:]\n",
    "                        if self.stateDim>0:\n",
    "                            mbOldMean[i,:]=oldMeans[dataIdx,:]\n",
    "                        mbAdvantages[i]=advantages[dataIdx]\n",
    "                    advantageMean=np.mean(mbAdvantages)\n",
    "                    temp,currLoss=sess.run([self.optimizePolicyMean,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages,self.logPiOldIn:mbLogPiOld, self.oldPolicyMean:mbOldMean})\n",
    "                    if verbose and (batchIdx % 100 == 0):\n",
    "                        print(\"Adapting policy mean, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx-nVarAdaptBatches,nBatches,advantageMean,currLoss))\n",
    "\n",
    "        else:\n",
    "            for batchIdx in range(nBatches + nVarAdaptBatches if self.separateVarAdapt else nBatches):\n",
    "                for i in range(nMinibatch):\n",
    "                    dataIdx=np.random.randint(0,nData)\n",
    "                    if self.stateDim!=0:\n",
    "                        mbStates[i,:]=scaledStates[dataIdx,:]\n",
    "                    mbActions[i,:]=actions[dataIdx,:]\n",
    "                    mbAdvantages[i]=advantages[dataIdx]\n",
    "                    mbLogPiOld[i]=logPiOld[dataIdx]\n",
    "                advantageMean=np.mean(mbAdvantages)\n",
    "                temp,currLoss=sess.run([self.optimizePolicy,self.policyLoss],feed_dict={self.stateIn:mbStates,self.actionIn:mbActions,self.advantagesIn:mbAdvantages,self.logPiOldIn:mbLogPiOld})\n",
    "                if verbose and (batchIdx % 100 == 0):\n",
    "                    print(\"Training policy, batch {}/{}, mean advantage {:.2f}, loss {}\".format(batchIdx,nBatches,advantageMean,currLoss))\n",
    "    def setGlobalStdev(self,relStdev:float, sess:tf.Session):\n",
    "        assert(self.globalVariance and (not self.trainableGlobalVariance))\n",
    "        stdev=relStdev*(self.actionMaxLimit-self.actionMinLimit)\n",
    "        var=np.square(stdev)\n",
    "        logVar=np.log(var)\n",
    "        self.globalLogVarVariable.load(logVar,sess)\n",
    "        \n",
    "    def sample(self,sess:tf.Session,observations:np.array,enforcedRelSigma:float=None):\n",
    "        obs=observations\n",
    "        nObs=obs.shape[0]\n",
    "        result=np.zeros([nObs,self.actionDim])\n",
    "        assert(self.initialized)\n",
    "        policyMean,policySigma=sess.run([self.policyMean,self.policySigma],feed_dict={self.stateIn:obs})\n",
    "        if np.any(np.isnan(policyMean)):\n",
    "            raise Exception(\"Policy mean is NaN\")\n",
    "        if np.any(np.isnan(policySigma)):\n",
    "            raise Exception(\"Policy sigma is NaN\")\n",
    "        #if np.any(policySigma<minSigma):\n",
    "        #    raise Exception(\"Policy sigma violates limits\")\n",
    "\n",
    "        for i in range(nObs):\n",
    "            if self.stateDim==0:\n",
    "                result[i,:]=np.random.normal(policyMean,policySigma,[self.actionDim])\n",
    "            else:\n",
    "                if self.globalVariance:\n",
    "                    result[i,:]=np.random.normal(policyMean[i,:],policySigma,[self.actionDim])\n",
    "                else:\n",
    "                    result[i,:]=np.random.normal(policyMean[i,:],policySigma[i,:],[self.actionDim])\n",
    "\n",
    "        #bookkeeping for logging\n",
    "        self.usedSigmaSum+=np.mean(policySigma)\n",
    "        self.usedSigmaSumCounter+=1\n",
    "        return result\n",
    "    def getExpectation(self,sess:tf.Session,observations:np.array):\n",
    "        return sess.run(self.policyMean,feed_dict={self.stateIn:observations})\n",
    "    def getSd(self,sess:tf.Session,observations:np.array):\n",
    "        return sess.run(self.policySigma,feed_dict={self.stateIn:observations})\n",
    "    #def get2dEllipse(self,observations:np.array):\n",
    "    #    def logProb(self,state:np.array,action:np.array):\n",
    "    #    def adapt(self,states:np.array,actions:np.array,advantages:np.array,batchSize:int,nEpochs:int):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "useGradientClipping=False\n",
    "maxGradientNorm=1\n",
    "\n",
    "#A function that behaves like abs, but has zero derivative at the origin, which should improve final convergence if this is \n",
    "#used in computing a loss \n",
    "def softAbs(x:tf.Tensor):\n",
    "    x=tf.abs(x)\n",
    "    return tf.where(x > 0.5, x-0.25, x*x)\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self,stateDim:int,nHidden:int,networkUnits:int,networkActivation,useSkips=False,learningRate:float=1e-3,nHistory:int=1,lossType=\"L2\"):\n",
    "        stateIn=tf.placeholder(dtype=tf.float32,shape=[None,stateDim])\n",
    "        valueIn=tf.placeholder(dtype=tf.float32,shape=[None,1])             #training targets for value network\n",
    "        critic,criticInit=mlp(stateIn,nHidden,networkUnits,1,networkActivation,firstLinearLayerUnits=0,useSkips=useSkips)  #need a handle for the DenseNet instance for network switching\n",
    "        diff=valueIn-critic\n",
    "        if lossType==\"L2\":\n",
    "            loss=tf.reduce_mean(tf.square(diff))    \n",
    "        elif lossType==\"L1\":\n",
    "            loss=tf.reduce_mean(tf.abs(diff))       #L1 loss, can be more stable\n",
    "        elif lossType==\"SoftL1\":\n",
    "            loss=tf.reduce_mean(softAbs(diff))       #L1 loss with zero gradient at optimum\n",
    "        else:\n",
    "            raise Exception(\"Loss type not recognized!\")\n",
    "        def optimize(loss):\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=learningRate)\n",
    "            if not useGradientClipping:\n",
    "                return optimizer.minimize(loss)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, maxGradientNorm)\n",
    "            return optimizer.apply_gradients(zip(gradients, variables))\n",
    "        optimizeCritic=optimize(loss)\n",
    "        #remember some of the tensors for later\n",
    "        self.loss=loss\n",
    "        self.nHistory=nHistory\n",
    "        self.history=deque()\n",
    "        self.criticInit=criticInit\n",
    "        self.stateIn=stateIn\n",
    "        self.valueIn=valueIn\n",
    "        self.initialized=False\n",
    "        self.stateDim=stateDim\n",
    "        self.critic=critic\n",
    "        self.optimize=optimizeCritic\n",
    "\n",
    "    def train(self,sess,states:np.array,values:np.array,nMinibatch:int,nEpochs:int,nBatches:int=0,verbose=True):\n",
    "        assert(np.all(np.isfinite(states)))\n",
    "        assert(np.all(np.isfinite(values)))\n",
    "        nData=states.shape[0]\n",
    "\n",
    "        #manage history\n",
    "        self.history.append([states.copy(),values.copy()])\n",
    "        if len(self.history)>self.nHistory:\n",
    "            self.history.popleft()\n",
    "\n",
    "        #train\n",
    "        nMinibatch=min([nData,nMinibatch])\n",
    "        if nBatches==0:\n",
    "            nBatches=max([1,int(nData*nEpochs/nMinibatch)])\n",
    "        mbState=np.zeros([nMinibatch,self.stateDim])\n",
    "        mbValue=np.zeros([nMinibatch,1])\n",
    "        for batchIdx in range(nBatches):\n",
    "            historyLen=len(self.history)\n",
    "            for i in range(nMinibatch):\n",
    "                histIdx=np.random.randint(0,historyLen)\n",
    "                h=self.history[histIdx]\n",
    "                nData=h[0].shape[0]\n",
    "                dataIdx=np.random.randint(0,nData)\n",
    "                mbState[i,:]=h[0][dataIdx,:]\n",
    "                mbValue[i]=h[1][dataIdx]\n",
    "            if batchIdx==0 and not self.initialized:\n",
    "                #init the MLP biases to prevent large values\n",
    "                temp,currLoss=sess.run([self.criticInit,self.loss],feed_dict={self.stateIn:mbState,self.valueIn:mbValue})\n",
    "                self.initialized=True\n",
    "            else:\n",
    "                temp,currLoss=sess.run([self.optimize,self.loss],feed_dict={self.stateIn:mbState,self.valueIn:mbValue})\n",
    "            if verbose and (batchIdx % 100 == 0):\n",
    "                print(\"Training critic, batch {}/{}, loss {}\".format(batchIdx,nBatches,currLoss))\n",
    "    def predict(self,sess,states):\n",
    "        return sess.run(self.critic,feed_dict={self.stateIn:states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Scaling Utilities\n",
    "Modified from code by Patrick Coady (pat-coady.github.io)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class Scaler(object):\n",
    "    \"\"\" Generate scale and offset based on running mean and stddev along axis=0\n",
    "        offset = running mean\n",
    "        scale = 1 / (2.0*stddev + epsilon) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, epsilon=0.001, clip=1e10, useOffset=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: dimension of axis=1\n",
    "        \"\"\"\n",
    "        self.xMean=np.zeros(obs_dim)\n",
    "        self.xSqMean=np.zeros(obs_dim)\n",
    "        self.scale=np.ones(obs_dim)\n",
    "        self.offset=np.zeros(obs_dim)\n",
    "        self.nUpdates=0\n",
    "        self.epsilon=epsilon\n",
    "        self.clip=clip\n",
    "        self.useOffset=useOffset\n",
    "\n",
    "    def update(self, x):\n",
    "        self.nUpdates+=1\n",
    "        newWeight=1/self.nUpdates\n",
    "        self.xSqMean=(1-newWeight)*self.xSqMean+newWeight*np.mean(np.square(x),axis=0) \n",
    "        self.xMean=(1-newWeight)*self.xMean+newWeight*np.mean(x,axis=0) \n",
    "        if self.useOffset:\n",
    "            mean=self.xMean\n",
    "        else:\n",
    "            mean=0\n",
    "        #var(x)=E((x-mean(x))^2)=E(x^2-2*x*E(x)+mean(x)^2)=E(x^2)-2*E(x)*E(x)+E(x)^2=E(x^2)-E(x)^2\n",
    "        var=self.xSqMean-np.square(mean)\n",
    "        var=np.maximum(0.0,var)\n",
    "        self.scale=np.minimum(self.scale,1.0/(2.0*np.sqrt(var)+self.epsilon))  \n",
    "        self.offset=mean\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\" returns 2-tuple: (scale, offset) \"\"\"\n",
    "        return self.scale,self.offset\n",
    "\n",
    "    def process(self,x:np.array):\n",
    "        return np.clip((x-self.offset)*self.scale,-self.clip,self.clip)\n",
    "    def unscale(self,x:np.array):\n",
    "        return x/self.scale+self.offset\n",
    "    \n",
    "\n",
    "    \n",
    "class MinMaxScaler(object):\n",
    "    \"\"\" Generate scale and offset based on low-pass filtered max and min vals\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, filter=0.9, epsilon=0.001, useOffset=False, scalarMode=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: dimension of axis=1\n",
    "        \"\"\"\n",
    "        self.minVals=-np.ones(obs_dim)\n",
    "        self.maxVals=np.zeros(obs_dim)\n",
    "        self.filteredMinVals=self.minVals.copy()\n",
    "        self.filteredMaxVals=self.maxVals.copy()\n",
    "        self.first_pass = True\n",
    "        self.filter=filter\n",
    "        self.epsilon=epsilon\n",
    "        self.scalarMode=scalarMode\n",
    "        self.useOffset=useOffset\n",
    "        self.scale=1 if scalarMode else np.ones(obs_dim)\n",
    "        self.offset=0 if scalarMode else np.zeros(obs_dim)\n",
    "\n",
    "    def update(self, x):\n",
    "        if self.first_pass:\n",
    "            self.minVals=np.min(x,axis=0)\n",
    "            self.maxVals=np.max(x,axis=0)\n",
    "            self.filteredMinVals=self.minVals.copy()\n",
    "            self.filteredMaxVals=self.maxVals.copy()\n",
    "        else:\n",
    "            self.minVals=np.minimum([self.minVals,np.min(x,axis=0)])\n",
    "            self.maxVals=np.maximum([self.maxVals,np.max(x,axis=0)])\n",
    "            self.filteredMinVals=self.filter*self.filteredMinVals+(1-self.filter)*self.minVals\n",
    "            self.filteredMaxVals=self.filter*self.filteredMaxVals+(1-self.filter)*self.maxVals\n",
    "        if self.scalarMode:\n",
    "            self.scale=np.minimum(self.scale,2.0/np.max((self.filteredMaxVals-self.filteredMinVals)+self.epsilon))\n",
    "        else:\n",
    "            self.scale=2.0/((self.filteredMaxVals-self.filteredMinVals)+self.epsilon)\n",
    "            if self.useOffset:\n",
    "                self.offset=0.5*(self.filteredMaxVals+self.filteredMinVals)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\" returns 2-tuple: (scale, offset) \"\"\"\n",
    "        return self.scale,self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Known issues (TODO): \n",
    "- policy.train() takes as input the scaler scale and offset, and uses them internally. However, the scaler also has a clip parameter, which is\n",
    "  by default set to a large value, but if one wants to use it, scaler.process() will then produce different results than the scaling in policy.train()\n",
    "  - fix: policy should take the Scaler instance as an argument, although this creates a dependency\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, these simple graphs run faster on CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Data structure for holding experience\n",
    "class Experience:\n",
    "    def __init__(self,s:np.array,a:np.array,r:float,s_next:np.array,terminated:bool,timeStep=0):\n",
    "        self.s=s.copy()\n",
    "        self.a=a\n",
    "        self.r=r\n",
    "        self.s_next=s_next.copy()\n",
    "        self.terminated=terminated\n",
    "        self.timeStep=timeStep     \n",
    "        self.V=r            #value, will be updated by the agent or the client. note: this is the on-policy value, i.e., averaged over children\n",
    "        self.Vselect=r      #value used for the UCB\n",
    "        self.advantage=0    #will be updated by the agent\n",
    "        self.nonDiscountedRewardSum=0   #used in some applications\n",
    "        self.fullState=None #used in tree search\n",
    "        #the following only used when building trajectory trees\n",
    "        self.parent=None\n",
    "        self.children=[]\n",
    "        self.depth=1\n",
    "        #visitation count used by UCB tree search\n",
    "        self.n=1\n",
    "\n",
    "    #Updates value upwards in the tree, called after adding a new trajectory to the tree.\n",
    "    #Also keeps track of tree depth\n",
    "    def propagateUpwards(self,gamma:float,bestGamma:float=1):\n",
    "        node=self\n",
    "        node.V=node.r\n",
    "        node.Vselect=node.r\n",
    "        while node.parent is not None:\n",
    "            node=node.parent\n",
    "            node.V=node.r\n",
    "            node.Vselect=-np.inf\n",
    "            nChildren=len(node.children)\n",
    "            node.depth=0\n",
    "            for child in node.children:\n",
    "                node.V+=gamma*child.V/nChildren\n",
    "                node.Vselect=max([node.Vselect,node.r+bestGamma*child.Vselect])\n",
    "                node.depth=max([node.depth,child.depth+1])\n",
    "            #node.Vselect=node.V\n",
    "\n",
    "    #adds a child to this node, updating tree linkage\n",
    "    def addChild(self,child):\n",
    "        self.children.append(child)\n",
    "        child.parent=self\n",
    "\n",
    "    #selects a child node at specific depth, using the UCB formula to visit the children\n",
    "    def selectChildAtDepth(self,depth,C_ucb):\n",
    "        node=self\n",
    "        self.n+=1       #keep track of visitation count\n",
    "        while depth>0:\n",
    "            nChildren=len(node.children)\n",
    "            if nChildren==1:\n",
    "                #if only one child, just move forward\n",
    "                node=node.children[0]\n",
    "            else:\n",
    "                #if multiple children, select the best-scoring one based on the UCB-1 formula\n",
    "                maxScore=-np.inf\n",
    "                maxScoringChild=None\n",
    "                for child in node.children:\n",
    "                    if child.depth>=depth:  #we only accept branches that will take us up to the desired depth\n",
    "                        score=node.Vselect+C_ucb*math.sqrt(math.log(node.n)/child.n)\n",
    "                        if score>maxScore:\n",
    "                            maxScoringChild=child\n",
    "                            maxScore=score\n",
    "                node=maxScoringChild\n",
    "            depth-=1        #keep track of depth\n",
    "            node.n+=1       #keep track of visitation count\n",
    "        return node\n",
    "\n",
    "class Agent:\n",
    "    #In most cases, one only needs to specify stateDim, actionDim, actionMin, and actionMax.\n",
    "    #The mode parameter defines the algorithm. PPO-CMA-m is the default, i.e., PPO-CMA using the sample mirroring trick.\n",
    "    #Other choices are \"PPO-CMA\", \"PPO\", \"PG\" and \"PG-pos\". The two last modes denote vanilla policy gradient, and the \"-pos\"\n",
    "    #means that only positive advantage actions are used. See DidacticExample.py for visualization of different modes in a simple quadratic problem.\n",
    "    def __init__(self, stateDim:int, actionDim:int, actionMin:np.array, actionMax:np.array, learningRate=0.0005\n",
    "                 , gamma=0.99, GAElambda=0.95, PPOepsilon=0.2, PPOentropyLossWeight=0, nHidden:int=2\n",
    "                 , nUnitsPerLayer:int=128, mode=\"PPO-CMA-m\", activation=\"lrelu\", H:int=9, entropyLossWeight:float=0\n",
    "                 , sdLowLimit=0.01, useScaler:bool=True, criticTimestepScale=0.001,initialMean:np.array=None,initialSd:np.array=None):\n",
    "        #Create policy network \n",
    "        print(\"Creating policy\")\n",
    "        self.actionMin=actionMin\n",
    "        self.actionMax=actionMax\n",
    "        self.actionDim=actionDim\n",
    "        self.stateDim=stateDim\n",
    "        self.useScaler=useScaler\n",
    "        if useScaler:\n",
    "            self.scaler=Scaler(stateDim)\n",
    "        self.scalerInitialized=False\n",
    "        self.normalizeAdvantages=True\n",
    "        self.gamma=gamma\n",
    "        self.GAElambda=GAElambda\n",
    "        self.criticTimestepScale=0 if gamma==0 else criticTimestepScale     #with gamma==0, no need for this\n",
    "        piEpsilon = None\n",
    "        nHistory = 1\n",
    "        negativeAdvantageAvoidanceSigma = 0\n",
    "        if mode==\"PPO-CMA\" or mode==\"PPO-CMA-m\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt=True\n",
    "            self.reluAdvantages=True if mode==\"PPO-CMA\" else False\n",
    "            nHistory=H             #policy mean adapts immediately, policy covariance as an aggreagate of this many past iterations\n",
    "            useSigmaSoftClip=True\n",
    "            negativeAdvantageAvoidanceSigma=1 if mode==\"PPO-CMA-m\" else 0\n",
    "        elif mode==\"PPO\":\n",
    "            usePPOLoss=True           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=False\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        elif mode==\"PG\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=False\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        elif mode==\"PG-pos\":\n",
    "            usePPOLoss=False           #if True, we use PPO's clipped surrogate loss function instead of the standard -A_i * log(pi(a_i | s_i))\n",
    "            separateVarAdapt = False\n",
    "            # separateSigmaAdapt=False\n",
    "            self.reluAdvantages=True\n",
    "            useSigmaSoftClip=True\n",
    "            piEpsilon=0\n",
    "        else:\n",
    "            raise(\"Unknown mode {}\".format(mode))\n",
    "        self.policy=Policy(stateDim, actionDim, actionMin, actionMax, entropyLossWeight=PPOentropyLossWeight\n",
    "                           , networkActivation=activation, networkDepth=nHidden, networkUnits=nUnitsPerLayer\n",
    "                           , networkSkips=False, learningRate=learningRate, minSigma=sdLowLimit, PPOepsilon=PPOepsilon\n",
    "                           , usePPOLoss=usePPOLoss, separateVarAdapt=separateVarAdapt, nHistory=nHistory\n",
    "                           , useSigmaSoftClip=useSigmaSoftClip, piEpsilon=piEpsilon\n",
    "                           , negativeAdvantageAvoidanceSigma=negativeAdvantageAvoidanceSigma)\n",
    "\n",
    "        #Create critic network, +1 stateDim because at least in OpenAI gym, episodes are time-limited and the value estimates thus depend on simulation time.\n",
    "        #Thus, we use time step as an additional feature for the critic.\n",
    "        #Note that this does not mess up generalization, as the feature is not used for the policy during training or at runtime\n",
    "        print(\"Creating critic network\")\n",
    "        self.critic=Critic(stateDim=stateDim+1,learningRate=learningRate,nHidden=nHidden,networkUnits=nUnitsPerLayer,networkActivation=activation,useSkips=False,lossType=\"L1\")\n",
    "\n",
    "        #Experience trajectory buffers for the memorize() and updateWithMemorized() methods\n",
    "        self.experienceTrajectories=[]\n",
    "        self.currentTrajectory=[]\n",
    "\n",
    "        #Init may take as argument a desired initial action mean and sd. These need to be remembered for the first iteration's act,\n",
    "        #which samples the initial mean and sd directly instead of utilizing the policy network.\n",
    "        if initialMean is not None:\n",
    "            self.initialMean=initialMean.copy()\n",
    "        else:\n",
    "            self.initialMean=0.5*(self.actionMin+self.actionMax)*np.ones(self.actionDim)\n",
    "        if initialSd is not None:\n",
    "            self.initialSd=initialSd.copy()\n",
    "        else:\n",
    "            self.initialSd=0.5*(self.actionMax-self.actionMin)*np.ones(self.actionDim)\n",
    "\n",
    "    #call this after tensorflow's global variables initializer\n",
    "    def init(self,sess:tf.Session,verbose=False):\n",
    "        #Pretrain the policy to output the initial Gaussian for all states\n",
    "        self.policy.init(sess,0,1,self.initialMean,self.initialSd,256,2000,verbose)\n",
    "    \n",
    "    #stateObs is an n-by-m tensor, where n = number of observations, m = number of observation variables\n",
    "    def act(self,sess:tf.Session,stateObs:np.array,deterministic=False,clipActionToLimits=True):\n",
    "        #Expand a single 1d-observation into a batch of 1 vectors\n",
    "        if len(stateObs.shape)==1:\n",
    "            stateObs=np.reshape(stateObs,[1,stateObs.shape[0]])\n",
    "        #Query the policy for the action, except for the first iteration where we sample directly from the initial exploration Gaussian\n",
    "        #that covers the whole action space.\n",
    "        #This is done because we don't know the scale of state observations a priori; thus, we can only init the state scaler in update(), \n",
    "        #after we have collected some experience.\n",
    "        if self.useScaler and (not self.scalerInitialized):\n",
    "            actions=np.random.normal(self.initialMean,self.initialSd,size=[stateObs.shape[0],self.actionDim])\n",
    "            if clipActionToLimits:\n",
    "                actions=np.clip(actions,np.reshape(self.actionMin,[1,self.actionDim]),np.reshape(self.actionMax,[1,self.actionDim]))\n",
    "            return actions\n",
    "        else:\n",
    "            if self.useScaler:\n",
    "                scaledObs=self.scaler.process(stateObs)\n",
    "            else:\n",
    "                scaledObs=stateObs\n",
    "            if deterministic:\n",
    "                actions=self.policy.getExpectation(sess,scaledObs)\n",
    "            else:\n",
    "                actions=self.policy.sample(sess,scaledObs)\n",
    "            if clipActionToLimits:\n",
    "                actions=np.clip(actions,self.actionMin,self.actionMax)\n",
    "            return actions\n",
    "    def memorize(self,observation:np.array,action:np.array,reward:float,nextObservation:np.array,done:bool):\n",
    "        e = Experience(observation, action, reward, nextObservation, done)\n",
    "        self.currentTrajectory.append(e)\n",
    "        if done:\n",
    "            self.experienceTrajectories.append(self.currentTrajectory)\n",
    "            self.currentTrajectory=[]\n",
    "\n",
    "    def getAverageActionStdev(self):\n",
    "        if self.useScaler and (not self.scalerInitialized):\n",
    "            return np.mean(0.5*(self.actionMax-self.actionMin))\n",
    "        else:\n",
    "            return self.policy.usedSigmaSum/(1e-20+self.policy.usedSigmaSumCounter)\n",
    "\n",
    "    #If you call memorize() after each action, you can update the agent with this method. \n",
    "    #If you handle the experience buffers yourself, e.g., due to a multithreaded implementation, use the update() method instead.\n",
    "    def updateWithMemorized(self,sess:tf.Session,batchSize:int=512,nBatches:int=100,verbose=True,valuesValid=False,timestepsValid=False):\n",
    "        self.update(sess,experienceTrajectories=self.experienceTrajectories,batchSize=batchSize,nBatches=nBatches,verbose=verbose,valuesValid=valuesValid,timestepsValid=timestepsValid)\n",
    "        averageEpisodeReturn=0\n",
    "        for t in self.experienceTrajectories:\n",
    "            episodeReturn=0\n",
    "            for e in t:\n",
    "                episodeReturn+=e.r\n",
    "            averageEpisodeReturn+=episodeReturn\n",
    "        averageEpisodeReturn/=len(self.experienceTrajectories)\n",
    "        self.experienceTrajectories=[]\n",
    "        self.currentTrajectory=[]\n",
    "        return averageEpisodeReturn\n",
    "\n",
    "    #experienceTrajectories is a list of lists of Experience instances such that each of the contained lists corresponds to an episode simulation trajectory\n",
    "    def update(self,sess:tf.Session,experienceTrajectories,batchSize:int=512,nBatches:int=100,verbose=True,valuesValid=False,timestepsValid=False):\n",
    "        trajectories=experienceTrajectories   #shorthand\n",
    "\n",
    "        #Collect all data into linear arrays for training. \n",
    "        nTrajectories=len(trajectories)\n",
    "        nData=0\n",
    "        for trajectory in trajectories:\n",
    "            nData+=len(trajectory)\n",
    "            #propagate values backwards along trajectory if not already done\n",
    "            if not valuesValid:\n",
    "                for i in reversed(range(len(trajectory)-1)):\n",
    "                    #value estimates, used for training the critic and estimating advantages\n",
    "                    trajectory[i].V=trajectory[i].r+self.gamma*trajectory[i+1].V\n",
    "            #update time steps if not updated\n",
    "            if not timestepsValid:\n",
    "                for i in range(len(trajectory)):\n",
    "                    trajectory[i].timeStep=i\n",
    "        allStates=np.zeros([nData,self.stateDim])\n",
    "        allActions=np.zeros([nData,self.actionDim])\n",
    "        allValues=np.zeros([nData])\n",
    "        allTimes=np.zeros([nData,1])\n",
    "        k=0\n",
    "        for trajectory in trajectories:\n",
    "            for e in trajectory:\n",
    "                allStates[k,:]=e.s\n",
    "                allValues[k]=e.V  \n",
    "                allActions[k,:]=e.a\n",
    "                allTimes[k,0]=e.timeStep*self.criticTimestepScale \n",
    "                k+=1\n",
    "\n",
    "\n",
    "        #Update scalers\n",
    "        if self.useScaler:\n",
    "            self.scaler.update(allStates)\n",
    "            scale, offset = self.scaler.get()\n",
    "            self.scalerInitialized=True\n",
    "        else:\n",
    "            offset=0\n",
    "            scale=1\n",
    " \n",
    "        #Scale the observations for training the critic\n",
    "        scaledStates=self.scaler.process(allStates)\n",
    "\n",
    "        #Train critic\n",
    "        def augmentCriticObs(obs:np.array,timeSteps:np.array):\n",
    "            return np.concatenate([obs,timeSteps],axis=1)\n",
    "        self.critic.train(sess,augmentCriticObs(scaledStates,allTimes),allValues,batchSize,nEpochs=0,nBatches=nBatches,verbose=verbose)\n",
    "\n",
    "        #Policy training needs advantages, which depend on the critic we just trained.\n",
    "        #We use Generalized Advantage Estimation by Schulman et al.\n",
    "        if verbose:\n",
    "            print(\"Estimating advantages...\".format(len(trajectories)))\n",
    "        for t in trajectories:\n",
    "            #query the critic values of all states of this trajectory in one big batch\n",
    "            nSteps=len(t)\n",
    "            states=np.zeros([nSteps+1,self.stateDim])\n",
    "            timeSteps=np.zeros([nSteps+1,1])\n",
    "            for i in range(nSteps):\n",
    "                states[i,:]=t[i].s\n",
    "                timeSteps[i,0]=t[i].timeStep*self.criticTimestepScale\n",
    "            states[nSteps,:]=t[nSteps-1].s_next\n",
    "            states=(states-offset)*scale\n",
    "            values=self.critic.predict(sess,augmentCriticObs(states,timeSteps))\n",
    "\n",
    "            #GAE loop, i.e., take the instantaneous advantage (how much value a single action brings, assuming that the\n",
    "            #values given by the critic are unbiased), and smooth those along the trajectory using 1st-order IIR filter.\n",
    "            advantage=0\n",
    "            for step in reversed(range(nSteps)):\n",
    "                delta_t=t[step].r+self.gamma*values[step+1] - values[step]\n",
    "                advantage=delta_t+self.GAElambda*self.gamma*advantage\n",
    "                t[step].advantage=advantage\n",
    "\n",
    "        #Gather the advantages to linear array and apply ReLU and normalization if needed\n",
    "        allAdvantages=np.zeros([nData])\n",
    "        k=0\n",
    "        for trajectory in trajectories:\n",
    "            for e in trajectory:\n",
    "                allAdvantages[k]=e.advantage  \n",
    "                k+=1\n",
    "\n",
    "        if self.reluAdvantages:\n",
    "            allAdvantages=np.clip(allAdvantages,0,np.inf)\n",
    "        if self.normalizeAdvantages:\n",
    "            aMean=np.mean(allAdvantages)\n",
    "            aSd=np.std(allAdvantages)\n",
    "            if verbose:\n",
    "                print(\"Advantage mean {}, sd{}\".format(aMean,aSd))\n",
    "            allAdvantages/=1e-10+aSd\n",
    "            #Clamp the normalized advantages to 3 sd:s, in case of outliers. \n",
    "            #Commented out for now to allow computing additional ICML results, as this was not yet implemented in the ICML version\n",
    "            #advantageLimit=3\n",
    "            #allAdvantages=np.clip(allAdvantages,-advantageLimit,advantageLimit)\n",
    "\n",
    "        #Train policy. Note that this uses original unscaled states, because the PPO-CMA variance training needs a history of\n",
    "        #states in the same scale\n",
    "        self.policy.train(sess,allStates,allActions,allAdvantages,batchSize,nEpochs=0,nBatches=nBatches,stateOffset=offset,stateScale=scale,verbose=verbose)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simulation environment\n",
      "Creating policy\n",
      "Creating critic network\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "#Simulation budget (steps) per iteration. This is the main parameter to tune.\n",
    "#8k works for relatively simple environments like the OpenAI Gym Roboschool 2D Hopper.\n",
    "#For more complex problems such as 3D humanoid locomotion, try 32k or even 64k.\n",
    "#Larger values are slower but more robust.\n",
    "N=2000\n",
    "\n",
    "# Stop training after this many steps\n",
    "max_steps=1000000\n",
    "\n",
    "# Init tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create environment (replace this with your own simulator)\n",
    "print(\"Creating simulation environment\")\n",
    "sim = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Create the agent using the default parameters for the neural network architecture\n",
    "agent=Agent(\n",
    "    stateDim=sim.observation_space.low.shape[0]\n",
    "    , actionDim=1\n",
    "    , actionMin=0\n",
    "    , actionMax=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation steps 2008, average episode return 22.065934065934066\n",
      "Simulation steps 4026, average episode return 23.741176470588236\n",
      "Simulation steps 6046, average episode return 33.666666666666664\n",
      "Simulation steps 8103, average episode return 57.138888888888886\n",
      "Simulation steps 10138, average episode return 53.55263157894737\n",
      "Simulation steps 12177, average episode return 63.71875\n",
      "Simulation steps 14206, average episode return 81.16\n",
      "Simulation steps 16242, average episode return 113.11111111111111\n",
      "Simulation steps 18255, average episode return 118.41176470588235\n",
      "Simulation steps 20290, average episode return 101.75\n",
      "Simulation steps 22377, average episode return 122.76470588235294\n",
      "Simulation steps 24465, average episode return 130.5\n",
      "Simulation steps 26479, average episode return 134.26666666666668\n",
      "Simulation steps 28569, average episode return 122.94117647058823\n",
      "Simulation steps 30644, average episode return 115.27777777777777\n",
      "Simulation steps 32728, average episode return 148.85714285714286\n",
      "Simulation steps 34904, average episode return 155.42857142857142\n",
      "Simulation steps 37071, average episode return 166.69230769230768\n",
      "Simulation steps 39105, average episode return 169.5\n",
      "Simulation steps 41230, average episode return 163.46153846153845\n",
      "Simulation steps 43379, average episode return 153.5\n",
      "Simulation steps 45453, average episode return 159.53846153846155\n",
      "Simulation steps 47503, average episode return 186.36363636363637\n",
      "Simulation steps 49553, average episode return 186.36363636363637\n",
      "Simulation steps 51561, average episode return 154.46153846153845\n",
      "Simulation steps 53644, average episode return 173.58333333333334\n",
      "Simulation steps 55676, average episode return 169.33333333333334\n",
      "Simulation steps 57770, average episode return 174.5\n",
      "Simulation steps 59855, average episode return 189.54545454545453\n",
      "Simulation steps 61988, average episode return 164.07692307692307\n",
      "Simulation steps 64072, average episode return 109.6842105263158\n",
      "Simulation steps 66129, average episode return 158.23076923076923\n",
      "Simulation steps 68152, average episode return 183.9090909090909\n",
      "Simulation steps 70152, average episode return 200.0\n",
      "Simulation steps 72263, average episode return 175.91666666666666\n",
      "Simulation steps 74431, average episode return 180.66666666666666\n",
      "Simulation steps 76554, average episode return 193.0\n",
      "Simulation steps 78737, average episode return 198.45454545454547\n",
      "Simulation steps 80804, average episode return 187.9090909090909\n",
      "Simulation steps 82921, average episode return 192.45454545454547\n",
      "Simulation steps 84973, average episode return 186.54545454545453\n",
      "Simulation steps 87137, average episode return 166.46153846153845\n",
      "Simulation steps 89160, average episode return 183.9090909090909\n",
      "Simulation steps 91279, average episode return 192.63636363636363\n",
      "Simulation steps 93392, average episode return 192.0909090909091\n",
      "Simulation steps 95556, average episode return 180.33333333333334\n",
      "Simulation steps 97648, average episode return 174.33333333333334\n",
      "Simulation steps 99648, average episode return 200.0\n",
      "Simulation steps 101741, average episode return 190.27272727272728\n",
      "Simulation steps 103741, average episode return 200.0\n",
      "Simulation steps 105741, average episode return 200.0\n",
      "Simulation steps 107741, average episode return 200.0\n",
      "Simulation steps 109741, average episode return 200.0\n",
      "Simulation steps 111741, average episode return 200.0\n",
      "Simulation steps 113741, average episode return 200.0\n",
      "Simulation steps 115741, average episode return 200.0\n",
      "Simulation steps 117741, average episode return 200.0\n",
      "Simulation steps 119741, average episode return 200.0\n",
      "Simulation steps 121897, average episode return 196.0\n",
      "Simulation steps 123897, average episode return 200.0\n",
      "Simulation steps 125897, average episode return 200.0\n",
      "Simulation steps 127897, average episode return 200.0\n",
      "Simulation steps 129897, average episode return 200.0\n",
      "Simulation steps 131897, average episode return 200.0\n",
      "Simulation steps 133897, average episode return 200.0\n",
      "Simulation steps 135897, average episode return 200.0\n",
      "Simulation steps 137897, average episode return 200.0\n",
      "Simulation steps 139897, average episode return 200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-b721da8bc513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Query the agent for action given the state observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-8a2ab543411f>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, sess, stateObs, deterministic, clipActionToLimits)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetExpectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaledObs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaledObs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclipActionToLimits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionMin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionMax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-220678415c49>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sess, observations, enforcedRelSigma)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnObs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionDim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mpolicyMean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicySigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicyMean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicySigma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicyMean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Policy mean is NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finalize initialization\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "agent.init(sess)  # must be called after TensorFlow global variables init\n",
    "\n",
    "# Main training loop\n",
    "totalSimSteps = 0\n",
    "while totalSimSteps < max_steps:\n",
    "\n",
    "    #Run episodes until the iteration simulation budget runs out\n",
    "    iterSimSteps = 0\n",
    "    while iterSimSteps < N:\n",
    "\n",
    "        # Reset the simulation\n",
    "        observation = sim.reset()\n",
    "\n",
    "        # Simulate this episode until done (e.g., due to time limit or failure)\n",
    "        done=False\n",
    "        while not done:\n",
    "            # Query the agent for action given the state observation\n",
    "            action = agent.act(sess,observation)\n",
    "            action = 1 if action > 0.5 else 0\n",
    "\n",
    "            # Simulate using the action\n",
    "            # Note: this tutorial does not repeat the same action for multiple steps,\n",
    "            # unlike the Run.py script used for the paper results.\n",
    "            # Repeating the action for multiple steps seems to yield better exploration\n",
    "            # in most cases, possibly because it reduces high-frequency action noise.\n",
    "            nextObservation, reward, done, info = sim.step(action)\n",
    "\n",
    "            # Save the experience point\n",
    "            agent.memorize(observation,action,reward,nextObservation,done)\n",
    "            observation=nextObservation\n",
    "\n",
    "            # Bookkeeping\n",
    "            iterSimSteps += 1\n",
    "\n",
    "    #All episodes of this iteration done, update the agent and print results\n",
    "    averageEpisodeReturn=agent.updateWithMemorized(sess,verbose=False)\n",
    "    totalSimSteps += iterSimSteps\n",
    "    print(\"Simulation steps {}, average episode return {}\".format(totalSimSteps,averageEpisodeReturn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
