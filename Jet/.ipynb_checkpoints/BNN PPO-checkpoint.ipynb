{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO in Stable Baselines\n",
    "\n",
    "In single-agent PPO, `MlpPolicy` was used in `PPO1` as follows:\n",
    "\n",
    "```\n",
    "model = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "```\n",
    "\n",
    "`MlpPolicy` is found in `stable_baselines/common/policies.py`, inheriting `FeedForwardPolicy`, which inherits from `ActorCriticPolicy`.\n",
    "\n",
    "In `FeedForwardPolicy`'s `__init__`, there contains the following:\n",
    "```\n",
    "if net_arch is None:\n",
    "    if layers is None:\n",
    "        layers = [64, 64]\n",
    "    net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=reuse):\n",
    "    if feature_extraction == \"cnn\":\n",
    "        pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "    else:\n",
    "        pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "    self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "    self._proba_distribution, self._policy, self.q_value = \\\n",
    "        self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "```\n",
    "\n",
    "Since `MlpPolicy` uses `feature_extraction=\"mlp\"`, look into `mlp_extractor` (here)[https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/policies.py].\n",
    "\n",
    "`mlp_extractor` constructs a MLP that receive observations as input and outputs a latent representation for the policy and a value network. Amount and size of hidden layers and how many shared between policy and value network can be spcified using `net_arch`.\n",
    "\n",
    "In `mlp_extractor`, it iterates through `net_arch` and creates layers, specifically using `latent = act_fun(linear(latent, ...))`. Therfore, look into `act_fun` and `linear`, which belongs in stable_baselines.common.tf_layers.\n",
    "\n",
    "`FeedForwardPolicy`'s default for `act_fun` is `tf.tanh`. Linear contains:\n",
    "\n",
    "```\n",
    "def linear(input_tensor, scope, n_hidden, *, init_scale=1.0, init_bias=0.0):\n",
    "    \"\"\"\n",
    "    Creates a fully connected layer for TensorFlow\n",
    "    :param input_tensor: (TensorFlow Tensor) The input tensor for the fully connected layer\n",
    "    :param scope: (str) The TensorFlow variable scope\n",
    "    :param n_hidden: (int) The number of hidden neurons\n",
    "    :param init_scale: (int) The initialization scale\n",
    "    :param init_bias: (int) The initialization offset bias\n",
    "    :return: (TensorFlow Tensor) fully connected layer\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_input = input_tensor.get_shape()[1].value\n",
    "        weight = tf.get_variable(\"w\", [n_input, n_hidden], initializer=ortho_init(init_scale))\n",
    "        bias = tf.get_variable(\"b\", [n_hidden], initializer=tf.constant_initializer(init_bias))\n",
    "        return tf.matmul(input_tensor, weight) + bias\n",
    "```\n",
    "\n",
    "Therefore, to transform this model into a Bayesian neural network, the linear layer needs to be changed into DenseVariational instead of a linear layer. We can do this by modifying `FeedForwardPolicy` (which `MlpPolicy` inherits) with a new `bnn_extractor`, then creating a `BnnPolicy` to replace `MlpPolicy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseVariational(layer_size, posterior_mean_field, prior_trainable, kl_weight=1))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseVariational(pi_layer_size, posterior_mean_field, prior_trainable, kl_weight=1)(latent_policy))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseVariational(vf_layer_size, posterior_mean_field, prior_trainable, kl_weight=1)(latent_value))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy\n",
    "        (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor\n",
    "        documentation for details).\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, net_arch=None,\n",
    "                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            if feature_extraction == \"cnn\":\n",
    "                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "            elif feature_extraction == \"bnn\":\n",
    "                pi_latent, vf_latent = bnn_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "            else:\n",
    "                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import zip_longest\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.distributions import make_proba_dist_type, CategoricalProbabilityDistribution, \\\n",
    "    MultiCategoricalProbabilityDistribution, DiagGaussianProbabilityDistribution, BernoulliProbabilityDistribution\n",
    "from stable_baselines.common.input import observation_input\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a Bayesian neural net (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Agent PPO with BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_ppo1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[721]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Train single CPU PPO1 on slimevolley.\n",
    "# Should solve it (beat existing AI on average over 1000 trials) in 3 hours on single CPU, within 3M steps.\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "from slimevolleygym import SurvivalRewardEnv\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "NUM_TIMESTEPS = int(5e6)\n",
    "SEED = 721\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "LOGDIR = \"bnn_ppo1\" # moved to zoo afterwards.\n",
    "\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "model = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
