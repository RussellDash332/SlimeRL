{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO in Stable Baselines\n",
    "\n",
    "In single-agent PPO, `MlpPolicy` was used in `PPO1` as follows:\n",
    "\n",
    "```\n",
    "model = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "```\n",
    "\n",
    "`MlpPolicy` is found in `stable_baselines/common/policies.py`, inheriting `FeedForwardPolicy`, which inherits from `ActorCriticPolicy`.\n",
    "\n",
    "In `FeedForwardPolicy`'s `__init__`, there contains the following:\n",
    "```\n",
    "if net_arch is None:\n",
    "    if layers is None:\n",
    "        layers = [64, 64]\n",
    "    net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "with tf.variable_scope(\"model\", reuse=reuse):\n",
    "    if feature_extraction == \"cnn\":\n",
    "        pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "    else:\n",
    "        pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "    self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "    self._proba_distribution, self._policy, self.q_value = \\\n",
    "        self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "```\n",
    "\n",
    "Since `MlpPolicy` uses `feature_extraction=\"mlp\"`, look into `mlp_extractor` (here)[https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/policies.py].\n",
    "\n",
    "`mlp_extractor` constructs a MLP that receive observations as input and outputs a latent representation for the policy and a value network. Amount and size of hidden layers and how many shared between policy and value network can be spcified using `net_arch`.\n",
    "\n",
    "In `mlp_extractor`, it iterates through `net_arch` and creates layers, specifically using `latent = act_fun(linear(latent, ...))`. Therfore, look into `act_fun` and `linear`, which belongs in stable_baselines.common.tf_layers.\n",
    "\n",
    "`FeedForwardPolicy`'s default for `act_fun` is `tf.tanh`. Linear contains:\n",
    "\n",
    "```\n",
    "def linear(input_tensor, scope, n_hidden, *, init_scale=1.0, init_bias=0.0):\n",
    "    \"\"\"\n",
    "    Creates a fully connected layer for TensorFlow\n",
    "    :param input_tensor: (TensorFlow Tensor) The input tensor for the fully connected layer\n",
    "    :param scope: (str) The TensorFlow variable scope\n",
    "    :param n_hidden: (int) The number of hidden neurons\n",
    "    :param init_scale: (int) The initialization scale\n",
    "    :param init_bias: (int) The initialization offset bias\n",
    "    :return: (TensorFlow Tensor) fully connected layer\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_input = input_tensor.get_shape()[1].value\n",
    "        weight = tf.get_variable(\"w\", [n_input, n_hidden], initializer=ortho_init(init_scale))\n",
    "        bias = tf.get_variable(\"b\", [n_hidden], initializer=tf.constant_initializer(init_bias))\n",
    "        return tf.matmul(input_tensor, weight) + bias\n",
    "```\n",
    "\n",
    "Therefore, to transform this model into a Bayesian neural network, the linear layer needs to be changed into DenseVariational instead of a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
