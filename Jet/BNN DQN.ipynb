{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "from stable_baselines.common.policies import BasePolicy, nature_cnn, register_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseFlipout(layer_size, name=\"shared_fc{}\".format(idx), activation='relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseFlipout(pi_layer_size, name=\"pi_fc{}\".format(idx), activation='relu', kernel_divergence_fn=kernel_divergence_fn)(latent_policy))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseFlipout(vf_layer_size, name=\"vf_fc{}\".format(idx), activation='relu', kernel_divergence_fn=kernel_divergence_fn)(latent_value))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tf_layers\n",
    "from stable_baselines.deepq.policies import DQNPolicy\n",
    "class FeedForwardPolicy(DQNPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements a DQN policy, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) The size of the Neural network for the policy (if None, default to [64, 64])\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param obs_phs: (TensorFlow Tensor, TensorFlow Tensor) a tuple containing an override for observation placeholder\n",
    "        and the processed observation placeholder respectively\n",
    "    :param layer_norm: (bool) enable layer normalisation\n",
    "    :param dueling: (bool) if true double the output MLP to compute a baseline for action scores\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None,\n",
    "                 cnn_extractor=nature_cnn, feature_extraction=\"cnn\",\n",
    "                 obs_phs=None, layer_norm=False, dueling=True, act_fun=tf.nn.relu, **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps,\n",
    "                                                n_batch, dueling=dueling, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"), obs_phs=obs_phs)\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is None:\n",
    "            layers = [64, 64]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            with tf.variable_scope(\"action_value\"):\n",
    "                if feature_extraction == \"cnn\":\n",
    "                    extracted_features = cnn_extractor(self.processed_obs, **kwargs)\n",
    "                    action_out = extracted_features\n",
    "                elif feature_extraction == \"mlp\":\n",
    "                    extracted_features = tf.layers.flatten(self.processed_obs)\n",
    "                    action_out = extracted_features\n",
    "                    for layer_size in layers:\n",
    "                        action_out = tf_layers.fully_connected(action_out, num_outputs=layer_size, activation_fn=None)\n",
    "                        if layer_norm:\n",
    "                            action_out = tf_layers.layer_norm(action_out, center=True, scale=True)\n",
    "                        action_out = act_fun(action_out)\n",
    "                elif feature_extraction == \"bnn\":\n",
    "                    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "                    extracted_features = tf.layers.flatten(self.processed_obs)\n",
    "                    action_out = extracted_features\n",
    "                    for layer_size in layers:\n",
    "                        action_out = tfp.layers.DenseFlipout(layer_size, activation=None, kernel_divergence_fn=kernel_divergence_fn)(action_out)\n",
    "                        if layer_norm:\n",
    "                            action_out = tf_layers.layer_norm(action_out, center=True, scale=True)\n",
    "                        action_out = act_fun(action_out)\n",
    "\n",
    "                action_scores = tf_layers.fully_connected(action_out, num_outputs=self.n_actions, activation_fn=None)\n",
    "\n",
    "            if self.dueling:\n",
    "                with tf.variable_scope(\"state_value\"):\n",
    "                    state_out = extracted_features\n",
    "                    for layer_size in layers:\n",
    "                        state_out = tf_layers.fully_connected(state_out, num_outputs=layer_size, activation_fn=None)\n",
    "                        if layer_norm:\n",
    "                            state_out = tf_layers.layer_norm(state_out, center=True, scale=True)\n",
    "                        state_out = act_fun(state_out)\n",
    "                    state_score = tf_layers.fully_connected(state_out, num_outputs=1, activation_fn=None)\n",
    "                action_scores_mean = tf.reduce_mean(action_scores, axis=1)\n",
    "                action_scores_centered = action_scores - tf.expand_dims(action_scores_mean, axis=1)\n",
    "                q_out = state_score + action_scores_centered\n",
    "            else:\n",
    "                q_out = action_scores\n",
    "\n",
    "        self.q_values = q_out\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=True):\n",
    "        q_values, actions_proba = self.sess.run([self.q_values, self.policy_proba], {self.obs_ph: obs})\n",
    "        if deterministic:\n",
    "            actions = np.argmax(q_values, axis=1)\n",
    "        else:\n",
    "            # Unefficient sampling\n",
    "            # TODO: replace the loop\n",
    "            # maybe with Gumbel-max trick ? (http://amid.fish/humble-gumbel)\n",
    "            actions = np.zeros((len(obs),), dtype=np.int64)\n",
    "            for action_idx in range(len(obs)):\n",
    "                actions[action_idx] = np.random.choice(self.n_actions, p=actions_proba[action_idx])\n",
    "\n",
    "        return actions, q_values, None\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements DQN policy, using a MLP (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param obs_phs: (TensorFlow Tensor, TensorFlow Tensor) a tuple containing an override for observation placeholder\n",
    "        and the processed observation placeholder respectively\n",
    "    :param dueling: (bool) if true double the output MLP to compute a baseline for action scores\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                 reuse=False, obs_phs=None, dueling=True, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", obs_phs=obs_phs, dueling=dueling,\n",
    "                                        layer_norm=False, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "from slimevolleygym import SurvivalRewardEnv\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines import A2C, ACER, ACKTR, DQN, HER, GAIL, TRPO\n",
    "\n",
    "import base64\n",
    "import IPython\n",
    "import imageio\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "def record_game(model, env, num_episodes=5, video_filename='video.mp4'):\n",
    "    with imageio.get_writer(video_filename, fps=60) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            while not done:\n",
    "                action, _steps = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            print(\"score:\", total_reward)\n",
    "\n",
    "def experiment(algo, policy, \n",
    "               seed=3244,\n",
    "               timesteps=15_000_000,\n",
    "               eval_freq=25_000,\n",
    "               eval_episodes=10,\n",
    "               record=True):\n",
    "    \"\"\"\n",
    "    algo: Choose from\n",
    "        a2c, acer, acktr, dqn, trpo, ppo1\n",
    "    policy: Choose from\n",
    "        mlp, bnn\n",
    "    \"\"\"\n",
    "    if algo == \"a2c\":\n",
    "        model = A2C\n",
    "    elif algo == \"acer\":\n",
    "        model = ACER\n",
    "    elif algo == \"acktr\":\n",
    "        model = ACKTR\n",
    "    elif algo == \"dqn\":\n",
    "        model = DQN\n",
    "        \n",
    "    elif algo == \"gail\":\n",
    "        model = GAIL\n",
    "    elif algo == \"trpo\":\n",
    "        model = TRPO\n",
    "    elif algo == \"ppo\":\n",
    "        model = PPO1        \n",
    "    \n",
    "    if policy == \"dnn\":\n",
    "        if algo == \"dqn\":\n",
    "            from stable_baselines.deepq.policies import MlpPolicy\n",
    "        else:\n",
    "            from stable_baselines.common.policies import MlpPolicy\n",
    "        policyFn = MlpPolicy\n",
    "    elif policy == \"bnn\":\n",
    "        if algo == \"dqn\":\n",
    "#             print(\"0\")\n",
    "            from dqn_model import BnnPolicy\n",
    "        else:\n",
    "            from model import BnnPolicy\n",
    "        policyFn = BnnPolicy\n",
    "        \n",
    "        \n",
    "    log_dir = f\"{algo}-{policy}\"\n",
    "    logger.configure(folder=log_dir)\n",
    "    \n",
    "    env = gym.make(\"SlimeVolley-v0\")\n",
    "    env.atari_mode = True\n",
    "    env.__init__()\n",
    "    env.seed(seed)\n",
    "\n",
    "    model = model(policyFn, env, verbose=2)\n",
    "    \n",
    "    eval_callback = EvalCallback(env,\n",
    "                                 best_model_save_path=log_dir,\n",
    "                                 log_path=log_dir,\n",
    "                                 eval_freq=eval_freq,\n",
    "                                 n_eval_episodes=eval_episodes)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps,\n",
    "                callback=eval_callback)\n",
    "    \n",
    "    model.save(os.path.join(log_dir, f\"trained-{algo}-{policy}\"))\n",
    "    print(\"Training complete, saved to:\", os.path.join(log_dir, f\"trained-{algo}-{policy}\"))\n",
    "    \n",
    "    if record:\n",
    "        print(\"Recording video of gameplay.\")\n",
    "        video_filename=os.path.join(log_dir, f\"trained-{algo}-{policy}.mp4\")\n",
    "        record_game(model=model,\n",
    "                    env=env,\n",
    "                    num_episodes=5,\n",
    "                    video_filename=video_filename)\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to dqn-bnn\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Slime-RL\\Jet\\dqn_model.py:125: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\layers\\util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\callbacks.py:287: UserWarning: Training and eval env are not of the same type<SlimeVolleyEnv<SlimeVolley-v0>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000197FB9900C8>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete, saved to: dqn-bnn\\trained-dqn-bnn\n",
      "Recording video of gameplay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 1200) to (512, 1200) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce17159e405e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dqn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bnn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-9bb435124648>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(algo, policy, seed, timesteps, eval_freq, eval_episodes, record)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                     video_filename=video_filename)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9bb435124648>\u001b[0m in \u001b[0;36mrecord_game\u001b[1;34m(model, env, num_episodes, video_filename)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\slimevolleygym\\slimevolley.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_action_meanings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36menable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mglPushMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mglTranslatef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# translate to GL loc ppint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mglRotatef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRAD2DEG\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment('dqn', 'bnn', timesteps=1000, record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
