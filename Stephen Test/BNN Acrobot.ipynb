{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseFlipout(layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseFlipout(pi_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseFlipout(vf_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import ActorCriticPolicy, nature_cnn\n",
    "\n",
    "class FeedForwardPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy\n",
    "        (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor\n",
    "        documentation for details).\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, net_arch=None,\n",
    "                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            if feature_extraction == \"cnn\":\n",
    "                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "            elif feature_extraction == \"bnn\":\n",
    "                pi_latent, vf_latent = bnn_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "            else:\n",
    "                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import zip_longest\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.distributions import make_proba_dist_type, CategoricalProbabilityDistribution, \\\n",
    "    MultiCategoricalProbabilityDistribution, DiagGaussianProbabilityDistribution, BernoulliProbabilityDistribution\n",
    "from stable_baselines.common.input import observation_input\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a Bayesian neural net (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMESTEPS = int(1e6)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 500\n",
    "EVAL_EPISODES = 10  # was 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to acrobot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGDIR = \"acrobot\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "dnn = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<AcrobotEnv<Acrobot-v1>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fbf0c6dce50>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=0, episode_reward=-417.40 +/- 94.19\n",
      "Episode length: 417.90 +/- 93.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=0, episode_reward=-406.00 +/- 116.83\n",
      "Episode length: 406.40 +/- 116.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=0, episode_reward=-423.50 +/- 110.73\n",
      "Episode length: 423.90 +/- 110.31\n",
      "Eval num_timesteps=0, episode_reward=-365.40 +/- 88.89\n",
      "Episode length: 366.20 +/- 88.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=0, episode_reward=-425.00 +/- 82.47\n",
      "Episode length: 425.50 +/- 82.01\n",
      "Eval num_timesteps=0, episode_reward=-429.90 +/- 85.21\n",
      "Episode length: 430.40 +/- 84.80\n",
      "Eval num_timesteps=0, episode_reward=-391.00 +/- 93.20\n",
      "Episode length: 391.70 +/- 92.85\n",
      "Eval num_timesteps=0, episode_reward=-423.20 +/- 96.66\n",
      "Episode length: 423.70 +/- 96.26\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00026 |       0.00000 |     239.78580 |       0.00069 |       1.09795\n",
      "     -0.00110 |       0.00000 |     178.64015 |       0.00160 |       1.09706\n",
      "     -0.00219 |       0.00000 |     115.50635 |       0.00319 |       1.09548\n",
      "     -0.00286 |       0.00000 |      68.61203 |       0.00509 |       1.09357\n",
      "     -0.00342 |       0.00000 |      40.98987 |       0.00667 |       1.09204\n",
      "     -0.00401 |       0.00000 |      25.38531 |       0.00790 |       1.09081\n",
      "     -0.00406 |       0.00000 |      16.41243 |       0.00758 |       1.09109\n",
      "     -0.00420 |       0.00000 |      11.18801 |       0.00759 |       1.09106\n",
      "     -0.00468 |       0.00000 |       8.16904 |       0.00796 |       1.09075\n",
      "     -0.00460 |       0.00000 |       6.45247 |       0.00944 |       1.08929\n",
      "Evaluating losses...\n",
      "     -0.00532 |       0.00000 |       5.87886 |       0.00855 |       1.09015\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 8             |\n",
      "| TimeElapsed     | 19.7          |\n",
      "| TimestepsSoFar  | 4096          |\n",
      "| ev_tdlam_before | -0.0829       |\n",
      "| loss_ent        | 1.0901456     |\n",
      "| loss_kl         | 0.008553814   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0053239507 |\n",
      "| loss_vf_loss    | 5.878858      |\n",
      "-----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00012 |       0.00000 |     157.08415 |       0.00029 |       1.09105\n",
      "     -0.00164 |       0.00000 |     103.20381 |       0.00293 |       1.08611\n",
      "     -0.00324 |       0.00000 |      72.04588 |       0.00757 |       1.08133\n",
      "     -0.00393 |       0.00000 |      52.31002 |       0.00931 |       1.07882\n",
      "     -0.00449 |       0.00000 |      39.23515 |       0.00994 |       1.07944\n",
      "     -0.00495 |       0.00000 |      30.43041 |       0.00949 |       1.07828\n",
      "     -0.00559 |       0.00000 |      24.48837 |       0.00948 |       1.07897\n",
      "     -0.00593 |       0.00000 |      20.48542 |       0.01059 |       1.07579\n",
      "     -0.00624 |       0.00000 |      17.84118 |       0.01042 |       1.07580\n",
      "     -0.00643 |       0.00000 |      16.11151 |       0.01039 |       1.07697\n",
      "Evaluating losses...\n",
      "     -0.00722 |       0.00000 |      15.44928 |       0.00975 |       1.07716\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 16           |\n",
      "| TimeElapsed     | 47.9         |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | -0.00249     |\n",
      "| loss_ent        | 1.0771606    |\n",
      "| loss_kl         | 0.0097492235 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.007219044 |\n",
      "| loss_vf_loss    | 15.449282    |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00026 |       0.00000 |     124.78302 |       0.00048 |       1.08030\n",
      "     -0.00089 |       0.00000 |      88.26209 |       0.00269 |       1.08299\n",
      "     -0.00224 |       0.00000 |      65.41407 |       0.00506 |       1.08356\n",
      "     -0.00269 |       0.00000 |      50.76550 |       0.00557 |       1.08340\n",
      "     -0.00295 |       0.00000 |      41.14816 |       0.00573 |       1.08249\n",
      "     -0.00308 |       0.00000 |      34.79521 |       0.00705 |       1.08267\n",
      "     -0.00347 |       0.00000 |      30.63312 |       0.00732 |       1.08166\n",
      "     -0.00356 |       0.00000 |      27.88704 |       0.00635 |       1.08159\n",
      "     -0.00359 |       0.00000 |      26.12887 |       0.00838 |       1.08238\n",
      "     -0.00361 |       0.00000 |      25.00261 |       0.00797 |       1.08089\n",
      "Evaluating losses...\n",
      "     -0.00451 |       0.00000 |      24.58261 |       0.00734 |       1.08192\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 24            |\n",
      "| TimeElapsed     | 71            |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | -0.019        |\n",
      "| loss_ent        | 1.0819219     |\n",
      "| loss_kl         | 0.0073436094  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0045111803 |\n",
      "| loss_vf_loss    | 24.582613     |\n",
      "-----------------------------------\n",
      "********** Iteration 3 ************\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00037 |       0.00000 |     102.92410 |       0.00054 |       1.08064\n",
      "     -0.00064 |       0.00000 |      77.03475 |       0.00311 |       1.07711\n",
      "     -0.00134 |       0.00000 |      60.87416 |       0.00397 |       1.07353\n",
      "     -0.00139 |       0.00000 |      50.78227 |       0.00377 |       1.07380\n",
      "     -0.00155 |       0.00000 |      44.44447 |       0.00346 |       1.07425\n",
      "     -0.00157 |       0.00000 |      40.42486 |       0.00541 |       1.07049\n",
      "     -0.00178 |       0.00000 |      37.93908 |       0.00542 |       1.07285\n",
      "     -0.00240 |       0.00000 |      36.38423 |       0.00506 |       1.07394\n",
      "     -0.00234 |       0.00000 |      35.44881 |       0.00561 |       1.07282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00173 |       0.00000 |      34.88096 |       0.00315 |       1.07641\n",
      "Evaluating losses...\n",
      "     -0.00296 |       0.00000 |      34.67294 |       0.00429 |       1.07431\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 32            |\n",
      "| TimeElapsed     | 96.1          |\n",
      "| TimestepsSoFar  | 16384         |\n",
      "| ev_tdlam_before | -0.00366      |\n",
      "| loss_ent        | 1.074305      |\n",
      "| loss_kl         | 0.004289391   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0029649534 |\n",
      "| loss_vf_loss    | 34.672935     |\n",
      "-----------------------------------\n",
      "********** Iteration 4 ************\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -4.12e-05 |       0.00000 |      90.40066 |       0.00134 |       1.07765\n",
      "     -0.00238 |       0.00000 |      71.60265 |       0.00405 |       1.07787\n",
      "     -0.00340 |       0.00000 |      60.41322 |       0.00644 |       1.07931\n",
      "     -0.00360 |       0.00000 |      53.91737 |       0.00727 |       1.07816\n",
      "     -0.00391 |       0.00000 |      50.07634 |       0.00624 |       1.07877\n",
      "     -0.00418 |       0.00000 |      47.86587 |       0.00682 |       1.07845\n",
      "     -0.00406 |       0.00000 |      46.56384 |       0.00788 |       1.07703\n",
      "     -0.00394 |       0.00000 |      45.85289 |       0.00746 |       1.07706\n",
      "     -0.00444 |       0.00000 |      45.43961 |       0.00620 |       1.07792\n",
      "     -0.00423 |       0.00000 |      45.21879 |       0.00766 |       1.07752\n",
      "Evaluating losses...\n",
      "     -0.00507 |       0.00000 |      45.15118 |       0.00686 |       1.07600\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 40            |\n",
      "| TimeElapsed     | 122           |\n",
      "| TimestepsSoFar  | 20480         |\n",
      "| ev_tdlam_before | 0.000535      |\n",
      "| loss_ent        | 1.0760014     |\n",
      "| loss_kl         | 0.006861901   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0050700703 |\n",
      "| loss_vf_loss    | 45.151184     |\n",
      "-----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00029 |       0.00000 |      89.06371 |      7.94e-05 |       1.07862\n",
      "    -8.47e-05 |       0.00000 |      76.11695 |       0.00043 |       1.07764\n",
      "     -0.00083 |       0.00000 |      69.08672 |       0.00099 |       1.07450\n",
      "     -0.00147 |       0.00000 |      65.40583 |       0.00153 |       1.07622\n",
      "     -0.00181 |       0.00000 |      63.46795 |       0.00330 |       1.06914\n",
      "     -0.00175 |       0.00000 |      62.45433 |       0.00337 |       1.07168\n",
      "     -0.00250 |       0.00000 |      61.93202 |       0.00400 |       1.07245\n",
      "     -0.00285 |       0.00000 |      61.67268 |       0.00404 |       1.07197\n",
      "     -0.00329 |       0.00000 |      61.54975 |       0.00450 |       1.06989\n",
      "     -0.00333 |       0.00000 |      61.48327 |       0.00554 |       1.07085\n",
      "Evaluating losses...\n",
      "     -0.00437 |       0.00000 |      61.46618 |       0.00478 |       1.07271\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 9            |\n",
      "| EpisodesSoFar   | 49           |\n",
      "| TimeElapsed     | 146          |\n",
      "| TimestepsSoFar  | 24576        |\n",
      "| ev_tdlam_before | 5.26e-05     |\n",
      "| loss_ent        | 1.0727122    |\n",
      "| loss_kl         | 0.0047832727 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004369167 |\n",
      "| loss_vf_loss    | 61.46618     |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24576, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00062 |       0.00000 |      83.36999 |       0.00015 |       1.07374\n",
      "     -0.00049 |       0.00000 |      73.38394 |       0.00046 |       1.07387\n",
      "     -0.00144 |       0.00000 |      68.61993 |       0.00228 |       1.07931\n",
      "     -0.00211 |       0.00000 |      66.44183 |       0.00428 |       1.08227\n",
      "     -0.00240 |       0.00000 |      65.42106 |       0.00403 |       1.08168\n",
      "     -0.00264 |       0.00000 |      64.97624 |       0.00514 |       1.08342\n",
      "     -0.00295 |       0.00000 |      64.77314 |       0.00561 |       1.08389\n",
      "     -0.00298 |       0.00000 |      64.69004 |       0.00523 |       1.08389\n",
      "     -0.00328 |       0.00000 |      64.65345 |       0.00627 |       1.08371\n",
      "     -0.00345 |       0.00000 |      64.63963 |       0.00674 |       1.08334\n",
      "Evaluating losses...\n",
      "     -0.00409 |       0.00000 |      64.63535 |       0.00572 |       1.08301\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 57            |\n",
      "| TimeElapsed     | 169           |\n",
      "| TimestepsSoFar  | 28672         |\n",
      "| ev_tdlam_before | 3.48e-05      |\n",
      "| loss_ent        | 1.0830135     |\n",
      "| loss_kl         | 0.005718206   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040912293 |\n",
      "| loss_vf_loss    | 64.63535      |\n",
      "-----------------------------------\n",
      "********** Iteration 7 ************\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=28672, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.71e-05 |       0.00000 |      84.81113 |       0.00066 |       1.08288\n",
      "     -0.00123 |       0.00000 |      77.75255 |       0.00201 |       1.07924\n",
      "     -0.00271 |       0.00000 |      74.90385 |       0.00718 |       1.06851\n",
      "     -0.00305 |       0.00000 |      73.79113 |       0.00797 |       1.06754\n",
      "     -0.00329 |       0.00000 |      73.36186 |       0.00895 |       1.06657\n",
      "     -0.00329 |       0.00000 |      73.20257 |       0.00888 |       1.06653\n",
      "     -0.00383 |       0.00000 |      73.14801 |       0.00841 |       1.06645\n",
      "     -0.00368 |       0.00000 |      73.12551 |       0.00898 |       1.06677\n",
      "     -0.00383 |       0.00000 |      73.12211 |       0.00834 |       1.07017\n",
      "     -0.00430 |       0.00000 |      73.12099 |       0.00994 |       1.06669\n",
      "Evaluating losses...\n",
      "     -0.00468 |       0.00000 |      73.11333 |       0.00704 |       1.07113\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 65            |\n",
      "| TimeElapsed     | 192           |\n",
      "| TimestepsSoFar  | 32768         |\n",
      "| ev_tdlam_before | 0.000181      |\n",
      "| loss_ent        | 1.0711294     |\n",
      "| loss_kl         | 0.00704156    |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0046769297 |\n",
      "| loss_vf_loss    | 73.113335     |\n",
      "-----------------------------------\n",
      "********** Iteration 8 ************\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=32768, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00038 |       0.00000 |      87.64541 |       0.00027 |       1.07416\n",
      "     -0.00078 |       0.00000 |      82.80272 |       0.00166 |       1.07775\n",
      "     -0.00140 |       0.00000 |      81.15712 |       0.00313 |       1.07502\n",
      "     -0.00240 |       0.00000 |      80.64140 |       0.00373 |       1.07429\n",
      "     -0.00280 |       0.00000 |      80.49424 |       0.00691 |       1.07560\n",
      "     -0.00321 |       0.00000 |      80.44463 |       0.00719 |       1.07212\n",
      "     -0.00325 |       0.00000 |      80.43310 |       0.00773 |       1.07257\n",
      "     -0.00364 |       0.00000 |      80.43472 |       0.00716 |       1.07444\n",
      "     -0.00362 |       0.00000 |      80.42265 |       0.00751 |       1.07431\n",
      "     -0.00383 |       0.00000 |      80.42882 |       0.00684 |       1.07517\n",
      "Evaluating losses...\n",
      "     -0.00456 |       0.00000 |      80.41418 |       0.00823 |       1.07631\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 73           |\n",
      "| TimeElapsed     | 214          |\n",
      "| TimestepsSoFar  | 36864        |\n",
      "| ev_tdlam_before | 0.000178     |\n",
      "| loss_ent        | 1.0763121    |\n",
      "| loss_kl         | 0.008226424  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004563186 |\n",
      "| loss_vf_loss    | 80.41418     |\n",
      "----------------------------------\n",
      "********** Iteration 9 ************\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=36864, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00030 |       0.00000 |      91.03973 |       0.00081 |       1.06942\n",
      "     -0.00302 |       0.00000 |      87.73305 |       0.00381 |       1.05984\n",
      "     -0.00423 |       0.00000 |      86.83504 |       0.00866 |       1.05250\n",
      "     -0.00497 |       0.00000 |      86.58795 |       0.00780 |       1.05526\n",
      "     -0.00502 |       0.00000 |      86.43415 |       0.00904 |       1.05099\n",
      "     -0.00548 |       0.00000 |      85.76324 |       0.00867 |       1.05163\n",
      "     -0.00550 |       0.00000 |      83.53931 |       0.00784 |       1.05343\n",
      "     -0.00589 |       0.00000 |      81.86192 |       0.00845 |       1.05247\n",
      "     -0.00542 |       0.00000 |      80.92217 |       0.00751 |       1.05559\n",
      "     -0.00599 |       0.00000 |      80.54567 |       0.00811 |       1.05283\n",
      "Evaluating losses...\n",
      "     -0.00690 |       0.00000 |      80.08618 |       0.00821 |       1.05200\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 81            |\n",
      "| TimeElapsed     | 237           |\n",
      "| TimestepsSoFar  | 40960         |\n",
      "| ev_tdlam_before | 0.000502      |\n",
      "| loss_ent        | 1.0520017     |\n",
      "| loss_kl         | 0.008209321   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0068997047 |\n",
      "| loss_vf_loss    | 80.08618      |\n",
      "-----------------------------------\n",
      "********** Iteration 10 ************\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=40960, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00091 |       0.00000 |      99.85835 |       0.00049 |       1.04716\n",
      "     -0.00022 |       0.00000 |      97.53941 |       0.00091 |       1.04594\n",
      "     -0.00135 |       0.00000 |      96.66373 |       0.00310 |       1.04491\n",
      "     -0.00196 |       0.00000 |      96.40784 |       0.00426 |       1.04446\n",
      "     -0.00160 |       0.00000 |      96.34962 |       0.00448 |       1.03528\n",
      "     -0.00246 |       0.00000 |      96.20914 |       0.00502 |       1.04102\n",
      "     -0.00168 |       0.00000 |      96.03398 |       0.00398 |       1.04602\n",
      "     -0.00236 |       0.00000 |      95.86470 |       0.00459 |       1.03659\n",
      "     -0.00256 |       0.00000 |      95.79871 |       0.00555 |       1.03590\n",
      "     -0.00288 |       0.00000 |      95.67014 |       0.00436 |       1.03938\n",
      "Evaluating losses...\n",
      "     -0.00386 |       0.00000 |      95.37724 |       0.00358 |       1.03883\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 9             |\n",
      "| EpisodesSoFar   | 90            |\n",
      "| TimeElapsed     | 262           |\n",
      "| TimestepsSoFar  | 45056         |\n",
      "| ev_tdlam_before | 0.0955        |\n",
      "| loss_ent        | 1.0388254     |\n",
      "| loss_kl         | 0.0035790685  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0038566373 |\n",
      "| loss_vf_loss    | 95.37724      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 11 ************\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=45056, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00010 |       0.00000 |     117.27459 |       0.00105 |       1.03836\n",
      "     -0.00299 |       0.00000 |     102.53395 |       0.00257 |       1.04230\n",
      "     -0.00347 |       0.00000 |      95.87666 |       0.00247 |       1.04005\n",
      "     -0.00377 |       0.00000 |      91.54907 |       0.00385 |       1.03743\n",
      "     -0.00435 |       0.00000 |      89.15131 |       0.00329 |       1.04107\n",
      "     -0.00407 |       0.00000 |      87.95895 |       0.00360 |       1.03650\n",
      "     -0.00459 |       0.00000 |      86.76653 |       0.00381 |       1.04070\n",
      "     -0.00426 |       0.00000 |      86.03233 |       0.00364 |       1.03940\n",
      "     -0.00485 |       0.00000 |      85.19842 |       0.00348 |       1.03902\n",
      "     -0.00488 |       0.00000 |      84.75169 |       0.00346 |       1.03973\n",
      "Evaluating losses...\n",
      "     -0.00578 |       0.00000 |      83.82361 |       0.00391 |       1.04105\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 98           |\n",
      "| TimeElapsed     | 285          |\n",
      "| TimestepsSoFar  | 49152        |\n",
      "| ev_tdlam_before | 0.245        |\n",
      "| loss_ent        | 1.041047     |\n",
      "| loss_kl         | 0.0039083916 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005782562 |\n",
      "| loss_vf_loss    | 83.82361     |\n",
      "----------------------------------\n",
      "********** Iteration 12 ************\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=49152, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00043 |       0.00000 |     109.44255 |       0.00111 |       1.03788\n",
      "     -0.00348 |       0.00000 |      97.04755 |       0.00353 |       1.03388\n",
      "     -0.00479 |       0.00000 |      91.41911 |       0.00427 |       1.02704\n",
      "     -0.00471 |       0.00000 |      87.22144 |       0.00580 |       1.02480\n",
      "     -0.00513 |       0.00000 |      84.53818 |       0.00540 |       1.02825\n",
      "     -0.00540 |       0.00000 |      82.76839 |       0.00522 |       1.03037\n",
      "     -0.00570 |       0.00000 |      81.18475 |       0.00531 |       1.02626\n",
      "     -0.00563 |       0.00000 |      80.18884 |       0.00602 |       1.02551\n",
      "     -0.00596 |       0.00000 |      79.37503 |       0.00593 |       1.02591\n",
      "     -0.00606 |       0.00000 |      78.71887 |       0.00485 |       1.02846\n",
      "Evaluating losses...\n",
      "     -0.00719 |       0.00000 |      78.23116 |       0.00497 |       1.02439\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 106          |\n",
      "| TimeElapsed     | 308          |\n",
      "| TimestepsSoFar  | 53248        |\n",
      "| ev_tdlam_before | 0.489        |\n",
      "| loss_ent        | 1.0243866    |\n",
      "| loss_kl         | 0.004973304  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.007194665 |\n",
      "| loss_vf_loss    | 78.23116     |\n",
      "----------------------------------\n",
      "********** Iteration 13 ************\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=53248, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00475 |       0.00000 |      61.99589 |       0.00239 |       1.01407\n",
      "     -0.00838 |       0.00000 |      58.16727 |       0.00558 |       1.00973\n",
      "     -0.00914 |       0.00000 |      56.34282 |       0.00685 |       1.00354\n",
      "     -0.00924 |       0.00000 |      54.89139 |       0.00649 |       1.00314\n",
      "     -0.01022 |       0.00000 |      53.87394 |       0.00877 |       1.01034\n",
      "     -0.00953 |       0.00000 |      52.92374 |       0.00559 |       1.00602\n",
      "     -0.01043 |       0.00000 |      52.29312 |       0.00783 |       1.00452\n",
      "     -0.01069 |       0.00000 |      51.61849 |       0.00719 |       1.00654\n",
      "     -0.01081 |       0.00000 |      51.15956 |       0.00790 |       1.00669\n",
      "     -0.01111 |       0.00000 |      50.58896 |       0.00744 |       1.00812\n",
      "Evaluating losses...\n",
      "     -0.01214 |       0.00000 |      50.07691 |       0.00759 |       1.00646\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 114          |\n",
      "| TimeElapsed     | 336          |\n",
      "| TimestepsSoFar  | 57344        |\n",
      "| ev_tdlam_before | 0.743        |\n",
      "| loss_ent        | 1.006456     |\n",
      "| loss_kl         | 0.0075923344 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.012143211 |\n",
      "| loss_vf_loss    | 50.07691     |\n",
      "----------------------------------\n",
      "********** Iteration 14 ************\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=57344, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00513 |       0.00000 |      28.76764 |       0.00269 |       0.97988\n",
      "     -0.00798 |       0.00000 |      25.87836 |       0.00673 |       0.97730\n",
      "     -0.00810 |       0.00000 |      24.62218 |       0.00674 |       0.97985\n",
      "     -0.00885 |       0.00000 |      23.63774 |       0.00774 |       0.97820\n",
      "     -0.00898 |       0.00000 |      22.88763 |       0.00741 |       0.97825\n",
      "     -0.00930 |       0.00000 |      22.36613 |       0.00773 |       0.97789\n",
      "     -0.00980 |       0.00000 |      21.74360 |       0.00749 |       0.97635\n",
      "     -0.00974 |       0.00000 |      21.36982 |       0.00725 |       0.97468\n",
      "     -0.01030 |       0.00000 |      20.88628 |       0.00768 |       0.97389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.01004 |       0.00000 |      20.48277 |       0.00783 |       0.97431\n",
      "Evaluating losses...\n",
      "     -0.01077 |       0.00000 |      20.09076 |       0.00649 |       0.98366\n",
      "---------------------------------\n",
      "| EpLenMean       | 498         |\n",
      "| EpRewMean       | -498        |\n",
      "| EpThisIter      | 9           |\n",
      "| EpisodesSoFar   | 123         |\n",
      "| TimeElapsed     | 365         |\n",
      "| TimestepsSoFar  | 61440       |\n",
      "| ev_tdlam_before | 0.923       |\n",
      "| loss_ent        | 0.98366356  |\n",
      "| loss_kl         | 0.006488354 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | -0.01077407 |\n",
      "| loss_vf_loss    | 20.090757   |\n",
      "---------------------------------\n",
      "********** Iteration 15 ************\n",
      "Eval num_timesteps=61440, episode_reward=-289.10 +/- 119.80\n",
      "Episode length: 289.90 +/- 119.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61440, episode_reward=-181.70 +/- 46.06\n",
      "Episode length: 182.70 +/- 46.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61440, episode_reward=-233.40 +/- 66.52\n",
      "Episode length: 234.40 +/- 66.52\n",
      "Eval num_timesteps=61440, episode_reward=-198.80 +/- 46.71\n",
      "Episode length: 199.80 +/- 46.71\n",
      "Eval num_timesteps=61440, episode_reward=-239.80 +/- 108.99\n",
      "Episode length: 240.70 +/- 108.75\n",
      "Eval num_timesteps=61440, episode_reward=-225.00 +/- 99.68\n",
      "Episode length: 225.90 +/- 99.40\n",
      "Eval num_timesteps=61440, episode_reward=-265.10 +/- 120.99\n",
      "Episode length: 265.90 +/- 120.61\n",
      "Eval num_timesteps=61440, episode_reward=-260.70 +/- 121.68\n",
      "Episode length: 261.60 +/- 121.49\n",
      "Eval num_timesteps=61440, episode_reward=-237.10 +/- 116.12\n",
      "Episode length: 238.00 +/- 115.89\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00581 |       0.00000 |      37.42098 |       0.00376 |       0.97116\n",
      "     -0.00928 |       0.00000 |      33.43354 |       0.00598 |       0.96384\n",
      "     -0.01064 |       0.00000 |      32.41467 |       0.00629 |       0.96026\n",
      "     -0.01126 |       0.00000 |      31.71802 |       0.00724 |       0.96534\n",
      "     -0.01179 |       0.00000 |      31.20524 |       0.00695 |       0.96453\n",
      "     -0.01162 |       0.00000 |      30.79128 |       0.00740 |       0.96100\n",
      "     -0.01240 |       0.00000 |      30.51728 |       0.00801 |       0.96311\n",
      "     -0.01259 |       0.00000 |      30.13923 |       0.00828 |       0.95869\n",
      "     -0.01269 |       0.00000 |      29.82768 |       0.00774 |       0.96284\n",
      "     -0.01280 |       0.00000 |      29.46410 |       0.00727 |       0.96428\n",
      "Evaluating losses...\n",
      "     -0.01365 |       0.00000 |      28.98959 |       0.00921 |       0.96467\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 131          |\n",
      "| TimeElapsed     | 381          |\n",
      "| TimestepsSoFar  | 65536        |\n",
      "| ev_tdlam_before | 0.894        |\n",
      "| loss_ent        | 0.9646712    |\n",
      "| loss_kl         | 0.009207512  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.013646354 |\n",
      "| loss_vf_loss    | 28.989592    |\n",
      "----------------------------------\n",
      "********** Iteration 16 ************\n",
      "Eval num_timesteps=65536, episode_reward=-154.10 +/- 24.25\n",
      "Episode length: 155.10 +/- 24.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65536, episode_reward=-156.70 +/- 34.85\n",
      "Episode length: 157.70 +/- 34.85\n",
      "Eval num_timesteps=65536, episode_reward=-204.10 +/- 106.54\n",
      "Episode length: 205.00 +/- 106.26\n",
      "Eval num_timesteps=65536, episode_reward=-149.40 +/- 21.25\n",
      "Episode length: 150.40 +/- 21.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65536, episode_reward=-165.10 +/- 69.94\n",
      "Episode length: 166.10 +/- 69.94\n",
      "Eval num_timesteps=65536, episode_reward=-194.70 +/- 105.48\n",
      "Episode length: 195.60 +/- 105.19\n",
      "Eval num_timesteps=65536, episode_reward=-164.30 +/- 30.06\n",
      "Episode length: 165.30 +/- 30.06\n",
      "Eval num_timesteps=65536, episode_reward=-173.50 +/- 35.74\n",
      "Episode length: 174.50 +/- 35.74\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00313 |       0.00000 |      62.94543 |       0.00344 |       0.92779\n",
      "     -0.00676 |       0.00000 |      59.53893 |       0.00485 |       0.92002\n",
      "     -0.00697 |       0.00000 |      57.74075 |       0.00674 |       0.91237\n",
      "     -0.00748 |       0.00000 |      56.61397 |       0.00651 |       0.92245\n",
      "     -0.00794 |       0.00000 |      55.82044 |       0.00686 |       0.91068\n",
      "     -0.00797 |       0.00000 |      55.10168 |       0.00728 |       0.91595\n",
      "     -0.00838 |       0.00000 |      54.67518 |       0.00604 |       0.91589\n",
      "     -0.00872 |       0.00000 |      54.14240 |       0.00727 |       0.91601\n",
      "     -0.00875 |       0.00000 |      53.79033 |       0.00669 |       0.91323\n",
      "     -0.00890 |       0.00000 |      53.15074 |       0.00806 |       0.90775\n",
      "Evaluating losses...\n",
      "     -0.00974 |       0.00000 |      52.52833 |       0.00717 |       0.91988\n",
      "----------------------------------\n",
      "| EpLenMean       | 478          |\n",
      "| EpRewMean       | -477         |\n",
      "| EpThisIter      | 12           |\n",
      "| EpisodesSoFar   | 143          |\n",
      "| TimeElapsed     | 391          |\n",
      "| TimestepsSoFar  | 69632        |\n",
      "| ev_tdlam_before | 0.841        |\n",
      "| loss_ent        | 0.9198791    |\n",
      "| loss_kl         | 0.0071748146 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.009739562 |\n",
      "| loss_vf_loss    | 52.528328    |\n",
      "----------------------------------\n",
      "********** Iteration 17 ************\n",
      "Eval num_timesteps=69632, episode_reward=-137.50 +/- 24.17\n",
      "Episode length: 138.50 +/- 24.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69632, episode_reward=-127.20 +/- 16.42\n",
      "Episode length: 128.20 +/- 16.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69632, episode_reward=-129.40 +/- 16.38\n",
      "Episode length: 130.40 +/- 16.38\n",
      "Eval num_timesteps=69632, episode_reward=-140.70 +/- 11.38\n",
      "Episode length: 141.70 +/- 11.38\n",
      "Eval num_timesteps=69632, episode_reward=-130.10 +/- 13.93\n",
      "Episode length: 131.10 +/- 13.93\n",
      "Eval num_timesteps=69632, episode_reward=-131.80 +/- 14.62\n",
      "Episode length: 132.80 +/- 14.62\n",
      "Eval num_timesteps=69632, episode_reward=-133.10 +/- 26.14\n",
      "Episode length: 134.10 +/- 26.14\n",
      "Eval num_timesteps=69632, episode_reward=-120.00 +/- 13.10\n",
      "Episode length: 121.00 +/- 13.10\n",
      "New best mean reward!\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00532 |       0.00000 |      56.11751 |       0.00383 |       0.87904\n",
      "     -0.01015 |       0.00000 |      53.44856 |       0.00656 |       0.86143\n",
      "     -0.01202 |       0.00000 |      52.21376 |       0.00889 |       0.85209\n",
      "     -0.01243 |       0.00000 |      51.31187 |       0.00859 |       0.85059\n",
      "     -0.01311 |       0.00000 |      50.43790 |       0.00918 |       0.85037\n",
      "     -0.01330 |       0.00000 |      49.76045 |       0.00778 |       0.85898\n",
      "     -0.01362 |       0.00000 |      49.44279 |       0.00827 |       0.85501\n",
      "     -0.01392 |       0.00000 |      48.60901 |       0.00861 |       0.85795\n",
      "     -0.01439 |       0.00000 |      47.98816 |       0.00926 |       0.85175\n",
      "     -0.01428 |       0.00000 |      47.87806 |       0.00906 |       0.85337\n",
      "Evaluating losses...\n",
      "     -0.01521 |       0.00000 |      47.27710 |       0.00817 |       0.86366\n",
      "----------------------------------\n",
      "| EpLenMean       | 447          |\n",
      "| EpRewMean       | -446         |\n",
      "| EpThisIter      | 15           |\n",
      "| EpisodesSoFar   | 158          |\n",
      "| TimeElapsed     | 400          |\n",
      "| TimestepsSoFar  | 73728        |\n",
      "| ev_tdlam_before | 0.876        |\n",
      "| loss_ent        | 0.8636559    |\n",
      "| loss_kl         | 0.008168963  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.015211299 |\n",
      "| loss_vf_loss    | 47.277103    |\n",
      "----------------------------------\n",
      "********** Iteration 18 ************\n",
      "Eval num_timesteps=73728, episode_reward=-135.50 +/- 37.28\n",
      "Episode length: 136.50 +/- 37.28\n",
      "Eval num_timesteps=73728, episode_reward=-110.80 +/- 15.92\n",
      "Episode length: 111.80 +/- 15.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73728, episode_reward=-152.90 +/- 58.80\n",
      "Episode length: 153.90 +/- 58.80\n",
      "Eval num_timesteps=73728, episode_reward=-126.70 +/- 53.24\n",
      "Episode length: 127.70 +/- 53.24\n",
      "Eval num_timesteps=73728, episode_reward=-116.50 +/- 22.20\n",
      "Episode length: 117.50 +/- 22.20\n",
      "Eval num_timesteps=73728, episode_reward=-117.30 +/- 21.33\n",
      "Episode length: 118.30 +/- 21.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=73728, episode_reward=-116.80 +/- 21.06\n",
      "Episode length: 117.80 +/- 21.06\n",
      "Eval num_timesteps=73728, episode_reward=-123.90 +/- 40.27\n",
      "Episode length: 124.90 +/- 40.27\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00376 |       0.00000 |      71.59366 |       0.00278 |       0.84910\n",
      "     -0.00707 |       0.00000 |      69.63634 |       0.00776 |       0.81839\n",
      "     -0.00805 |       0.00000 |      68.73539 |       0.00683 |       0.82801\n",
      "     -0.00863 |       0.00000 |      68.21500 |       0.00941 |       0.81238\n",
      "     -0.00870 |       0.00000 |      67.34636 |       0.00839 |       0.82055\n",
      "     -0.00917 |       0.00000 |      66.71078 |       0.00707 |       0.82310\n",
      "     -0.00923 |       0.00000 |      66.52464 |       0.00865 |       0.81330\n",
      "     -0.00964 |       0.00000 |      65.88665 |       0.00941 |       0.81342\n",
      "     -0.00982 |       0.00000 |      65.46067 |       0.00787 |       0.82089\n",
      "     -0.00986 |       0.00000 |      65.09216 |       0.00902 |       0.81646\n",
      "Evaluating losses...\n",
      "     -0.01112 |       0.00000 |      64.34788 |       0.00831 |       0.81615\n",
      "----------------------------------\n",
      "| EpLenMean       | 392          |\n",
      "| EpRewMean       | -391         |\n",
      "| EpThisIter      | 19           |\n",
      "| EpisodesSoFar   | 177          |\n",
      "| TimeElapsed     | 409          |\n",
      "| TimestepsSoFar  | 77824        |\n",
      "| ev_tdlam_before | 0.847        |\n",
      "| loss_ent        | 0.8161529    |\n",
      "| loss_kl         | 0.00831438   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.011117975 |\n",
      "| loss_vf_loss    | 64.34788     |\n",
      "----------------------------------\n",
      "********** Iteration 19 ************\n",
      "Eval num_timesteps=77824, episode_reward=-107.20 +/- 23.68\n",
      "Episode length: 108.20 +/- 23.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77824, episode_reward=-106.40 +/- 24.29\n",
      "Episode length: 107.40 +/- 24.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77824, episode_reward=-100.00 +/- 7.80\n",
      "Episode length: 101.00 +/- 7.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77824, episode_reward=-102.90 +/- 8.26\n",
      "Episode length: 103.90 +/- 8.26\n",
      "Eval num_timesteps=77824, episode_reward=-103.50 +/- 10.30\n",
      "Episode length: 104.50 +/- 10.30\n",
      "Eval num_timesteps=77824, episode_reward=-97.10 +/- 17.98\n",
      "Episode length: 98.10 +/- 17.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77824, episode_reward=-131.00 +/- 51.48\n",
      "Episode length: 132.00 +/- 51.48\n",
      "Eval num_timesteps=77824, episode_reward=-116.30 +/- 22.90\n",
      "Episode length: 117.30 +/- 22.90\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00154 |       0.00000 |      75.67926 |       0.00248 |       0.81358\n",
      "     -0.00541 |       0.00000 |      70.35636 |       0.00583 |       0.79005\n",
      "     -0.00604 |       0.00000 |      68.59484 |       0.00677 |       0.78347\n",
      "     -0.00741 |       0.00000 |      67.54038 |       0.00765 |       0.78340\n",
      "     -0.00629 |       0.00000 |      66.62247 |       0.00702 |       0.78000\n",
      "     -0.00780 |       0.00000 |      66.05760 |       0.00824 |       0.78017\n",
      "     -0.00701 |       0.00000 |      65.41311 |       0.00881 |       0.77250\n",
      "     -0.00768 |       0.00000 |      64.97221 |       0.00841 |       0.78150\n",
      "     -0.00821 |       0.00000 |      64.64162 |       0.00897 |       0.77794\n",
      "     -0.00829 |       0.00000 |      64.03493 |       0.00946 |       0.77313\n",
      "Evaluating losses...\n",
      "     -0.00948 |       0.00000 |      63.33445 |       0.00874 |       0.77374\n",
      "----------------------------------\n",
      "| EpLenMean       | 328          |\n",
      "| EpRewMean       | -327         |\n",
      "| EpThisIter      | 21           |\n",
      "| EpisodesSoFar   | 198          |\n",
      "| TimeElapsed     | 419          |\n",
      "| TimestepsSoFar  | 81920        |\n",
      "| ev_tdlam_before | 0.828        |\n",
      "| loss_ent        | 0.77374196   |\n",
      "| loss_kl         | 0.00874112   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.009481644 |\n",
      "| loss_vf_loss    | 63.33445     |\n",
      "----------------------------------\n",
      "********** Iteration 20 ************\n",
      "Eval num_timesteps=81920, episode_reward=-174.30 +/- 154.74\n",
      "Episode length: 175.20 +/- 154.53\n",
      "Eval num_timesteps=81920, episode_reward=-99.20 +/- 9.22\n",
      "Episode length: 100.20 +/- 9.22\n",
      "Eval num_timesteps=81920, episode_reward=-114.10 +/- 31.75\n",
      "Episode length: 115.10 +/- 31.75\n",
      "Eval num_timesteps=81920, episode_reward=-104.10 +/- 27.23\n",
      "Episode length: 105.10 +/- 27.23\n",
      "Eval num_timesteps=81920, episode_reward=-108.80 +/- 22.25\n",
      "Episode length: 109.80 +/- 22.25\n",
      "Eval num_timesteps=81920, episode_reward=-129.60 +/- 75.14\n",
      "Episode length: 130.60 +/- 75.14\n",
      "Eval num_timesteps=81920, episode_reward=-140.90 +/- 121.18\n",
      "Episode length: 141.80 +/- 120.89\n",
      "Eval num_timesteps=81920, episode_reward=-128.80 +/- 37.28\n",
      "Episode length: 129.80 +/- 37.28\n",
      "Eval num_timesteps=81920, episode_reward=-96.90 +/- 10.72\n",
      "Episode length: 97.90 +/- 10.72\n",
      "New best mean reward!\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00057 |       0.00000 |      72.31779 |       0.00150 |       0.75405\n",
      "     -0.00291 |       0.00000 |      68.27941 |       0.00350 |       0.74121\n",
      "     -0.00364 |       0.00000 |      67.16126 |       0.00435 |       0.73099\n",
      "     -0.00377 |       0.00000 |      66.94232 |       0.00398 |       0.73792\n",
      "     -0.00469 |       0.00000 |      66.00922 |       0.00537 |       0.72858\n",
      "     -0.00423 |       0.00000 |      65.77263 |       0.00546 |       0.72516\n",
      "     -0.00480 |       0.00000 |      65.43728 |       0.00573 |       0.72459\n",
      "     -0.00433 |       0.00000 |      65.15104 |       0.00427 |       0.73270\n",
      "     -0.00527 |       0.00000 |      64.78956 |       0.00546 |       0.72126\n",
      "     -0.00512 |       0.00000 |      64.55978 |       0.00497 |       0.72889\n",
      "Evaluating losses...\n",
      "     -0.00596 |       0.00000 |      63.91943 |       0.00655 |       0.73169\n",
      "----------------------------------\n",
      "| EpLenMean       | 231          |\n",
      "| EpRewMean       | -230         |\n",
      "| EpThisIter      | 28           |\n",
      "| EpisodesSoFar   | 226          |\n",
      "| TimeElapsed     | 429          |\n",
      "| TimestepsSoFar  | 86016        |\n",
      "| ev_tdlam_before | 0.859        |\n",
      "| loss_ent        | 0.73169315   |\n",
      "| loss_kl         | 0.006546186  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005958724 |\n",
      "| loss_vf_loss    | 63.919426    |\n",
      "----------------------------------\n",
      "********** Iteration 21 ************\n",
      "Eval num_timesteps=86016, episode_reward=-119.70 +/- 55.92\n",
      "Episode length: 120.70 +/- 55.92\n",
      "Eval num_timesteps=86016, episode_reward=-110.70 +/- 32.14\n",
      "Episode length: 111.70 +/- 32.14\n",
      "Eval num_timesteps=86016, episode_reward=-99.90 +/- 21.11\n",
      "Episode length: 100.90 +/- 21.11\n",
      "Eval num_timesteps=86016, episode_reward=-106.90 +/- 19.82\n",
      "Episode length: 107.90 +/- 19.82\n",
      "Eval num_timesteps=86016, episode_reward=-98.30 +/- 20.32\n",
      "Episode length: 99.30 +/- 20.32\n",
      "Eval num_timesteps=86016, episode_reward=-113.20 +/- 41.12\n",
      "Episode length: 114.20 +/- 41.12\n",
      "Eval num_timesteps=86016, episode_reward=-91.80 +/- 13.08\n",
      "Episode length: 92.80 +/- 13.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86016, episode_reward=-97.10 +/- 23.86\n",
      "Episode length: 98.10 +/- 23.86\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00495 |       0.00000 |      71.27611 |       0.00366 |       0.70003\n",
      "     -0.00743 |       0.00000 |      67.32857 |       0.00752 |       0.67199\n",
      "     -0.00857 |       0.00000 |      64.98991 |       0.00727 |       0.67289\n",
      "     -0.00844 |       0.00000 |      63.70869 |       0.00704 |       0.67412\n",
      "     -0.00921 |       0.00000 |      62.53046 |       0.00844 |       0.66503\n",
      "     -0.00954 |       0.00000 |      61.33287 |       0.00792 |       0.66907\n",
      "     -0.01000 |       0.00000 |      60.55754 |       0.00919 |       0.66658\n",
      "     -0.00906 |       0.00000 |      59.95622 |       0.00782 |       0.66897\n",
      "     -0.01033 |       0.00000 |      59.21263 |       0.00800 |       0.66950\n",
      "     -0.01037 |       0.00000 |      58.73362 |       0.00862 |       0.66718\n",
      "Evaluating losses...\n",
      "     -0.01147 |       0.00000 |      58.25672 |       0.00806 |       0.66735\n",
      "----------------------------------\n",
      "| EpLenMean       | 176          |\n",
      "| EpRewMean       | -175         |\n",
      "| EpThisIter      | 27           |\n",
      "| EpisodesSoFar   | 253          |\n",
      "| TimeElapsed     | 437          |\n",
      "| TimestepsSoFar  | 90112        |\n",
      "| ev_tdlam_before | 0.843        |\n",
      "| loss_ent        | 0.66734654   |\n",
      "| loss_kl         | 0.008057704  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.011474779 |\n",
      "| loss_vf_loss    | 58.256718    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 22 ************\n",
      "Eval num_timesteps=90112, episode_reward=-86.40 +/- 12.24\n",
      "Episode length: 87.40 +/- 12.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90112, episode_reward=-112.90 +/- 59.36\n",
      "Episode length: 113.90 +/- 59.36\n",
      "Eval num_timesteps=90112, episode_reward=-147.20 +/- 128.79\n",
      "Episode length: 148.10 +/- 128.52\n",
      "Eval num_timesteps=90112, episode_reward=-90.60 +/- 16.64\n",
      "Episode length: 91.60 +/- 16.64\n",
      "Eval num_timesteps=90112, episode_reward=-109.80 +/- 44.88\n",
      "Episode length: 110.80 +/- 44.88\n",
      "Eval num_timesteps=90112, episode_reward=-88.00 +/- 10.85\n",
      "Episode length: 89.00 +/- 10.85\n",
      "Eval num_timesteps=90112, episode_reward=-89.10 +/- 23.78\n",
      "Episode length: 90.10 +/- 23.78\n",
      "Eval num_timesteps=90112, episode_reward=-97.70 +/- 20.92\n",
      "Episode length: 98.70 +/- 20.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00163 |       0.00000 |      65.67307 |       0.00274 |       0.62807\n",
      "     -0.00403 |       0.00000 |      58.62514 |       0.00302 |       0.62450\n",
      "     -0.00501 |       0.00000 |      56.41529 |       0.00390 |       0.62436\n",
      "     -0.00585 |       0.00000 |      55.41789 |       0.00364 |       0.62341\n",
      "     -0.00563 |       0.00000 |      54.43526 |       0.00536 |       0.62060\n",
      "     -0.00593 |       0.00000 |      54.21024 |       0.00484 |       0.62071\n",
      "     -0.00655 |       0.00000 |      53.40757 |       0.00479 |       0.62175\n",
      "     -0.00645 |       0.00000 |      52.94986 |       0.00602 |       0.61475\n",
      "     -0.00703 |       0.00000 |      52.66110 |       0.00493 |       0.62058\n",
      "     -0.00723 |       0.00000 |      52.40764 |       0.00551 |       0.61945\n",
      "Evaluating losses...\n",
      "     -0.00753 |       0.00000 |      51.94690 |       0.00381 |       0.63043\n",
      "-----------------------------------\n",
      "| EpLenMean       | 158           |\n",
      "| EpRewMean       | -157          |\n",
      "| EpThisIter      | 28            |\n",
      "| EpisodesSoFar   | 281           |\n",
      "| TimeElapsed     | 446           |\n",
      "| TimestepsSoFar  | 94208         |\n",
      "| ev_tdlam_before | 0.824         |\n",
      "| loss_ent        | 0.63043207    |\n",
      "| loss_kl         | 0.0038108919  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0075305765 |\n",
      "| loss_vf_loss    | 51.9469       |\n",
      "-----------------------------------\n",
      "********** Iteration 23 ************\n",
      "Eval num_timesteps=94208, episode_reward=-88.40 +/- 30.55\n",
      "Episode length: 89.40 +/- 30.55\n",
      "Eval num_timesteps=94208, episode_reward=-91.00 +/- 23.73\n",
      "Episode length: 92.00 +/- 23.73\n",
      "Eval num_timesteps=94208, episode_reward=-84.00 +/- 25.36\n",
      "Episode length: 85.00 +/- 25.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94208, episode_reward=-78.10 +/- 12.30\n",
      "Episode length: 79.10 +/- 12.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94208, episode_reward=-93.70 +/- 30.34\n",
      "Episode length: 94.70 +/- 30.34\n",
      "Eval num_timesteps=94208, episode_reward=-80.30 +/- 21.58\n",
      "Episode length: 81.30 +/- 21.58\n",
      "Eval num_timesteps=94208, episode_reward=-80.50 +/- 9.32\n",
      "Episode length: 81.50 +/- 9.32\n",
      "Eval num_timesteps=94208, episode_reward=-79.60 +/- 15.58\n",
      "Episode length: 80.60 +/- 15.58\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00018 |       0.00000 |      67.54493 |       0.00173 |       0.61552\n",
      "     -0.00269 |       0.00000 |      65.67570 |       0.00211 |       0.60426\n",
      "     -0.00306 |       0.00000 |      64.75227 |       0.00322 |       0.59716\n",
      "     -0.00371 |       0.00000 |      64.06355 |       0.00308 |       0.60074\n",
      "     -0.00368 |       0.00000 |      63.73587 |       0.00299 |       0.60384\n",
      "     -0.00443 |       0.00000 |      63.20181 |       0.00328 |       0.60301\n",
      "     -0.00456 |       0.00000 |      62.75099 |       0.00365 |       0.60149\n",
      "     -0.00510 |       0.00000 |      62.46344 |       0.00336 |       0.59740\n",
      "     -0.00515 |       0.00000 |      62.20658 |       0.00367 |       0.60203\n",
      "     -0.00511 |       0.00000 |      61.83293 |       0.00378 |       0.59777\n",
      "Evaluating losses...\n",
      "     -0.00608 |       0.00000 |      61.27546 |       0.00339 |       0.60210\n",
      "-----------------------------------\n",
      "| EpLenMean       | 146           |\n",
      "| EpRewMean       | -145          |\n",
      "| EpThisIter      | 29            |\n",
      "| EpisodesSoFar   | 310           |\n",
      "| TimeElapsed     | 454           |\n",
      "| TimestepsSoFar  | 98304         |\n",
      "| ev_tdlam_before | 0.828         |\n",
      "| loss_ent        | 0.60209537    |\n",
      "| loss_kl         | 0.0033854402  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0060849288 |\n",
      "| loss_vf_loss    | 61.275463     |\n",
      "-----------------------------------\n",
      "********** Iteration 24 ************\n",
      "Eval num_timesteps=98304, episode_reward=-81.50 +/- 18.38\n",
      "Episode length: 82.50 +/- 18.38\n",
      "Eval num_timesteps=98304, episode_reward=-75.30 +/- 9.35\n",
      "Episode length: 76.30 +/- 9.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98304, episode_reward=-79.00 +/- 10.85\n",
      "Episode length: 80.00 +/- 10.85\n",
      "Eval num_timesteps=98304, episode_reward=-82.20 +/- 24.07\n",
      "Episode length: 83.20 +/- 24.07\n",
      "Eval num_timesteps=98304, episode_reward=-86.90 +/- 33.51\n",
      "Episode length: 87.90 +/- 33.51\n",
      "Eval num_timesteps=98304, episode_reward=-79.00 +/- 11.51\n",
      "Episode length: 80.00 +/- 11.51\n",
      "Eval num_timesteps=98304, episode_reward=-76.10 +/- 9.71\n",
      "Episode length: 77.10 +/- 9.71\n",
      "Eval num_timesteps=98304, episode_reward=-119.60 +/- 60.42\n",
      "Episode length: 120.60 +/- 60.42\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00115 |       0.00000 |      58.08091 |       0.00151 |       0.58633\n",
      "     -0.00495 |       0.00000 |      56.12344 |       0.00318 |       0.56639\n",
      "     -0.00606 |       0.00000 |      55.09511 |       0.00417 |       0.56089\n",
      "     -0.00505 |       0.00000 |      54.49690 |       0.00419 |       0.55952\n",
      "     -0.00626 |       0.00000 |      53.87285 |       0.00384 |       0.56427\n",
      "     -0.00691 |       0.00000 |      53.33945 |       0.00382 |       0.56614\n",
      "     -0.00665 |       0.00000 |      53.08710 |       0.00442 |       0.56136\n",
      "     -0.00687 |       0.00000 |      52.51968 |       0.00478 |       0.56064\n",
      "     -0.00766 |       0.00000 |      52.35576 |       0.00440 |       0.56312\n",
      "     -0.00749 |       0.00000 |      51.96252 |       0.00500 |       0.56175\n",
      "Evaluating losses...\n",
      "     -0.00785 |       0.00000 |      51.37418 |       0.00578 |       0.55462\n",
      "----------------------------------\n",
      "| EpLenMean       | 139          |\n",
      "| EpRewMean       | -138         |\n",
      "| EpThisIter      | 33           |\n",
      "| EpisodesSoFar   | 343          |\n",
      "| TimeElapsed     | 461          |\n",
      "| TimestepsSoFar  | 102400       |\n",
      "| ev_tdlam_before | 0.855        |\n",
      "| loss_ent        | 0.55461615   |\n",
      "| loss_kl         | 0.0057795364 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.007846164 |\n",
      "| loss_vf_loss    | 51.374184    |\n",
      "----------------------------------\n",
      "********** Iteration 25 ************\n",
      "Eval num_timesteps=102400, episode_reward=-75.00 +/- 11.30\n",
      "Episode length: 76.00 +/- 11.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102400, episode_reward=-99.20 +/- 52.65\n",
      "Episode length: 100.20 +/- 52.65\n",
      "Eval num_timesteps=102400, episode_reward=-76.40 +/- 12.54\n",
      "Episode length: 77.40 +/- 12.54\n",
      "Eval num_timesteps=102400, episode_reward=-79.30 +/- 9.80\n",
      "Episode length: 80.30 +/- 9.80\n",
      "Eval num_timesteps=102400, episode_reward=-78.40 +/- 10.31\n",
      "Episode length: 79.40 +/- 10.31\n",
      "Eval num_timesteps=102400, episode_reward=-104.80 +/- 49.82\n",
      "Episode length: 105.80 +/- 49.82\n",
      "Eval num_timesteps=102400, episode_reward=-77.40 +/- 13.62\n",
      "Episode length: 78.40 +/- 13.62\n",
      "Eval num_timesteps=102400, episode_reward=-81.70 +/- 18.16\n",
      "Episode length: 82.70 +/- 18.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00044 |       0.00000 |      74.02960 |       0.00160 |       0.54246\n",
      "     -0.00303 |       0.00000 |      67.94772 |       0.00242 |       0.52937\n",
      "     -0.00368 |       0.00000 |      66.15121 |       0.00359 |       0.54062\n",
      "     -0.00419 |       0.00000 |      65.15527 |       0.00456 |       0.51955\n",
      "     -0.00509 |       0.00000 |      64.30662 |       0.00418 |       0.53266\n",
      "     -0.00541 |       0.00000 |      63.95998 |       0.00401 |       0.52617\n",
      "     -0.00631 |       0.00000 |      63.05457 |       0.00461 |       0.52296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00666 |       0.00000 |      62.71004 |       0.00452 |       0.52829\n",
      "     -0.00700 |       0.00000 |      62.26494 |       0.00414 |       0.51903\n",
      "     -0.00745 |       0.00000 |      61.77905 |       0.00526 |       0.52435\n",
      "Evaluating losses...\n",
      "     -0.00838 |       0.00000 |      61.32778 |       0.00450 |       0.51959\n",
      "----------------------------------\n",
      "| EpLenMean       | 135          |\n",
      "| EpRewMean       | -134         |\n",
      "| EpThisIter      | 31           |\n",
      "| EpisodesSoFar   | 374          |\n",
      "| TimeElapsed     | 468          |\n",
      "| TimestepsSoFar  | 106496       |\n",
      "| ev_tdlam_before | 0.789        |\n",
      "| loss_ent        | 0.5195855    |\n",
      "| loss_kl         | 0.004497221  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.008375313 |\n",
      "| loss_vf_loss    | 61.32778     |\n",
      "----------------------------------\n",
      "********** Iteration 26 ************\n",
      "Eval num_timesteps=106496, episode_reward=-101.60 +/- 39.00\n",
      "Episode length: 102.60 +/- 39.00\n",
      "Eval num_timesteps=106496, episode_reward=-94.90 +/- 28.78\n",
      "Episode length: 95.90 +/- 28.78\n",
      "Eval num_timesteps=106496, episode_reward=-77.20 +/- 11.40\n",
      "Episode length: 78.20 +/- 11.40\n",
      "Eval num_timesteps=106496, episode_reward=-84.50 +/- 17.52\n",
      "Episode length: 85.50 +/- 17.52\n",
      "Eval num_timesteps=106496, episode_reward=-87.10 +/- 27.27\n",
      "Episode length: 88.10 +/- 27.27\n",
      "Eval num_timesteps=106496, episode_reward=-87.00 +/- 16.66\n",
      "Episode length: 88.00 +/- 16.66\n",
      "Eval num_timesteps=106496, episode_reward=-94.10 +/- 30.61\n",
      "Episode length: 95.10 +/- 30.61\n",
      "Eval num_timesteps=106496, episode_reward=-96.60 +/- 26.48\n",
      "Episode length: 97.60 +/- 26.48\n",
      "Eval num_timesteps=106496, episode_reward=-90.40 +/- 19.14\n",
      "Episode length: 91.40 +/- 19.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00011 |       0.00000 |      55.16575 |       0.00076 |       0.52580\n",
      "     -0.00236 |       0.00000 |      51.01049 |       0.00226 |       0.50117\n",
      "     -0.00217 |       0.00000 |      49.28699 |       0.00231 |       0.49932\n",
      "     -0.00380 |       0.00000 |      48.26013 |       0.00256 |       0.50062\n",
      "     -0.00371 |       0.00000 |      47.32147 |       0.00293 |       0.50101\n",
      "     -0.00441 |       0.00000 |      46.76705 |       0.00337 |       0.49726\n",
      "     -0.00451 |       0.00000 |      46.15488 |       0.00330 |       0.49916\n",
      "     -0.00504 |       0.00000 |      45.99400 |       0.00341 |       0.49644\n",
      "     -0.00501 |       0.00000 |      45.55265 |       0.00303 |       0.50287\n",
      "     -0.00542 |       0.00000 |      45.11900 |       0.00345 |       0.50138\n",
      "Evaluating losses...\n",
      "     -0.00594 |       0.00000 |      44.85167 |       0.00304 |       0.51015\n",
      "-----------------------------------\n",
      "| EpLenMean       | 122           |\n",
      "| EpRewMean       | -121          |\n",
      "| EpThisIter      | 36            |\n",
      "| EpisodesSoFar   | 410           |\n",
      "| TimeElapsed     | 475           |\n",
      "| TimestepsSoFar  | 110592        |\n",
      "| ev_tdlam_before | 0.84          |\n",
      "| loss_ent        | 0.5101534     |\n",
      "| loss_kl         | 0.0030402     |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0059416634 |\n",
      "| loss_vf_loss    | 44.85167      |\n",
      "-----------------------------------\n",
      "********** Iteration 27 ************\n",
      "Eval num_timesteps=110592, episode_reward=-76.50 +/- 9.29\n",
      "Episode length: 77.50 +/- 9.29\n",
      "Eval num_timesteps=110592, episode_reward=-81.70 +/- 9.52\n",
      "Episode length: 82.70 +/- 9.52\n",
      "Eval num_timesteps=110592, episode_reward=-99.60 +/- 48.10\n",
      "Episode length: 100.60 +/- 48.10\n",
      "Eval num_timesteps=110592, episode_reward=-99.50 +/- 49.09\n",
      "Episode length: 100.50 +/- 49.09\n",
      "Eval num_timesteps=110592, episode_reward=-95.20 +/- 34.18\n",
      "Episode length: 96.20 +/- 34.18\n",
      "Eval num_timesteps=110592, episode_reward=-85.60 +/- 29.51\n",
      "Episode length: 86.60 +/- 29.51\n",
      "Eval num_timesteps=110592, episode_reward=-80.60 +/- 10.23\n",
      "Episode length: 81.60 +/- 10.23\n",
      "Eval num_timesteps=110592, episode_reward=-97.40 +/- 35.92\n",
      "Episode length: 98.40 +/- 35.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00025 |       0.00000 |      45.26387 |       0.00055 |       0.49354\n",
      "     -0.00164 |       0.00000 |      43.23772 |       0.00144 |       0.47160\n",
      "     -0.00255 |       0.00000 |      42.60367 |       0.00185 |       0.47064\n",
      "     -0.00329 |       0.00000 |      42.11247 |       0.00199 |       0.47544\n",
      "     -0.00370 |       0.00000 |      41.57631 |       0.00228 |       0.47308\n",
      "     -0.00425 |       0.00000 |      41.22748 |       0.00274 |       0.46573\n",
      "     -0.00434 |       0.00000 |      41.09272 |       0.00294 |       0.46899\n",
      "     -0.00431 |       0.00000 |      40.86747 |       0.00296 |       0.47209\n",
      "     -0.00489 |       0.00000 |      40.61338 |       0.00316 |       0.47040\n",
      "     -0.00456 |       0.00000 |      40.40897 |       0.00335 |       0.47000\n",
      "Evaluating losses...\n",
      "     -0.00596 |       0.00000 |      40.18046 |       0.00290 |       0.47209\n",
      "----------------------------------\n",
      "| EpLenMean       | 115          |\n",
      "| EpRewMean       | -114         |\n",
      "| EpThisIter      | 38           |\n",
      "| EpisodesSoFar   | 448          |\n",
      "| TimeElapsed     | 482          |\n",
      "| TimestepsSoFar  | 114688       |\n",
      "| ev_tdlam_before | 0.872        |\n",
      "| loss_ent        | 0.47208607   |\n",
      "| loss_kl         | 0.0029019078 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.00596384  |\n",
      "| loss_vf_loss    | 40.180458    |\n",
      "----------------------------------\n",
      "********** Iteration 28 ************\n",
      "Eval num_timesteps=114688, episode_reward=-98.10 +/- 37.64\n",
      "Episode length: 99.10 +/- 37.64\n",
      "Eval num_timesteps=114688, episode_reward=-86.90 +/- 16.98\n",
      "Episode length: 87.90 +/- 16.98\n",
      "Eval num_timesteps=114688, episode_reward=-90.10 +/- 30.07\n",
      "Episode length: 91.10 +/- 30.07\n",
      "Eval num_timesteps=114688, episode_reward=-120.80 +/- 126.69\n",
      "Episode length: 121.70 +/- 126.39\n",
      "Eval num_timesteps=114688, episode_reward=-89.10 +/- 18.05\n",
      "Episode length: 90.10 +/- 18.05\n",
      "Eval num_timesteps=114688, episode_reward=-156.30 +/- 123.44\n",
      "Episode length: 157.20 +/- 123.16\n",
      "Eval num_timesteps=114688, episode_reward=-85.40 +/- 16.98\n",
      "Episode length: 86.40 +/- 16.98\n",
      "Eval num_timesteps=114688, episode_reward=-76.00 +/- 7.22\n",
      "Episode length: 77.00 +/- 7.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00014 |       0.00000 |      51.30941 |       0.00090 |       0.46374\n",
      "     -0.00176 |       0.00000 |      46.99244 |       0.00232 |       0.45413\n",
      "     -0.00265 |       0.00000 |      45.87357 |       0.00236 |       0.45502\n",
      "     -0.00303 |       0.00000 |      45.31826 |       0.00249 |       0.45392\n",
      "     -0.00306 |       0.00000 |      44.95144 |       0.00268 |       0.44959\n",
      "     -0.00349 |       0.00000 |      44.55204 |       0.00240 |       0.44735\n",
      "     -0.00395 |       0.00000 |      44.20308 |       0.00252 |       0.45174\n",
      "     -0.00353 |       0.00000 |      43.88662 |       0.00287 |       0.44590\n",
      "     -0.00444 |       0.00000 |      43.72314 |       0.00271 |       0.44506\n",
      "     -0.00411 |       0.00000 |      43.49077 |       0.00276 |       0.44183\n",
      "Evaluating losses...\n",
      "     -0.00540 |       0.00000 |      42.97682 |       0.00218 |       0.44566\n",
      "----------------------------------\n",
      "| EpLenMean       | 108          |\n",
      "| EpRewMean       | -107         |\n",
      "| EpThisIter      | 39           |\n",
      "| EpisodesSoFar   | 487          |\n",
      "| TimeElapsed     | 489          |\n",
      "| TimestepsSoFar  | 118784       |\n",
      "| ev_tdlam_before | 0.824        |\n",
      "| loss_ent        | 0.44566292   |\n",
      "| loss_kl         | 0.0021764624 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005399743 |\n",
      "| loss_vf_loss    | 42.97682     |\n",
      "----------------------------------\n",
      "********** Iteration 29 ************\n",
      "Eval num_timesteps=118784, episode_reward=-97.90 +/- 36.43\n",
      "Episode length: 98.90 +/- 36.43\n",
      "Eval num_timesteps=118784, episode_reward=-80.70 +/- 9.34\n",
      "Episode length: 81.70 +/- 9.34\n",
      "Eval num_timesteps=118784, episode_reward=-98.90 +/- 33.67\n",
      "Episode length: 99.90 +/- 33.67\n",
      "Eval num_timesteps=118784, episode_reward=-78.30 +/- 10.39\n",
      "Episode length: 79.30 +/- 10.39\n",
      "Eval num_timesteps=118784, episode_reward=-89.10 +/- 29.15\n",
      "Episode length: 90.10 +/- 29.15\n",
      "Eval num_timesteps=118784, episode_reward=-84.00 +/- 12.88\n",
      "Episode length: 85.00 +/- 12.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=118784, episode_reward=-113.00 +/- 54.91\n",
      "Episode length: 114.00 +/- 54.91\n",
      "Eval num_timesteps=118784, episode_reward=-107.00 +/- 57.62\n",
      "Episode length: 108.00 +/- 57.62\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00020 |       0.00000 |      49.52408 |       0.00067 |       0.42662\n",
      "     -0.00169 |       0.00000 |      44.28245 |       0.00176 |       0.41617\n",
      "     -0.00193 |       0.00000 |      42.48097 |       0.00188 |       0.41407\n",
      "     -0.00249 |       0.00000 |      41.49489 |       0.00271 |       0.40766\n",
      "     -0.00269 |       0.00000 |      40.84729 |       0.00229 |       0.41496\n",
      "     -0.00346 |       0.00000 |      40.36279 |       0.00397 |       0.40566\n",
      "     -0.00306 |       0.00000 |      39.93815 |       0.00255 |       0.41437\n",
      "     -0.00367 |       0.00000 |      39.70503 |       0.00352 |       0.40267\n",
      "     -0.00374 |       0.00000 |      39.35730 |       0.00310 |       0.40816\n",
      "     -0.00417 |       0.00000 |      39.14217 |       0.00331 |       0.40743\n",
      "Evaluating losses...\n",
      "     -0.00458 |       0.00000 |      38.91206 |       0.00291 |       0.40667\n",
      "-----------------------------------\n",
      "| EpLenMean       | 109           |\n",
      "| EpRewMean       | -108          |\n",
      "| EpThisIter      | 36            |\n",
      "| EpisodesSoFar   | 523           |\n",
      "| TimeElapsed     | 495           |\n",
      "| TimestepsSoFar  | 122880        |\n",
      "| ev_tdlam_before | 0.815         |\n",
      "| loss_ent        | 0.40667066    |\n",
      "| loss_kl         | 0.002914546   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0045839343 |\n",
      "| loss_vf_loss    | 38.912056     |\n",
      "-----------------------------------\n",
      "********** Iteration 30 ************\n",
      "Eval num_timesteps=122880, episode_reward=-87.70 +/- 33.70\n",
      "Episode length: 88.70 +/- 33.70\n",
      "Eval num_timesteps=122880, episode_reward=-88.80 +/- 18.88\n",
      "Episode length: 89.80 +/- 18.88\n",
      "Eval num_timesteps=122880, episode_reward=-83.30 +/- 11.75\n",
      "Episode length: 84.30 +/- 11.75\n",
      "Eval num_timesteps=122880, episode_reward=-82.10 +/- 16.43\n",
      "Episode length: 83.10 +/- 16.43\n",
      "Eval num_timesteps=122880, episode_reward=-96.70 +/- 30.80\n",
      "Episode length: 97.70 +/- 30.80\n",
      "Eval num_timesteps=122880, episode_reward=-82.80 +/- 13.68\n",
      "Episode length: 83.80 +/- 13.68\n",
      "Eval num_timesteps=122880, episode_reward=-86.90 +/- 8.80\n",
      "Episode length: 87.90 +/- 8.80\n",
      "Eval num_timesteps=122880, episode_reward=-83.00 +/- 8.69\n",
      "Episode length: 84.00 +/- 8.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00086 |       0.00000 |      60.14125 |       0.00072 |       0.41977\n",
      "     -0.00150 |       0.00000 |      54.27612 |       0.00154 |       0.41141\n",
      "     -0.00245 |       0.00000 |      52.51985 |       0.00226 |       0.41319\n",
      "     -0.00258 |       0.00000 |      51.46885 |       0.00237 |       0.40819\n",
      "     -0.00382 |       0.00000 |      50.78693 |       0.00270 |       0.41145\n",
      "     -0.00395 |       0.00000 |      50.45251 |       0.00268 |       0.40609\n",
      "     -0.00383 |       0.00000 |      50.08369 |       0.00285 |       0.41133\n",
      "     -0.00467 |       0.00000 |      49.66169 |       0.00301 |       0.40808\n",
      "     -0.00477 |       0.00000 |      49.59206 |       0.00390 |       0.40234\n",
      "     -0.00493 |       0.00000 |      49.25415 |       0.00339 |       0.40686\n",
      "Evaluating losses...\n",
      "     -0.00593 |       0.00000 |      48.92686 |       0.00309 |       0.40326\n",
      "----------------------------------\n",
      "| EpLenMean       | 109          |\n",
      "| EpRewMean       | -108         |\n",
      "| EpThisIter      | 38           |\n",
      "| EpisodesSoFar   | 561          |\n",
      "| TimeElapsed     | 502          |\n",
      "| TimestepsSoFar  | 126976       |\n",
      "| ev_tdlam_before | 0.78         |\n",
      "| loss_ent        | 0.403262     |\n",
      "| loss_kl         | 0.0030931896 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005934247 |\n",
      "| loss_vf_loss    | 48.926857    |\n",
      "----------------------------------\n",
      "********** Iteration 31 ************\n",
      "Eval num_timesteps=126976, episode_reward=-84.30 +/- 7.62\n",
      "Episode length: 85.30 +/- 7.62\n",
      "Eval num_timesteps=126976, episode_reward=-88.30 +/- 18.03\n",
      "Episode length: 89.30 +/- 18.03\n",
      "Eval num_timesteps=126976, episode_reward=-86.00 +/- 8.21\n",
      "Episode length: 87.00 +/- 8.21\n",
      "Eval num_timesteps=126976, episode_reward=-123.10 +/- 80.19\n",
      "Episode length: 124.10 +/- 80.19\n",
      "Eval num_timesteps=126976, episode_reward=-113.10 +/- 50.83\n",
      "Episode length: 114.10 +/- 50.83\n",
      "Eval num_timesteps=126976, episode_reward=-84.40 +/- 13.48\n",
      "Episode length: 85.40 +/- 13.48\n",
      "Eval num_timesteps=126976, episode_reward=-84.30 +/- 8.66\n",
      "Episode length: 85.30 +/- 8.66\n",
      "Eval num_timesteps=126976, episode_reward=-84.00 +/- 13.36\n",
      "Episode length: 85.00 +/- 13.36\n",
      "Eval num_timesteps=126976, episode_reward=-90.20 +/- 22.52\n",
      "Episode length: 91.20 +/- 22.52\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.32e-05 |       0.00000 |      62.45548 |       0.00030 |       0.38599\n",
      "     -0.00201 |       0.00000 |      59.70929 |       0.00174 |       0.36668\n",
      "     -0.00292 |       0.00000 |      58.86190 |       0.00201 |       0.36810\n",
      "     -0.00323 |       0.00000 |      58.28955 |       0.00222 |       0.36406\n",
      "     -0.00352 |       0.00000 |      57.99272 |       0.00291 |       0.36153\n",
      "     -0.00404 |       0.00000 |      57.63765 |       0.00279 |       0.36286\n",
      "     -0.00421 |       0.00000 |      57.48659 |       0.00299 |       0.36287\n",
      "     -0.00427 |       0.00000 |      57.19559 |       0.00278 |       0.36701\n",
      "     -0.00400 |       0.00000 |      57.10922 |       0.00404 |       0.35575\n",
      "     -0.00457 |       0.00000 |      56.78957 |       0.00270 |       0.36434\n",
      "Evaluating losses...\n",
      "     -0.00516 |       0.00000 |      56.44283 |       0.00330 |       0.36103\n",
      "----------------------------------\n",
      "| EpLenMean       | 109          |\n",
      "| EpRewMean       | -108         |\n",
      "| EpThisIter      | 37           |\n",
      "| EpisodesSoFar   | 598          |\n",
      "| TimeElapsed     | 509          |\n",
      "| TimestepsSoFar  | 131072       |\n",
      "| ev_tdlam_before | 0.756        |\n",
      "| loss_ent        | 0.36102682   |\n",
      "| loss_kl         | 0.00330372   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005156043 |\n",
      "| loss_vf_loss    | 56.44283     |\n",
      "----------------------------------\n",
      "********** Iteration 32 ************\n",
      "Eval num_timesteps=131072, episode_reward=-76.70 +/- 9.25\n",
      "Episode length: 77.70 +/- 9.25\n",
      "Eval num_timesteps=131072, episode_reward=-100.00 +/- 26.61\n",
      "Episode length: 101.00 +/- 26.61\n",
      "Eval num_timesteps=131072, episode_reward=-82.50 +/- 11.00\n",
      "Episode length: 83.50 +/- 11.00\n",
      "Eval num_timesteps=131072, episode_reward=-84.90 +/- 13.72\n",
      "Episode length: 85.90 +/- 13.72\n",
      "Eval num_timesteps=131072, episode_reward=-88.80 +/- 17.89\n",
      "Episode length: 89.80 +/- 17.89\n",
      "Eval num_timesteps=131072, episode_reward=-94.20 +/- 23.05\n",
      "Episode length: 95.20 +/- 23.05\n",
      "Eval num_timesteps=131072, episode_reward=-80.80 +/- 12.41\n",
      "Episode length: 81.80 +/- 12.41\n",
      "Eval num_timesteps=131072, episode_reward=-90.80 +/- 22.22\n",
      "Episode length: 91.80 +/- 22.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00016 |       0.00000 |      49.36722 |       0.00096 |       0.35836\n",
      "     -0.00078 |       0.00000 |      47.08034 |       0.00071 |       0.36288\n",
      "     -0.00224 |       0.00000 |      46.20980 |       0.00143 |       0.35660\n",
      "     -0.00331 |       0.00000 |      45.64219 |       0.00181 |       0.35426\n",
      "     -0.00386 |       0.00000 |      45.28122 |       0.00210 |       0.35709\n",
      "     -0.00422 |       0.00000 |      45.07370 |       0.00228 |       0.35405\n",
      "     -0.00484 |       0.00000 |      44.87445 |       0.00275 |       0.35703\n",
      "     -0.00501 |       0.00000 |      44.54655 |       0.00267 |       0.35822\n",
      "     -0.00527 |       0.00000 |      44.40786 |       0.00261 |       0.35751\n",
      "     -0.00569 |       0.00000 |      44.26640 |       0.00272 |       0.35725\n",
      "Evaluating losses...\n",
      "     -0.00660 |       0.00000 |      43.83141 |       0.00269 |       0.35614\n",
      "----------------------------------\n",
      "| EpLenMean       | 105          |\n",
      "| EpRewMean       | -104         |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 639          |\n",
      "| TimeElapsed     | 517          |\n",
      "| TimestepsSoFar  | 135168       |\n",
      "| ev_tdlam_before | 0.829        |\n",
      "| loss_ent        | 0.3561433    |\n",
      "| loss_kl         | 0.0026856612 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.006602508 |\n",
      "| loss_vf_loss    | 43.831413    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 33 ************\n",
      "Eval num_timesteps=135168, episode_reward=-80.70 +/- 7.17\n",
      "Episode length: 81.70 +/- 7.17\n",
      "Eval num_timesteps=135168, episode_reward=-86.20 +/- 10.32\n",
      "Episode length: 87.20 +/- 10.32\n",
      "Eval num_timesteps=135168, episode_reward=-80.40 +/- 12.50\n",
      "Episode length: 81.40 +/- 12.50\n",
      "Eval num_timesteps=135168, episode_reward=-96.00 +/- 26.99\n",
      "Episode length: 97.00 +/- 26.99\n",
      "Eval num_timesteps=135168, episode_reward=-96.10 +/- 47.12\n",
      "Episode length: 97.10 +/- 47.12\n",
      "Eval num_timesteps=135168, episode_reward=-79.40 +/- 8.10\n",
      "Episode length: 80.40 +/- 8.10\n",
      "Eval num_timesteps=135168, episode_reward=-87.50 +/- 33.39\n",
      "Episode length: 88.50 +/- 33.39\n",
      "Eval num_timesteps=135168, episode_reward=-110.60 +/- 59.67\n",
      "Episode length: 111.60 +/- 59.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.46e-05 |       0.00000 |      46.38059 |       0.00039 |       0.34297\n",
      "     -0.00196 |       0.00000 |      45.32023 |       0.00117 |       0.34019\n",
      "     -0.00260 |       0.00000 |      44.84409 |       0.00168 |       0.34015\n",
      "     -0.00319 |       0.00000 |      44.61003 |       0.00176 |       0.33555\n",
      "     -0.00347 |       0.00000 |      44.16241 |       0.00179 |       0.33917\n",
      "     -0.00368 |       0.00000 |      44.08101 |       0.00206 |       0.34112\n",
      "     -0.00374 |       0.00000 |      43.92577 |       0.00215 |       0.33915\n",
      "     -0.00401 |       0.00000 |      43.56530 |       0.00216 |       0.33695\n",
      "     -0.00399 |       0.00000 |      43.60381 |       0.00197 |       0.34206\n",
      "     -0.00443 |       0.00000 |      43.34344 |       0.00202 |       0.34345\n",
      "Evaluating losses...\n",
      "     -0.00491 |       0.00000 |      43.02958 |       0.00191 |       0.34852\n",
      "-----------------------------------\n",
      "| EpLenMean       | 100           |\n",
      "| EpRewMean       | -99           |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 680           |\n",
      "| TimeElapsed     | 524           |\n",
      "| TimestepsSoFar  | 139264        |\n",
      "| ev_tdlam_before | 0.839         |\n",
      "| loss_ent        | 0.3485225     |\n",
      "| loss_kl         | 0.0019103214  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0049145715 |\n",
      "| loss_vf_loss    | 43.029583     |\n",
      "-----------------------------------\n",
      "********** Iteration 34 ************\n",
      "Eval num_timesteps=139264, episode_reward=-76.70 +/- 4.94\n",
      "Episode length: 77.70 +/- 4.94\n",
      "Eval num_timesteps=139264, episode_reward=-83.20 +/- 9.21\n",
      "Episode length: 84.20 +/- 9.21\n",
      "Eval num_timesteps=139264, episode_reward=-79.90 +/- 9.76\n",
      "Episode length: 80.90 +/- 9.76\n",
      "Eval num_timesteps=139264, episode_reward=-84.40 +/- 11.24\n",
      "Episode length: 85.40 +/- 11.24\n",
      "Eval num_timesteps=139264, episode_reward=-94.80 +/- 28.67\n",
      "Episode length: 95.80 +/- 28.67\n",
      "Eval num_timesteps=139264, episode_reward=-90.10 +/- 32.21\n",
      "Episode length: 91.10 +/- 32.21\n",
      "Eval num_timesteps=139264, episode_reward=-85.60 +/- 17.14\n",
      "Episode length: 86.60 +/- 17.14\n",
      "Eval num_timesteps=139264, episode_reward=-80.40 +/- 6.81\n",
      "Episode length: 81.40 +/- 6.81\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.93e-05 |       0.00000 |      44.23420 |       0.00054 |       0.35144\n",
      "     -0.00113 |       0.00000 |      41.60892 |       0.00139 |       0.34083\n",
      "     -0.00199 |       0.00000 |      40.71589 |       0.00180 |       0.34386\n",
      "     -0.00171 |       0.00000 |      40.34758 |       0.00158 |       0.34801\n",
      "     -0.00202 |       0.00000 |      39.93540 |       0.00170 |       0.33727\n",
      "     -0.00321 |       0.00000 |      39.66532 |       0.00216 |       0.34215\n",
      "     -0.00269 |       0.00000 |      39.41256 |       0.00174 |       0.34911\n",
      "     -0.00316 |       0.00000 |      39.21294 |       0.00156 |       0.34740\n",
      "     -0.00330 |       0.00000 |      38.85900 |       0.00206 |       0.34217\n",
      "     -0.00400 |       0.00000 |      38.78337 |       0.00206 |       0.34228\n",
      "Evaluating losses...\n",
      "     -0.00482 |       0.00000 |      38.55400 |       0.00248 |       0.33984\n",
      "-----------------------------------\n",
      "| EpLenMean       | 102           |\n",
      "| EpRewMean       | -101          |\n",
      "| EpThisIter      | 39            |\n",
      "| EpisodesSoFar   | 719           |\n",
      "| TimeElapsed     | 531           |\n",
      "| TimestepsSoFar  | 143360        |\n",
      "| ev_tdlam_before | 0.833         |\n",
      "| loss_ent        | 0.33983797    |\n",
      "| loss_kl         | 0.0024816915  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0048181424 |\n",
      "| loss_vf_loss    | 38.554        |\n",
      "-----------------------------------\n",
      "********** Iteration 35 ************\n",
      "Eval num_timesteps=143360, episode_reward=-102.80 +/- 41.17\n",
      "Episode length: 103.80 +/- 41.17\n",
      "Eval num_timesteps=143360, episode_reward=-80.10 +/- 6.89\n",
      "Episode length: 81.10 +/- 6.89\n",
      "Eval num_timesteps=143360, episode_reward=-81.30 +/- 5.29\n",
      "Episode length: 82.30 +/- 5.29\n",
      "Eval num_timesteps=143360, episode_reward=-98.50 +/- 24.37\n",
      "Episode length: 99.50 +/- 24.37\n",
      "Eval num_timesteps=143360, episode_reward=-87.70 +/- 13.87\n",
      "Episode length: 88.70 +/- 13.87\n",
      "Eval num_timesteps=143360, episode_reward=-81.70 +/- 11.50\n",
      "Episode length: 82.70 +/- 11.50\n",
      "Eval num_timesteps=143360, episode_reward=-89.70 +/- 18.31\n",
      "Episode length: 90.70 +/- 18.31\n",
      "Eval num_timesteps=143360, episode_reward=-80.60 +/- 7.95\n",
      "Episode length: 81.60 +/- 7.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00040 |       0.00000 |      41.51923 |       0.00079 |       0.32009\n",
      "     -0.00230 |       0.00000 |      40.71635 |       0.00144 |       0.31664\n",
      "     -0.00250 |       0.00000 |      40.34824 |       0.00165 |       0.31122\n",
      "     -0.00290 |       0.00000 |      39.93983 |       0.00172 |       0.31239\n",
      "     -0.00317 |       0.00000 |      39.58669 |       0.00178 |       0.31026\n",
      "     -0.00337 |       0.00000 |      39.28392 |       0.00154 |       0.31285\n",
      "     -0.00346 |       0.00000 |      39.09396 |       0.00173 |       0.31349\n",
      "     -0.00340 |       0.00000 |      38.90934 |       0.00210 |       0.31250\n",
      "     -0.00359 |       0.00000 |      38.79223 |       0.00176 |       0.31285\n",
      "     -0.00368 |       0.00000 |      38.47265 |       0.00208 |       0.31331\n",
      "Evaluating losses...\n",
      "     -0.00453 |       0.00000 |      38.13230 |       0.00176 |       0.31180\n",
      "-----------------------------------\n",
      "| EpLenMean       | 102           |\n",
      "| EpRewMean       | -101          |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 760           |\n",
      "| TimeElapsed     | 538           |\n",
      "| TimestepsSoFar  | 147456        |\n",
      "| ev_tdlam_before | 0.853         |\n",
      "| loss_ent        | 0.31180423    |\n",
      "| loss_kl         | 0.0017600881  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0045343144 |\n",
      "| loss_vf_loss    | 38.132298     |\n",
      "-----------------------------------\n",
      "********** Iteration 36 ************\n",
      "Eval num_timesteps=147456, episode_reward=-86.10 +/- 19.64\n",
      "Episode length: 87.10 +/- 19.64\n",
      "Eval num_timesteps=147456, episode_reward=-84.40 +/- 6.07\n",
      "Episode length: 85.40 +/- 6.07\n",
      "Eval num_timesteps=147456, episode_reward=-85.40 +/- 15.50\n",
      "Episode length: 86.40 +/- 15.50\n",
      "Eval num_timesteps=147456, episode_reward=-80.00 +/- 6.71\n",
      "Episode length: 81.00 +/- 6.71\n",
      "Eval num_timesteps=147456, episode_reward=-91.50 +/- 28.19\n",
      "Episode length: 92.50 +/- 28.19\n",
      "Eval num_timesteps=147456, episode_reward=-101.60 +/- 21.75\n",
      "Episode length: 102.60 +/- 21.75\n",
      "Eval num_timesteps=147456, episode_reward=-79.70 +/- 8.84\n",
      "Episode length: 80.70 +/- 8.84\n",
      "Eval num_timesteps=147456, episode_reward=-92.60 +/- 17.41\n",
      "Episode length: 93.60 +/- 17.41\n",
      "Eval num_timesteps=147456, episode_reward=-101.90 +/- 26.88\n",
      "Episode length: 102.90 +/- 26.88\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -4.75e-05 |       0.00000 |      44.20482 |       0.00094 |       0.30567\n",
      "     -0.00122 |       0.00000 |      42.55164 |       0.00136 |       0.30610\n",
      "     -0.00162 |       0.00000 |      41.98912 |       0.00140 |       0.30928\n",
      "     -0.00220 |       0.00000 |      41.47130 |       0.00193 |       0.30707\n",
      "     -0.00287 |       0.00000 |      41.00486 |       0.00129 |       0.31001\n",
      "     -0.00282 |       0.00000 |      40.79384 |       0.00202 |       0.30727\n",
      "     -0.00330 |       0.00000 |      40.43823 |       0.00218 |       0.30768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00296 |       0.00000 |      40.14505 |       0.00205 |       0.30780\n",
      "     -0.00394 |       0.00000 |      40.20996 |       0.00142 |       0.31526\n",
      "     -0.00379 |       0.00000 |      39.86070 |       0.00219 |       0.30921\n",
      "Evaluating losses...\n",
      "     -0.00492 |       0.00000 |      39.48935 |       0.00185 |       0.30734\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.5          |\n",
      "| EpRewMean       | -97.5         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 802           |\n",
      "| TimeElapsed     | 545           |\n",
      "| TimestepsSoFar  | 151552        |\n",
      "| ev_tdlam_before | 0.841         |\n",
      "| loss_ent        | 0.30733806    |\n",
      "| loss_kl         | 0.0018458668  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0049177753 |\n",
      "| loss_vf_loss    | 39.48935      |\n",
      "-----------------------------------\n",
      "********** Iteration 37 ************\n",
      "Eval num_timesteps=151552, episode_reward=-82.70 +/- 14.76\n",
      "Episode length: 83.70 +/- 14.76\n",
      "Eval num_timesteps=151552, episode_reward=-114.40 +/- 72.67\n",
      "Episode length: 115.40 +/- 72.67\n",
      "Eval num_timesteps=151552, episode_reward=-77.60 +/- 7.53\n",
      "Episode length: 78.60 +/- 7.53\n",
      "Eval num_timesteps=151552, episode_reward=-90.20 +/- 7.24\n",
      "Episode length: 91.20 +/- 7.24\n",
      "Eval num_timesteps=151552, episode_reward=-85.60 +/- 7.64\n",
      "Episode length: 86.60 +/- 7.64\n",
      "Eval num_timesteps=151552, episode_reward=-100.70 +/- 60.71\n",
      "Episode length: 101.70 +/- 60.71\n",
      "Eval num_timesteps=151552, episode_reward=-107.80 +/- 56.58\n",
      "Episode length: 108.80 +/- 56.58\n",
      "Eval num_timesteps=151552, episode_reward=-98.60 +/- 46.38\n",
      "Episode length: 99.60 +/- 46.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00143 |       0.00000 |      37.10701 |       0.00109 |       0.30581\n",
      "     -0.00231 |       0.00000 |      35.51231 |       0.00196 |       0.30647\n",
      "     -0.00247 |       0.00000 |      34.75629 |       0.00210 |       0.30361\n",
      "     -0.00288 |       0.00000 |      34.17871 |       0.00214 |       0.30746\n",
      "     -0.00328 |       0.00000 |      33.81734 |       0.00212 |       0.29826\n",
      "     -0.00291 |       0.00000 |      33.48698 |       0.00211 |       0.30546\n",
      "     -0.00335 |       0.00000 |      33.21758 |       0.00227 |       0.30429\n",
      "     -0.00363 |       0.00000 |      33.11981 |       0.00241 |       0.30120\n",
      "     -0.00392 |       0.00000 |      32.73245 |       0.00223 |       0.30297\n",
      "     -0.00379 |       0.00000 |      32.51579 |       0.00222 |       0.29842\n",
      "Evaluating losses...\n",
      "     -0.00467 |       0.00000 |      32.23861 |       0.00212 |       0.30249\n",
      "----------------------------------\n",
      "| EpLenMean       | 97.2         |\n",
      "| EpRewMean       | -96.2        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 847          |\n",
      "| TimeElapsed     | 552          |\n",
      "| TimestepsSoFar  | 155648       |\n",
      "| ev_tdlam_before | 0.872        |\n",
      "| loss_ent        | 0.3024928    |\n",
      "| loss_kl         | 0.002115307  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004673559 |\n",
      "| loss_vf_loss    | 32.23861     |\n",
      "----------------------------------\n",
      "********** Iteration 38 ************\n",
      "Eval num_timesteps=155648, episode_reward=-82.20 +/- 10.01\n",
      "Episode length: 83.20 +/- 10.01\n",
      "Eval num_timesteps=155648, episode_reward=-89.30 +/- 20.90\n",
      "Episode length: 90.30 +/- 20.90\n",
      "Eval num_timesteps=155648, episode_reward=-85.70 +/- 15.75\n",
      "Episode length: 86.70 +/- 15.75\n",
      "Eval num_timesteps=155648, episode_reward=-80.90 +/- 11.95\n",
      "Episode length: 81.90 +/- 11.95\n",
      "Eval num_timesteps=155648, episode_reward=-81.10 +/- 10.34\n",
      "Episode length: 82.10 +/- 10.34\n",
      "Eval num_timesteps=155648, episode_reward=-76.40 +/- 5.90\n",
      "Episode length: 77.40 +/- 5.90\n",
      "Eval num_timesteps=155648, episode_reward=-82.20 +/- 6.06\n",
      "Episode length: 83.20 +/- 6.06\n",
      "Eval num_timesteps=155648, episode_reward=-92.70 +/- 22.33\n",
      "Episode length: 93.70 +/- 22.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00076 |       0.00000 |      45.88593 |       0.00039 |       0.31126\n",
      "     -0.00097 |       0.00000 |      43.11571 |       0.00132 |       0.30022\n",
      "     -0.00202 |       0.00000 |      41.98912 |       0.00141 |       0.29764\n",
      "     -0.00259 |       0.00000 |      41.35918 |       0.00176 |       0.29915\n",
      "     -0.00347 |       0.00000 |      40.81472 |       0.00187 |       0.29166\n",
      "     -0.00305 |       0.00000 |      40.48260 |       0.00215 |       0.29773\n",
      "     -0.00357 |       0.00000 |      40.01075 |       0.00223 |       0.29618\n",
      "     -0.00432 |       0.00000 |      39.71514 |       0.00187 |       0.29501\n",
      "     -0.00422 |       0.00000 |      39.58410 |       0.00215 |       0.29393\n",
      "     -0.00432 |       0.00000 |      39.26171 |       0.00235 |       0.29558\n",
      "Evaluating losses...\n",
      "     -0.00528 |       0.00000 |      38.97783 |       0.00292 |       0.29027\n",
      "-----------------------------------\n",
      "| EpLenMean       | 97            |\n",
      "| EpRewMean       | -96           |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 888           |\n",
      "| TimeElapsed     | 559           |\n",
      "| TimestepsSoFar  | 159744        |\n",
      "| ev_tdlam_before | 0.826         |\n",
      "| loss_ent        | 0.29027206    |\n",
      "| loss_kl         | 0.0029235685  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0052776122 |\n",
      "| loss_vf_loss    | 38.977825     |\n",
      "-----------------------------------\n",
      "********** Iteration 39 ************\n",
      "Eval num_timesteps=159744, episode_reward=-90.20 +/- 32.42\n",
      "Episode length: 91.20 +/- 32.42\n",
      "Eval num_timesteps=159744, episode_reward=-86.70 +/- 19.14\n",
      "Episode length: 87.70 +/- 19.14\n",
      "Eval num_timesteps=159744, episode_reward=-80.70 +/- 10.07\n",
      "Episode length: 81.70 +/- 10.07\n",
      "Eval num_timesteps=159744, episode_reward=-94.20 +/- 31.38\n",
      "Episode length: 95.20 +/- 31.38\n",
      "Eval num_timesteps=159744, episode_reward=-90.30 +/- 34.43\n",
      "Episode length: 91.30 +/- 34.43\n",
      "Eval num_timesteps=159744, episode_reward=-82.60 +/- 9.08\n",
      "Episode length: 83.60 +/- 9.08\n",
      "Eval num_timesteps=159744, episode_reward=-88.20 +/- 26.68\n",
      "Episode length: 89.20 +/- 26.68\n",
      "Eval num_timesteps=159744, episode_reward=-81.60 +/- 12.21\n",
      "Episode length: 82.60 +/- 12.21\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00094 |       0.00000 |      38.88361 |       0.00066 |       0.28979\n",
      "     -0.00231 |       0.00000 |      37.27338 |       0.00120 |       0.28322\n",
      "     -0.00322 |       0.00000 |      36.64571 |       0.00148 |       0.27851\n",
      "     -0.00320 |       0.00000 |      36.18989 |       0.00128 |       0.28299\n",
      "     -0.00358 |       0.00000 |      35.92041 |       0.00152 |       0.28095\n",
      "     -0.00407 |       0.00000 |      35.71523 |       0.00137 |       0.28055\n",
      "     -0.00402 |       0.00000 |      35.43670 |       0.00171 |       0.27980\n",
      "     -0.00401 |       0.00000 |      35.30173 |       0.00151 |       0.28289\n",
      "     -0.00450 |       0.00000 |      35.20196 |       0.00167 |       0.28211\n",
      "     -0.00452 |       0.00000 |      35.04370 |       0.00156 |       0.28309\n",
      "Evaluating losses...\n",
      "     -0.00542 |       0.00000 |      34.68851 |       0.00160 |       0.27970\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.8          |\n",
      "| EpRewMean       | -94.8         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 932           |\n",
      "| TimeElapsed     | 566           |\n",
      "| TimestepsSoFar  | 163840        |\n",
      "| ev_tdlam_before | 0.858         |\n",
      "| loss_ent        | 0.27970237    |\n",
      "| loss_kl         | 0.0016005975  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0054238695 |\n",
      "| loss_vf_loss    | 34.688515     |\n",
      "-----------------------------------\n",
      "********** Iteration 40 ************\n",
      "Eval num_timesteps=163840, episode_reward=-78.40 +/- 6.86\n",
      "Episode length: 79.40 +/- 6.86\n",
      "Eval num_timesteps=163840, episode_reward=-83.30 +/- 7.03\n",
      "Episode length: 84.30 +/- 7.03\n",
      "Eval num_timesteps=163840, episode_reward=-80.80 +/- 5.36\n",
      "Episode length: 81.80 +/- 5.36\n",
      "Eval num_timesteps=163840, episode_reward=-101.10 +/- 49.75\n",
      "Episode length: 102.10 +/- 49.75\n",
      "Eval num_timesteps=163840, episode_reward=-85.70 +/- 7.28\n",
      "Episode length: 86.70 +/- 7.28\n",
      "Eval num_timesteps=163840, episode_reward=-84.40 +/- 15.59\n",
      "Episode length: 85.40 +/- 15.59\n",
      "Eval num_timesteps=163840, episode_reward=-80.90 +/- 8.24\n",
      "Episode length: 81.90 +/- 8.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=163840, episode_reward=-78.90 +/- 8.36\n",
      "Episode length: 79.90 +/- 8.36\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00108 |       0.00000 |      48.87150 |       0.00110 |       0.27348\n",
      "     -0.00227 |       0.00000 |      46.64653 |       0.00123 |       0.27551\n",
      "     -0.00234 |       0.00000 |      46.04998 |       0.00117 |       0.27549\n",
      "     -0.00252 |       0.00000 |      45.46159 |       0.00142 |       0.27624\n",
      "     -0.00276 |       0.00000 |      45.18456 |       0.00125 |       0.27709\n",
      "     -0.00321 |       0.00000 |      44.85375 |       0.00115 |       0.27627\n",
      "     -0.00309 |       0.00000 |      44.55554 |       0.00151 |       0.27438\n",
      "     -0.00322 |       0.00000 |      44.48428 |       0.00129 |       0.27817\n",
      "     -0.00349 |       0.00000 |      44.38503 |       0.00101 |       0.27644\n",
      "     -0.00381 |       0.00000 |      43.93021 |       0.00123 |       0.27471\n",
      "Evaluating losses...\n",
      "     -0.00444 |       0.00000 |      43.61921 |       0.00141 |       0.27522\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.7          |\n",
      "| EpRewMean       | -95.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 974           |\n",
      "| TimeElapsed     | 574           |\n",
      "| TimestepsSoFar  | 167936        |\n",
      "| ev_tdlam_before | 0.795         |\n",
      "| loss_ent        | 0.27522314    |\n",
      "| loss_kl         | 0.0014120351  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0044448036 |\n",
      "| loss_vf_loss    | 43.619213     |\n",
      "-----------------------------------\n",
      "********** Iteration 41 ************\n",
      "Eval num_timesteps=167936, episode_reward=-92.70 +/- 13.17\n",
      "Episode length: 93.70 +/- 13.17\n",
      "Eval num_timesteps=167936, episode_reward=-88.40 +/- 14.78\n",
      "Episode length: 89.40 +/- 14.78\n",
      "Eval num_timesteps=167936, episode_reward=-79.90 +/- 7.30\n",
      "Episode length: 80.90 +/- 7.30\n",
      "Eval num_timesteps=167936, episode_reward=-83.40 +/- 10.86\n",
      "Episode length: 84.40 +/- 10.86\n",
      "Eval num_timesteps=167936, episode_reward=-77.70 +/- 8.23\n",
      "Episode length: 78.70 +/- 8.23\n",
      "Eval num_timesteps=167936, episode_reward=-103.40 +/- 45.67\n",
      "Episode length: 104.40 +/- 45.67\n",
      "Eval num_timesteps=167936, episode_reward=-89.60 +/- 21.20\n",
      "Episode length: 90.60 +/- 21.20\n",
      "Eval num_timesteps=167936, episode_reward=-97.20 +/- 28.06\n",
      "Episode length: 98.20 +/- 28.06\n",
      "Eval num_timesteps=167936, episode_reward=-81.20 +/- 8.51\n",
      "Episode length: 82.20 +/- 8.51\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00075 |       0.00000 |      49.36105 |       0.00061 |       0.27282\n",
      "     -0.00066 |       0.00000 |      47.66652 |       0.00071 |       0.27362\n",
      "     -0.00080 |       0.00000 |      46.89658 |       0.00068 |       0.26958\n",
      "     -0.00174 |       0.00000 |      46.29150 |       0.00121 |       0.26706\n",
      "     -0.00184 |       0.00000 |      45.79866 |       0.00116 |       0.27015\n",
      "     -0.00245 |       0.00000 |      45.51635 |       0.00119 |       0.26817\n",
      "     -0.00245 |       0.00000 |      45.09380 |       0.00133 |       0.26531\n",
      "     -0.00299 |       0.00000 |      44.92606 |       0.00131 |       0.26758\n",
      "     -0.00295 |       0.00000 |      44.54761 |       0.00135 |       0.26660\n",
      "     -0.00349 |       0.00000 |      44.43732 |       0.00132 |       0.26701\n",
      "Evaluating losses...\n",
      "     -0.00372 |       0.00000 |      43.94450 |       0.00100 |       0.27280\n",
      "-----------------------------------\n",
      "| EpLenMean       | 97.5          |\n",
      "| EpRewMean       | -96.5         |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 1014          |\n",
      "| TimeElapsed     | 581           |\n",
      "| TimestepsSoFar  | 172032        |\n",
      "| ev_tdlam_before | 0.801         |\n",
      "| loss_ent        | 0.27280167    |\n",
      "| loss_kl         | 0.0009984917  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0037152222 |\n",
      "| loss_vf_loss    | 43.944504     |\n",
      "-----------------------------------\n",
      "********** Iteration 42 ************\n",
      "Eval num_timesteps=172032, episode_reward=-88.70 +/- 21.65\n",
      "Episode length: 89.70 +/- 21.65\n",
      "Eval num_timesteps=172032, episode_reward=-86.30 +/- 15.48\n",
      "Episode length: 87.30 +/- 15.48\n",
      "Eval num_timesteps=172032, episode_reward=-81.20 +/- 8.94\n",
      "Episode length: 82.20 +/- 8.94\n",
      "Eval num_timesteps=172032, episode_reward=-83.30 +/- 7.38\n",
      "Episode length: 84.30 +/- 7.38\n",
      "Eval num_timesteps=172032, episode_reward=-88.80 +/- 17.95\n",
      "Episode length: 89.80 +/- 17.95\n",
      "Eval num_timesteps=172032, episode_reward=-88.80 +/- 16.12\n",
      "Episode length: 89.80 +/- 16.12\n",
      "Eval num_timesteps=172032, episode_reward=-114.90 +/- 47.94\n",
      "Episode length: 115.90 +/- 47.94\n",
      "Eval num_timesteps=172032, episode_reward=-85.10 +/- 10.21\n",
      "Episode length: 86.10 +/- 10.21\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00039 |       0.00000 |      43.92812 |       0.00057 |       0.27080\n",
      "     -0.00097 |       0.00000 |      41.02510 |       0.00110 |       0.26788\n",
      "     -0.00170 |       0.00000 |      39.87958 |       0.00102 |       0.26585\n",
      "     -0.00218 |       0.00000 |      39.11430 |       0.00110 |       0.26486\n",
      "     -0.00238 |       0.00000 |      38.61209 |       0.00155 |       0.26281\n",
      "     -0.00300 |       0.00000 |      38.16777 |       0.00154 |       0.26750\n",
      "     -0.00277 |       0.00000 |      37.83910 |       0.00141 |       0.26415\n",
      "     -0.00368 |       0.00000 |      37.64912 |       0.00162 |       0.26278\n",
      "     -0.00368 |       0.00000 |      37.25772 |       0.00187 |       0.26334\n",
      "     -0.00374 |       0.00000 |      37.08617 |       0.00182 |       0.26255\n",
      "Evaluating losses...\n",
      "     -0.00458 |       0.00000 |      36.64721 |       0.00186 |       0.26343\n",
      "-----------------------------------\n",
      "| EpLenMean       | 101           |\n",
      "| EpRewMean       | -100          |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 1055          |\n",
      "| TimeElapsed     | 589           |\n",
      "| TimestepsSoFar  | 176128        |\n",
      "| ev_tdlam_before | 0.811         |\n",
      "| loss_ent        | 0.2634284     |\n",
      "| loss_kl         | 0.001860558   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0045785718 |\n",
      "| loss_vf_loss    | 36.647205     |\n",
      "-----------------------------------\n",
      "********** Iteration 43 ************\n",
      "Eval num_timesteps=176128, episode_reward=-87.40 +/- 17.25\n",
      "Episode length: 88.40 +/- 17.25\n",
      "Eval num_timesteps=176128, episode_reward=-89.90 +/- 29.80\n",
      "Episode length: 90.90 +/- 29.80\n",
      "Eval num_timesteps=176128, episode_reward=-83.70 +/- 8.22\n",
      "Episode length: 84.70 +/- 8.22\n",
      "Eval num_timesteps=176128, episode_reward=-85.80 +/- 14.88\n",
      "Episode length: 86.80 +/- 14.88\n",
      "Eval num_timesteps=176128, episode_reward=-93.60 +/- 17.45\n",
      "Episode length: 94.60 +/- 17.45\n",
      "Eval num_timesteps=176128, episode_reward=-81.70 +/- 6.33\n",
      "Episode length: 82.70 +/- 6.33\n",
      "Eval num_timesteps=176128, episode_reward=-91.20 +/- 23.33\n",
      "Episode length: 92.20 +/- 23.33\n",
      "Eval num_timesteps=176128, episode_reward=-90.30 +/- 26.91\n",
      "Episode length: 91.30 +/- 26.91\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00121 |       0.00000 |      49.96259 |       0.00121 |       0.28149\n",
      "     -0.00510 |       0.00000 |      46.03873 |       0.00452 |       0.29198\n",
      "     -0.00631 |       0.00000 |      45.43683 |       0.00449 |       0.29171\n",
      "     -0.00620 |       0.00000 |      45.12915 |       0.00436 |       0.28519\n",
      "     -0.00719 |       0.00000 |      44.66697 |       0.00483 |       0.28900\n",
      "     -0.00707 |       0.00000 |      44.25783 |       0.00475 |       0.28721\n",
      "     -0.00837 |       0.00000 |      44.14661 |       0.00487 |       0.28848\n",
      "     -0.00814 |       0.00000 |      43.99423 |       0.00501 |       0.29059\n",
      "     -0.00864 |       0.00000 |      43.95583 |       0.00534 |       0.28908\n",
      "     -0.00917 |       0.00000 |      43.65638 |       0.00544 |       0.28613\n",
      "Evaluating losses...\n",
      "     -0.00976 |       0.00000 |      43.30803 |       0.00447 |       0.28705\n",
      "-----------------------------------\n",
      "| EpLenMean       | 107           |\n",
      "| EpRewMean       | -106          |\n",
      "| EpThisIter      | 36            |\n",
      "| EpisodesSoFar   | 1091          |\n",
      "| TimeElapsed     | 596           |\n",
      "| TimestepsSoFar  | 180224        |\n",
      "| ev_tdlam_before | 0.776         |\n",
      "| loss_ent        | 0.2870461     |\n",
      "| loss_kl         | 0.004467563   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0097646285 |\n",
      "| loss_vf_loss    | 43.30803      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 44 ************\n",
      "Eval num_timesteps=180224, episode_reward=-83.50 +/- 12.12\n",
      "Episode length: 84.50 +/- 12.12\n",
      "Eval num_timesteps=180224, episode_reward=-107.00 +/- 69.54\n",
      "Episode length: 108.00 +/- 69.54\n",
      "Eval num_timesteps=180224, episode_reward=-81.80 +/- 10.06\n",
      "Episode length: 82.80 +/- 10.06\n",
      "Eval num_timesteps=180224, episode_reward=-109.40 +/- 50.10\n",
      "Episode length: 110.40 +/- 50.10\n",
      "Eval num_timesteps=180224, episode_reward=-93.80 +/- 22.64\n",
      "Episode length: 94.80 +/- 22.64\n",
      "Eval num_timesteps=180224, episode_reward=-82.60 +/- 13.92\n",
      "Episode length: 83.60 +/- 13.92\n",
      "Eval num_timesteps=180224, episode_reward=-86.20 +/- 19.46\n",
      "Episode length: 87.20 +/- 19.46\n",
      "Eval num_timesteps=180224, episode_reward=-88.30 +/- 12.59\n",
      "Episode length: 89.30 +/- 12.59\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00080 |       0.00000 |      47.57548 |       0.00086 |       0.26358\n",
      "     -0.00204 |       0.00000 |      45.64415 |       0.00118 |       0.26315\n",
      "     -0.00277 |       0.00000 |      45.08881 |       0.00131 |       0.26607\n",
      "     -0.00306 |       0.00000 |      44.66404 |       0.00128 |       0.26193\n",
      "     -0.00337 |       0.00000 |      44.28663 |       0.00122 |       0.26507\n",
      "     -0.00355 |       0.00000 |      44.22316 |       0.00142 |       0.26473\n",
      "     -0.00404 |       0.00000 |      43.93417 |       0.00158 |       0.26419\n",
      "     -0.00394 |       0.00000 |      43.67923 |       0.00143 |       0.26220\n",
      "     -0.00422 |       0.00000 |      43.63634 |       0.00138 |       0.25822\n",
      "     -0.00421 |       0.00000 |      43.42547 |       0.00168 |       0.26229\n",
      "Evaluating losses...\n",
      "     -0.00485 |       0.00000 |      42.93277 |       0.00154 |       0.26150\n",
      "-----------------------------------\n",
      "| EpLenMean       | 100           |\n",
      "| EpRewMean       | -99.4         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 1136          |\n",
      "| TimeElapsed     | 603           |\n",
      "| TimestepsSoFar  | 184320        |\n",
      "| ev_tdlam_before | 0.82          |\n",
      "| loss_ent        | 0.261502      |\n",
      "| loss_kl         | 0.0015433269  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0048505883 |\n",
      "| loss_vf_loss    | 42.932774     |\n",
      "-----------------------------------\n",
      "********** Iteration 45 ************\n",
      "Eval num_timesteps=184320, episode_reward=-80.40 +/- 7.09\n",
      "Episode length: 81.40 +/- 7.09\n",
      "Eval num_timesteps=184320, episode_reward=-89.20 +/- 16.20\n",
      "Episode length: 90.20 +/- 16.20\n",
      "Eval num_timesteps=184320, episode_reward=-84.40 +/- 14.18\n",
      "Episode length: 85.40 +/- 14.18\n",
      "Eval num_timesteps=184320, episode_reward=-90.20 +/- 28.71\n",
      "Episode length: 91.20 +/- 28.71\n",
      "Eval num_timesteps=184320, episode_reward=-80.50 +/- 5.92\n",
      "Episode length: 81.50 +/- 5.92\n",
      "Eval num_timesteps=184320, episode_reward=-103.50 +/- 41.44\n",
      "Episode length: 104.50 +/- 41.44\n",
      "Eval num_timesteps=184320, episode_reward=-80.80 +/- 7.18\n",
      "Episode length: 81.80 +/- 7.18\n",
      "Eval num_timesteps=184320, episode_reward=-91.50 +/- 15.67\n",
      "Episode length: 92.50 +/- 15.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00035 |       0.00000 |      49.71473 |       0.00044 |       0.26412\n",
      "     -0.00200 |       0.00000 |      48.37352 |       0.00107 |       0.25892\n",
      "     -0.00337 |       0.00000 |      47.65823 |       0.00110 |       0.26280\n",
      "     -0.00389 |       0.00000 |      47.36480 |       0.00144 |       0.25657\n",
      "     -0.00425 |       0.00000 |      46.86983 |       0.00139 |       0.26393\n",
      "     -0.00444 |       0.00000 |      46.57405 |       0.00171 |       0.25963\n",
      "     -0.00434 |       0.00000 |      46.36968 |       0.00151 |       0.25705\n",
      "     -0.00484 |       0.00000 |      46.15444 |       0.00151 |       0.25897\n",
      "     -0.00504 |       0.00000 |      45.94510 |       0.00171 |       0.26043\n",
      "     -0.00510 |       0.00000 |      45.82480 |       0.00162 |       0.25628\n",
      "Evaluating losses...\n",
      "     -0.00592 |       0.00000 |      45.32330 |       0.00150 |       0.25392\n",
      "----------------------------------\n",
      "| EpLenMean       | 99.7         |\n",
      "| EpRewMean       | -98.7        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 1178         |\n",
      "| TimeElapsed     | 610          |\n",
      "| TimestepsSoFar  | 188416       |\n",
      "| ev_tdlam_before | 0.814        |\n",
      "| loss_ent        | 0.2539193    |\n",
      "| loss_kl         | 0.0015025691 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005915305 |\n",
      "| loss_vf_loss    | 45.3233      |\n",
      "----------------------------------\n",
      "********** Iteration 46 ************\n",
      "Eval num_timesteps=188416, episode_reward=-89.70 +/- 12.81\n",
      "Episode length: 90.70 +/- 12.81\n",
      "Eval num_timesteps=188416, episode_reward=-104.50 +/- 54.16\n",
      "Episode length: 105.50 +/- 54.16\n",
      "Eval num_timesteps=188416, episode_reward=-110.60 +/- 53.31\n",
      "Episode length: 111.60 +/- 53.31\n",
      "Eval num_timesteps=188416, episode_reward=-85.00 +/- 20.16\n",
      "Episode length: 86.00 +/- 20.16\n",
      "Eval num_timesteps=188416, episode_reward=-98.20 +/- 31.46\n",
      "Episode length: 99.20 +/- 31.46\n",
      "Eval num_timesteps=188416, episode_reward=-89.70 +/- 14.63\n",
      "Episode length: 90.70 +/- 14.63\n",
      "Eval num_timesteps=188416, episode_reward=-88.30 +/- 19.67\n",
      "Episode length: 89.30 +/- 19.67\n",
      "Eval num_timesteps=188416, episode_reward=-86.00 +/- 9.88\n",
      "Episode length: 87.00 +/- 9.88\n",
      "Eval num_timesteps=188416, episode_reward=-82.20 +/- 5.69\n",
      "Episode length: 83.20 +/- 5.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00052 |       0.00000 |      56.21908 |       0.00057 |       0.29307\n",
      "     -0.00142 |       0.00000 |      50.18470 |       0.00102 |       0.28247\n",
      "     -0.00203 |       0.00000 |      49.08231 |       0.00114 |       0.28758\n",
      "     -0.00224 |       0.00000 |      48.66174 |       0.00119 |       0.28781\n",
      "     -0.00278 |       0.00000 |      48.05386 |       0.00136 |       0.28546\n",
      "     -0.00292 |       0.00000 |      47.83087 |       0.00131 |       0.28568\n",
      "     -0.00342 |       0.00000 |      47.51663 |       0.00166 |       0.28285\n",
      "     -0.00326 |       0.00000 |      47.41479 |       0.00162 |       0.28316\n",
      "     -0.00371 |       0.00000 |      47.12772 |       0.00140 |       0.28431\n",
      "     -0.00376 |       0.00000 |      46.85133 |       0.00175 |       0.28252\n",
      "Evaluating losses...\n",
      "     -0.00432 |       0.00000 |      46.45648 |       0.00208 |       0.28402\n",
      "-----------------------------------\n",
      "| EpLenMean       | 103           |\n",
      "| EpRewMean       | -102          |\n",
      "| EpThisIter      | 35            |\n",
      "| EpisodesSoFar   | 1213          |\n",
      "| TimeElapsed     | 617           |\n",
      "| TimestepsSoFar  | 192512        |\n",
      "| ev_tdlam_before | 0.722         |\n",
      "| loss_ent        | 0.28401685    |\n",
      "| loss_kl         | 0.0020782156  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0043200315 |\n",
      "| loss_vf_loss    | 46.45648      |\n",
      "-----------------------------------\n",
      "********** Iteration 47 ************\n",
      "Eval num_timesteps=192512, episode_reward=-88.60 +/- 24.13\n",
      "Episode length: 89.60 +/- 24.13\n",
      "Eval num_timesteps=192512, episode_reward=-90.50 +/- 23.20\n",
      "Episode length: 91.50 +/- 23.20\n",
      "Eval num_timesteps=192512, episode_reward=-137.70 +/- 65.87\n",
      "Episode length: 138.70 +/- 65.87\n",
      "Eval num_timesteps=192512, episode_reward=-91.20 +/- 33.66\n",
      "Episode length: 92.20 +/- 33.66\n",
      "Eval num_timesteps=192512, episode_reward=-90.00 +/- 27.88\n",
      "Episode length: 91.00 +/- 27.88\n",
      "Eval num_timesteps=192512, episode_reward=-88.60 +/- 10.92\n",
      "Episode length: 89.60 +/- 10.92\n",
      "Eval num_timesteps=192512, episode_reward=-94.20 +/- 35.19\n",
      "Episode length: 95.20 +/- 35.19\n",
      "Eval num_timesteps=192512, episode_reward=-81.80 +/- 6.58\n",
      "Episode length: 82.80 +/- 6.58\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -8.95e-05 |       0.00000 |      51.58370 |       0.00055 |       0.25595\n",
      "     -0.00169 |       0.00000 |      49.30202 |       0.00125 |       0.25184\n",
      "     -0.00220 |       0.00000 |      48.40354 |       0.00143 |       0.25383\n",
      "     -0.00312 |       0.00000 |      47.75035 |       0.00153 |       0.25260\n",
      "     -0.00359 |       0.00000 |      47.17022 |       0.00188 |       0.25046\n",
      "     -0.00404 |       0.00000 |      46.70530 |       0.00218 |       0.25592\n",
      "     -0.00391 |       0.00000 |      46.24548 |       0.00178 |       0.25228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00407 |       0.00000 |      45.99550 |       0.00201 |       0.25194\n",
      "     -0.00432 |       0.00000 |      45.85917 |       0.00166 |       0.25236\n",
      "     -0.00443 |       0.00000 |      45.53883 |       0.00211 |       0.25079\n",
      "Evaluating losses...\n",
      "     -0.00586 |       0.00000 |      45.07151 |       0.00169 |       0.25264\n",
      "----------------------------------\n",
      "| EpLenMean       | 105          |\n",
      "| EpRewMean       | -104         |\n",
      "| EpThisIter      | 40           |\n",
      "| EpisodesSoFar   | 1253         |\n",
      "| TimeElapsed     | 625          |\n",
      "| TimestepsSoFar  | 196608       |\n",
      "| ev_tdlam_before | 0.783        |\n",
      "| loss_ent        | 0.25263885   |\n",
      "| loss_kl         | 0.0016904395 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005855032 |\n",
      "| loss_vf_loss    | 45.071507    |\n",
      "----------------------------------\n",
      "********** Iteration 48 ************\n",
      "Eval num_timesteps=196608, episode_reward=-92.10 +/- 22.90\n",
      "Episode length: 93.10 +/- 22.90\n",
      "Eval num_timesteps=196608, episode_reward=-80.30 +/- 13.33\n",
      "Episode length: 81.30 +/- 13.33\n",
      "Eval num_timesteps=196608, episode_reward=-83.40 +/- 13.38\n",
      "Episode length: 84.40 +/- 13.38\n",
      "Eval num_timesteps=196608, episode_reward=-88.50 +/- 15.49\n",
      "Episode length: 89.50 +/- 15.49\n",
      "Eval num_timesteps=196608, episode_reward=-80.60 +/- 7.64\n",
      "Episode length: 81.60 +/- 7.64\n",
      "Eval num_timesteps=196608, episode_reward=-86.10 +/- 9.94\n",
      "Episode length: 87.10 +/- 9.94\n",
      "Eval num_timesteps=196608, episode_reward=-96.70 +/- 39.69\n",
      "Episode length: 97.70 +/- 39.69\n",
      "Eval num_timesteps=196608, episode_reward=-106.00 +/- 44.91\n",
      "Episode length: 107.00 +/- 44.91\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00127 |       0.00000 |      47.98606 |       0.00085 |       0.24410\n",
      "     -0.00172 |       0.00000 |      45.11348 |       0.00086 |       0.24588\n",
      "     -0.00294 |       0.00000 |      44.32015 |       0.00131 |       0.24301\n",
      "     -0.00306 |       0.00000 |      43.88681 |       0.00130 |       0.24696\n",
      "     -0.00377 |       0.00000 |      43.66598 |       0.00165 |       0.24357\n",
      "     -0.00467 |       0.00000 |      43.42794 |       0.00153 |       0.24328\n",
      "     -0.00423 |       0.00000 |      43.34804 |       0.00162 |       0.24300\n",
      "     -0.00469 |       0.00000 |      43.17496 |       0.00210 |       0.24486\n",
      "     -0.00514 |       0.00000 |      42.91941 |       0.00182 |       0.24049\n",
      "     -0.00468 |       0.00000 |      42.87589 |       0.00191 |       0.24374\n",
      "Evaluating losses...\n",
      "     -0.00647 |       0.00000 |      42.44067 |       0.00192 |       0.24061\n",
      "----------------------------------\n",
      "| EpLenMean       | 100          |\n",
      "| EpRewMean       | -99.3        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 1295         |\n",
      "| TimeElapsed     | 632          |\n",
      "| TimestepsSoFar  | 200704       |\n",
      "| ev_tdlam_before | 0.805        |\n",
      "| loss_ent        | 0.24061054   |\n",
      "| loss_kl         | 0.001922807  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.006467957 |\n",
      "| loss_vf_loss    | 42.440666    |\n",
      "----------------------------------\n",
      "********** Iteration 49 ************\n",
      "Eval num_timesteps=200704, episode_reward=-79.70 +/- 6.13\n",
      "Episode length: 80.70 +/- 6.13\n",
      "Eval num_timesteps=200704, episode_reward=-78.20 +/- 7.51\n",
      "Episode length: 79.20 +/- 7.51\n",
      "Eval num_timesteps=200704, episode_reward=-84.40 +/- 18.37\n",
      "Episode length: 85.40 +/- 18.37\n",
      "Eval num_timesteps=200704, episode_reward=-84.00 +/- 7.84\n",
      "Episode length: 85.00 +/- 7.84\n",
      "Eval num_timesteps=200704, episode_reward=-81.50 +/- 6.96\n",
      "Episode length: 82.50 +/- 6.96\n",
      "Eval num_timesteps=200704, episode_reward=-89.50 +/- 17.98\n",
      "Episode length: 90.50 +/- 17.98\n",
      "Eval num_timesteps=200704, episode_reward=-83.70 +/- 13.48\n",
      "Episode length: 84.70 +/- 13.48\n",
      "Eval num_timesteps=200704, episode_reward=-84.40 +/- 10.38\n",
      "Episode length: 85.40 +/- 10.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -7.04e-05 |       0.00000 |      49.78730 |       0.00076 |       0.23996\n",
      "     -0.00181 |       0.00000 |      48.32495 |       0.00103 |       0.23583\n",
      "     -0.00253 |       0.00000 |      47.58514 |       0.00081 |       0.23854\n",
      "     -0.00259 |       0.00000 |      47.08361 |       0.00113 |       0.23997\n",
      "     -0.00260 |       0.00000 |      46.58599 |       0.00124 |       0.24407\n",
      "     -0.00346 |       0.00000 |      46.15012 |       0.00137 |       0.24022\n",
      "     -0.00309 |       0.00000 |      45.99669 |       0.00132 |       0.24549\n",
      "     -0.00382 |       0.00000 |      45.71216 |       0.00132 |       0.24355\n",
      "     -0.00365 |       0.00000 |      45.67002 |       0.00136 |       0.24526\n",
      "     -0.00427 |       0.00000 |      45.28601 |       0.00134 |       0.24478\n",
      "Evaluating losses...\n",
      "     -0.00462 |       0.00000 |      44.71889 |       0.00160 |       0.24277\n",
      "----------------------------------\n",
      "| EpLenMean       | 101          |\n",
      "| EpRewMean       | -99.6        |\n",
      "| EpThisIter      | 40           |\n",
      "| EpisodesSoFar   | 1335         |\n",
      "| TimeElapsed     | 638          |\n",
      "| TimestepsSoFar  | 204800       |\n",
      "| ev_tdlam_before | 0.815        |\n",
      "| loss_ent        | 0.24277021   |\n",
      "| loss_kl         | 0.0015969598 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004624808 |\n",
      "| loss_vf_loss    | 44.718887    |\n",
      "----------------------------------\n",
      "********** Iteration 50 ************\n",
      "Eval num_timesteps=204800, episode_reward=-82.80 +/- 9.10\n",
      "Episode length: 83.80 +/- 9.10\n",
      "Eval num_timesteps=204800, episode_reward=-89.70 +/- 18.02\n",
      "Episode length: 90.70 +/- 18.02\n",
      "Eval num_timesteps=204800, episode_reward=-81.90 +/- 5.22\n",
      "Episode length: 82.90 +/- 5.22\n",
      "Eval num_timesteps=204800, episode_reward=-101.40 +/- 44.36\n",
      "Episode length: 102.40 +/- 44.36\n",
      "Eval num_timesteps=204800, episode_reward=-82.30 +/- 5.12\n",
      "Episode length: 83.30 +/- 5.12\n",
      "Eval num_timesteps=204800, episode_reward=-85.90 +/- 10.82\n",
      "Episode length: 86.90 +/- 10.82\n",
      "Eval num_timesteps=204800, episode_reward=-88.10 +/- 21.38\n",
      "Episode length: 89.10 +/- 21.38\n",
      "Eval num_timesteps=204800, episode_reward=-81.00 +/- 12.41\n",
      "Episode length: 82.00 +/- 12.41\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00061 |       0.00000 |      53.12950 |       0.00048 |       0.25108\n",
      "     -0.00089 |       0.00000 |      47.68847 |       0.00096 |       0.25097\n",
      "     -0.00195 |       0.00000 |      46.60538 |       0.00100 |       0.24884\n",
      "     -0.00190 |       0.00000 |      46.01601 |       0.00109 |       0.25422\n",
      "     -0.00295 |       0.00000 |      45.49752 |       0.00113 |       0.25315\n",
      "     -0.00259 |       0.00000 |      45.18863 |       0.00099 |       0.24947\n",
      "     -0.00305 |       0.00000 |      44.82011 |       0.00096 |       0.25146\n",
      "     -0.00296 |       0.00000 |      44.64212 |       0.00146 |       0.25144\n",
      "     -0.00346 |       0.00000 |      44.38914 |       0.00117 |       0.24965\n",
      "     -0.00367 |       0.00000 |      44.25248 |       0.00116 |       0.25492\n",
      "Evaluating losses...\n",
      "     -0.00448 |       0.00000 |      43.76323 |       0.00086 |       0.24986\n",
      "-----------------------------------\n",
      "| EpLenMean       | 103           |\n",
      "| EpRewMean       | -102          |\n",
      "| EpThisIter      | 37            |\n",
      "| EpisodesSoFar   | 1372          |\n",
      "| TimeElapsed     | 645           |\n",
      "| TimestepsSoFar  | 208896        |\n",
      "| ev_tdlam_before | 0.789         |\n",
      "| loss_ent        | 0.24985948    |\n",
      "| loss_kl         | 0.00086042896 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.004477188  |\n",
      "| loss_vf_loss    | 43.76323      |\n",
      "-----------------------------------\n",
      "********** Iteration 51 ************\n",
      "Eval num_timesteps=208896, episode_reward=-88.00 +/- 15.22\n",
      "Episode length: 89.00 +/- 15.22\n",
      "Eval num_timesteps=208896, episode_reward=-78.70 +/- 7.46\n",
      "Episode length: 79.70 +/- 7.46\n",
      "Eval num_timesteps=208896, episode_reward=-78.20 +/- 8.23\n",
      "Episode length: 79.20 +/- 8.23\n",
      "Eval num_timesteps=208896, episode_reward=-82.00 +/- 10.33\n",
      "Episode length: 83.00 +/- 10.33\n",
      "Eval num_timesteps=208896, episode_reward=-80.50 +/- 9.58\n",
      "Episode length: 81.50 +/- 9.58\n",
      "Eval num_timesteps=208896, episode_reward=-146.10 +/- 135.34\n",
      "Episode length: 147.00 +/- 135.08\n",
      "Eval num_timesteps=208896, episode_reward=-79.60 +/- 9.12\n",
      "Episode length: 80.60 +/- 9.12\n",
      "Eval num_timesteps=208896, episode_reward=-83.30 +/- 9.22\n",
      "Episode length: 84.30 +/- 9.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00075 |       0.00000 |      57.14777 |       0.00050 |       0.24278\n",
      "     -0.00041 |       0.00000 |      52.27797 |       0.00141 |       0.24891\n",
      "     -0.00137 |       0.00000 |      50.24709 |       0.00133 |       0.24412\n",
      "     -0.00251 |       0.00000 |      49.00555 |       0.00122 |       0.24721\n",
      "     -0.00266 |       0.00000 |      47.94008 |       0.00117 |       0.24895\n",
      "     -0.00332 |       0.00000 |      47.36482 |       0.00119 |       0.24682\n",
      "     -0.00320 |       0.00000 |      46.60488 |       0.00150 |       0.25102\n",
      "     -0.00354 |       0.00000 |      46.08847 |       0.00139 |       0.24553\n",
      "     -0.00373 |       0.00000 |      45.72231 |       0.00163 |       0.24647\n",
      "     -0.00378 |       0.00000 |      45.35067 |       0.00173 |       0.24880\n",
      "Evaluating losses...\n",
      "     -0.00533 |       0.00000 |      44.74464 |       0.00199 |       0.24939\n",
      "-----------------------------------\n",
      "| EpLenMean       | 111           |\n",
      "| EpRewMean       | -110          |\n",
      "| EpThisIter      | 37            |\n",
      "| EpisodesSoFar   | 1409          |\n",
      "| TimeElapsed     | 651           |\n",
      "| TimestepsSoFar  | 212992        |\n",
      "| ev_tdlam_before | 0.793         |\n",
      "| loss_ent        | 0.2493878     |\n",
      "| loss_kl         | 0.0019882673  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0053261984 |\n",
      "| loss_vf_loss    | 44.744644     |\n",
      "-----------------------------------\n",
      "********** Iteration 52 ************\n",
      "Eval num_timesteps=212992, episode_reward=-80.40 +/- 17.37\n",
      "Episode length: 81.40 +/- 17.37\n",
      "Eval num_timesteps=212992, episode_reward=-89.40 +/- 23.58\n",
      "Episode length: 90.40 +/- 23.58\n",
      "Eval num_timesteps=212992, episode_reward=-82.90 +/- 8.03\n",
      "Episode length: 83.90 +/- 8.03\n",
      "Eval num_timesteps=212992, episode_reward=-82.80 +/- 10.39\n",
      "Episode length: 83.80 +/- 10.39\n",
      "Eval num_timesteps=212992, episode_reward=-83.50 +/- 13.99\n",
      "Episode length: 84.50 +/- 13.99\n",
      "Eval num_timesteps=212992, episode_reward=-100.20 +/- 38.70\n",
      "Episode length: 101.20 +/- 38.70\n",
      "Eval num_timesteps=212992, episode_reward=-88.70 +/- 19.54\n",
      "Episode length: 89.70 +/- 19.54\n",
      "Eval num_timesteps=212992, episode_reward=-79.10 +/- 7.44\n",
      "Episode length: 80.10 +/- 7.44\n",
      "Eval num_timesteps=212992, episode_reward=-82.40 +/- 7.64\n",
      "Episode length: 83.40 +/- 7.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00079 |       0.00000 |      51.19240 |       0.00033 |       0.22133\n",
      "     -0.00062 |       0.00000 |      48.09345 |       0.00102 |       0.21332\n",
      "     -0.00117 |       0.00000 |      46.65722 |       0.00086 |       0.21270\n",
      "     -0.00163 |       0.00000 |      45.58443 |       0.00106 |       0.21514\n",
      "     -0.00188 |       0.00000 |      44.80357 |       0.00115 |       0.21610\n",
      "     -0.00215 |       0.00000 |      44.13238 |       0.00124 |       0.21381\n",
      "     -0.00202 |       0.00000 |      43.72975 |       0.00101 |       0.21975\n",
      "     -0.00286 |       0.00000 |      43.48942 |       0.00133 |       0.21551\n",
      "     -0.00279 |       0.00000 |      43.11570 |       0.00129 |       0.21856\n",
      "     -0.00323 |       0.00000 |      42.82438 |       0.00105 |       0.21560\n",
      "Evaluating losses...\n",
      "     -0.00399 |       0.00000 |      42.31608 |       0.00116 |       0.21498\n",
      "----------------------------------\n",
      "| EpLenMean       | 99.2         |\n",
      "| EpRewMean       | -98.2        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 1453         |\n",
      "| TimeElapsed     | 658          |\n",
      "| TimestepsSoFar  | 217088       |\n",
      "| ev_tdlam_before | 0.808        |\n",
      "| loss_ent        | 0.21498339   |\n",
      "| loss_kl         | 0.0011630903 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.00399058  |\n",
      "| loss_vf_loss    | 42.31608     |\n",
      "----------------------------------\n",
      "********** Iteration 53 ************\n",
      "Eval num_timesteps=217088, episode_reward=-77.90 +/- 8.70\n",
      "Episode length: 78.90 +/- 8.70\n",
      "Eval num_timesteps=217088, episode_reward=-99.00 +/- 38.56\n",
      "Episode length: 100.00 +/- 38.56\n",
      "Eval num_timesteps=217088, episode_reward=-83.20 +/- 9.65\n",
      "Episode length: 84.20 +/- 9.65\n",
      "Eval num_timesteps=217088, episode_reward=-95.20 +/- 39.48\n",
      "Episode length: 96.20 +/- 39.48\n",
      "Eval num_timesteps=217088, episode_reward=-106.70 +/- 43.25\n",
      "Episode length: 107.70 +/- 43.25\n",
      "Eval num_timesteps=217088, episode_reward=-87.70 +/- 11.88\n",
      "Episode length: 88.70 +/- 11.88\n",
      "Eval num_timesteps=217088, episode_reward=-82.60 +/- 5.57\n",
      "Episode length: 83.60 +/- 5.57\n",
      "Eval num_timesteps=217088, episode_reward=-90.50 +/- 14.46\n",
      "Episode length: 91.50 +/- 14.46\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00041 |       0.00000 |      63.54680 |       0.00047 |       0.24210\n",
      "     -0.00168 |       0.00000 |      60.74160 |       0.00138 |       0.24234\n",
      "     -0.00242 |       0.00000 |      59.93178 |       0.00135 |       0.24063\n",
      "     -0.00212 |       0.00000 |      58.96096 |       0.00139 |       0.24238\n",
      "     -0.00274 |       0.00000 |      58.47712 |       0.00123 |       0.23928\n",
      "     -0.00311 |       0.00000 |      58.05045 |       0.00141 |       0.24450\n",
      "     -0.00315 |       0.00000 |      57.85004 |       0.00121 |       0.24473\n",
      "     -0.00328 |       0.00000 |      57.62584 |       0.00147 |       0.24223\n",
      "     -0.00344 |       0.00000 |      57.39205 |       0.00129 |       0.24636\n",
      "     -0.00375 |       0.00000 |      57.10958 |       0.00136 |       0.23991\n",
      "Evaluating losses...\n",
      "     -0.00477 |       0.00000 |      56.71646 |       0.00143 |       0.24052\n",
      "-----------------------------------\n",
      "| EpLenMean       | 104           |\n",
      "| EpRewMean       | -103          |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 1491          |\n",
      "| TimeElapsed     | 665           |\n",
      "| TimestepsSoFar  | 221184        |\n",
      "| ev_tdlam_before | 0.732         |\n",
      "| loss_ent        | 0.24052301    |\n",
      "| loss_kl         | 0.0014279574  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0047655827 |\n",
      "| loss_vf_loss    | 56.716465     |\n",
      "-----------------------------------\n",
      "********** Iteration 54 ************\n",
      "Eval num_timesteps=221184, episode_reward=-85.30 +/- 11.40\n",
      "Episode length: 86.30 +/- 11.40\n",
      "Eval num_timesteps=221184, episode_reward=-97.10 +/- 31.03\n",
      "Episode length: 98.10 +/- 31.03\n",
      "Eval num_timesteps=221184, episode_reward=-77.80 +/- 9.43\n",
      "Episode length: 78.80 +/- 9.43\n",
      "Eval num_timesteps=221184, episode_reward=-84.80 +/- 10.93\n",
      "Episode length: 85.80 +/- 10.93\n",
      "Eval num_timesteps=221184, episode_reward=-77.20 +/- 11.36\n",
      "Episode length: 78.20 +/- 11.36\n",
      "Eval num_timesteps=221184, episode_reward=-83.50 +/- 5.52\n",
      "Episode length: 84.50 +/- 5.52\n",
      "Eval num_timesteps=221184, episode_reward=-89.40 +/- 22.66\n",
      "Episode length: 90.40 +/- 22.66\n",
      "Eval num_timesteps=221184, episode_reward=-126.50 +/- 84.82\n",
      "Episode length: 127.50 +/- 84.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00025 |       0.00000 |      54.63964 |       0.00066 |       0.23693\n",
      "     -0.00116 |       0.00000 |      53.35571 |       0.00101 |       0.23749\n",
      "     -0.00227 |       0.00000 |      52.89807 |       0.00082 |       0.23247\n",
      "     -0.00264 |       0.00000 |      52.13777 |       0.00127 |       0.23473\n",
      "     -0.00303 |       0.00000 |      51.80779 |       0.00105 |       0.23152\n",
      "     -0.00317 |       0.00000 |      51.53432 |       0.00119 |       0.23227\n",
      "     -0.00296 |       0.00000 |      51.33556 |       0.00166 |       0.22855\n",
      "     -0.00372 |       0.00000 |      50.89965 |       0.00147 |       0.23108\n",
      "     -0.00368 |       0.00000 |      50.72701 |       0.00134 |       0.22501\n",
      "     -0.00301 |       0.00000 |      50.45492 |       0.00194 |       0.22923\n",
      "Evaluating losses...\n",
      "     -0.00490 |       0.00000 |      50.00874 |       0.00144 |       0.23042\n",
      "-----------------------------------\n",
      "| EpLenMean       | 101           |\n",
      "| EpRewMean       | -99.9         |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 1531          |\n",
      "| TimeElapsed     | 672           |\n",
      "| TimestepsSoFar  | 225280        |\n",
      "| ev_tdlam_before | 0.782         |\n",
      "| loss_ent        | 0.23041603    |\n",
      "| loss_kl         | 0.0014418159  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0048976787 |\n",
      "| loss_vf_loss    | 50.00874      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 55 ************\n",
      "Eval num_timesteps=225280, episode_reward=-78.50 +/- 7.21\n",
      "Episode length: 79.50 +/- 7.21\n",
      "Eval num_timesteps=225280, episode_reward=-83.70 +/- 6.56\n",
      "Episode length: 84.70 +/- 6.56\n",
      "Eval num_timesteps=225280, episode_reward=-96.70 +/- 51.83\n",
      "Episode length: 97.70 +/- 51.83\n",
      "Eval num_timesteps=225280, episode_reward=-86.00 +/- 7.32\n",
      "Episode length: 87.00 +/- 7.32\n",
      "Eval num_timesteps=225280, episode_reward=-80.40 +/- 7.59\n",
      "Episode length: 81.40 +/- 7.59\n",
      "Eval num_timesteps=225280, episode_reward=-75.60 +/- 6.87\n",
      "Episode length: 76.60 +/- 6.87\n",
      "Eval num_timesteps=225280, episode_reward=-83.40 +/- 5.85\n",
      "Episode length: 84.40 +/- 5.85\n",
      "Eval num_timesteps=225280, episode_reward=-86.10 +/- 15.69\n",
      "Episode length: 87.10 +/- 15.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00048 |       0.00000 |      58.18375 |       0.00037 |       0.22236\n",
      "     -0.00184 |       0.00000 |      53.68308 |       0.00101 |       0.22057\n",
      "     -0.00171 |       0.00000 |      52.54560 |       0.00113 |       0.22035\n",
      "     -0.00257 |       0.00000 |      51.83853 |       0.00096 |       0.22257\n",
      "     -0.00319 |       0.00000 |      51.29700 |       0.00112 |       0.22364\n",
      "     -0.00366 |       0.00000 |      51.05497 |       0.00097 |       0.22850\n",
      "     -0.00406 |       0.00000 |      50.73329 |       0.00138 |       0.22961\n",
      "     -0.00458 |       0.00000 |      50.49870 |       0.00127 |       0.23186\n",
      "     -0.00499 |       0.00000 |      50.18155 |       0.00187 |       0.23092\n",
      "     -0.00526 |       0.00000 |      50.18624 |       0.00213 |       0.23479\n",
      "Evaluating losses...\n",
      "     -0.00630 |       0.00000 |      49.52584 |       0.00203 |       0.23593\n",
      "----------------------------------\n",
      "| EpLenMean       | 98.3         |\n",
      "| EpRewMean       | -97.3        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 1573         |\n",
      "| TimeElapsed     | 678          |\n",
      "| TimestepsSoFar  | 229376       |\n",
      "| ev_tdlam_before | 0.764        |\n",
      "| loss_ent        | 0.23593442   |\n",
      "| loss_kl         | 0.002025014  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.006300747 |\n",
      "| loss_vf_loss    | 49.525837    |\n",
      "----------------------------------\n",
      "********** Iteration 56 ************\n",
      "Eval num_timesteps=229376, episode_reward=-84.80 +/- 9.35\n",
      "Episode length: 85.80 +/- 9.35\n",
      "Eval num_timesteps=229376, episode_reward=-103.20 +/- 40.97\n",
      "Episode length: 104.20 +/- 40.97\n",
      "Eval num_timesteps=229376, episode_reward=-81.20 +/- 8.98\n",
      "Episode length: 82.20 +/- 8.98\n",
      "Eval num_timesteps=229376, episode_reward=-83.90 +/- 10.39\n",
      "Episode length: 84.90 +/- 10.39\n",
      "Eval num_timesteps=229376, episode_reward=-89.70 +/- 6.68\n",
      "Episode length: 90.70 +/- 6.68\n",
      "Eval num_timesteps=229376, episode_reward=-89.60 +/- 27.23\n",
      "Episode length: 90.60 +/- 27.23\n",
      "Eval num_timesteps=229376, episode_reward=-97.70 +/- 23.53\n",
      "Episode length: 98.70 +/- 23.53\n",
      "Eval num_timesteps=229376, episode_reward=-82.50 +/- 8.00\n",
      "Episode length: 83.50 +/- 8.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00035 |       0.00000 |      55.54331 |       0.00044 |       0.23887\n",
      "     -0.00082 |       0.00000 |      51.76117 |       0.00099 |       0.23290\n",
      "     -0.00128 |       0.00000 |      50.86609 |       0.00105 |       0.24227\n",
      "     -0.00155 |       0.00000 |      50.23105 |       0.00104 |       0.23750\n",
      "     -0.00230 |       0.00000 |      49.75882 |       0.00099 |       0.23684\n",
      "     -0.00235 |       0.00000 |      49.29039 |       0.00117 |       0.23776\n",
      "     -0.00237 |       0.00000 |      48.96177 |       0.00120 |       0.23419\n",
      "     -0.00281 |       0.00000 |      48.67243 |       0.00146 |       0.24249\n",
      "     -0.00265 |       0.00000 |      48.31496 |       0.00144 |       0.23473\n",
      "     -0.00352 |       0.00000 |      48.20541 |       0.00109 |       0.23816\n",
      "Evaluating losses...\n",
      "     -0.00402 |       0.00000 |      47.53482 |       0.00180 |       0.23774\n",
      "-----------------------------------\n",
      "| EpLenMean       | 101           |\n",
      "| EpRewMean       | -99.6         |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 1611          |\n",
      "| TimeElapsed     | 685           |\n",
      "| TimestepsSoFar  | 233472        |\n",
      "| ev_tdlam_before | 0.783         |\n",
      "| loss_ent        | 0.23774454    |\n",
      "| loss_kl         | 0.0017980534  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040205615 |\n",
      "| loss_vf_loss    | 47.534817     |\n",
      "-----------------------------------\n",
      "********** Iteration 57 ************\n",
      "Eval num_timesteps=233472, episode_reward=-90.20 +/- 34.55\n",
      "Episode length: 91.20 +/- 34.55\n",
      "Eval num_timesteps=233472, episode_reward=-91.60 +/- 30.47\n",
      "Episode length: 92.60 +/- 30.47\n",
      "Eval num_timesteps=233472, episode_reward=-113.60 +/- 54.70\n",
      "Episode length: 114.60 +/- 54.70\n",
      "Eval num_timesteps=233472, episode_reward=-86.50 +/- 17.16\n",
      "Episode length: 87.50 +/- 17.16\n",
      "Eval num_timesteps=233472, episode_reward=-84.50 +/- 12.03\n",
      "Episode length: 85.50 +/- 12.03\n",
      "Eval num_timesteps=233472, episode_reward=-88.50 +/- 14.07\n",
      "Episode length: 89.50 +/- 14.07\n",
      "Eval num_timesteps=233472, episode_reward=-82.10 +/- 11.55\n",
      "Episode length: 83.10 +/- 11.55\n",
      "Eval num_timesteps=233472, episode_reward=-131.90 +/- 123.97\n",
      "Episode length: 132.80 +/- 123.68\n",
      "Eval num_timesteps=233472, episode_reward=-98.40 +/- 39.68\n",
      "Episode length: 99.40 +/- 39.68\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00033 |       0.00000 |      62.65885 |       0.00039 |       0.21952\n",
      "     -0.00086 |       0.00000 |      61.11049 |       0.00076 |       0.21938\n",
      "     -0.00189 |       0.00000 |      60.06616 |       0.00115 |       0.22483\n",
      "     -0.00149 |       0.00000 |      59.35620 |       0.00118 |       0.22159\n",
      "     -0.00239 |       0.00000 |      58.53074 |       0.00105 |       0.22443\n",
      "     -0.00230 |       0.00000 |      58.10617 |       0.00117 |       0.22631\n",
      "     -0.00220 |       0.00000 |      57.63338 |       0.00113 |       0.22733\n",
      "     -0.00257 |       0.00000 |      57.19331 |       0.00135 |       0.22642\n",
      "     -0.00311 |       0.00000 |      56.97531 |       0.00114 |       0.22565\n",
      "     -0.00292 |       0.00000 |      56.60614 |       0.00139 |       0.22986\n",
      "Evaluating losses...\n",
      "     -0.00406 |       0.00000 |      56.11137 |       0.00113 |       0.22436\n",
      "-----------------------------------\n",
      "| EpLenMean       | 105           |\n",
      "| EpRewMean       | -104          |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 1652          |\n",
      "| TimeElapsed     | 693           |\n",
      "| TimestepsSoFar  | 237568        |\n",
      "| ev_tdlam_before | 0.778         |\n",
      "| loss_ent        | 0.2243561     |\n",
      "| loss_kl         | 0.0011334094  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040648673 |\n",
      "| loss_vf_loss    | 56.11137      |\n",
      "-----------------------------------\n",
      "********** Iteration 58 ************\n",
      "Eval num_timesteps=237568, episode_reward=-108.80 +/- 59.74\n",
      "Episode length: 109.80 +/- 59.74\n",
      "Eval num_timesteps=237568, episode_reward=-86.40 +/- 23.35\n",
      "Episode length: 87.40 +/- 23.35\n",
      "Eval num_timesteps=237568, episode_reward=-81.20 +/- 6.26\n",
      "Episode length: 82.20 +/- 6.26\n",
      "Eval num_timesteps=237568, episode_reward=-87.00 +/- 16.11\n",
      "Episode length: 88.00 +/- 16.11\n",
      "Eval num_timesteps=237568, episode_reward=-81.50 +/- 10.59\n",
      "Episode length: 82.50 +/- 10.59\n",
      "Eval num_timesteps=237568, episode_reward=-79.30 +/- 6.29\n",
      "Episode length: 80.30 +/- 6.29\n",
      "Eval num_timesteps=237568, episode_reward=-88.90 +/- 20.27\n",
      "Episode length: 89.90 +/- 20.27\n",
      "Eval num_timesteps=237568, episode_reward=-90.20 +/- 28.95\n",
      "Episode length: 91.20 +/- 28.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.84e-05 |       0.00000 |      53.36690 |       0.00065 |       0.22576\n",
      "     -0.00123 |       0.00000 |      50.80606 |       0.00076 |       0.22187\n",
      "     -0.00240 |       0.00000 |      49.34991 |       0.00095 |       0.22297\n",
      "     -0.00283 |       0.00000 |      48.46284 |       0.00073 |       0.22196\n",
      "     -0.00331 |       0.00000 |      47.79720 |       0.00109 |       0.22369\n",
      "     -0.00362 |       0.00000 |      47.35050 |       0.00131 |       0.22129\n",
      "     -0.00416 |       0.00000 |      46.94672 |       0.00112 |       0.22295\n",
      "     -0.00437 |       0.00000 |      46.67770 |       0.00096 |       0.22172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00435 |       0.00000 |      46.31434 |       0.00120 |       0.22349\n",
      "     -0.00492 |       0.00000 |      46.15588 |       0.00109 |       0.22268\n",
      "Evaluating losses...\n",
      "     -0.00542 |       0.00000 |      45.76516 |       0.00092 |       0.21932\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99.7          |\n",
      "| EpRewMean       | -98.7         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 1696          |\n",
      "| TimeElapsed     | 699           |\n",
      "| TimestepsSoFar  | 241664        |\n",
      "| ev_tdlam_before | 0.79          |\n",
      "| loss_ent        | 0.21932416    |\n",
      "| loss_kl         | 0.00092041126 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.005419703  |\n",
      "| loss_vf_loss    | 45.765163     |\n",
      "-----------------------------------\n",
      "********** Iteration 59 ************\n",
      "Eval num_timesteps=241664, episode_reward=-98.00 +/- 32.67\n",
      "Episode length: 99.00 +/- 32.67\n",
      "Eval num_timesteps=241664, episode_reward=-90.10 +/- 25.27\n",
      "Episode length: 91.10 +/- 25.27\n",
      "Eval num_timesteps=241664, episode_reward=-79.90 +/- 9.14\n",
      "Episode length: 80.90 +/- 9.14\n",
      "Eval num_timesteps=241664, episode_reward=-81.60 +/- 9.99\n",
      "Episode length: 82.60 +/- 9.99\n",
      "Eval num_timesteps=241664, episode_reward=-84.20 +/- 8.47\n",
      "Episode length: 85.20 +/- 8.47\n",
      "Eval num_timesteps=241664, episode_reward=-79.10 +/- 8.44\n",
      "Episode length: 80.10 +/- 8.44\n",
      "Eval num_timesteps=241664, episode_reward=-84.50 +/- 12.14\n",
      "Episode length: 85.50 +/- 12.14\n",
      "Eval num_timesteps=241664, episode_reward=-79.40 +/- 7.23\n",
      "Episode length: 80.40 +/- 7.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.27e-05 |       0.00000 |      45.29373 |       0.00030 |       0.21562\n",
      "     -0.00175 |       0.00000 |      43.41475 |       0.00111 |       0.21797\n",
      "     -0.00214 |       0.00000 |      42.55727 |       0.00106 |       0.21991\n",
      "     -0.00208 |       0.00000 |      42.04597 |       0.00122 |       0.22263\n",
      "     -0.00282 |       0.00000 |      41.62233 |       0.00134 |       0.21871\n",
      "     -0.00256 |       0.00000 |      41.45156 |       0.00171 |       0.22152\n",
      "     -0.00359 |       0.00000 |      41.02813 |       0.00143 |       0.22317\n",
      "     -0.00377 |       0.00000 |      40.88443 |       0.00164 |       0.21993\n",
      "     -0.00406 |       0.00000 |      40.81659 |       0.00131 |       0.22042\n",
      "     -0.00424 |       0.00000 |      40.48473 |       0.00155 |       0.21921\n",
      "Evaluating losses...\n",
      "     -0.00490 |       0.00000 |      40.11050 |       0.00125 |       0.22456\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.8          |\n",
      "| EpRewMean       | -92.8         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 1738          |\n",
      "| TimeElapsed     | 706           |\n",
      "| TimestepsSoFar  | 245760        |\n",
      "| ev_tdlam_before | 0.824         |\n",
      "| loss_ent        | 0.22456425    |\n",
      "| loss_kl         | 0.0012525314  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0049025384 |\n",
      "| loss_vf_loss    | 40.110497     |\n",
      "-----------------------------------\n",
      "********** Iteration 60 ************\n",
      "Eval num_timesteps=245760, episode_reward=-91.60 +/- 18.28\n",
      "Episode length: 92.60 +/- 18.28\n",
      "Eval num_timesteps=245760, episode_reward=-85.10 +/- 18.20\n",
      "Episode length: 86.10 +/- 18.20\n",
      "Eval num_timesteps=245760, episode_reward=-106.50 +/- 47.09\n",
      "Episode length: 107.50 +/- 47.09\n",
      "Eval num_timesteps=245760, episode_reward=-84.30 +/- 15.26\n",
      "Episode length: 85.30 +/- 15.26\n",
      "Eval num_timesteps=245760, episode_reward=-81.70 +/- 6.63\n",
      "Episode length: 82.70 +/- 6.63\n",
      "Eval num_timesteps=245760, episode_reward=-80.30 +/- 12.92\n",
      "Episode length: 81.30 +/- 12.92\n",
      "Eval num_timesteps=245760, episode_reward=-89.50 +/- 30.82\n",
      "Episode length: 90.50 +/- 30.82\n",
      "Eval num_timesteps=245760, episode_reward=-78.70 +/- 6.48\n",
      "Episode length: 79.70 +/- 6.48\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00026 |       0.00000 |      35.12665 |       0.00031 |       0.21101\n",
      "     -0.00103 |       0.00000 |      33.43819 |       0.00068 |       0.21363\n",
      "     -0.00205 |       0.00000 |      32.70583 |       0.00067 |       0.21254\n",
      "     -0.00255 |       0.00000 |      32.34724 |       0.00092 |       0.21110\n",
      "     -0.00283 |       0.00000 |      31.90802 |       0.00112 |       0.21163\n",
      "     -0.00329 |       0.00000 |      31.72297 |       0.00101 |       0.21004\n",
      "     -0.00339 |       0.00000 |      31.46250 |       0.00106 |       0.20986\n",
      "     -0.00325 |       0.00000 |      31.24544 |       0.00105 |       0.20994\n",
      "     -0.00353 |       0.00000 |      31.19637 |       0.00119 |       0.21049\n",
      "     -0.00422 |       0.00000 |      30.99772 |       0.00130 |       0.20915\n",
      "Evaluating losses...\n",
      "     -0.00488 |       0.00000 |      30.73595 |       0.00120 |       0.20978\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.9         |\n",
      "| EpRewMean       | -92.9        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 1782         |\n",
      "| TimeElapsed     | 712          |\n",
      "| TimestepsSoFar  | 249856       |\n",
      "| ev_tdlam_before | 0.868        |\n",
      "| loss_ent        | 0.20978402   |\n",
      "| loss_kl         | 0.0012021061 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004875752 |\n",
      "| loss_vf_loss    | 30.735954    |\n",
      "----------------------------------\n",
      "********** Iteration 61 ************\n",
      "Eval num_timesteps=249856, episode_reward=-89.20 +/- 32.22\n",
      "Episode length: 90.20 +/- 32.22\n",
      "Eval num_timesteps=249856, episode_reward=-90.90 +/- 19.07\n",
      "Episode length: 91.90 +/- 19.07\n",
      "Eval num_timesteps=249856, episode_reward=-82.10 +/- 8.19\n",
      "Episode length: 83.10 +/- 8.19\n",
      "Eval num_timesteps=249856, episode_reward=-80.70 +/- 4.58\n",
      "Episode length: 81.70 +/- 4.58\n",
      "Eval num_timesteps=249856, episode_reward=-82.20 +/- 12.43\n",
      "Episode length: 83.20 +/- 12.43\n",
      "Eval num_timesteps=249856, episode_reward=-89.70 +/- 32.45\n",
      "Episode length: 90.70 +/- 32.45\n",
      "Eval num_timesteps=249856, episode_reward=-82.10 +/- 5.82\n",
      "Episode length: 83.10 +/- 5.82\n",
      "Eval num_timesteps=249856, episode_reward=-81.80 +/- 5.23\n",
      "Episode length: 82.80 +/- 5.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00019 |       0.00000 |      39.19404 |       0.00107 |       0.20166\n",
      "     -0.00179 |       0.00000 |      38.06627 |       0.00089 |       0.20051\n",
      "     -0.00174 |       0.00000 |      37.38383 |       0.00111 |       0.20475\n",
      "     -0.00249 |       0.00000 |      36.86541 |       0.00085 |       0.20505\n",
      "     -0.00230 |       0.00000 |      36.63707 |       0.00072 |       0.20763\n",
      "     -0.00309 |       0.00000 |      36.20621 |       0.00085 |       0.20722\n",
      "     -0.00306 |       0.00000 |      35.97897 |       0.00155 |       0.20630\n",
      "     -0.00356 |       0.00000 |      35.76093 |       0.00104 |       0.20873\n",
      "     -0.00402 |       0.00000 |      35.40483 |       0.00127 |       0.20945\n",
      "     -0.00414 |       0.00000 |      35.19563 |       0.00145 |       0.20937\n",
      "Evaluating losses...\n",
      "     -0.00467 |       0.00000 |      34.87035 |       0.00147 |       0.20916\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.2         |\n",
      "| EpRewMean       | -91.2        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 1828         |\n",
      "| TimeElapsed     | 718          |\n",
      "| TimestepsSoFar  | 253952       |\n",
      "| ev_tdlam_before | 0.858        |\n",
      "| loss_ent        | 0.2091578    |\n",
      "| loss_kl         | 0.0014690134 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004665628 |\n",
      "| loss_vf_loss    | 34.87035     |\n",
      "----------------------------------\n",
      "********** Iteration 62 ************\n",
      "Eval num_timesteps=253952, episode_reward=-86.70 +/- 6.00\n",
      "Episode length: 87.70 +/- 6.00\n",
      "Eval num_timesteps=253952, episode_reward=-100.40 +/- 31.06\n",
      "Episode length: 101.40 +/- 31.06\n",
      "Eval num_timesteps=253952, episode_reward=-82.60 +/- 6.64\n",
      "Episode length: 83.60 +/- 6.64\n",
      "Eval num_timesteps=253952, episode_reward=-84.00 +/- 6.42\n",
      "Episode length: 85.00 +/- 6.42\n",
      "Eval num_timesteps=253952, episode_reward=-79.90 +/- 12.47\n",
      "Episode length: 80.90 +/- 12.47\n",
      "Eval num_timesteps=253952, episode_reward=-80.10 +/- 5.13\n",
      "Episode length: 81.10 +/- 5.13\n",
      "Eval num_timesteps=253952, episode_reward=-103.40 +/- 51.37\n",
      "Episode length: 104.40 +/- 51.37\n",
      "Eval num_timesteps=253952, episode_reward=-88.80 +/- 11.96\n",
      "Episode length: 89.80 +/- 11.96\n",
      "Eval num_timesteps=253952, episode_reward=-95.30 +/- 34.99\n",
      "Episode length: 96.30 +/- 34.99\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00035 |       0.00000 |      43.14809 |       0.00048 |       0.22471\n",
      "     -0.00164 |       0.00000 |      39.61824 |       0.00169 |       0.23529\n",
      "     -0.00273 |       0.00000 |      38.71197 |       0.00103 |       0.22851\n",
      "     -0.00317 |       0.00000 |      38.01984 |       0.00158 |       0.23001\n",
      "     -0.00331 |       0.00000 |      37.67554 |       0.00174 |       0.22995\n",
      "     -0.00389 |       0.00000 |      37.29551 |       0.00199 |       0.23359\n",
      "     -0.00382 |       0.00000 |      36.99758 |       0.00202 |       0.23438\n",
      "     -0.00424 |       0.00000 |      36.75315 |       0.00203 |       0.23010\n",
      "     -0.00430 |       0.00000 |      36.54439 |       0.00208 |       0.23732\n",
      "     -0.00444 |       0.00000 |      36.41252 |       0.00229 |       0.23241\n",
      "Evaluating losses...\n",
      "     -0.00507 |       0.00000 |      35.96553 |       0.00242 |       0.23693\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.4          |\n",
      "| EpRewMean       | -97.4         |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 1866          |\n",
      "| TimeElapsed     | 725           |\n",
      "| TimestepsSoFar  | 258048        |\n",
      "| ev_tdlam_before | 0.8           |\n",
      "| loss_ent        | 0.23692964    |\n",
      "| loss_kl         | 0.002424554   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0050689187 |\n",
      "| loss_vf_loss    | 35.96553      |\n",
      "-----------------------------------\n",
      "********** Iteration 63 ************\n",
      "Eval num_timesteps=258048, episode_reward=-98.30 +/- 30.71\n",
      "Episode length: 99.30 +/- 30.71\n",
      "Eval num_timesteps=258048, episode_reward=-84.20 +/- 8.46\n",
      "Episode length: 85.20 +/- 8.46\n",
      "Eval num_timesteps=258048, episode_reward=-86.10 +/- 14.71\n",
      "Episode length: 87.10 +/- 14.71\n",
      "Eval num_timesteps=258048, episode_reward=-81.70 +/- 6.21\n",
      "Episode length: 82.70 +/- 6.21\n",
      "Eval num_timesteps=258048, episode_reward=-89.10 +/- 9.73\n",
      "Episode length: 90.10 +/- 9.73\n",
      "Eval num_timesteps=258048, episode_reward=-95.60 +/- 30.07\n",
      "Episode length: 96.60 +/- 30.07\n",
      "Eval num_timesteps=258048, episode_reward=-81.30 +/- 7.25\n",
      "Episode length: 82.30 +/- 7.25\n",
      "Eval num_timesteps=258048, episode_reward=-77.40 +/- 7.57\n",
      "Episode length: 78.40 +/- 7.57\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00020 |       0.00000 |      39.35325 |       0.00030 |       0.20474\n",
      "     -0.00257 |       0.00000 |      37.74172 |       0.00048 |       0.20212\n",
      "     -0.00308 |       0.00000 |      37.11907 |       0.00096 |       0.20203\n",
      "     -0.00338 |       0.00000 |      36.62731 |       0.00094 |       0.19928\n",
      "     -0.00402 |       0.00000 |      36.26970 |       0.00102 |       0.19876\n",
      "     -0.00391 |       0.00000 |      35.93125 |       0.00117 |       0.19809\n",
      "     -0.00404 |       0.00000 |      35.69284 |       0.00119 |       0.20123\n",
      "     -0.00415 |       0.00000 |      35.46056 |       0.00137 |       0.19740\n",
      "     -0.00455 |       0.00000 |      35.31700 |       0.00149 |       0.19720\n",
      "     -0.00452 |       0.00000 |      35.19164 |       0.00155 |       0.19678\n",
      "Evaluating losses...\n",
      "     -0.00524 |       0.00000 |      34.70994 |       0.00147 |       0.19618\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.6          |\n",
      "| EpRewMean       | -95.6         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 1911          |\n",
      "| TimeElapsed     | 732           |\n",
      "| TimestepsSoFar  | 262144        |\n",
      "| ev_tdlam_before | 0.851         |\n",
      "| loss_ent        | 0.19618224    |\n",
      "| loss_kl         | 0.001473376   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0052422793 |\n",
      "| loss_vf_loss    | 34.70994      |\n",
      "-----------------------------------\n",
      "********** Iteration 64 ************\n",
      "Eval num_timesteps=262144, episode_reward=-81.10 +/- 17.41\n",
      "Episode length: 82.10 +/- 17.41\n",
      "Eval num_timesteps=262144, episode_reward=-81.40 +/- 9.13\n",
      "Episode length: 82.40 +/- 9.13\n",
      "Eval num_timesteps=262144, episode_reward=-82.80 +/- 9.38\n",
      "Episode length: 83.80 +/- 9.38\n",
      "Eval num_timesteps=262144, episode_reward=-82.50 +/- 9.14\n",
      "Episode length: 83.50 +/- 9.14\n",
      "Eval num_timesteps=262144, episode_reward=-83.90 +/- 11.22\n",
      "Episode length: 84.90 +/- 11.22\n",
      "Eval num_timesteps=262144, episode_reward=-82.20 +/- 12.80\n",
      "Episode length: 83.20 +/- 12.80\n",
      "Eval num_timesteps=262144, episode_reward=-84.70 +/- 6.78\n",
      "Episode length: 85.70 +/- 6.78\n",
      "Eval num_timesteps=262144, episode_reward=-98.40 +/- 52.85\n",
      "Episode length: 99.40 +/- 52.85\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00042 |       0.00000 |      40.59795 |       0.00032 |       0.21519\n",
      "     -0.00112 |       0.00000 |      39.34318 |       0.00092 |       0.21434\n",
      "     -0.00188 |       0.00000 |      38.59328 |       0.00099 |       0.21723\n",
      "     -0.00216 |       0.00000 |      38.29187 |       0.00093 |       0.21628\n",
      "     -0.00256 |       0.00000 |      37.89851 |       0.00114 |       0.21530\n",
      "     -0.00274 |       0.00000 |      37.55289 |       0.00097 |       0.21842\n",
      "     -0.00321 |       0.00000 |      37.33176 |       0.00144 |       0.21723\n",
      "     -0.00312 |       0.00000 |      37.07550 |       0.00116 |       0.21933\n",
      "     -0.00343 |       0.00000 |      36.89299 |       0.00133 |       0.21860\n",
      "     -0.00307 |       0.00000 |      36.70673 |       0.00124 |       0.21970\n",
      "Evaluating losses...\n",
      "     -0.00405 |       0.00000 |      36.29501 |       0.00135 |       0.22222\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.8          |\n",
      "| EpRewMean       | -91.8         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 1954          |\n",
      "| TimeElapsed     | 738           |\n",
      "| TimestepsSoFar  | 266240        |\n",
      "| ev_tdlam_before | 0.845         |\n",
      "| loss_ent        | 0.2222162     |\n",
      "| loss_kl         | 0.001349514   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040534413 |\n",
      "| loss_vf_loss    | 36.295006     |\n",
      "-----------------------------------\n",
      "********** Iteration 65 ************\n",
      "Eval num_timesteps=266240, episode_reward=-85.70 +/- 14.99\n",
      "Episode length: 86.70 +/- 14.99\n",
      "Eval num_timesteps=266240, episode_reward=-83.20 +/- 4.96\n",
      "Episode length: 84.20 +/- 4.96\n",
      "Eval num_timesteps=266240, episode_reward=-79.70 +/- 9.62\n",
      "Episode length: 80.70 +/- 9.62\n",
      "Eval num_timesteps=266240, episode_reward=-88.20 +/- 16.61\n",
      "Episode length: 89.20 +/- 16.61\n",
      "Eval num_timesteps=266240, episode_reward=-83.90 +/- 13.25\n",
      "Episode length: 84.90 +/- 13.25\n",
      "Eval num_timesteps=266240, episode_reward=-91.60 +/- 15.00\n",
      "Episode length: 92.60 +/- 15.00\n",
      "Eval num_timesteps=266240, episode_reward=-92.50 +/- 29.49\n",
      "Episode length: 93.50 +/- 29.49\n",
      "Eval num_timesteps=266240, episode_reward=-80.60 +/- 10.08\n",
      "Episode length: 81.60 +/- 10.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00026 |       0.00000 |      33.43142 |       0.00044 |       0.20295\n",
      "     -0.00201 |       0.00000 |      32.55576 |       0.00091 |       0.20127\n",
      "     -0.00259 |       0.00000 |      31.96308 |       0.00105 |       0.20507\n",
      "     -0.00338 |       0.00000 |      31.73837 |       0.00117 |       0.20061\n",
      "     -0.00368 |       0.00000 |      31.36555 |       0.00108 |       0.19998\n",
      "     -0.00396 |       0.00000 |      31.17666 |       0.00114 |       0.20021\n",
      "     -0.00447 |       0.00000 |      31.02671 |       0.00132 |       0.20092\n",
      "     -0.00417 |       0.00000 |      30.80636 |       0.00125 |       0.20082\n",
      "     -0.00472 |       0.00000 |      30.62938 |       0.00145 |       0.20252\n",
      "     -0.00491 |       0.00000 |      30.47705 |       0.00162 |       0.20174\n",
      "Evaluating losses...\n",
      "     -0.00533 |       0.00000 |      30.11434 |       0.00155 |       0.19997\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.3         |\n",
      "| EpRewMean       | -91.3        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 1999         |\n",
      "| TimeElapsed     | 745          |\n",
      "| TimestepsSoFar  | 270336       |\n",
      "| ev_tdlam_before | 0.868        |\n",
      "| loss_ent        | 0.19996597   |\n",
      "| loss_kl         | 0.001546663  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005326029 |\n",
      "| loss_vf_loss    | 30.114336    |\n",
      "----------------------------------\n",
      "********** Iteration 66 ************\n",
      "Eval num_timesteps=270336, episode_reward=-83.30 +/- 10.33\n",
      "Episode length: 84.30 +/- 10.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=270336, episode_reward=-98.60 +/- 38.30\n",
      "Episode length: 99.60 +/- 38.30\n",
      "Eval num_timesteps=270336, episode_reward=-78.00 +/- 9.77\n",
      "Episode length: 79.00 +/- 9.77\n",
      "Eval num_timesteps=270336, episode_reward=-77.30 +/- 6.21\n",
      "Episode length: 78.30 +/- 6.21\n",
      "Eval num_timesteps=270336, episode_reward=-80.40 +/- 8.51\n",
      "Episode length: 81.40 +/- 8.51\n",
      "Eval num_timesteps=270336, episode_reward=-79.80 +/- 12.64\n",
      "Episode length: 80.80 +/- 12.64\n",
      "Eval num_timesteps=270336, episode_reward=-78.00 +/- 6.68\n",
      "Episode length: 79.00 +/- 6.68\n",
      "Eval num_timesteps=270336, episode_reward=-96.70 +/- 33.84\n",
      "Episode length: 97.70 +/- 33.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -6.88e-05 |       0.00000 |      37.19320 |       0.00038 |       0.20061\n",
      "     -0.00167 |       0.00000 |      36.32717 |       0.00095 |       0.19772\n",
      "     -0.00191 |       0.00000 |      35.77822 |       0.00068 |       0.19377\n",
      "     -0.00209 |       0.00000 |      35.60236 |       0.00058 |       0.19686\n",
      "     -0.00287 |       0.00000 |      35.34407 |       0.00086 |       0.19823\n",
      "     -0.00310 |       0.00000 |      35.12269 |       0.00069 |       0.19527\n",
      "     -0.00322 |       0.00000 |      34.94532 |       0.00133 |       0.19455\n",
      "     -0.00339 |       0.00000 |      34.78278 |       0.00095 |       0.19576\n",
      "     -0.00343 |       0.00000 |      34.88008 |       0.00092 |       0.19550\n",
      "     -0.00374 |       0.00000 |      34.70613 |       0.00119 |       0.19523\n",
      "Evaluating losses...\n",
      "     -0.00446 |       0.00000 |      34.37589 |       0.00128 |       0.19559\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.5          |\n",
      "| EpRewMean       | -90.5         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2043          |\n",
      "| TimeElapsed     | 752           |\n",
      "| TimestepsSoFar  | 274432        |\n",
      "| ev_tdlam_before | 0.86          |\n",
      "| loss_ent        | 0.19559452    |\n",
      "| loss_kl         | 0.0012811237  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0044589285 |\n",
      "| loss_vf_loss    | 34.375893     |\n",
      "-----------------------------------\n",
      "********** Iteration 67 ************\n",
      "Eval num_timesteps=274432, episode_reward=-80.60 +/- 9.56\n",
      "Episode length: 81.60 +/- 9.56\n",
      "Eval num_timesteps=274432, episode_reward=-83.60 +/- 15.50\n",
      "Episode length: 84.60 +/- 15.50\n",
      "Eval num_timesteps=274432, episode_reward=-83.70 +/- 7.32\n",
      "Episode length: 84.70 +/- 7.32\n",
      "Eval num_timesteps=274432, episode_reward=-88.60 +/- 29.68\n",
      "Episode length: 89.60 +/- 29.68\n",
      "Eval num_timesteps=274432, episode_reward=-87.10 +/- 22.51\n",
      "Episode length: 88.10 +/- 22.51\n",
      "Eval num_timesteps=274432, episode_reward=-80.80 +/- 7.88\n",
      "Episode length: 81.80 +/- 7.88\n",
      "Eval num_timesteps=274432, episode_reward=-96.30 +/- 46.96\n",
      "Episode length: 97.30 +/- 46.96\n",
      "Eval num_timesteps=274432, episode_reward=-82.60 +/- 13.41\n",
      "Episode length: 83.60 +/- 13.41\n",
      "Eval num_timesteps=274432, episode_reward=-79.00 +/- 9.06\n",
      "Episode length: 80.00 +/- 9.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00035 |       0.00000 |      38.14724 |       0.00039 |       0.18862\n",
      "     -0.00095 |       0.00000 |      37.01427 |       0.00045 |       0.18820\n",
      "     -0.00220 |       0.00000 |      36.71339 |       0.00069 |       0.18859\n",
      "     -0.00212 |       0.00000 |      36.28425 |       0.00088 |       0.18884\n",
      "     -0.00227 |       0.00000 |      35.95674 |       0.00108 |       0.18689\n",
      "     -0.00254 |       0.00000 |      35.65555 |       0.00103 |       0.18969\n",
      "     -0.00210 |       0.00000 |      35.45675 |       0.00112 |       0.18441\n",
      "     -0.00278 |       0.00000 |      35.21503 |       0.00117 |       0.18635\n",
      "     -0.00294 |       0.00000 |      35.11738 |       0.00139 |       0.18630\n",
      "     -0.00331 |       0.00000 |      34.91264 |       0.00111 |       0.18805\n",
      "Evaluating losses...\n",
      "     -0.00374 |       0.00000 |      34.65435 |       0.00135 |       0.18332\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.5          |\n",
      "| EpRewMean       | -89.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 2090          |\n",
      "| TimeElapsed     | 759           |\n",
      "| TimestepsSoFar  | 278528        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.18332191    |\n",
      "| loss_kl         | 0.0013471196  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0037422772 |\n",
      "| loss_vf_loss    | 34.65435      |\n",
      "-----------------------------------\n",
      "********** Iteration 68 ************\n",
      "Eval num_timesteps=278528, episode_reward=-87.10 +/- 16.21\n",
      "Episode length: 88.10 +/- 16.21\n",
      "Eval num_timesteps=278528, episode_reward=-84.80 +/- 5.11\n",
      "Episode length: 85.80 +/- 5.11\n",
      "Eval num_timesteps=278528, episode_reward=-81.90 +/- 9.66\n",
      "Episode length: 82.90 +/- 9.66\n",
      "Eval num_timesteps=278528, episode_reward=-77.00 +/- 9.10\n",
      "Episode length: 78.00 +/- 9.10\n",
      "Eval num_timesteps=278528, episode_reward=-76.20 +/- 6.48\n",
      "Episode length: 77.20 +/- 6.48\n",
      "Eval num_timesteps=278528, episode_reward=-78.00 +/- 6.02\n",
      "Episode length: 79.00 +/- 6.02\n",
      "Eval num_timesteps=278528, episode_reward=-94.40 +/- 32.07\n",
      "Episode length: 95.40 +/- 32.07\n",
      "Eval num_timesteps=278528, episode_reward=-88.40 +/- 19.16\n",
      "Episode length: 89.40 +/- 19.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00083 |       0.00000 |      37.27404 |       0.00050 |       0.20352\n",
      "     -0.00220 |       0.00000 |      35.46309 |       0.00095 |       0.20449\n",
      "     -0.00310 |       0.00000 |      35.06991 |       0.00139 |       0.20540\n",
      "     -0.00345 |       0.00000 |      34.69835 |       0.00099 |       0.21066\n",
      "     -0.00358 |       0.00000 |      34.37003 |       0.00152 |       0.20754\n",
      "     -0.00398 |       0.00000 |      34.10664 |       0.00167 |       0.20890\n",
      "     -0.00424 |       0.00000 |      33.88154 |       0.00147 |       0.20934\n",
      "     -0.00417 |       0.00000 |      33.73182 |       0.00155 |       0.21048\n",
      "     -0.00408 |       0.00000 |      33.51388 |       0.00150 |       0.21157\n",
      "     -0.00462 |       0.00000 |      33.48271 |       0.00157 |       0.21081\n",
      "Evaluating losses...\n",
      "     -0.00534 |       0.00000 |      33.10305 |       0.00170 |       0.20904\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.1          |\n",
      "| EpRewMean       | -89.1         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 2132          |\n",
      "| TimeElapsed     | 766           |\n",
      "| TimestepsSoFar  | 282624        |\n",
      "| ev_tdlam_before | 0.853         |\n",
      "| loss_ent        | 0.20904395    |\n",
      "| loss_kl         | 0.0017034383  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0053430535 |\n",
      "| loss_vf_loss    | 33.103046     |\n",
      "-----------------------------------\n",
      "********** Iteration 69 ************\n",
      "Eval num_timesteps=282624, episode_reward=-84.30 +/- 8.92\n",
      "Episode length: 85.30 +/- 8.92\n",
      "Eval num_timesteps=282624, episode_reward=-80.50 +/- 6.39\n",
      "Episode length: 81.50 +/- 6.39\n",
      "Eval num_timesteps=282624, episode_reward=-83.70 +/- 7.14\n",
      "Episode length: 84.70 +/- 7.14\n",
      "Eval num_timesteps=282624, episode_reward=-104.70 +/- 43.93\n",
      "Episode length: 105.70 +/- 43.93\n",
      "Eval num_timesteps=282624, episode_reward=-88.80 +/- 24.00\n",
      "Episode length: 89.80 +/- 24.00\n",
      "Eval num_timesteps=282624, episode_reward=-83.60 +/- 12.88\n",
      "Episode length: 84.60 +/- 12.88\n",
      "Eval num_timesteps=282624, episode_reward=-82.10 +/- 9.69\n",
      "Episode length: 83.10 +/- 9.69\n",
      "Eval num_timesteps=282624, episode_reward=-83.00 +/- 6.34\n",
      "Episode length: 84.00 +/- 6.34\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00028 |       0.00000 |      43.25715 |       0.00042 |       0.21051\n",
      "     -0.00121 |       0.00000 |      39.67707 |       0.00062 |       0.21469\n",
      "     -0.00114 |       0.00000 |      38.88591 |       0.00092 |       0.21690\n",
      "     -0.00141 |       0.00000 |      38.32114 |       0.00094 |       0.21634\n",
      "     -0.00214 |       0.00000 |      37.85325 |       0.00093 |       0.21651\n",
      "     -0.00255 |       0.00000 |      37.49491 |       0.00084 |       0.21650\n",
      "     -0.00251 |       0.00000 |      37.12038 |       0.00100 |       0.21733\n",
      "     -0.00273 |       0.00000 |      36.94283 |       0.00116 |       0.21846\n",
      "     -0.00273 |       0.00000 |      36.77517 |       0.00113 |       0.22025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00310 |       0.00000 |      36.51596 |       0.00131 |       0.21959\n",
      "Evaluating losses...\n",
      "     -0.00400 |       0.00000 |      36.13499 |       0.00106 |       0.21902\n",
      "----------------------------------\n",
      "| EpLenMean       | 96           |\n",
      "| EpRewMean       | -95          |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 2173         |\n",
      "| TimeElapsed     | 774          |\n",
      "| TimestepsSoFar  | 286720       |\n",
      "| ev_tdlam_before | 0.815        |\n",
      "| loss_ent        | 0.21902236   |\n",
      "| loss_kl         | 0.0010628252 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003995313 |\n",
      "| loss_vf_loss    | 36.13499     |\n",
      "----------------------------------\n",
      "********** Iteration 70 ************\n",
      "Eval num_timesteps=286720, episode_reward=-83.30 +/- 7.11\n",
      "Episode length: 84.30 +/- 7.11\n",
      "Eval num_timesteps=286720, episode_reward=-90.10 +/- 13.22\n",
      "Episode length: 91.10 +/- 13.22\n",
      "Eval num_timesteps=286720, episode_reward=-85.60 +/- 9.45\n",
      "Episode length: 86.60 +/- 9.45\n",
      "Eval num_timesteps=286720, episode_reward=-84.00 +/- 8.29\n",
      "Episode length: 85.00 +/- 8.29\n",
      "Eval num_timesteps=286720, episode_reward=-81.10 +/- 9.46\n",
      "Episode length: 82.10 +/- 9.46\n",
      "Eval num_timesteps=286720, episode_reward=-109.90 +/- 66.89\n",
      "Episode length: 110.90 +/- 66.89\n",
      "Eval num_timesteps=286720, episode_reward=-85.20 +/- 16.42\n",
      "Episode length: 86.20 +/- 16.42\n",
      "Eval num_timesteps=286720, episode_reward=-83.20 +/- 7.14\n",
      "Episode length: 84.20 +/- 7.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -5.83e-05 |       0.00000 |      37.19801 |       0.00054 |       0.20513\n",
      "     -0.00115 |       0.00000 |      35.32097 |       0.00082 |       0.20641\n",
      "     -0.00146 |       0.00000 |      34.84109 |       0.00108 |       0.20419\n",
      "     -0.00211 |       0.00000 |      34.48479 |       0.00121 |       0.20313\n",
      "     -0.00253 |       0.00000 |      34.33950 |       0.00143 |       0.20247\n",
      "     -0.00240 |       0.00000 |      34.16749 |       0.00077 |       0.20476\n",
      "     -0.00294 |       0.00000 |      33.95745 |       0.00154 |       0.20313\n",
      "     -0.00302 |       0.00000 |      33.82732 |       0.00111 |       0.20363\n",
      "     -0.00315 |       0.00000 |      33.68772 |       0.00119 |       0.20341\n",
      "     -0.00289 |       0.00000 |      33.53451 |       0.00126 |       0.20337\n",
      "Evaluating losses...\n",
      "     -0.00344 |       0.00000 |      33.23758 |       0.00198 |       0.20010\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.5          |\n",
      "| EpRewMean       | -93.5         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 2216          |\n",
      "| TimeElapsed     | 781           |\n",
      "| TimestepsSoFar  | 290816        |\n",
      "| ev_tdlam_before | 0.848         |\n",
      "| loss_ent        | 0.20010142    |\n",
      "| loss_kl         | 0.0019752886  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034427722 |\n",
      "| loss_vf_loss    | 33.237583     |\n",
      "-----------------------------------\n",
      "********** Iteration 71 ************\n",
      "Eval num_timesteps=290816, episode_reward=-84.60 +/- 5.89\n",
      "Episode length: 85.60 +/- 5.89\n",
      "Eval num_timesteps=290816, episode_reward=-85.80 +/- 9.31\n",
      "Episode length: 86.80 +/- 9.31\n",
      "Eval num_timesteps=290816, episode_reward=-78.50 +/- 8.85\n",
      "Episode length: 79.50 +/- 8.85\n",
      "Eval num_timesteps=290816, episode_reward=-80.00 +/- 9.91\n",
      "Episode length: 81.00 +/- 9.91\n",
      "Eval num_timesteps=290816, episode_reward=-97.60 +/- 57.62\n",
      "Episode length: 98.60 +/- 57.62\n",
      "Eval num_timesteps=290816, episode_reward=-89.70 +/- 27.91\n",
      "Episode length: 90.70 +/- 27.91\n",
      "Eval num_timesteps=290816, episode_reward=-97.60 +/- 39.37\n",
      "Episode length: 98.60 +/- 39.37\n",
      "Eval num_timesteps=290816, episode_reward=-89.50 +/- 29.00\n",
      "Episode length: 90.50 +/- 29.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00057 |       0.00000 |      22.66063 |       0.00061 |       0.20680\n",
      "     -0.00211 |       0.00000 |      21.66446 |       0.00078 |       0.20385\n",
      "     -0.00276 |       0.00000 |      21.34925 |       0.00081 |       0.20484\n",
      "     -0.00364 |       0.00000 |      21.13552 |       0.00089 |       0.20258\n",
      "     -0.00392 |       0.00000 |      20.93345 |       0.00088 |       0.20244\n",
      "     -0.00425 |       0.00000 |      20.81995 |       0.00110 |       0.20153\n",
      "     -0.00428 |       0.00000 |      20.68790 |       0.00122 |       0.20026\n",
      "     -0.00455 |       0.00000 |      20.63252 |       0.00133 |       0.20129\n",
      "     -0.00491 |       0.00000 |      20.44804 |       0.00113 |       0.20203\n",
      "     -0.00481 |       0.00000 |      20.39849 |       0.00113 |       0.19934\n",
      "Evaluating losses...\n",
      "     -0.00564 |       0.00000 |      20.18906 |       0.00123 |       0.19894\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 2263         |\n",
      "| TimeElapsed     | 788          |\n",
      "| TimestepsSoFar  | 294912       |\n",
      "| ev_tdlam_before | 0.916        |\n",
      "| loss_ent        | 0.19894001   |\n",
      "| loss_kl         | 0.0012287976 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.00563637  |\n",
      "| loss_vf_loss    | 20.189064    |\n",
      "----------------------------------\n",
      "********** Iteration 72 ************\n",
      "Eval num_timesteps=294912, episode_reward=-82.10 +/- 11.19\n",
      "Episode length: 83.10 +/- 11.19\n",
      "Eval num_timesteps=294912, episode_reward=-79.60 +/- 7.51\n",
      "Episode length: 80.60 +/- 7.51\n",
      "Eval num_timesteps=294912, episode_reward=-81.50 +/- 10.52\n",
      "Episode length: 82.50 +/- 10.52\n",
      "Eval num_timesteps=294912, episode_reward=-79.00 +/- 6.74\n",
      "Episode length: 80.00 +/- 6.74\n",
      "Eval num_timesteps=294912, episode_reward=-97.80 +/- 49.11\n",
      "Episode length: 98.80 +/- 49.11\n",
      "Eval num_timesteps=294912, episode_reward=-127.30 +/- 72.72\n",
      "Episode length: 128.30 +/- 72.72\n",
      "Eval num_timesteps=294912, episode_reward=-86.30 +/- 9.18\n",
      "Episode length: 87.30 +/- 9.18\n",
      "Eval num_timesteps=294912, episode_reward=-83.30 +/- 12.73\n",
      "Episode length: 84.30 +/- 12.73\n",
      "Eval num_timesteps=294912, episode_reward=-94.10 +/- 27.60\n",
      "Episode length: 95.10 +/- 27.60\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00098 |       0.00000 |      39.12114 |       0.00060 |       0.20205\n",
      "     -0.00331 |       0.00000 |      36.32928 |       0.00120 |       0.20110\n",
      "     -0.00378 |       0.00000 |      35.55600 |       0.00138 |       0.20124\n",
      "     -0.00440 |       0.00000 |      35.11274 |       0.00139 |       0.20022\n",
      "     -0.00451 |       0.00000 |      34.79250 |       0.00148 |       0.20134\n",
      "     -0.00459 |       0.00000 |      34.45429 |       0.00195 |       0.20294\n",
      "     -0.00536 |       0.00000 |      34.15913 |       0.00159 |       0.20258\n",
      "     -0.00557 |       0.00000 |      33.93987 |       0.00172 |       0.20382\n",
      "     -0.00551 |       0.00000 |      33.81868 |       0.00184 |       0.20210\n",
      "     -0.00549 |       0.00000 |      33.58399 |       0.00181 |       0.20523\n",
      "Evaluating losses...\n",
      "     -0.00604 |       0.00000 |      33.18964 |       0.00185 |       0.19916\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.8         |\n",
      "| EpRewMean       | -90.8        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 2307         |\n",
      "| TimeElapsed     | 795          |\n",
      "| TimestepsSoFar  | 299008       |\n",
      "| ev_tdlam_before | 0.853        |\n",
      "| loss_ent        | 0.19916327   |\n",
      "| loss_kl         | 0.0018539024 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.006035382 |\n",
      "| loss_vf_loss    | 33.189644    |\n",
      "----------------------------------\n",
      "********** Iteration 73 ************\n",
      "Eval num_timesteps=299008, episode_reward=-123.80 +/- 125.61\n",
      "Episode length: 124.70 +/- 125.31\n",
      "Eval num_timesteps=299008, episode_reward=-88.30 +/- 17.65\n",
      "Episode length: 89.30 +/- 17.65\n",
      "Eval num_timesteps=299008, episode_reward=-81.70 +/- 6.75\n",
      "Episode length: 82.70 +/- 6.75\n",
      "Eval num_timesteps=299008, episode_reward=-76.80 +/- 7.44\n",
      "Episode length: 77.80 +/- 7.44\n",
      "Eval num_timesteps=299008, episode_reward=-88.50 +/- 30.13\n",
      "Episode length: 89.50 +/- 30.13\n",
      "Eval num_timesteps=299008, episode_reward=-84.40 +/- 11.52\n",
      "Episode length: 85.40 +/- 11.52\n",
      "Eval num_timesteps=299008, episode_reward=-86.80 +/- 27.00\n",
      "Episode length: 87.80 +/- 27.00\n",
      "Eval num_timesteps=299008, episode_reward=-104.30 +/- 55.55\n",
      "Episode length: 105.30 +/- 55.55\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00048 |       0.00000 |      28.08971 |       0.00056 |       0.19312\n",
      "     -0.00156 |       0.00000 |      27.25854 |       0.00108 |       0.18861\n",
      "     -0.00136 |       0.00000 |      26.68860 |       0.00092 |       0.19095\n",
      "     -0.00219 |       0.00000 |      26.38553 |       0.00098 |       0.18921\n",
      "     -0.00261 |       0.00000 |      26.00408 |       0.00104 |       0.18925\n",
      "     -0.00275 |       0.00000 |      25.79635 |       0.00117 |       0.18974\n",
      "     -0.00261 |       0.00000 |      25.58769 |       0.00108 |       0.18935\n",
      "     -0.00318 |       0.00000 |      25.43223 |       0.00133 |       0.19073\n",
      "     -0.00318 |       0.00000 |      25.29128 |       0.00135 |       0.18868\n",
      "     -0.00346 |       0.00000 |      25.20425 |       0.00107 |       0.18915\n",
      "Evaluating losses...\n",
      "     -0.00404 |       0.00000 |      24.94002 |       0.00151 |       0.19116\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.2          |\n",
      "| EpRewMean       | -92.2         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 2350          |\n",
      "| TimeElapsed     | 802           |\n",
      "| TimestepsSoFar  | 303104        |\n",
      "| ev_tdlam_before | 0.896         |\n",
      "| loss_ent        | 0.19115576    |\n",
      "| loss_kl         | 0.0015149417  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040389732 |\n",
      "| loss_vf_loss    | 24.940016     |\n",
      "-----------------------------------\n",
      "********** Iteration 74 ************\n",
      "Eval num_timesteps=303104, episode_reward=-77.00 +/- 9.44\n",
      "Episode length: 78.00 +/- 9.44\n",
      "Eval num_timesteps=303104, episode_reward=-86.10 +/- 17.00\n",
      "Episode length: 87.10 +/- 17.00\n",
      "Eval num_timesteps=303104, episode_reward=-82.00 +/- 12.56\n",
      "Episode length: 83.00 +/- 12.56\n",
      "Eval num_timesteps=303104, episode_reward=-86.00 +/- 24.22\n",
      "Episode length: 87.00 +/- 24.22\n",
      "Eval num_timesteps=303104, episode_reward=-78.20 +/- 5.86\n",
      "Episode length: 79.20 +/- 5.86\n",
      "Eval num_timesteps=303104, episode_reward=-82.90 +/- 12.54\n",
      "Episode length: 83.90 +/- 12.54\n",
      "Eval num_timesteps=303104, episode_reward=-86.00 +/- 20.61\n",
      "Episode length: 87.00 +/- 20.61\n",
      "Eval num_timesteps=303104, episode_reward=-78.10 +/- 12.00\n",
      "Episode length: 79.10 +/- 12.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.56e-05 |       0.00000 |      33.05473 |       0.00022 |       0.19680\n",
      "     -0.00100 |       0.00000 |      31.79129 |       0.00135 |       0.20129\n",
      "     -0.00148 |       0.00000 |      31.04436 |       0.00074 |       0.19856\n",
      "     -0.00182 |       0.00000 |      30.66326 |       0.00086 |       0.19863\n",
      "     -0.00199 |       0.00000 |      30.28473 |       0.00087 |       0.19843\n",
      "     -0.00237 |       0.00000 |      29.97511 |       0.00114 |       0.20137\n",
      "     -0.00237 |       0.00000 |      29.90744 |       0.00105 |       0.19954\n",
      "     -0.00295 |       0.00000 |      29.63306 |       0.00102 |       0.20019\n",
      "     -0.00307 |       0.00000 |      29.40419 |       0.00120 |       0.20147\n",
      "     -0.00325 |       0.00000 |      29.37410 |       0.00131 |       0.20088\n",
      "Evaluating losses...\n",
      "     -0.00408 |       0.00000 |      28.93736 |       0.00117 |       0.20033\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.5          |\n",
      "| EpRewMean       | -91.5         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2394          |\n",
      "| TimeElapsed     | 811           |\n",
      "| TimestepsSoFar  | 307200        |\n",
      "| ev_tdlam_before | 0.873         |\n",
      "| loss_ent        | 0.20033115    |\n",
      "| loss_kl         | 0.001170587   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0040802923 |\n",
      "| loss_vf_loss    | 28.937365     |\n",
      "-----------------------------------\n",
      "********** Iteration 75 ************\n",
      "Eval num_timesteps=307200, episode_reward=-81.30 +/- 5.18\n",
      "Episode length: 82.30 +/- 5.18\n",
      "Eval num_timesteps=307200, episode_reward=-81.10 +/- 14.53\n",
      "Episode length: 82.10 +/- 14.53\n",
      "Eval num_timesteps=307200, episode_reward=-89.80 +/- 32.51\n",
      "Episode length: 90.80 +/- 32.51\n",
      "Eval num_timesteps=307200, episode_reward=-79.80 +/- 8.01\n",
      "Episode length: 80.80 +/- 8.01\n",
      "Eval num_timesteps=307200, episode_reward=-79.80 +/- 4.58\n",
      "Episode length: 80.80 +/- 4.58\n",
      "Eval num_timesteps=307200, episode_reward=-94.40 +/- 49.95\n",
      "Episode length: 95.40 +/- 49.95\n",
      "Eval num_timesteps=307200, episode_reward=-88.50 +/- 14.95\n",
      "Episode length: 89.50 +/- 14.95\n",
      "Eval num_timesteps=307200, episode_reward=-100.60 +/- 41.87\n",
      "Episode length: 101.60 +/- 41.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00081 |       0.00000 |      19.84602 |       0.00038 |       0.19287\n",
      "    -9.36e-05 |       0.00000 |      19.41484 |       0.00047 |       0.19366\n",
      "     -0.00085 |       0.00000 |      19.22038 |       0.00098 |       0.19528\n",
      "     -0.00130 |       0.00000 |      19.04729 |       0.00091 |       0.18880\n",
      "     -0.00203 |       0.00000 |      18.91358 |       0.00078 |       0.19225\n",
      "     -0.00231 |       0.00000 |      18.78158 |       0.00094 |       0.19202\n",
      "     -0.00225 |       0.00000 |      18.65756 |       0.00110 |       0.18943\n",
      "     -0.00268 |       0.00000 |      18.67838 |       0.00100 |       0.19185\n",
      "     -0.00263 |       0.00000 |      18.54491 |       0.00123 |       0.19251\n",
      "     -0.00291 |       0.00000 |      18.45749 |       0.00094 |       0.19221\n",
      "Evaluating losses...\n",
      "     -0.00343 |       0.00000 |      18.52607 |       0.00092 |       0.18984\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.8          |\n",
      "| EpRewMean       | -88.8         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 2442          |\n",
      "| TimeElapsed     | 822           |\n",
      "| TimestepsSoFar  | 311296        |\n",
      "| ev_tdlam_before | 0.927         |\n",
      "| loss_ent        | 0.18983524    |\n",
      "| loss_kl         | 0.00091786496 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034326802 |\n",
      "| loss_vf_loss    | 18.52607      |\n",
      "-----------------------------------\n",
      "********** Iteration 76 ************\n",
      "Eval num_timesteps=311296, episode_reward=-88.00 +/- 12.05\n",
      "Episode length: 89.00 +/- 12.05\n",
      "Eval num_timesteps=311296, episode_reward=-92.70 +/- 26.90\n",
      "Episode length: 93.70 +/- 26.90\n",
      "Eval num_timesteps=311296, episode_reward=-93.00 +/- 24.04\n",
      "Episode length: 94.00 +/- 24.04\n",
      "Eval num_timesteps=311296, episode_reward=-90.50 +/- 27.92\n",
      "Episode length: 91.50 +/- 27.92\n",
      "Eval num_timesteps=311296, episode_reward=-83.90 +/- 8.96\n",
      "Episode length: 84.90 +/- 8.96\n",
      "Eval num_timesteps=311296, episode_reward=-91.50 +/- 26.99\n",
      "Episode length: 92.50 +/- 26.99\n",
      "Eval num_timesteps=311296, episode_reward=-83.00 +/- 12.01\n",
      "Episode length: 84.00 +/- 12.01\n",
      "Eval num_timesteps=311296, episode_reward=-89.40 +/- 10.33\n",
      "Episode length: 90.40 +/- 10.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.50e-05 |       0.00000 |      28.03222 |       0.00021 |       0.18849\n",
      "     -0.00157 |       0.00000 |      25.67834 |       0.00070 |       0.19338\n",
      "     -0.00343 |       0.00000 |      25.20536 |       0.00138 |       0.19715\n",
      "     -0.00605 |       0.00000 |      24.74253 |       0.00293 |       0.20225\n",
      "     -0.00855 |       0.00000 |      24.52553 |       0.00507 |       0.20926\n",
      "     -0.00948 |       0.00000 |      24.31253 |       0.00476 |       0.20752\n",
      "     -0.01014 |       0.00000 |      24.11267 |       0.00595 |       0.20971\n",
      "     -0.01043 |       0.00000 |      23.97205 |       0.00572 |       0.21204\n",
      "     -0.01076 |       0.00000 |      23.87751 |       0.00548 |       0.20856\n",
      "     -0.01126 |       0.00000 |      23.62749 |       0.00567 |       0.20812\n",
      "Evaluating losses...\n",
      "     -0.01163 |       0.00000 |      23.62133 |       0.00597 |       0.20939\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.8         |\n",
      "| EpRewMean       | -85.8        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 2489         |\n",
      "| TimeElapsed     | 833          |\n",
      "| TimestepsSoFar  | 315392       |\n",
      "| ev_tdlam_before | 0.89         |\n",
      "| loss_ent        | 0.20938934   |\n",
      "| loss_kl         | 0.0059701405 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.011634561 |\n",
      "| loss_vf_loss    | 23.621328    |\n",
      "----------------------------------\n",
      "********** Iteration 77 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=315392, episode_reward=-96.60 +/- 49.15\n",
      "Episode length: 97.60 +/- 49.15\n",
      "Eval num_timesteps=315392, episode_reward=-97.30 +/- 24.57\n",
      "Episode length: 98.30 +/- 24.57\n",
      "Eval num_timesteps=315392, episode_reward=-95.40 +/- 25.95\n",
      "Episode length: 96.40 +/- 25.95\n",
      "Eval num_timesteps=315392, episode_reward=-84.10 +/- 9.31\n",
      "Episode length: 85.10 +/- 9.31\n",
      "Eval num_timesteps=315392, episode_reward=-83.40 +/- 13.78\n",
      "Episode length: 84.40 +/- 13.78\n",
      "Eval num_timesteps=315392, episode_reward=-82.30 +/- 5.35\n",
      "Episode length: 83.30 +/- 5.35\n",
      "Eval num_timesteps=315392, episode_reward=-93.40 +/- 20.54\n",
      "Episode length: 94.40 +/- 20.54\n",
      "Eval num_timesteps=315392, episode_reward=-89.70 +/- 19.42\n",
      "Episode length: 90.70 +/- 19.42\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00018 |       0.00000 |      32.95494 |       0.00050 |       0.20967\n",
      "     -0.00139 |       0.00000 |      31.57215 |       0.00136 |       0.21022\n",
      "     -0.00205 |       0.00000 |      30.79147 |       0.00116 |       0.21286\n",
      "     -0.00263 |       0.00000 |      30.36046 |       0.00158 |       0.21405\n",
      "     -0.00315 |       0.00000 |      30.08891 |       0.00173 |       0.21854\n",
      "     -0.00351 |       0.00000 |      29.88371 |       0.00197 |       0.21744\n",
      "     -0.00391 |       0.00000 |      29.78076 |       0.00212 |       0.22059\n",
      "     -0.00420 |       0.00000 |      29.48527 |       0.00270 |       0.22273\n",
      "     -0.00435 |       0.00000 |      29.32481 |       0.00277 |       0.22191\n",
      "     -0.00469 |       0.00000 |      29.19689 |       0.00287 |       0.22256\n",
      "Evaluating losses...\n",
      "     -0.00564 |       0.00000 |      28.86149 |       0.00308 |       0.22521\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.6          |\n",
      "| EpRewMean       | -86.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2535          |\n",
      "| TimeElapsed     | 842           |\n",
      "| TimestepsSoFar  | 319488        |\n",
      "| ev_tdlam_before | 0.872         |\n",
      "| loss_ent        | 0.22520931    |\n",
      "| loss_kl         | 0.0030796085  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0056365966 |\n",
      "| loss_vf_loss    | 28.861494     |\n",
      "-----------------------------------\n",
      "********** Iteration 78 ************\n",
      "Eval num_timesteps=319488, episode_reward=-89.30 +/- 15.70\n",
      "Episode length: 90.30 +/- 15.70\n",
      "Eval num_timesteps=319488, episode_reward=-80.60 +/- 10.56\n",
      "Episode length: 81.60 +/- 10.56\n",
      "Eval num_timesteps=319488, episode_reward=-76.40 +/- 9.11\n",
      "Episode length: 77.40 +/- 9.11\n",
      "Eval num_timesteps=319488, episode_reward=-85.20 +/- 7.43\n",
      "Episode length: 86.20 +/- 7.43\n",
      "Eval num_timesteps=319488, episode_reward=-83.60 +/- 22.11\n",
      "Episode length: 84.60 +/- 22.11\n",
      "Eval num_timesteps=319488, episode_reward=-82.70 +/- 7.28\n",
      "Episode length: 83.70 +/- 7.28\n",
      "Eval num_timesteps=319488, episode_reward=-91.20 +/- 13.70\n",
      "Episode length: 92.20 +/- 13.70\n",
      "Eval num_timesteps=319488, episode_reward=-96.70 +/- 36.38\n",
      "Episode length: 97.70 +/- 36.38\n",
      "Eval num_timesteps=319488, episode_reward=-86.60 +/- 13.02\n",
      "Episode length: 87.60 +/- 13.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -9.48e-05 |       0.00000 |      40.94931 |       0.00045 |       0.22454\n",
      "     -0.00102 |       0.00000 |      39.58424 |       0.00072 |       0.21871\n",
      "     -0.00246 |       0.00000 |      38.90527 |       0.00066 |       0.21980\n",
      "     -0.00231 |       0.00000 |      38.40932 |       0.00091 |       0.21535\n",
      "     -0.00327 |       0.00000 |      38.00612 |       0.00092 |       0.21650\n",
      "     -0.00308 |       0.00000 |      37.75175 |       0.00102 |       0.21535\n",
      "     -0.00370 |       0.00000 |      37.52616 |       0.00105 |       0.21487\n",
      "     -0.00392 |       0.00000 |      37.22983 |       0.00115 |       0.21307\n",
      "     -0.00413 |       0.00000 |      37.00775 |       0.00104 |       0.21489\n",
      "     -0.00444 |       0.00000 |      36.79705 |       0.00104 |       0.21585\n",
      "Evaluating losses...\n",
      "     -0.00473 |       0.00000 |      36.39405 |       0.00123 |       0.21223\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.3          |\n",
      "| EpRewMean       | -90.3         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2579          |\n",
      "| TimeElapsed     | 851           |\n",
      "| TimestepsSoFar  | 323584        |\n",
      "| ev_tdlam_before | 0.829         |\n",
      "| loss_ent        | 0.21223086    |\n",
      "| loss_kl         | 0.001234358   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0047278227 |\n",
      "| loss_vf_loss    | 36.39405      |\n",
      "-----------------------------------\n",
      "********** Iteration 79 ************\n",
      "Eval num_timesteps=323584, episode_reward=-88.10 +/- 17.00\n",
      "Episode length: 89.10 +/- 17.00\n",
      "Eval num_timesteps=323584, episode_reward=-77.50 +/- 7.84\n",
      "Episode length: 78.50 +/- 7.84\n",
      "Eval num_timesteps=323584, episode_reward=-90.30 +/- 15.89\n",
      "Episode length: 91.30 +/- 15.89\n",
      "Eval num_timesteps=323584, episode_reward=-85.50 +/- 13.14\n",
      "Episode length: 86.50 +/- 13.14\n",
      "Eval num_timesteps=323584, episode_reward=-91.60 +/- 39.63\n",
      "Episode length: 92.60 +/- 39.63\n",
      "Eval num_timesteps=323584, episode_reward=-82.70 +/- 14.34\n",
      "Episode length: 83.70 +/- 14.34\n",
      "Eval num_timesteps=323584, episode_reward=-94.60 +/- 41.22\n",
      "Episode length: 95.60 +/- 41.22\n",
      "Eval num_timesteps=323584, episode_reward=-83.30 +/- 5.14\n",
      "Episode length: 84.30 +/- 5.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00057 |       0.00000 |      40.46249 |       0.00033 |       0.20538\n",
      "     -0.00062 |       0.00000 |      38.77137 |       0.00070 |       0.20575\n",
      "     -0.00068 |       0.00000 |      37.84985 |       0.00118 |       0.20065\n",
      "     -0.00148 |       0.00000 |      37.22976 |       0.00052 |       0.20510\n",
      "     -0.00180 |       0.00000 |      36.82207 |       0.00084 |       0.20339\n",
      "     -0.00241 |       0.00000 |      36.45052 |       0.00094 |       0.20308\n",
      "     -0.00179 |       0.00000 |      36.13092 |       0.00085 |       0.20410\n",
      "     -0.00214 |       0.00000 |      35.89793 |       0.00086 |       0.20408\n",
      "     -0.00266 |       0.00000 |      35.66928 |       0.00105 |       0.20307\n",
      "     -0.00316 |       0.00000 |      35.35118 |       0.00095 |       0.20347\n",
      "Evaluating losses...\n",
      "     -0.00370 |       0.00000 |      35.09475 |       0.00106 |       0.20562\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.8         |\n",
      "| EpRewMean       | -92.8        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 2622         |\n",
      "| TimeElapsed     | 860          |\n",
      "| TimestepsSoFar  | 327680       |\n",
      "| ev_tdlam_before | 0.829        |\n",
      "| loss_ent        | 0.20562062   |\n",
      "| loss_kl         | 0.0010637424 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003704648 |\n",
      "| loss_vf_loss    | 35.094753    |\n",
      "----------------------------------\n",
      "********** Iteration 80 ************\n",
      "Eval num_timesteps=327680, episode_reward=-90.20 +/- 19.86\n",
      "Episode length: 91.20 +/- 19.86\n",
      "Eval num_timesteps=327680, episode_reward=-99.50 +/- 67.07\n",
      "Episode length: 100.50 +/- 67.07\n",
      "Eval num_timesteps=327680, episode_reward=-104.30 +/- 54.48\n",
      "Episode length: 105.30 +/- 54.48\n",
      "Eval num_timesteps=327680, episode_reward=-81.80 +/- 9.52\n",
      "Episode length: 82.80 +/- 9.52\n",
      "Eval num_timesteps=327680, episode_reward=-82.90 +/- 4.50\n",
      "Episode length: 83.90 +/- 4.50\n",
      "Eval num_timesteps=327680, episode_reward=-77.70 +/- 8.71\n",
      "Episode length: 78.70 +/- 8.71\n",
      "Eval num_timesteps=327680, episode_reward=-77.90 +/- 7.15\n",
      "Episode length: 78.90 +/- 7.15\n",
      "Eval num_timesteps=327680, episode_reward=-79.70 +/- 7.81\n",
      "Episode length: 80.70 +/- 7.81\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00031 |       0.00000 |      41.03838 |       0.00033 |       0.22214\n",
      "     -0.00133 |       0.00000 |      37.44108 |       0.00063 |       0.22346\n",
      "     -0.00135 |       0.00000 |      36.34135 |       0.00062 |       0.23039\n",
      "     -0.00213 |       0.00000 |      35.71225 |       0.00068 |       0.22747\n",
      "     -0.00305 |       0.00000 |      35.28482 |       0.00086 |       0.22606\n",
      "     -0.00260 |       0.00000 |      34.86665 |       0.00078 |       0.23054\n",
      "     -0.00310 |       0.00000 |      34.68985 |       0.00110 |       0.22826\n",
      "     -0.00371 |       0.00000 |      34.39356 |       0.00084 |       0.22646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00396 |       0.00000 |      34.18450 |       0.00101 |       0.22897\n",
      "     -0.00411 |       0.00000 |      34.03606 |       0.00146 |       0.23202\n",
      "Evaluating losses...\n",
      "     -0.00516 |       0.00000 |      33.57907 |       0.00138 |       0.23331\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.1         |\n",
      "| EpRewMean       | -94.1        |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 2663         |\n",
      "| TimeElapsed     | 868          |\n",
      "| TimestepsSoFar  | 331776       |\n",
      "| ev_tdlam_before | 0.8          |\n",
      "| loss_ent        | 0.23330921   |\n",
      "| loss_kl         | 0.0013757108 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005157956 |\n",
      "| loss_vf_loss    | 33.57907     |\n",
      "----------------------------------\n",
      "********** Iteration 81 ************\n",
      "Eval num_timesteps=331776, episode_reward=-102.70 +/- 34.58\n",
      "Episode length: 103.70 +/- 34.58\n",
      "Eval num_timesteps=331776, episode_reward=-85.80 +/- 11.44\n",
      "Episode length: 86.80 +/- 11.44\n",
      "Eval num_timesteps=331776, episode_reward=-85.70 +/- 5.51\n",
      "Episode length: 86.70 +/- 5.51\n",
      "Eval num_timesteps=331776, episode_reward=-96.00 +/- 42.03\n",
      "Episode length: 97.00 +/- 42.03\n",
      "Eval num_timesteps=331776, episode_reward=-99.10 +/- 54.37\n",
      "Episode length: 100.10 +/- 54.37\n",
      "Eval num_timesteps=331776, episode_reward=-94.20 +/- 37.32\n",
      "Episode length: 95.20 +/- 37.32\n",
      "Eval num_timesteps=331776, episode_reward=-82.10 +/- 6.30\n",
      "Episode length: 83.10 +/- 6.30\n",
      "Eval num_timesteps=331776, episode_reward=-93.40 +/- 23.39\n",
      "Episode length: 94.40 +/- 23.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00078 |       0.00000 |      45.21027 |       0.00027 |       0.22500\n",
      "     -0.00069 |       0.00000 |      42.13715 |       0.00062 |       0.22438\n",
      "     -0.00083 |       0.00000 |      40.84797 |       0.00066 |       0.22490\n",
      "     -0.00164 |       0.00000 |      40.28570 |       0.00073 |       0.22495\n",
      "     -0.00212 |       0.00000 |      39.62109 |       0.00120 |       0.22699\n",
      "     -0.00233 |       0.00000 |      39.08551 |       0.00098 |       0.23090\n",
      "     -0.00274 |       0.00000 |      38.78195 |       0.00095 |       0.22691\n",
      "     -0.00308 |       0.00000 |      38.47490 |       0.00098 |       0.22946\n",
      "     -0.00365 |       0.00000 |      38.08883 |       0.00106 |       0.22715\n",
      "     -0.00360 |       0.00000 |      37.84978 |       0.00110 |       0.22889\n",
      "Evaluating losses...\n",
      "     -0.00433 |       0.00000 |      37.47444 |       0.00144 |       0.22749\n",
      "----------------------------------\n",
      "| EpLenMean       | 99.3         |\n",
      "| EpRewMean       | -98.3        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 2705         |\n",
      "| TimeElapsed     | 878          |\n",
      "| TimestepsSoFar  | 335872       |\n",
      "| ev_tdlam_before | 0.811        |\n",
      "| loss_ent        | 0.2274896    |\n",
      "| loss_kl         | 0.0014351354 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004325131 |\n",
      "| loss_vf_loss    | 37.474438    |\n",
      "----------------------------------\n",
      "********** Iteration 82 ************\n",
      "Eval num_timesteps=335872, episode_reward=-83.90 +/- 5.20\n",
      "Episode length: 84.90 +/- 5.20\n",
      "Eval num_timesteps=335872, episode_reward=-85.70 +/- 9.10\n",
      "Episode length: 86.70 +/- 9.10\n",
      "Eval num_timesteps=335872, episode_reward=-89.40 +/- 33.87\n",
      "Episode length: 90.40 +/- 33.87\n",
      "Eval num_timesteps=335872, episode_reward=-91.10 +/- 28.52\n",
      "Episode length: 92.10 +/- 28.52\n",
      "Eval num_timesteps=335872, episode_reward=-90.10 +/- 27.30\n",
      "Episode length: 91.10 +/- 27.30\n",
      "Eval num_timesteps=335872, episode_reward=-89.50 +/- 29.88\n",
      "Episode length: 90.50 +/- 29.88\n",
      "Eval num_timesteps=335872, episode_reward=-84.70 +/- 20.40\n",
      "Episode length: 85.70 +/- 20.40\n",
      "Eval num_timesteps=335872, episode_reward=-80.00 +/- 8.17\n",
      "Episode length: 81.00 +/- 8.17\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00073 |       0.00000 |      38.75188 |       0.00027 |       0.22666\n",
      "     -0.00078 |       0.00000 |      37.08595 |       0.00054 |       0.22786\n",
      "     -0.00102 |       0.00000 |      36.02620 |       0.00047 |       0.22582\n",
      "     -0.00185 |       0.00000 |      35.54502 |       0.00067 |       0.22429\n",
      "     -0.00216 |       0.00000 |      34.99599 |       0.00084 |       0.22436\n",
      "     -0.00254 |       0.00000 |      34.73229 |       0.00069 |       0.22346\n",
      "     -0.00243 |       0.00000 |      34.55589 |       0.00073 |       0.22592\n",
      "     -0.00267 |       0.00000 |      34.19697 |       0.00067 |       0.22690\n",
      "     -0.00300 |       0.00000 |      33.98812 |       0.00085 |       0.22302\n",
      "     -0.00307 |       0.00000 |      33.76274 |       0.00084 |       0.22492\n",
      "Evaluating losses...\n",
      "     -0.00411 |       0.00000 |      33.36860 |       0.00086 |       0.22253\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.4          |\n",
      "| EpRewMean       | -94.4         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2749          |\n",
      "| TimeElapsed     | 887           |\n",
      "| TimestepsSoFar  | 339968        |\n",
      "| ev_tdlam_before | 0.842         |\n",
      "| loss_ent        | 0.22253256    |\n",
      "| loss_kl         | 0.00086076366 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0041091987 |\n",
      "| loss_vf_loss    | 33.3686       |\n",
      "-----------------------------------\n",
      "********** Iteration 83 ************\n",
      "Eval num_timesteps=339968, episode_reward=-81.10 +/- 5.92\n",
      "Episode length: 82.10 +/- 5.92\n",
      "Eval num_timesteps=339968, episode_reward=-83.90 +/- 6.74\n",
      "Episode length: 84.90 +/- 6.74\n",
      "Eval num_timesteps=339968, episode_reward=-78.90 +/- 10.45\n",
      "Episode length: 79.90 +/- 10.45\n",
      "Eval num_timesteps=339968, episode_reward=-80.10 +/- 5.19\n",
      "Episode length: 81.10 +/- 5.19\n",
      "Eval num_timesteps=339968, episode_reward=-80.30 +/- 3.23\n",
      "Episode length: 81.30 +/- 3.23\n",
      "Eval num_timesteps=339968, episode_reward=-83.00 +/- 11.46\n",
      "Episode length: 84.00 +/- 11.46\n",
      "Eval num_timesteps=339968, episode_reward=-84.80 +/- 13.64\n",
      "Episode length: 85.80 +/- 13.64\n",
      "Eval num_timesteps=339968, episode_reward=-80.50 +/- 10.21\n",
      "Episode length: 81.50 +/- 10.21\n",
      "Eval num_timesteps=339968, episode_reward=-79.20 +/- 7.04\n",
      "Episode length: 80.20 +/- 7.04\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00029 |       0.00000 |      44.71143 |       0.00028 |       0.21273\n",
      "     -0.00108 |       0.00000 |      43.16435 |       0.00071 |       0.20790\n",
      "     -0.00137 |       0.00000 |      42.46360 |       0.00073 |       0.20466\n",
      "     -0.00145 |       0.00000 |      41.94045 |       0.00080 |       0.20823\n",
      "     -0.00161 |       0.00000 |      41.43967 |       0.00102 |       0.20533\n",
      "     -0.00229 |       0.00000 |      41.14865 |       0.00085 |       0.20813\n",
      "     -0.00244 |       0.00000 |      40.81792 |       0.00090 |       0.21006\n",
      "     -0.00259 |       0.00000 |      40.66099 |       0.00103 |       0.20687\n",
      "     -0.00257 |       0.00000 |      40.47944 |       0.00107 |       0.21098\n",
      "     -0.00305 |       0.00000 |      40.24227 |       0.00097 |       0.20819\n",
      "Evaluating losses...\n",
      "     -0.00391 |       0.00000 |      39.74825 |       0.00083 |       0.20985\n",
      "----------------------------------\n",
      "| EpLenMean       | 93           |\n",
      "| EpRewMean       | -92          |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 2791         |\n",
      "| TimeElapsed     | 895          |\n",
      "| TimestepsSoFar  | 344064       |\n",
      "| ev_tdlam_before | 0.823        |\n",
      "| loss_ent        | 0.20984839   |\n",
      "| loss_kl         | 0.0008285787 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003911591 |\n",
      "| loss_vf_loss    | 39.748253    |\n",
      "----------------------------------\n",
      "********** Iteration 84 ************\n",
      "Eval num_timesteps=344064, episode_reward=-77.30 +/- 9.12\n",
      "Episode length: 78.30 +/- 9.12\n",
      "Eval num_timesteps=344064, episode_reward=-82.50 +/- 13.30\n",
      "Episode length: 83.50 +/- 13.30\n",
      "Eval num_timesteps=344064, episode_reward=-79.90 +/- 11.20\n",
      "Episode length: 80.90 +/- 11.20\n",
      "Eval num_timesteps=344064, episode_reward=-81.00 +/- 8.60\n",
      "Episode length: 82.00 +/- 8.60\n",
      "Eval num_timesteps=344064, episode_reward=-78.30 +/- 10.15\n",
      "Episode length: 79.30 +/- 10.15\n",
      "Eval num_timesteps=344064, episode_reward=-85.30 +/- 13.51\n",
      "Episode length: 86.30 +/- 13.51\n",
      "Eval num_timesteps=344064, episode_reward=-83.20 +/- 9.61\n",
      "Episode length: 84.20 +/- 9.61\n",
      "Eval num_timesteps=344064, episode_reward=-77.00 +/- 13.26\n",
      "Episode length: 78.00 +/- 13.26\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00022 |       0.00000 |      36.24587 |       0.00045 |       0.21113\n",
      "     -0.00112 |       0.00000 |      34.50078 |       0.00092 |       0.22046\n",
      "     -0.00167 |       0.00000 |      33.96748 |       0.00088 |       0.21704\n",
      "     -0.00197 |       0.00000 |      33.46991 |       0.00076 |       0.21691\n",
      "     -0.00248 |       0.00000 |      33.10609 |       0.00107 |       0.21873\n",
      "     -0.00305 |       0.00000 |      32.78245 |       0.00107 |       0.22031\n",
      "     -0.00318 |       0.00000 |      32.53029 |       0.00131 |       0.21677\n",
      "     -0.00343 |       0.00000 |      32.34286 |       0.00127 |       0.22111\n",
      "     -0.00397 |       0.00000 |      32.01126 |       0.00159 |       0.22515\n",
      "     -0.00402 |       0.00000 |      31.80573 |       0.00173 |       0.22156\n",
      "Evaluating losses...\n",
      "     -0.00453 |       0.00000 |      31.45842 |       0.00145 |       0.21959\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.5         |\n",
      "| EpRewMean       | -91.5        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 2835         |\n",
      "| TimeElapsed     | 903          |\n",
      "| TimestepsSoFar  | 348160       |\n",
      "| ev_tdlam_before | 0.863        |\n",
      "| loss_ent        | 0.21959177   |\n",
      "| loss_kl         | 0.0014528526 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004533909 |\n",
      "| loss_vf_loss    | 31.458422    |\n",
      "----------------------------------\n",
      "********** Iteration 85 ************\n",
      "Eval num_timesteps=348160, episode_reward=-88.20 +/- 15.45\n",
      "Episode length: 89.20 +/- 15.45\n",
      "Eval num_timesteps=348160, episode_reward=-88.30 +/- 37.51\n",
      "Episode length: 89.30 +/- 37.51\n",
      "Eval num_timesteps=348160, episode_reward=-80.80 +/- 9.47\n",
      "Episode length: 81.80 +/- 9.47\n",
      "Eval num_timesteps=348160, episode_reward=-83.60 +/- 17.61\n",
      "Episode length: 84.60 +/- 17.61\n",
      "Eval num_timesteps=348160, episode_reward=-77.30 +/- 6.25\n",
      "Episode length: 78.30 +/- 6.25\n",
      "Eval num_timesteps=348160, episode_reward=-81.60 +/- 4.61\n",
      "Episode length: 82.60 +/- 4.61\n",
      "Eval num_timesteps=348160, episode_reward=-80.30 +/- 10.76\n",
      "Episode length: 81.30 +/- 10.76\n",
      "Eval num_timesteps=348160, episode_reward=-89.10 +/- 30.60\n",
      "Episode length: 90.10 +/- 30.60\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00014 |       0.00000 |      51.44931 |       0.00032 |       0.26024\n",
      "     -0.00104 |       0.00000 |      48.05105 |       0.00048 |       0.25948\n",
      "     -0.00167 |       0.00000 |      47.35407 |       0.00080 |       0.26156\n",
      "     -0.00207 |       0.00000 |      47.00120 |       0.00063 |       0.25851\n",
      "     -0.00226 |       0.00000 |      46.56281 |       0.00081 |       0.26053\n",
      "     -0.00249 |       0.00000 |      46.24448 |       0.00070 |       0.25715\n",
      "     -0.00234 |       0.00000 |      45.96173 |       0.00106 |       0.26508\n",
      "     -0.00328 |       0.00000 |      45.79251 |       0.00089 |       0.25967\n",
      "     -0.00343 |       0.00000 |      45.72980 |       0.00077 |       0.25621\n",
      "     -0.00332 |       0.00000 |      45.38616 |       0.00121 |       0.26209\n",
      "Evaluating losses...\n",
      "     -0.00433 |       0.00000 |      45.04723 |       0.00085 |       0.25728\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99.6          |\n",
      "| EpRewMean       | -98.6         |\n",
      "| EpThisIter      | 36            |\n",
      "| EpisodesSoFar   | 2871          |\n",
      "| TimeElapsed     | 911           |\n",
      "| TimestepsSoFar  | 352256        |\n",
      "| ev_tdlam_before | 0.791         |\n",
      "| loss_ent        | 0.25727558    |\n",
      "| loss_kl         | 0.000850951   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0043340903 |\n",
      "| loss_vf_loss    | 45.04723      |\n",
      "-----------------------------------\n",
      "********** Iteration 86 ************\n",
      "Eval num_timesteps=352256, episode_reward=-79.50 +/- 4.34\n",
      "Episode length: 80.50 +/- 4.34\n",
      "Eval num_timesteps=352256, episode_reward=-80.00 +/- 6.31\n",
      "Episode length: 81.00 +/- 6.31\n",
      "Eval num_timesteps=352256, episode_reward=-84.80 +/- 8.70\n",
      "Episode length: 85.80 +/- 8.70\n",
      "Eval num_timesteps=352256, episode_reward=-79.30 +/- 7.28\n",
      "Episode length: 80.30 +/- 7.28\n",
      "Eval num_timesteps=352256, episode_reward=-77.20 +/- 4.21\n",
      "Episode length: 78.20 +/- 4.21\n",
      "Eval num_timesteps=352256, episode_reward=-91.10 +/- 25.04\n",
      "Episode length: 92.10 +/- 25.04\n",
      "Eval num_timesteps=352256, episode_reward=-81.00 +/- 6.32\n",
      "Episode length: 82.00 +/- 6.32\n",
      "Eval num_timesteps=352256, episode_reward=-100.60 +/- 61.05\n",
      "Episode length: 101.60 +/- 61.05\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00035 |       0.00000 |      29.81712 |       0.00027 |       0.21101\n",
      "     -0.00132 |       0.00000 |      27.77515 |       0.00061 |       0.21111\n",
      "     -0.00182 |       0.00000 |      27.02116 |       0.00067 |       0.21102\n",
      "     -0.00201 |       0.00000 |      26.50370 |       0.00077 |       0.20931\n",
      "     -0.00171 |       0.00000 |      26.08640 |       0.00099 |       0.20903\n",
      "     -0.00256 |       0.00000 |      25.81196 |       0.00097 |       0.20907\n",
      "     -0.00284 |       0.00000 |      25.58169 |       0.00076 |       0.20952\n",
      "     -0.00305 |       0.00000 |      25.34548 |       0.00090 |       0.20879\n",
      "     -0.00342 |       0.00000 |      25.22141 |       0.00090 |       0.20964\n",
      "     -0.00332 |       0.00000 |      25.03025 |       0.00111 |       0.20733\n",
      "Evaluating losses...\n",
      "     -0.00411 |       0.00000 |      24.76066 |       0.00081 |       0.20774\n",
      "-----------------------------------\n",
      "| EpLenMean       | 102           |\n",
      "| EpRewMean       | -101          |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2915          |\n",
      "| TimeElapsed     | 918           |\n",
      "| TimestepsSoFar  | 356352        |\n",
      "| ev_tdlam_before | 0.891         |\n",
      "| loss_ent        | 0.20774353    |\n",
      "| loss_kl         | 0.00080947246 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.004106779  |\n",
      "| loss_vf_loss    | 24.76066      |\n",
      "-----------------------------------\n",
      "********** Iteration 87 ************\n",
      "Eval num_timesteps=356352, episode_reward=-91.90 +/- 32.66\n",
      "Episode length: 92.90 +/- 32.66\n",
      "Eval num_timesteps=356352, episode_reward=-87.80 +/- 28.69\n",
      "Episode length: 88.80 +/- 28.69\n",
      "Eval num_timesteps=356352, episode_reward=-81.00 +/- 8.54\n",
      "Episode length: 82.00 +/- 8.54\n",
      "Eval num_timesteps=356352, episode_reward=-74.00 +/- 8.53\n",
      "Episode length: 75.00 +/- 8.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=356352, episode_reward=-80.50 +/- 11.56\n",
      "Episode length: 81.50 +/- 11.56\n",
      "Eval num_timesteps=356352, episode_reward=-97.30 +/- 53.51\n",
      "Episode length: 98.30 +/- 53.51\n",
      "Eval num_timesteps=356352, episode_reward=-82.60 +/- 3.75\n",
      "Episode length: 83.60 +/- 3.75\n",
      "Eval num_timesteps=356352, episode_reward=-85.10 +/- 17.16\n",
      "Episode length: 86.10 +/- 17.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00098 |       0.00000 |      36.29878 |       0.00047 |       0.20442\n",
      "     -0.00200 |       0.00000 |      34.55342 |       0.00134 |       0.20400\n",
      "     -0.00217 |       0.00000 |      33.60852 |       0.00114 |       0.20778\n",
      "     -0.00296 |       0.00000 |      33.05192 |       0.00110 |       0.20491\n",
      "     -0.00336 |       0.00000 |      32.50759 |       0.00110 |       0.20441\n",
      "     -0.00380 |       0.00000 |      32.22179 |       0.00097 |       0.20273\n",
      "     -0.00396 |       0.00000 |      31.90550 |       0.00131 |       0.20286\n",
      "     -0.00372 |       0.00000 |      31.70547 |       0.00124 |       0.20282\n",
      "     -0.00422 |       0.00000 |      31.50104 |       0.00123 |       0.20520\n",
      "     -0.00422 |       0.00000 |      31.34064 |       0.00103 |       0.20412\n",
      "Evaluating losses...\n",
      "     -0.00483 |       0.00000 |      31.02488 |       0.00137 |       0.20382\n",
      "----------------------------------\n",
      "| EpLenMean       | 96.2         |\n",
      "| EpRewMean       | -95.2        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 2960         |\n",
      "| TimeElapsed     | 925          |\n",
      "| TimestepsSoFar  | 360448       |\n",
      "| ev_tdlam_before | 0.865        |\n",
      "| loss_ent        | 0.20382415   |\n",
      "| loss_kl         | 0.0013725859 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004829042 |\n",
      "| loss_vf_loss    | 31.02488     |\n",
      "----------------------------------\n",
      "********** Iteration 88 ************\n",
      "Eval num_timesteps=360448, episode_reward=-84.70 +/- 23.07\n",
      "Episode length: 85.70 +/- 23.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=360448, episode_reward=-82.00 +/- 9.49\n",
      "Episode length: 83.00 +/- 9.49\n",
      "Eval num_timesteps=360448, episode_reward=-82.90 +/- 8.60\n",
      "Episode length: 83.90 +/- 8.60\n",
      "Eval num_timesteps=360448, episode_reward=-86.20 +/- 6.43\n",
      "Episode length: 87.20 +/- 6.43\n",
      "Eval num_timesteps=360448, episode_reward=-85.20 +/- 19.89\n",
      "Episode length: 86.20 +/- 19.89\n",
      "Eval num_timesteps=360448, episode_reward=-80.70 +/- 4.58\n",
      "Episode length: 81.70 +/- 4.58\n",
      "Eval num_timesteps=360448, episode_reward=-76.20 +/- 8.08\n",
      "Episode length: 77.20 +/- 8.08\n",
      "Eval num_timesteps=360448, episode_reward=-88.80 +/- 21.62\n",
      "Episode length: 89.80 +/- 21.62\n",
      "Eval num_timesteps=360448, episode_reward=-82.20 +/- 8.03\n",
      "Episode length: 83.20 +/- 8.03\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00073 |       0.00000 |      25.64713 |       0.00032 |       0.20060\n",
      "     -0.00056 |       0.00000 |      24.68119 |       0.00036 |       0.19776\n",
      "     -0.00135 |       0.00000 |      24.24102 |       0.00072 |       0.19728\n",
      "     -0.00163 |       0.00000 |      23.94965 |       0.00079 |       0.20051\n",
      "     -0.00224 |       0.00000 |      23.74633 |       0.00074 |       0.19945\n",
      "     -0.00237 |       0.00000 |      23.58965 |       0.00074 |       0.19996\n",
      "     -0.00224 |       0.00000 |      23.54450 |       0.00086 |       0.20289\n",
      "     -0.00290 |       0.00000 |      23.36367 |       0.00085 |       0.20090\n",
      "     -0.00327 |       0.00000 |      23.22157 |       0.00081 |       0.20025\n",
      "     -0.00339 |       0.00000 |      23.11038 |       0.00073 |       0.20111\n",
      "Evaluating losses...\n",
      "     -0.00408 |       0.00000 |      22.91756 |       0.00076 |       0.20167\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.5         |\n",
      "| EpRewMean       | -89.5        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 3006         |\n",
      "| TimeElapsed     | 932          |\n",
      "| TimestepsSoFar  | 364544       |\n",
      "| ev_tdlam_before | 0.905        |\n",
      "| loss_ent        | 0.20167491   |\n",
      "| loss_kl         | 0.0007564707 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004077642 |\n",
      "| loss_vf_loss    | 22.917557    |\n",
      "----------------------------------\n",
      "********** Iteration 89 ************\n",
      "Eval num_timesteps=364544, episode_reward=-79.60 +/- 5.85\n",
      "Episode length: 80.60 +/- 5.85\n",
      "Eval num_timesteps=364544, episode_reward=-85.50 +/- 12.47\n",
      "Episode length: 86.50 +/- 12.47\n",
      "Eval num_timesteps=364544, episode_reward=-80.90 +/- 6.11\n",
      "Episode length: 81.90 +/- 6.11\n",
      "Eval num_timesteps=364544, episode_reward=-90.90 +/- 32.45\n",
      "Episode length: 91.90 +/- 32.45\n",
      "Eval num_timesteps=364544, episode_reward=-80.70 +/- 10.24\n",
      "Episode length: 81.70 +/- 10.24\n",
      "Eval num_timesteps=364544, episode_reward=-77.70 +/- 6.23\n",
      "Episode length: 78.70 +/- 6.23\n",
      "Eval num_timesteps=364544, episode_reward=-90.20 +/- 25.50\n",
      "Episode length: 91.20 +/- 25.50\n",
      "Eval num_timesteps=364544, episode_reward=-84.00 +/- 25.91\n",
      "Episode length: 85.00 +/- 25.91\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.77e-05 |       0.00000 |      39.99576 |       0.00036 |       0.20396\n",
      "     -0.00138 |       0.00000 |      36.83573 |       0.00052 |       0.20635\n",
      "     -0.00223 |       0.00000 |      36.03819 |       0.00074 |       0.20582\n",
      "     -0.00270 |       0.00000 |      35.76582 |       0.00080 |       0.20745\n",
      "     -0.00332 |       0.00000 |      35.45554 |       0.00107 |       0.20794\n",
      "     -0.00378 |       0.00000 |      35.14983 |       0.00094 |       0.20725\n",
      "     -0.00422 |       0.00000 |      34.91496 |       0.00113 |       0.20853\n",
      "     -0.00444 |       0.00000 |      34.74736 |       0.00144 |       0.21003\n",
      "     -0.00419 |       0.00000 |      34.61192 |       0.00159 |       0.21049\n",
      "     -0.00485 |       0.00000 |      34.49279 |       0.00153 |       0.21081\n",
      "Evaluating losses...\n",
      "     -0.00531 |       0.00000 |      34.14176 |       0.00151 |       0.20983\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.2         |\n",
      "| EpRewMean       | -92.2        |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 3047         |\n",
      "| TimeElapsed     | 940          |\n",
      "| TimestepsSoFar  | 368640       |\n",
      "| ev_tdlam_before | 0.831        |\n",
      "| loss_ent        | 0.20982654   |\n",
      "| loss_kl         | 0.0015077074 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005307532 |\n",
      "| loss_vf_loss    | 34.14176     |\n",
      "----------------------------------\n",
      "********** Iteration 90 ************\n",
      "Eval num_timesteps=368640, episode_reward=-84.80 +/- 7.57\n",
      "Episode length: 85.80 +/- 7.57\n",
      "Eval num_timesteps=368640, episode_reward=-79.10 +/- 5.97\n",
      "Episode length: 80.10 +/- 5.97\n",
      "Eval num_timesteps=368640, episode_reward=-83.20 +/- 8.41\n",
      "Episode length: 84.20 +/- 8.41\n",
      "Eval num_timesteps=368640, episode_reward=-82.60 +/- 8.81\n",
      "Episode length: 83.60 +/- 8.81\n",
      "Eval num_timesteps=368640, episode_reward=-97.00 +/- 33.80\n",
      "Episode length: 98.00 +/- 33.80\n",
      "Eval num_timesteps=368640, episode_reward=-81.20 +/- 9.36\n",
      "Episode length: 82.20 +/- 9.36\n",
      "Eval num_timesteps=368640, episode_reward=-83.90 +/- 10.42\n",
      "Episode length: 84.90 +/- 10.42\n",
      "Eval num_timesteps=368640, episode_reward=-84.60 +/- 20.28\n",
      "Episode length: 85.60 +/- 20.28\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00050 |       0.00000 |      33.55219 |       0.00027 |       0.20038\n",
      "     -0.00096 |       0.00000 |      32.67982 |       0.00041 |       0.20043\n",
      "     -0.00163 |       0.00000 |      32.16489 |       0.00051 |       0.20130\n",
      "     -0.00186 |       0.00000 |      31.87222 |       0.00068 |       0.20035\n",
      "     -0.00221 |       0.00000 |      31.54406 |       0.00079 |       0.20075\n",
      "     -0.00252 |       0.00000 |      31.31587 |       0.00092 |       0.20026\n",
      "     -0.00266 |       0.00000 |      31.17994 |       0.00093 |       0.20144\n",
      "     -0.00266 |       0.00000 |      31.10633 |       0.00103 |       0.20158\n",
      "     -0.00298 |       0.00000 |      30.94156 |       0.00095 |       0.20159\n",
      "     -0.00307 |       0.00000 |      30.82650 |       0.00109 |       0.20049\n",
      "Evaluating losses...\n",
      "     -0.00388 |       0.00000 |      30.44608 |       0.00100 |       0.20172\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94            |\n",
      "| EpRewMean       | -93           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3092          |\n",
      "| TimeElapsed     | 947           |\n",
      "| TimestepsSoFar  | 372736        |\n",
      "| ev_tdlam_before | 0.873         |\n",
      "| loss_ent        | 0.20171587    |\n",
      "| loss_kl         | 0.0010049911  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0038780223 |\n",
      "| loss_vf_loss    | 30.446085     |\n",
      "-----------------------------------\n",
      "********** Iteration 91 ************\n",
      "Eval num_timesteps=372736, episode_reward=-85.10 +/- 11.33\n",
      "Episode length: 86.10 +/- 11.33\n",
      "Eval num_timesteps=372736, episode_reward=-76.30 +/- 7.76\n",
      "Episode length: 77.30 +/- 7.76\n",
      "Eval num_timesteps=372736, episode_reward=-94.40 +/- 23.32\n",
      "Episode length: 95.40 +/- 23.32\n",
      "Eval num_timesteps=372736, episode_reward=-85.50 +/- 20.67\n",
      "Episode length: 86.50 +/- 20.67\n",
      "Eval num_timesteps=372736, episode_reward=-85.40 +/- 13.08\n",
      "Episode length: 86.40 +/- 13.08\n",
      "Eval num_timesteps=372736, episode_reward=-80.60 +/- 4.65\n",
      "Episode length: 81.60 +/- 4.65\n",
      "Eval num_timesteps=372736, episode_reward=-96.50 +/- 39.91\n",
      "Episode length: 97.50 +/- 39.91\n",
      "Eval num_timesteps=372736, episode_reward=-79.40 +/- 9.39\n",
      "Episode length: 80.40 +/- 9.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00011 |       0.00000 |      40.35912 |       0.00028 |       0.20914\n",
      "     -0.00166 |       0.00000 |      38.99284 |       0.00044 |       0.20764\n",
      "     -0.00206 |       0.00000 |      38.61719 |       0.00089 |       0.21036\n",
      "     -0.00272 |       0.00000 |      38.16011 |       0.00085 |       0.20846\n",
      "     -0.00309 |       0.00000 |      37.99026 |       0.00107 |       0.21124\n",
      "     -0.00323 |       0.00000 |      37.76534 |       0.00124 |       0.20941\n",
      "     -0.00378 |       0.00000 |      37.57449 |       0.00136 |       0.21172\n",
      "     -0.00366 |       0.00000 |      37.48874 |       0.00119 |       0.21062\n",
      "     -0.00390 |       0.00000 |      37.26664 |       0.00125 |       0.21295\n",
      "     -0.00408 |       0.00000 |      37.25304 |       0.00141 |       0.21263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "     -0.00458 |       0.00000 |      36.78093 |       0.00120 |       0.21031\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.9          |\n",
      "| EpRewMean       | -91.9         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 3135          |\n",
      "| TimeElapsed     | 953           |\n",
      "| TimestepsSoFar  | 376832        |\n",
      "| ev_tdlam_before | 0.851         |\n",
      "| loss_ent        | 0.21031196    |\n",
      "| loss_kl         | 0.0012001864  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0045818407 |\n",
      "| loss_vf_loss    | 36.780926     |\n",
      "-----------------------------------\n",
      "********** Iteration 92 ************\n",
      "Eval num_timesteps=376832, episode_reward=-87.00 +/- 26.20\n",
      "Episode length: 88.00 +/- 26.20\n",
      "Eval num_timesteps=376832, episode_reward=-93.10 +/- 39.16\n",
      "Episode length: 94.10 +/- 39.16\n",
      "Eval num_timesteps=376832, episode_reward=-81.60 +/- 8.91\n",
      "Episode length: 82.60 +/- 8.91\n",
      "Eval num_timesteps=376832, episode_reward=-77.90 +/- 6.53\n",
      "Episode length: 78.90 +/- 6.53\n",
      "Eval num_timesteps=376832, episode_reward=-88.00 +/- 20.63\n",
      "Episode length: 89.00 +/- 20.63\n",
      "Eval num_timesteps=376832, episode_reward=-81.20 +/- 6.72\n",
      "Episode length: 82.20 +/- 6.72\n",
      "Eval num_timesteps=376832, episode_reward=-79.20 +/- 7.97\n",
      "Episode length: 80.20 +/- 7.97\n",
      "Eval num_timesteps=376832, episode_reward=-84.10 +/- 16.16\n",
      "Episode length: 85.10 +/- 16.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00033 |       0.00000 |      48.04804 |       0.00025 |       0.20356\n",
      "     -0.00151 |       0.00000 |      46.71733 |       0.00052 |       0.20039\n",
      "     -0.00163 |       0.00000 |      45.97234 |       0.00061 |       0.20048\n",
      "     -0.00203 |       0.00000 |      45.47169 |       0.00067 |       0.20146\n",
      "     -0.00205 |       0.00000 |      45.01186 |       0.00082 |       0.20292\n",
      "     -0.00255 |       0.00000 |      44.65682 |       0.00063 |       0.20572\n",
      "     -0.00277 |       0.00000 |      44.42187 |       0.00079 |       0.20160\n",
      "     -0.00289 |       0.00000 |      44.18944 |       0.00090 |       0.20091\n",
      "     -0.00305 |       0.00000 |      43.88641 |       0.00087 |       0.20566\n",
      "     -0.00328 |       0.00000 |      43.83748 |       0.00095 |       0.20468\n",
      "Evaluating losses...\n",
      "     -0.00413 |       0.00000 |      43.37906 |       0.00091 |       0.20533\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.5         |\n",
      "| EpRewMean       | -94.5        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 3178         |\n",
      "| TimeElapsed     | 960          |\n",
      "| TimestepsSoFar  | 380928       |\n",
      "| ev_tdlam_before | 0.813        |\n",
      "| loss_ent        | 0.20532787   |\n",
      "| loss_kl         | 0.0009098335 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004127673 |\n",
      "| loss_vf_loss    | 43.379063    |\n",
      "----------------------------------\n",
      "********** Iteration 93 ************\n",
      "Eval num_timesteps=380928, episode_reward=-81.60 +/- 9.00\n",
      "Episode length: 82.60 +/- 9.00\n",
      "Eval num_timesteps=380928, episode_reward=-79.50 +/- 13.42\n",
      "Episode length: 80.50 +/- 13.42\n",
      "Eval num_timesteps=380928, episode_reward=-81.10 +/- 6.06\n",
      "Episode length: 82.10 +/- 6.06\n",
      "Eval num_timesteps=380928, episode_reward=-96.10 +/- 40.26\n",
      "Episode length: 97.10 +/- 40.26\n",
      "Eval num_timesteps=380928, episode_reward=-84.80 +/- 12.44\n",
      "Episode length: 85.80 +/- 12.44\n",
      "Eval num_timesteps=380928, episode_reward=-78.70 +/- 7.34\n",
      "Episode length: 79.70 +/- 7.34\n",
      "Eval num_timesteps=380928, episode_reward=-83.50 +/- 8.26\n",
      "Episode length: 84.50 +/- 8.26\n",
      "Eval num_timesteps=380928, episode_reward=-96.10 +/- 44.02\n",
      "Episode length: 97.10 +/- 44.02\n",
      "Eval num_timesteps=380928, episode_reward=-80.10 +/- 7.57\n",
      "Episode length: 81.10 +/- 7.57\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00040 |       0.00000 |      45.79194 |       0.00027 |       0.21138\n",
      "     -0.00166 |       0.00000 |      43.23557 |       0.00058 |       0.21300\n",
      "     -0.00217 |       0.00000 |      42.37012 |       0.00080 |       0.21221\n",
      "     -0.00315 |       0.00000 |      41.77879 |       0.00102 |       0.21634\n",
      "     -0.00221 |       0.00000 |      41.27444 |       0.00108 |       0.21093\n",
      "     -0.00311 |       0.00000 |      40.86812 |       0.00109 |       0.21445\n",
      "     -0.00312 |       0.00000 |      40.40296 |       0.00121 |       0.21190\n",
      "     -0.00370 |       0.00000 |      40.08805 |       0.00129 |       0.21672\n",
      "     -0.00424 |       0.00000 |      39.81231 |       0.00117 |       0.21625\n",
      "     -0.00437 |       0.00000 |      39.56641 |       0.00124 |       0.21360\n",
      "Evaluating losses...\n",
      "     -0.00485 |       0.00000 |      39.50237 |       0.00174 |       0.21840\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99            |\n",
      "| EpRewMean       | -98           |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 3220          |\n",
      "| TimeElapsed     | 967           |\n",
      "| TimestepsSoFar  | 385024        |\n",
      "| ev_tdlam_before | 0.796         |\n",
      "| loss_ent        | 0.218403      |\n",
      "| loss_kl         | 0.0017376646  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0048535974 |\n",
      "| loss_vf_loss    | 39.502373     |\n",
      "-----------------------------------\n",
      "********** Iteration 94 ************\n",
      "Eval num_timesteps=385024, episode_reward=-81.70 +/- 19.99\n",
      "Episode length: 82.70 +/- 19.99\n",
      "Eval num_timesteps=385024, episode_reward=-93.70 +/- 35.18\n",
      "Episode length: 94.70 +/- 35.18\n",
      "Eval num_timesteps=385024, episode_reward=-77.40 +/- 9.61\n",
      "Episode length: 78.40 +/- 9.61\n",
      "Eval num_timesteps=385024, episode_reward=-80.20 +/- 8.20\n",
      "Episode length: 81.20 +/- 8.20\n",
      "Eval num_timesteps=385024, episode_reward=-85.30 +/- 15.11\n",
      "Episode length: 86.30 +/- 15.11\n",
      "Eval num_timesteps=385024, episode_reward=-77.50 +/- 6.62\n",
      "Episode length: 78.50 +/- 6.62\n",
      "Eval num_timesteps=385024, episode_reward=-87.80 +/- 18.12\n",
      "Episode length: 88.80 +/- 18.12\n",
      "Eval num_timesteps=385024, episode_reward=-79.40 +/- 6.53\n",
      "Episode length: 80.40 +/- 6.53\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.92e-05 |       0.00000 |      33.63673 |       0.00019 |       0.19778\n",
      "     -0.00154 |       0.00000 |      32.12585 |       0.00068 |       0.19437\n",
      "     -0.00248 |       0.00000 |      31.38075 |       0.00081 |       0.19303\n",
      "     -0.00304 |       0.00000 |      30.96220 |       0.00093 |       0.19362\n",
      "     -0.00344 |       0.00000 |      30.49948 |       0.00104 |       0.19476\n",
      "     -0.00380 |       0.00000 |      30.21046 |       0.00139 |       0.19462\n",
      "     -0.00384 |       0.00000 |      30.01652 |       0.00125 |       0.19492\n",
      "     -0.00457 |       0.00000 |      29.79144 |       0.00130 |       0.19716\n",
      "     -0.00450 |       0.00000 |      29.51773 |       0.00139 |       0.19580\n",
      "     -0.00482 |       0.00000 |      29.36104 |       0.00148 |       0.19867\n",
      "Evaluating losses...\n",
      "     -0.00555 |       0.00000 |      29.10021 |       0.00157 |       0.19699\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.8         |\n",
      "| EpRewMean       | -90.8        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 3266         |\n",
      "| TimeElapsed     | 975          |\n",
      "| TimestepsSoFar  | 389120       |\n",
      "| ev_tdlam_before | 0.871        |\n",
      "| loss_ent        | 0.19699116   |\n",
      "| loss_kl         | 0.0015662881 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005548985 |\n",
      "| loss_vf_loss    | 29.100206    |\n",
      "----------------------------------\n",
      "********** Iteration 95 ************\n",
      "Eval num_timesteps=389120, episode_reward=-93.10 +/- 27.04\n",
      "Episode length: 94.10 +/- 27.04\n",
      "Eval num_timesteps=389120, episode_reward=-76.50 +/- 6.79\n",
      "Episode length: 77.50 +/- 6.79\n",
      "Eval num_timesteps=389120, episode_reward=-89.50 +/- 30.55\n",
      "Episode length: 90.50 +/- 30.55\n",
      "Eval num_timesteps=389120, episode_reward=-81.00 +/- 8.11\n",
      "Episode length: 82.00 +/- 8.11\n",
      "Eval num_timesteps=389120, episode_reward=-85.80 +/- 27.86\n",
      "Episode length: 86.80 +/- 27.86\n",
      "Eval num_timesteps=389120, episode_reward=-86.80 +/- 20.78\n",
      "Episode length: 87.80 +/- 20.78\n",
      "Eval num_timesteps=389120, episode_reward=-79.40 +/- 8.18\n",
      "Episode length: 80.40 +/- 8.18\n",
      "Eval num_timesteps=389120, episode_reward=-80.20 +/- 6.23\n",
      "Episode length: 81.20 +/- 6.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00012 |       0.00000 |      43.45531 |       0.00018 |       0.20796\n",
      "     -0.00117 |       0.00000 |      40.83772 |       0.00040 |       0.20760\n",
      "     -0.00167 |       0.00000 |      39.66636 |       0.00052 |       0.20956\n",
      "     -0.00192 |       0.00000 |      38.92306 |       0.00065 |       0.20923\n",
      "     -0.00253 |       0.00000 |      38.45744 |       0.00070 |       0.20728\n",
      "     -0.00277 |       0.00000 |      38.09492 |       0.00062 |       0.20939\n",
      "     -0.00303 |       0.00000 |      37.81869 |       0.00066 |       0.20665\n",
      "     -0.00320 |       0.00000 |      37.50113 |       0.00067 |       0.20930\n",
      "     -0.00328 |       0.00000 |      37.33324 |       0.00076 |       0.20854\n",
      "     -0.00335 |       0.00000 |      37.09382 |       0.00068 |       0.20771\n",
      "Evaluating losses...\n",
      "     -0.00391 |       0.00000 |      36.70542 |       0.00072 |       0.20927\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.6         |\n",
      "| EpRewMean       | -91.6        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 3308         |\n",
      "| TimeElapsed     | 984          |\n",
      "| TimestepsSoFar  | 393216       |\n",
      "| ev_tdlam_before | 0.822        |\n",
      "| loss_ent        | 0.20926785   |\n",
      "| loss_kl         | 0.0007236161 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003911394 |\n",
      "| loss_vf_loss    | 36.70542     |\n",
      "----------------------------------\n",
      "********** Iteration 96 ************\n",
      "Eval num_timesteps=393216, episode_reward=-97.10 +/- 33.74\n",
      "Episode length: 98.10 +/- 33.74\n",
      "Eval num_timesteps=393216, episode_reward=-100.70 +/- 51.33\n",
      "Episode length: 101.70 +/- 51.33\n",
      "Eval num_timesteps=393216, episode_reward=-85.20 +/- 16.49\n",
      "Episode length: 86.20 +/- 16.49\n",
      "Eval num_timesteps=393216, episode_reward=-93.00 +/- 24.65\n",
      "Episode length: 94.00 +/- 24.65\n",
      "Eval num_timesteps=393216, episode_reward=-78.60 +/- 8.91\n",
      "Episode length: 79.60 +/- 8.91\n",
      "Eval num_timesteps=393216, episode_reward=-81.70 +/- 14.70\n",
      "Episode length: 82.70 +/- 14.70\n",
      "Eval num_timesteps=393216, episode_reward=-84.20 +/- 9.47\n",
      "Episode length: 85.20 +/- 9.47\n",
      "Eval num_timesteps=393216, episode_reward=-77.60 +/- 8.11\n",
      "Episode length: 78.60 +/- 8.11\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00121 |       0.00000 |      43.45488 |       0.00024 |       0.20553\n",
      "     -0.00106 |       0.00000 |      41.85364 |       0.00073 |       0.20438\n",
      "     -0.00185 |       0.00000 |      40.96207 |       0.00056 |       0.20363\n",
      "     -0.00205 |       0.00000 |      40.28558 |       0.00076 |       0.20176\n",
      "     -0.00244 |       0.00000 |      39.71440 |       0.00088 |       0.20600\n",
      "     -0.00251 |       0.00000 |      39.26497 |       0.00093 |       0.20286\n",
      "     -0.00271 |       0.00000 |      38.84661 |       0.00096 |       0.20603\n",
      "     -0.00310 |       0.00000 |      38.49207 |       0.00106 |       0.20542\n",
      "     -0.00326 |       0.00000 |      38.18378 |       0.00120 |       0.20645\n",
      "     -0.00335 |       0.00000 |      38.01404 |       0.00103 |       0.20697\n",
      "Evaluating losses...\n",
      "     -0.00414 |       0.00000 |      37.50500 |       0.00115 |       0.20498\n",
      "----------------------------------\n",
      "| EpLenMean       | 94.4         |\n",
      "| EpRewMean       | -93.4        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 3352         |\n",
      "| TimeElapsed     | 992          |\n",
      "| TimestepsSoFar  | 397312       |\n",
      "| ev_tdlam_before | 0.829        |\n",
      "| loss_ent        | 0.20498462   |\n",
      "| loss_kl         | 0.0011452883 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004143579 |\n",
      "| loss_vf_loss    | 37.505005    |\n",
      "----------------------------------\n",
      "********** Iteration 97 ************\n",
      "Eval num_timesteps=397312, episode_reward=-79.60 +/- 11.52\n",
      "Episode length: 80.60 +/- 11.52\n",
      "Eval num_timesteps=397312, episode_reward=-84.00 +/- 6.26\n",
      "Episode length: 85.00 +/- 6.26\n",
      "Eval num_timesteps=397312, episode_reward=-93.30 +/- 25.44\n",
      "Episode length: 94.30 +/- 25.44\n",
      "Eval num_timesteps=397312, episode_reward=-80.40 +/- 12.55\n",
      "Episode length: 81.40 +/- 12.55\n",
      "Eval num_timesteps=397312, episode_reward=-92.60 +/- 32.93\n",
      "Episode length: 93.60 +/- 32.93\n",
      "Eval num_timesteps=397312, episode_reward=-84.90 +/- 18.25\n",
      "Episode length: 85.90 +/- 18.25\n",
      "Eval num_timesteps=397312, episode_reward=-80.90 +/- 8.17\n",
      "Episode length: 81.90 +/- 8.17\n",
      "Eval num_timesteps=397312, episode_reward=-80.30 +/- 12.95\n",
      "Episode length: 81.30 +/- 12.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00062 |       0.00000 |      34.76995 |       0.00023 |       0.19836\n",
      "     -0.00138 |       0.00000 |      33.96870 |       0.00074 |       0.19984\n",
      "     -0.00167 |       0.00000 |      33.41809 |       0.00072 |       0.19825\n",
      "     -0.00267 |       0.00000 |      33.10047 |       0.00085 |       0.19769\n",
      "     -0.00286 |       0.00000 |      32.91215 |       0.00092 |       0.19878\n",
      "     -0.00336 |       0.00000 |      32.68558 |       0.00098 |       0.19718\n",
      "     -0.00388 |       0.00000 |      32.49331 |       0.00117 |       0.19955\n",
      "     -0.00420 |       0.00000 |      32.34503 |       0.00111 |       0.19666\n",
      "     -0.00450 |       0.00000 |      32.14596 |       0.00108 |       0.19648\n",
      "     -0.00452 |       0.00000 |      32.09978 |       0.00104 |       0.19701\n",
      "Evaluating losses...\n",
      "     -0.00516 |       0.00000 |      31.72771 |       0.00109 |       0.19585\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.9         |\n",
      "| EpRewMean       | -90.9        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 3396         |\n",
      "| TimeElapsed     | 1e+03        |\n",
      "| TimestepsSoFar  | 401408       |\n",
      "| ev_tdlam_before | 0.863        |\n",
      "| loss_ent        | 0.19585235   |\n",
      "| loss_kl         | 0.0010899404 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005161968 |\n",
      "| loss_vf_loss    | 31.727709    |\n",
      "----------------------------------\n",
      "********** Iteration 98 ************\n",
      "Eval num_timesteps=401408, episode_reward=-103.70 +/- 50.30\n",
      "Episode length: 104.70 +/- 50.30\n",
      "Eval num_timesteps=401408, episode_reward=-97.80 +/- 39.88\n",
      "Episode length: 98.80 +/- 39.88\n",
      "Eval num_timesteps=401408, episode_reward=-83.80 +/- 4.64\n",
      "Episode length: 84.80 +/- 4.64\n",
      "Eval num_timesteps=401408, episode_reward=-86.50 +/- 4.88\n",
      "Episode length: 87.50 +/- 4.88\n",
      "Eval num_timesteps=401408, episode_reward=-78.40 +/- 6.77\n",
      "Episode length: 79.40 +/- 6.77\n",
      "Eval num_timesteps=401408, episode_reward=-79.20 +/- 7.12\n",
      "Episode length: 80.20 +/- 7.12\n",
      "Eval num_timesteps=401408, episode_reward=-97.00 +/- 51.90\n",
      "Episode length: 98.00 +/- 51.90\n",
      "Eval num_timesteps=401408, episode_reward=-82.90 +/- 13.55\n",
      "Episode length: 83.90 +/- 13.55\n",
      "Eval num_timesteps=401408, episode_reward=-85.70 +/- 12.80\n",
      "Episode length: 86.70 +/- 12.80\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.91e-05 |       0.00000 |      46.10194 |       0.00055 |       0.21212\n",
      "     -0.00114 |       0.00000 |      43.04276 |       0.00062 |       0.20838\n",
      "     -0.00116 |       0.00000 |      42.30081 |       0.00059 |       0.20796\n",
      "     -0.00176 |       0.00000 |      41.92929 |       0.00057 |       0.21081\n",
      "     -0.00190 |       0.00000 |      41.61979 |       0.00071 |       0.20770\n",
      "     -0.00206 |       0.00000 |      41.45998 |       0.00088 |       0.20831\n",
      "     -0.00269 |       0.00000 |      41.18180 |       0.00086 |       0.20870\n",
      "     -0.00255 |       0.00000 |      41.05993 |       0.00078 |       0.21142\n",
      "     -0.00312 |       0.00000 |      40.86425 |       0.00077 |       0.20924\n",
      "     -0.00306 |       0.00000 |      40.78537 |       0.00060 |       0.21033\n",
      "Evaluating losses...\n",
      "     -0.00394 |       0.00000 |      40.39916 |       0.00086 |       0.20972\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.7          |\n",
      "| EpRewMean       | -95.7         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 3437          |\n",
      "| TimeElapsed     | 1.01e+03      |\n",
      "| TimestepsSoFar  | 405504        |\n",
      "| ev_tdlam_before | 0.814         |\n",
      "| loss_ent        | 0.20971952    |\n",
      "| loss_kl         | 0.00085984636 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0039398214 |\n",
      "| loss_vf_loss    | 40.39916      |\n",
      "-----------------------------------\n",
      "********** Iteration 99 ************\n",
      "Eval num_timesteps=405504, episode_reward=-76.40 +/- 5.75\n",
      "Episode length: 77.40 +/- 5.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=405504, episode_reward=-79.00 +/- 5.57\n",
      "Episode length: 80.00 +/- 5.57\n",
      "Eval num_timesteps=405504, episode_reward=-81.30 +/- 7.59\n",
      "Episode length: 82.30 +/- 7.59\n",
      "Eval num_timesteps=405504, episode_reward=-81.30 +/- 6.63\n",
      "Episode length: 82.30 +/- 6.63\n",
      "Eval num_timesteps=405504, episode_reward=-79.40 +/- 7.85\n",
      "Episode length: 80.40 +/- 7.85\n",
      "Eval num_timesteps=405504, episode_reward=-80.50 +/- 24.45\n",
      "Episode length: 81.50 +/- 24.45\n",
      "Eval num_timesteps=405504, episode_reward=-82.00 +/- 6.34\n",
      "Episode length: 83.00 +/- 6.34\n",
      "Eval num_timesteps=405504, episode_reward=-75.40 +/- 5.16\n",
      "Episode length: 76.40 +/- 5.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -9.03e-06 |       0.00000 |      25.14244 |       0.00026 |       0.18603\n",
      "     -0.00105 |       0.00000 |      23.72073 |       0.00049 |       0.18439\n",
      "     -0.00127 |       0.00000 |      22.98232 |       0.00067 |       0.18413\n",
      "     -0.00177 |       0.00000 |      22.55887 |       0.00071 |       0.18383\n",
      "     -0.00264 |       0.00000 |      22.30371 |       0.00058 |       0.18480\n",
      "     -0.00258 |       0.00000 |      21.99958 |       0.00086 |       0.18468\n",
      "     -0.00296 |       0.00000 |      21.85707 |       0.00072 |       0.18381\n",
      "     -0.00292 |       0.00000 |      21.69592 |       0.00083 |       0.18328\n",
      "     -0.00289 |       0.00000 |      21.54691 |       0.00112 |       0.18356\n",
      "     -0.00302 |       0.00000 |      21.45498 |       0.00096 |       0.18203\n",
      "Evaluating losses...\n",
      "     -0.00378 |       0.00000 |      21.23019 |       0.00086 |       0.18323\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 3484          |\n",
      "| TimeElapsed     | 1.02e+03      |\n",
      "| TimestepsSoFar  | 409600        |\n",
      "| ev_tdlam_before | 0.907         |\n",
      "| loss_ent        | 0.18322828    |\n",
      "| loss_kl         | 0.00086228887 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0037844763 |\n",
      "| loss_vf_loss    | 21.230188     |\n",
      "-----------------------------------\n",
      "********** Iteration 100 ************\n",
      "Eval num_timesteps=409600, episode_reward=-77.90 +/- 3.73\n",
      "Episode length: 78.90 +/- 3.73\n",
      "Eval num_timesteps=409600, episode_reward=-100.60 +/- 38.90\n",
      "Episode length: 101.60 +/- 38.90\n",
      "Eval num_timesteps=409600, episode_reward=-81.30 +/- 13.86\n",
      "Episode length: 82.30 +/- 13.86\n",
      "Eval num_timesteps=409600, episode_reward=-84.30 +/- 9.38\n",
      "Episode length: 85.30 +/- 9.38\n",
      "Eval num_timesteps=409600, episode_reward=-88.30 +/- 25.32\n",
      "Episode length: 89.30 +/- 25.32\n",
      "Eval num_timesteps=409600, episode_reward=-86.70 +/- 14.87\n",
      "Episode length: 87.70 +/- 14.87\n",
      "Eval num_timesteps=409600, episode_reward=-82.00 +/- 8.40\n",
      "Episode length: 83.00 +/- 8.40\n",
      "Eval num_timesteps=409600, episode_reward=-80.60 +/- 7.90\n",
      "Episode length: 81.60 +/- 7.90\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00063 |       0.00000 |      40.30344 |       0.00020 |       0.18683\n",
      "     -0.00028 |       0.00000 |      38.19512 |       0.00045 |       0.18902\n",
      "     -0.00095 |       0.00000 |      37.24528 |       0.00051 |       0.18843\n",
      "     -0.00104 |       0.00000 |      36.74305 |       0.00048 |       0.18823\n",
      "     -0.00165 |       0.00000 |      36.41110 |       0.00060 |       0.18763\n",
      "     -0.00187 |       0.00000 |      36.09044 |       0.00058 |       0.18968\n",
      "     -0.00230 |       0.00000 |      35.77071 |       0.00064 |       0.18984\n",
      "     -0.00234 |       0.00000 |      35.52824 |       0.00081 |       0.19061\n",
      "     -0.00255 |       0.00000 |      35.29567 |       0.00068 |       0.19025\n",
      "     -0.00267 |       0.00000 |      35.15133 |       0.00068 |       0.18948\n",
      "Evaluating losses...\n",
      "     -0.00324 |       0.00000 |      34.79589 |       0.00076 |       0.18738\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.1          |\n",
      "| EpRewMean       | -92.1         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 3527          |\n",
      "| TimeElapsed     | 1.02e+03      |\n",
      "| TimestepsSoFar  | 413696        |\n",
      "| ev_tdlam_before | 0.839         |\n",
      "| loss_ent        | 0.18737942    |\n",
      "| loss_kl         | 0.0007594767  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0032416778 |\n",
      "| loss_vf_loss    | 34.795895     |\n",
      "-----------------------------------\n",
      "********** Iteration 101 ************\n",
      "Eval num_timesteps=413696, episode_reward=-81.90 +/- 8.08\n",
      "Episode length: 82.90 +/- 8.08\n",
      "Eval num_timesteps=413696, episode_reward=-95.20 +/- 39.66\n",
      "Episode length: 96.20 +/- 39.66\n",
      "Eval num_timesteps=413696, episode_reward=-78.10 +/- 4.61\n",
      "Episode length: 79.10 +/- 4.61\n",
      "Eval num_timesteps=413696, episode_reward=-83.70 +/- 11.16\n",
      "Episode length: 84.70 +/- 11.16\n",
      "Eval num_timesteps=413696, episode_reward=-78.10 +/- 6.64\n",
      "Episode length: 79.10 +/- 6.64\n",
      "Eval num_timesteps=413696, episode_reward=-82.20 +/- 8.40\n",
      "Episode length: 83.20 +/- 8.40\n",
      "Eval num_timesteps=413696, episode_reward=-83.60 +/- 16.34\n",
      "Episode length: 84.60 +/- 16.34\n",
      "Eval num_timesteps=413696, episode_reward=-79.30 +/- 10.15\n",
      "Episode length: 80.30 +/- 10.15\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00027 |       0.00000 |      37.55453 |       0.00031 |       0.20077\n",
      "     -0.00133 |       0.00000 |      34.43403 |       0.00063 |       0.19960\n",
      "     -0.00131 |       0.00000 |      33.43027 |       0.00076 |       0.19856\n",
      "     -0.00161 |       0.00000 |      32.97920 |       0.00079 |       0.19926\n",
      "     -0.00238 |       0.00000 |      32.40258 |       0.00073 |       0.19976\n",
      "     -0.00232 |       0.00000 |      32.13572 |       0.00091 |       0.20079\n",
      "     -0.00336 |       0.00000 |      31.83150 |       0.00089 |       0.19815\n",
      "     -0.00306 |       0.00000 |      31.60925 |       0.00091 |       0.20149\n",
      "     -0.00358 |       0.00000 |      31.37870 |       0.00102 |       0.19959\n",
      "     -0.00374 |       0.00000 |      31.30024 |       0.00094 |       0.19818\n",
      "Evaluating losses...\n",
      "     -0.00384 |       0.00000 |      30.83138 |       0.00161 |       0.20349\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.4          |\n",
      "| EpRewMean       | -93.4         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 3569          |\n",
      "| TimeElapsed     | 1.03e+03      |\n",
      "| TimestepsSoFar  | 417792        |\n",
      "| ev_tdlam_before | 0.837         |\n",
      "| loss_ent        | 0.20349273    |\n",
      "| loss_kl         | 0.0016100439  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0038422036 |\n",
      "| loss_vf_loss    | 30.831377     |\n",
      "-----------------------------------\n",
      "********** Iteration 102 ************\n",
      "Eval num_timesteps=417792, episode_reward=-81.50 +/- 9.66\n",
      "Episode length: 82.50 +/- 9.66\n",
      "Eval num_timesteps=417792, episode_reward=-80.30 +/- 9.75\n",
      "Episode length: 81.30 +/- 9.75\n",
      "Eval num_timesteps=417792, episode_reward=-80.50 +/- 8.02\n",
      "Episode length: 81.50 +/- 8.02\n",
      "Eval num_timesteps=417792, episode_reward=-96.90 +/- 34.19\n",
      "Episode length: 97.90 +/- 34.19\n",
      "Eval num_timesteps=417792, episode_reward=-89.80 +/- 36.16\n",
      "Episode length: 90.80 +/- 36.16\n",
      "Eval num_timesteps=417792, episode_reward=-77.90 +/- 8.64\n",
      "Episode length: 78.90 +/- 8.64\n",
      "Eval num_timesteps=417792, episode_reward=-97.00 +/- 35.45\n",
      "Episode length: 98.00 +/- 35.45\n",
      "Eval num_timesteps=417792, episode_reward=-79.80 +/- 9.77\n",
      "Episode length: 80.80 +/- 9.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00064 |       0.00000 |      36.27258 |       0.00030 |       0.18343\n",
      "     -0.00053 |       0.00000 |      35.00912 |       0.00068 |       0.18076\n",
      "     -0.00189 |       0.00000 |      34.31008 |       0.00065 |       0.17959\n",
      "     -0.00202 |       0.00000 |      33.87160 |       0.00081 |       0.17931\n",
      "     -0.00247 |       0.00000 |      33.50429 |       0.00101 |       0.17950\n",
      "     -0.00272 |       0.00000 |      33.16316 |       0.00099 |       0.17989\n",
      "     -0.00293 |       0.00000 |      32.90253 |       0.00099 |       0.17802\n",
      "     -0.00302 |       0.00000 |      32.65674 |       0.00111 |       0.17979\n",
      "     -0.00321 |       0.00000 |      32.43287 |       0.00127 |       0.17780\n",
      "     -0.00341 |       0.00000 |      32.21629 |       0.00134 |       0.17866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "     -0.00401 |       0.00000 |      32.00617 |       0.00118 |       0.17732\n",
      "----------------------------------\n",
      "| EpLenMean       | 94.1         |\n",
      "| EpRewMean       | -93.1        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 3613         |\n",
      "| TimeElapsed     | 1.04e+03     |\n",
      "| TimestepsSoFar  | 421888       |\n",
      "| ev_tdlam_before | 0.86         |\n",
      "| loss_ent        | 0.17731664   |\n",
      "| loss_kl         | 0.001180807  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004008326 |\n",
      "| loss_vf_loss    | 32.00617     |\n",
      "----------------------------------\n",
      "********** Iteration 103 ************\n",
      "Eval num_timesteps=421888, episode_reward=-93.50 +/- 39.70\n",
      "Episode length: 94.50 +/- 39.70\n",
      "Eval num_timesteps=421888, episode_reward=-95.00 +/- 29.21\n",
      "Episode length: 96.00 +/- 29.21\n",
      "Eval num_timesteps=421888, episode_reward=-85.00 +/- 16.66\n",
      "Episode length: 86.00 +/- 16.66\n",
      "Eval num_timesteps=421888, episode_reward=-91.10 +/- 27.22\n",
      "Episode length: 92.10 +/- 27.22\n",
      "Eval num_timesteps=421888, episode_reward=-80.60 +/- 12.40\n",
      "Episode length: 81.60 +/- 12.40\n",
      "Eval num_timesteps=421888, episode_reward=-79.70 +/- 9.50\n",
      "Episode length: 80.70 +/- 9.50\n",
      "Eval num_timesteps=421888, episode_reward=-79.90 +/- 4.57\n",
      "Episode length: 80.90 +/- 4.57\n",
      "Eval num_timesteps=421888, episode_reward=-82.10 +/- 9.66\n",
      "Episode length: 83.10 +/- 9.66\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00044 |       0.00000 |      37.11362 |       0.00029 |       0.18144\n",
      "     -0.00132 |       0.00000 |      36.21795 |       0.00061 |       0.17953\n",
      "     -0.00111 |       0.00000 |      35.73420 |       0.00076 |       0.17857\n",
      "     -0.00169 |       0.00000 |      35.69595 |       0.00074 |       0.17978\n",
      "     -0.00194 |       0.00000 |      35.30307 |       0.00105 |       0.17744\n",
      "     -0.00248 |       0.00000 |      35.15128 |       0.00083 |       0.17841\n",
      "     -0.00259 |       0.00000 |      34.95918 |       0.00073 |       0.17675\n",
      "     -0.00278 |       0.00000 |      34.83778 |       0.00083 |       0.17832\n",
      "     -0.00264 |       0.00000 |      34.69012 |       0.00076 |       0.17975\n",
      "     -0.00277 |       0.00000 |      34.64337 |       0.00089 |       0.17714\n",
      "Evaluating losses...\n",
      "     -0.00349 |       0.00000 |      34.43801 |       0.00092 |       0.18007\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.5         |\n",
      "| EpRewMean       | -89.5        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 3658         |\n",
      "| TimeElapsed     | 1.05e+03     |\n",
      "| TimestepsSoFar  | 425984       |\n",
      "| ev_tdlam_before | 0.855        |\n",
      "| loss_ent        | 0.18007341   |\n",
      "| loss_kl         | 0.000924349  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003490706 |\n",
      "| loss_vf_loss    | 34.43801     |\n",
      "----------------------------------\n",
      "********** Iteration 104 ************\n",
      "Eval num_timesteps=425984, episode_reward=-82.90 +/- 7.54\n",
      "Episode length: 83.90 +/- 7.54\n",
      "Eval num_timesteps=425984, episode_reward=-89.90 +/- 26.18\n",
      "Episode length: 90.90 +/- 26.18\n",
      "Eval num_timesteps=425984, episode_reward=-82.60 +/- 9.70\n",
      "Episode length: 83.60 +/- 9.70\n",
      "Eval num_timesteps=425984, episode_reward=-76.80 +/- 3.22\n",
      "Episode length: 77.80 +/- 3.22\n",
      "Eval num_timesteps=425984, episode_reward=-76.00 +/- 8.96\n",
      "Episode length: 77.00 +/- 8.96\n",
      "Eval num_timesteps=425984, episode_reward=-90.60 +/- 39.05\n",
      "Episode length: 91.60 +/- 39.05\n",
      "Eval num_timesteps=425984, episode_reward=-91.90 +/- 40.54\n",
      "Episode length: 92.90 +/- 40.54\n",
      "Eval num_timesteps=425984, episode_reward=-83.90 +/- 16.74\n",
      "Episode length: 84.90 +/- 16.74\n",
      "Eval num_timesteps=425984, episode_reward=-80.40 +/- 8.69\n",
      "Episode length: 81.40 +/- 8.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00018 |       0.00000 |      28.79311 |       0.00022 |       0.17632\n",
      "     -0.00081 |       0.00000 |      28.29945 |       0.00055 |       0.17695\n",
      "     -0.00113 |       0.00000 |      27.91357 |       0.00040 |       0.17654\n",
      "     -0.00111 |       0.00000 |      27.68607 |       0.00055 |       0.17959\n",
      "     -0.00169 |       0.00000 |      27.47409 |       0.00058 |       0.17730\n",
      "     -0.00201 |       0.00000 |      27.28529 |       0.00063 |       0.17734\n",
      "     -0.00202 |       0.00000 |      27.20728 |       0.00075 |       0.17700\n",
      "     -0.00192 |       0.00000 |      27.13106 |       0.00066 |       0.17701\n",
      "     -0.00203 |       0.00000 |      26.91952 |       0.00087 |       0.17629\n",
      "     -0.00251 |       0.00000 |      26.86959 |       0.00080 |       0.17703\n",
      "Evaluating losses...\n",
      "     -0.00291 |       0.00000 |      26.62436 |       0.00059 |       0.17892\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.7          |\n",
      "| EpRewMean       | -90.7         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 3702          |\n",
      "| TimeElapsed     | 1.06e+03      |\n",
      "| TimestepsSoFar  | 430080        |\n",
      "| ev_tdlam_before | 0.892         |\n",
      "| loss_ent        | 0.17892344    |\n",
      "| loss_kl         | 0.0005940273  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0029083039 |\n",
      "| loss_vf_loss    | 26.624357     |\n",
      "-----------------------------------\n",
      "********** Iteration 105 ************\n",
      "Eval num_timesteps=430080, episode_reward=-82.80 +/- 5.42\n",
      "Episode length: 83.80 +/- 5.42\n",
      "Eval num_timesteps=430080, episode_reward=-81.70 +/- 8.32\n",
      "Episode length: 82.70 +/- 8.32\n",
      "Eval num_timesteps=430080, episode_reward=-79.50 +/- 4.88\n",
      "Episode length: 80.50 +/- 4.88\n",
      "Eval num_timesteps=430080, episode_reward=-80.80 +/- 4.92\n",
      "Episode length: 81.80 +/- 4.92\n",
      "Eval num_timesteps=430080, episode_reward=-79.20 +/- 10.98\n",
      "Episode length: 80.20 +/- 10.98\n",
      "Eval num_timesteps=430080, episode_reward=-120.30 +/- 71.94\n",
      "Episode length: 121.30 +/- 71.94\n",
      "Eval num_timesteps=430080, episode_reward=-78.70 +/- 11.32\n",
      "Episode length: 79.70 +/- 11.32\n",
      "Eval num_timesteps=430080, episode_reward=-81.90 +/- 5.68\n",
      "Episode length: 82.90 +/- 5.68\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00048 |       0.00000 |      34.60013 |       0.00046 |       0.18369\n",
      "     -0.00093 |       0.00000 |      32.95035 |       0.00058 |       0.18544\n",
      "     -0.00136 |       0.00000 |      32.53547 |       0.00041 |       0.18488\n",
      "     -0.00175 |       0.00000 |      32.20662 |       0.00056 |       0.18311\n",
      "     -0.00219 |       0.00000 |      32.03645 |       0.00048 |       0.18387\n",
      "     -0.00234 |       0.00000 |      31.77555 |       0.00043 |       0.18494\n",
      "     -0.00262 |       0.00000 |      31.59580 |       0.00053 |       0.18396\n",
      "     -0.00285 |       0.00000 |      31.39290 |       0.00049 |       0.18445\n",
      "     -0.00274 |       0.00000 |      31.24908 |       0.00042 |       0.18422\n",
      "     -0.00301 |       0.00000 |      31.07300 |       0.00049 |       0.18420\n",
      "Evaluating losses...\n",
      "     -0.00367 |       0.00000 |      30.80068 |       0.00052 |       0.18390\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95            |\n",
      "| EpRewMean       | -94           |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 3744          |\n",
      "| TimeElapsed     | 1.06e+03      |\n",
      "| TimestepsSoFar  | 434176        |\n",
      "| ev_tdlam_before | 0.866         |\n",
      "| loss_ent        | 0.18390173    |\n",
      "| loss_kl         | 0.00051672355 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036714198 |\n",
      "| loss_vf_loss    | 30.800682     |\n",
      "-----------------------------------\n",
      "********** Iteration 106 ************\n",
      "Eval num_timesteps=434176, episode_reward=-95.10 +/- 47.52\n",
      "Episode length: 96.10 +/- 47.52\n",
      "Eval num_timesteps=434176, episode_reward=-81.20 +/- 9.54\n",
      "Episode length: 82.20 +/- 9.54\n",
      "Eval num_timesteps=434176, episode_reward=-95.30 +/- 32.34\n",
      "Episode length: 96.30 +/- 32.34\n",
      "Eval num_timesteps=434176, episode_reward=-86.60 +/- 6.95\n",
      "Episode length: 87.60 +/- 6.95\n",
      "Eval num_timesteps=434176, episode_reward=-81.60 +/- 9.28\n",
      "Episode length: 82.60 +/- 9.28\n",
      "Eval num_timesteps=434176, episode_reward=-87.50 +/- 25.64\n",
      "Episode length: 88.50 +/- 25.64\n",
      "Eval num_timesteps=434176, episode_reward=-83.20 +/- 12.96\n",
      "Episode length: 84.20 +/- 12.96\n",
      "Eval num_timesteps=434176, episode_reward=-86.30 +/- 10.41\n",
      "Episode length: 87.30 +/- 10.41\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00067 |       0.00000 |      55.31504 |       0.00034 |       0.18778\n",
      "     -0.00100 |       0.00000 |      52.94976 |       0.00040 |       0.18852\n",
      "     -0.00108 |       0.00000 |      52.17689 |       0.00065 |       0.19032\n",
      "     -0.00162 |       0.00000 |      51.58362 |       0.00048 |       0.19036\n",
      "     -0.00149 |       0.00000 |      51.11480 |       0.00071 |       0.19022\n",
      "     -0.00214 |       0.00000 |      50.74818 |       0.00059 |       0.19254\n",
      "     -0.00255 |       0.00000 |      50.40960 |       0.00088 |       0.19158\n",
      "     -0.00278 |       0.00000 |      50.13593 |       0.00084 |       0.19391\n",
      "     -0.00258 |       0.00000 |      49.98455 |       0.00102 |       0.19292\n",
      "     -0.00225 |       0.00000 |      49.84377 |       0.00088 |       0.19193\n",
      "Evaluating losses...\n",
      "     -0.00341 |       0.00000 |      49.28003 |       0.00075 |       0.19199\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.3          |\n",
      "| EpRewMean       | -97.3         |\n",
      "| EpThisIter      | 39            |\n",
      "| EpisodesSoFar   | 3783          |\n",
      "| TimeElapsed     | 1.07e+03      |\n",
      "| TimestepsSoFar  | 438272        |\n",
      "| ev_tdlam_before | 0.772         |\n",
      "| loss_ent        | 0.19198696    |\n",
      "| loss_kl         | 0.0007469675  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034085987 |\n",
      "| loss_vf_loss    | 49.280033     |\n",
      "-----------------------------------\n",
      "********** Iteration 107 ************\n",
      "Eval num_timesteps=438272, episode_reward=-79.60 +/- 4.52\n",
      "Episode length: 80.60 +/- 4.52\n",
      "Eval num_timesteps=438272, episode_reward=-75.90 +/- 6.07\n",
      "Episode length: 76.90 +/- 6.07\n",
      "Eval num_timesteps=438272, episode_reward=-77.60 +/- 7.35\n",
      "Episode length: 78.60 +/- 7.35\n",
      "Eval num_timesteps=438272, episode_reward=-83.10 +/- 7.79\n",
      "Episode length: 84.10 +/- 7.79\n",
      "Eval num_timesteps=438272, episode_reward=-76.40 +/- 6.90\n",
      "Episode length: 77.40 +/- 6.90\n",
      "Eval num_timesteps=438272, episode_reward=-80.30 +/- 7.06\n",
      "Episode length: 81.30 +/- 7.06\n",
      "Eval num_timesteps=438272, episode_reward=-79.50 +/- 7.03\n",
      "Episode length: 80.50 +/- 7.03\n",
      "Eval num_timesteps=438272, episode_reward=-94.90 +/- 36.36\n",
      "Episode length: 95.90 +/- 36.36\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.94e-05 |       0.00000 |      36.36009 |       0.00029 |       0.18717\n",
      "     -0.00046 |       0.00000 |      34.62402 |       0.00037 |       0.18752\n",
      "     -0.00120 |       0.00000 |      33.70457 |       0.00039 |       0.18806\n",
      "     -0.00116 |       0.00000 |      33.00777 |       0.00053 |       0.18772\n",
      "     -0.00140 |       0.00000 |      32.52998 |       0.00035 |       0.18812\n",
      "     -0.00165 |       0.00000 |      32.02258 |       0.00061 |       0.18977\n",
      "     -0.00163 |       0.00000 |      31.72243 |       0.00049 |       0.18729\n",
      "     -0.00150 |       0.00000 |      31.39964 |       0.00065 |       0.18805\n",
      "     -0.00193 |       0.00000 |      31.21080 |       0.00059 |       0.18751\n",
      "     -0.00237 |       0.00000 |      30.97400 |       0.00054 |       0.18814\n",
      "Evaluating losses...\n",
      "     -0.00312 |       0.00000 |      30.60216 |       0.00050 |       0.18858\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.8          |\n",
      "| EpRewMean       | -95.8         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 3827          |\n",
      "| TimeElapsed     | 1.08e+03      |\n",
      "| TimestepsSoFar  | 442368        |\n",
      "| ev_tdlam_before | 0.853         |\n",
      "| loss_ent        | 0.18857929    |\n",
      "| loss_kl         | 0.0004969515  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031160507 |\n",
      "| loss_vf_loss    | 30.602156     |\n",
      "-----------------------------------\n",
      "********** Iteration 108 ************\n",
      "Eval num_timesteps=442368, episode_reward=-81.50 +/- 11.24\n",
      "Episode length: 82.50 +/- 11.24\n",
      "Eval num_timesteps=442368, episode_reward=-90.30 +/- 29.22\n",
      "Episode length: 91.30 +/- 29.22\n",
      "Eval num_timesteps=442368, episode_reward=-84.10 +/- 5.70\n",
      "Episode length: 85.10 +/- 5.70\n",
      "Eval num_timesteps=442368, episode_reward=-77.40 +/- 4.52\n",
      "Episode length: 78.40 +/- 4.52\n",
      "Eval num_timesteps=442368, episode_reward=-78.50 +/- 4.48\n",
      "Episode length: 79.50 +/- 4.48\n",
      "Eval num_timesteps=442368, episode_reward=-77.80 +/- 8.28\n",
      "Episode length: 78.80 +/- 8.28\n",
      "Eval num_timesteps=442368, episode_reward=-79.40 +/- 7.88\n",
      "Episode length: 80.40 +/- 7.88\n",
      "Eval num_timesteps=442368, episode_reward=-78.20 +/- 5.62\n",
      "Episode length: 79.20 +/- 5.62\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.12e-05 |       0.00000 |      34.20913 |       0.00015 |       0.18198\n",
      "     -0.00127 |       0.00000 |      33.35210 |       0.00048 |       0.18149\n",
      "     -0.00134 |       0.00000 |      32.86060 |       0.00080 |       0.18079\n",
      "     -0.00162 |       0.00000 |      32.46451 |       0.00060 |       0.18197\n",
      "     -0.00187 |       0.00000 |      32.07911 |       0.00064 |       0.18409\n",
      "     -0.00221 |       0.00000 |      31.78883 |       0.00082 |       0.18134\n",
      "     -0.00221 |       0.00000 |      31.58742 |       0.00072 |       0.18349\n",
      "     -0.00244 |       0.00000 |      31.30354 |       0.00080 |       0.18275\n",
      "     -0.00232 |       0.00000 |      31.20024 |       0.00108 |       0.18311\n",
      "     -0.00276 |       0.00000 |      30.94254 |       0.00079 |       0.18345\n",
      "Evaluating losses...\n",
      "     -0.00310 |       0.00000 |      30.68163 |       0.00099 |       0.18175\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.8          |\n",
      "| EpRewMean       | -91.8         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3872          |\n",
      "| TimeElapsed     | 1.09e+03      |\n",
      "| TimestepsSoFar  | 446464        |\n",
      "| ev_tdlam_before | 0.87          |\n",
      "| loss_ent        | 0.18175362    |\n",
      "| loss_kl         | 0.0009915713  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031000427 |\n",
      "| loss_vf_loss    | 30.681633     |\n",
      "-----------------------------------\n",
      "********** Iteration 109 ************\n",
      "Eval num_timesteps=446464, episode_reward=-78.80 +/- 6.45\n",
      "Episode length: 79.80 +/- 6.45\n",
      "Eval num_timesteps=446464, episode_reward=-83.50 +/- 7.94\n",
      "Episode length: 84.50 +/- 7.94\n",
      "Eval num_timesteps=446464, episode_reward=-74.60 +/- 5.35\n",
      "Episode length: 75.60 +/- 5.35\n",
      "Eval num_timesteps=446464, episode_reward=-97.70 +/- 45.34\n",
      "Episode length: 98.70 +/- 45.34\n",
      "Eval num_timesteps=446464, episode_reward=-83.60 +/- 13.29\n",
      "Episode length: 84.60 +/- 13.29\n",
      "Eval num_timesteps=446464, episode_reward=-78.50 +/- 9.80\n",
      "Episode length: 79.50 +/- 9.80\n",
      "Eval num_timesteps=446464, episode_reward=-78.60 +/- 4.90\n",
      "Episode length: 79.60 +/- 4.90\n",
      "Eval num_timesteps=446464, episode_reward=-78.80 +/- 6.55\n",
      "Episode length: 79.80 +/- 6.55\n",
      "Eval num_timesteps=446464, episode_reward=-81.80 +/- 7.67\n",
      "Episode length: 82.80 +/- 7.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     6.17e-05 |       0.00000 |      25.26769 |       0.00027 |       0.18245\n",
      "     -0.00108 |       0.00000 |      24.35120 |       0.00036 |       0.18212\n",
      "     -0.00148 |       0.00000 |      23.96011 |       0.00075 |       0.18451\n",
      "     -0.00170 |       0.00000 |      23.66809 |       0.00049 |       0.18326\n",
      "     -0.00193 |       0.00000 |      23.42645 |       0.00056 |       0.18320\n",
      "     -0.00192 |       0.00000 |      23.18214 |       0.00068 |       0.18468\n",
      "     -0.00166 |       0.00000 |      23.04818 |       0.00065 |       0.18356\n",
      "     -0.00226 |       0.00000 |      22.92559 |       0.00074 |       0.18407\n",
      "     -0.00229 |       0.00000 |      22.79613 |       0.00074 |       0.18659\n",
      "     -0.00252 |       0.00000 |      22.62464 |       0.00083 |       0.18508\n",
      "Evaluating losses...\n",
      "     -0.00324 |       0.00000 |      22.48117 |       0.00077 |       0.18538\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.5          |\n",
      "| EpRewMean       | -88.5         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3917          |\n",
      "| TimeElapsed     | 1.09e+03      |\n",
      "| TimestepsSoFar  | 450560        |\n",
      "| ev_tdlam_before | 0.909         |\n",
      "| loss_ent        | 0.18538387    |\n",
      "| loss_kl         | 0.0007699933  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0032428417 |\n",
      "| loss_vf_loss    | 22.48117      |\n",
      "-----------------------------------\n",
      "********** Iteration 110 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=450560, episode_reward=-80.70 +/- 8.64\n",
      "Episode length: 81.70 +/- 8.64\n",
      "Eval num_timesteps=450560, episode_reward=-77.40 +/- 5.39\n",
      "Episode length: 78.40 +/- 5.39\n",
      "Eval num_timesteps=450560, episode_reward=-99.30 +/- 41.75\n",
      "Episode length: 100.30 +/- 41.75\n",
      "Eval num_timesteps=450560, episode_reward=-77.60 +/- 3.29\n",
      "Episode length: 78.60 +/- 3.29\n",
      "Eval num_timesteps=450560, episode_reward=-79.20 +/- 9.41\n",
      "Episode length: 80.20 +/- 9.41\n",
      "Eval num_timesteps=450560, episode_reward=-84.50 +/- 9.69\n",
      "Episode length: 85.50 +/- 9.69\n",
      "Eval num_timesteps=450560, episode_reward=-79.40 +/- 5.57\n",
      "Episode length: 80.40 +/- 5.57\n",
      "Eval num_timesteps=450560, episode_reward=-84.20 +/- 9.01\n",
      "Episode length: 85.20 +/- 9.01\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.34e-05 |       0.00000 |      23.37991 |       0.00032 |       0.18380\n",
      "     -0.00058 |       0.00000 |      22.58945 |       0.00064 |       0.18048\n",
      "     -0.00136 |       0.00000 |      22.39792 |       0.00056 |       0.18244\n",
      "     -0.00163 |       0.00000 |      22.10907 |       0.00050 |       0.18058\n",
      "     -0.00177 |       0.00000 |      21.89521 |       0.00071 |       0.18176\n",
      "     -0.00177 |       0.00000 |      21.79189 |       0.00069 |       0.18076\n",
      "     -0.00194 |       0.00000 |      21.60802 |       0.00064 |       0.18103\n",
      "     -0.00256 |       0.00000 |      21.57342 |       0.00064 |       0.18087\n",
      "     -0.00256 |       0.00000 |      21.44736 |       0.00065 |       0.18182\n",
      "     -0.00276 |       0.00000 |      21.31301 |       0.00070 |       0.18216\n",
      "Evaluating losses...\n",
      "     -0.00334 |       0.00000 |      21.15007 |       0.00073 |       0.18085\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.8          |\n",
      "| EpRewMean       | -88.8         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 3965          |\n",
      "| TimeElapsed     | 1.1e+03       |\n",
      "| TimestepsSoFar  | 454656        |\n",
      "| ev_tdlam_before | 0.918         |\n",
      "| loss_ent        | 0.18084802    |\n",
      "| loss_kl         | 0.0007342335  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0033386853 |\n",
      "| loss_vf_loss    | 21.150074     |\n",
      "-----------------------------------\n",
      "********** Iteration 111 ************\n",
      "Eval num_timesteps=454656, episode_reward=-85.20 +/- 15.48\n",
      "Episode length: 86.20 +/- 15.48\n",
      "Eval num_timesteps=454656, episode_reward=-82.40 +/- 9.19\n",
      "Episode length: 83.40 +/- 9.19\n",
      "Eval num_timesteps=454656, episode_reward=-84.70 +/- 11.27\n",
      "Episode length: 85.70 +/- 11.27\n",
      "Eval num_timesteps=454656, episode_reward=-77.40 +/- 4.94\n",
      "Episode length: 78.40 +/- 4.94\n",
      "Eval num_timesteps=454656, episode_reward=-89.40 +/- 35.66\n",
      "Episode length: 90.40 +/- 35.66\n",
      "Eval num_timesteps=454656, episode_reward=-77.10 +/- 5.49\n",
      "Episode length: 78.10 +/- 5.49\n",
      "Eval num_timesteps=454656, episode_reward=-78.90 +/- 6.35\n",
      "Episode length: 79.90 +/- 6.35\n",
      "Eval num_timesteps=454656, episode_reward=-85.10 +/- 13.21\n",
      "Episode length: 86.10 +/- 13.21\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00011 |       0.00000 |      41.58438 |       0.00020 |       0.19349\n",
      "     -0.00124 |       0.00000 |      40.42184 |       0.00045 |       0.19716\n",
      "     -0.00179 |       0.00000 |      39.93425 |       0.00069 |       0.20026\n",
      "     -0.00190 |       0.00000 |      39.62699 |       0.00099 |       0.20336\n",
      "     -0.00218 |       0.00000 |      39.35666 |       0.00092 |       0.20215\n",
      "     -0.00266 |       0.00000 |      39.08818 |       0.00134 |       0.20352\n",
      "     -0.00270 |       0.00000 |      38.85933 |       0.00135 |       0.20319\n",
      "     -0.00318 |       0.00000 |      38.65864 |       0.00206 |       0.20789\n",
      "     -0.00362 |       0.00000 |      38.45209 |       0.00208 |       0.20746\n",
      "     -0.00389 |       0.00000 |      38.34909 |       0.00263 |       0.20925\n",
      "Evaluating losses...\n",
      "     -0.00446 |       0.00000 |      37.92955 |       0.00233 |       0.20963\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.9          |\n",
      "| EpRewMean       | -91.9         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 4006          |\n",
      "| TimeElapsed     | 1.11e+03      |\n",
      "| TimestepsSoFar  | 458752        |\n",
      "| ev_tdlam_before | 0.844         |\n",
      "| loss_ent        | 0.20962907    |\n",
      "| loss_kl         | 0.0023296957  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0044601695 |\n",
      "| loss_vf_loss    | 37.92955      |\n",
      "-----------------------------------\n",
      "********** Iteration 112 ************\n",
      "Eval num_timesteps=458752, episode_reward=-80.20 +/- 18.25\n",
      "Episode length: 81.20 +/- 18.25\n",
      "Eval num_timesteps=458752, episode_reward=-78.80 +/- 7.36\n",
      "Episode length: 79.80 +/- 7.36\n",
      "Eval num_timesteps=458752, episode_reward=-81.80 +/- 8.95\n",
      "Episode length: 82.80 +/- 8.95\n",
      "Eval num_timesteps=458752, episode_reward=-79.20 +/- 8.39\n",
      "Episode length: 80.20 +/- 8.39\n",
      "Eval num_timesteps=458752, episode_reward=-79.30 +/- 8.10\n",
      "Episode length: 80.30 +/- 8.10\n",
      "Eval num_timesteps=458752, episode_reward=-84.40 +/- 4.48\n",
      "Episode length: 85.40 +/- 4.48\n",
      "Eval num_timesteps=458752, episode_reward=-78.60 +/- 7.30\n",
      "Episode length: 79.60 +/- 7.30\n",
      "Eval num_timesteps=458752, episode_reward=-82.20 +/- 8.06\n",
      "Episode length: 83.20 +/- 8.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     7.01e-05 |       0.00000 |      24.50830 |       0.00033 |       0.18842\n",
      "     -0.00089 |       0.00000 |      23.84347 |       0.00039 |       0.18662\n",
      "     -0.00127 |       0.00000 |      23.51190 |       0.00060 |       0.18513\n",
      "     -0.00151 |       0.00000 |      23.26884 |       0.00065 |       0.18765\n",
      "     -0.00178 |       0.00000 |      23.06727 |       0.00079 |       0.18854\n",
      "     -0.00179 |       0.00000 |      22.94705 |       0.00068 |       0.18763\n",
      "     -0.00213 |       0.00000 |      22.81949 |       0.00068 |       0.18671\n",
      "     -0.00232 |       0.00000 |      22.66186 |       0.00079 |       0.18695\n",
      "     -0.00195 |       0.00000 |      22.62200 |       0.00079 |       0.18491\n",
      "     -0.00226 |       0.00000 |      22.46438 |       0.00075 |       0.18724\n",
      "Evaluating losses...\n",
      "     -0.00302 |       0.00000 |      22.28195 |       0.00080 |       0.18674\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.5          |\n",
      "| EpRewMean       | -92.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 4053          |\n",
      "| TimeElapsed     | 1.12e+03      |\n",
      "| TimestepsSoFar  | 462848        |\n",
      "| ev_tdlam_before | 0.909         |\n",
      "| loss_ent        | 0.18673569    |\n",
      "| loss_kl         | 0.00080158596 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0030231266 |\n",
      "| loss_vf_loss    | 22.281954     |\n",
      "-----------------------------------\n",
      "********** Iteration 113 ************\n",
      "Eval num_timesteps=462848, episode_reward=-76.80 +/- 4.47\n",
      "Episode length: 77.80 +/- 4.47\n",
      "Eval num_timesteps=462848, episode_reward=-89.20 +/- 30.59\n",
      "Episode length: 90.20 +/- 30.59\n",
      "Eval num_timesteps=462848, episode_reward=-92.00 +/- 30.53\n",
      "Episode length: 93.00 +/- 30.53\n",
      "Eval num_timesteps=462848, episode_reward=-78.40 +/- 4.13\n",
      "Episode length: 79.40 +/- 4.13\n",
      "Eval num_timesteps=462848, episode_reward=-86.60 +/- 33.12\n",
      "Episode length: 87.60 +/- 33.12\n",
      "Eval num_timesteps=462848, episode_reward=-85.20 +/- 19.58\n",
      "Episode length: 86.20 +/- 19.58\n",
      "Eval num_timesteps=462848, episode_reward=-81.30 +/- 7.28\n",
      "Episode length: 82.30 +/- 7.28\n",
      "Eval num_timesteps=462848, episode_reward=-74.70 +/- 6.23\n",
      "Episode length: 75.70 +/- 6.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00037 |       0.00000 |      33.61184 |       0.00043 |       0.19297\n",
      "     -0.00047 |       0.00000 |      32.08567 |       0.00041 |       0.19322\n",
      "     -0.00084 |       0.00000 |      31.38770 |       0.00059 |       0.19225\n",
      "     -0.00153 |       0.00000 |      30.80914 |       0.00060 |       0.19339\n",
      "     -0.00168 |       0.00000 |      30.45871 |       0.00057 |       0.19557\n",
      "     -0.00206 |       0.00000 |      30.21849 |       0.00061 |       0.19508\n",
      "     -0.00206 |       0.00000 |      29.99523 |       0.00073 |       0.19561\n",
      "     -0.00240 |       0.00000 |      29.79152 |       0.00060 |       0.19523\n",
      "     -0.00245 |       0.00000 |      29.61484 |       0.00060 |       0.19371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00265 |       0.00000 |      29.44022 |       0.00076 |       0.19439\n",
      "Evaluating losses...\n",
      "     -0.00315 |       0.00000 |      29.16477 |       0.00066 |       0.19480\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.3          |\n",
      "| EpRewMean       | -89.3         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4097          |\n",
      "| TimeElapsed     | 1.12e+03      |\n",
      "| TimestepsSoFar  | 466944        |\n",
      "| ev_tdlam_before | 0.873         |\n",
      "| loss_ent        | 0.1947984     |\n",
      "| loss_kl         | 0.00066229753 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031494973 |\n",
      "| loss_vf_loss    | 29.164766     |\n",
      "-----------------------------------\n",
      "********** Iteration 114 ************\n",
      "Eval num_timesteps=466944, episode_reward=-79.10 +/- 6.93\n",
      "Episode length: 80.10 +/- 6.93\n",
      "Eval num_timesteps=466944, episode_reward=-78.70 +/- 8.50\n",
      "Episode length: 79.70 +/- 8.50\n",
      "Eval num_timesteps=466944, episode_reward=-82.80 +/- 5.98\n",
      "Episode length: 83.80 +/- 5.98\n",
      "Eval num_timesteps=466944, episode_reward=-77.20 +/- 6.93\n",
      "Episode length: 78.20 +/- 6.93\n",
      "Eval num_timesteps=466944, episode_reward=-81.40 +/- 5.37\n",
      "Episode length: 82.40 +/- 5.37\n",
      "Eval num_timesteps=466944, episode_reward=-76.00 +/- 5.60\n",
      "Episode length: 77.00 +/- 5.60\n",
      "Eval num_timesteps=466944, episode_reward=-77.90 +/- 6.20\n",
      "Episode length: 78.90 +/- 6.20\n",
      "Eval num_timesteps=466944, episode_reward=-75.20 +/- 4.45\n",
      "Episode length: 76.20 +/- 4.45\n",
      "Eval num_timesteps=466944, episode_reward=-79.70 +/- 7.14\n",
      "Episode length: 80.70 +/- 7.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00060 |       0.00000 |      33.87946 |       0.00012 |       0.18727\n",
      "     -0.00013 |       0.00000 |      32.40213 |       0.00053 |       0.18831\n",
      "     -0.00081 |       0.00000 |      31.73524 |       0.00047 |       0.18545\n",
      "     -0.00110 |       0.00000 |      31.20910 |       0.00054 |       0.18627\n",
      "     -0.00174 |       0.00000 |      30.91859 |       0.00054 |       0.18578\n",
      "     -0.00185 |       0.00000 |      30.68326 |       0.00055 |       0.18531\n",
      "     -0.00199 |       0.00000 |      30.53061 |       0.00079 |       0.18795\n",
      "     -0.00235 |       0.00000 |      30.42552 |       0.00092 |       0.18859\n",
      "     -0.00255 |       0.00000 |      30.24708 |       0.00083 |       0.18691\n",
      "     -0.00250 |       0.00000 |      30.12671 |       0.00092 |       0.18740\n",
      "Evaluating losses...\n",
      "     -0.00317 |       0.00000 |      29.93762 |       0.00117 |       0.18912\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.6          |\n",
      "| EpRewMean       | -90.6         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 4142          |\n",
      "| TimeElapsed     | 1.13e+03      |\n",
      "| TimestepsSoFar  | 471040        |\n",
      "| ev_tdlam_before | 0.868         |\n",
      "| loss_ent        | 0.18912378    |\n",
      "| loss_kl         | 0.0011658801  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031657056 |\n",
      "| loss_vf_loss    | 29.937624     |\n",
      "-----------------------------------\n",
      "********** Iteration 115 ************\n",
      "Eval num_timesteps=471040, episode_reward=-92.80 +/- 32.00\n",
      "Episode length: 93.80 +/- 32.00\n",
      "Eval num_timesteps=471040, episode_reward=-83.30 +/- 9.79\n",
      "Episode length: 84.30 +/- 9.79\n",
      "Eval num_timesteps=471040, episode_reward=-76.90 +/- 4.91\n",
      "Episode length: 77.90 +/- 4.91\n",
      "Eval num_timesteps=471040, episode_reward=-76.80 +/- 4.04\n",
      "Episode length: 77.80 +/- 4.04\n",
      "Eval num_timesteps=471040, episode_reward=-82.00 +/- 6.87\n",
      "Episode length: 83.00 +/- 6.87\n",
      "Eval num_timesteps=471040, episode_reward=-82.00 +/- 13.97\n",
      "Episode length: 83.00 +/- 13.97\n",
      "Eval num_timesteps=471040, episode_reward=-81.00 +/- 7.86\n",
      "Episode length: 82.00 +/- 7.86\n",
      "Eval num_timesteps=471040, episode_reward=-86.20 +/- 21.26\n",
      "Episode length: 87.20 +/- 21.26\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.69e-05 |       0.00000 |      44.55482 |       0.00018 |       0.19378\n",
      "     -0.00074 |       0.00000 |      43.45194 |       0.00043 |       0.19361\n",
      "     -0.00119 |       0.00000 |      42.89947 |       0.00056 |       0.19151\n",
      "     -0.00136 |       0.00000 |      42.32542 |       0.00059 |       0.19152\n",
      "     -0.00149 |       0.00000 |      42.06122 |       0.00051 |       0.19339\n",
      "     -0.00189 |       0.00000 |      41.74100 |       0.00063 |       0.19400\n",
      "     -0.00209 |       0.00000 |      41.41245 |       0.00064 |       0.19347\n",
      "     -0.00218 |       0.00000 |      41.21708 |       0.00066 |       0.19569\n",
      "     -0.00227 |       0.00000 |      40.99868 |       0.00058 |       0.19356\n",
      "     -0.00226 |       0.00000 |      40.88946 |       0.00070 |       0.19535\n",
      "Evaluating losses...\n",
      "     -0.00274 |       0.00000 |      40.57954 |       0.00084 |       0.19593\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4186          |\n",
      "| TimeElapsed     | 1.14e+03      |\n",
      "| TimestepsSoFar  | 475136        |\n",
      "| ev_tdlam_before | 0.827         |\n",
      "| loss_ent        | 0.19592893    |\n",
      "| loss_kl         | 0.0008396499  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0027390826 |\n",
      "| loss_vf_loss    | 40.579536     |\n",
      "-----------------------------------\n",
      "********** Iteration 116 ************\n",
      "Eval num_timesteps=475136, episode_reward=-82.30 +/- 11.57\n",
      "Episode length: 83.30 +/- 11.57\n",
      "Eval num_timesteps=475136, episode_reward=-77.20 +/- 9.13\n",
      "Episode length: 78.20 +/- 9.13\n",
      "Eval num_timesteps=475136, episode_reward=-85.40 +/- 12.31\n",
      "Episode length: 86.40 +/- 12.31\n",
      "Eval num_timesteps=475136, episode_reward=-82.60 +/- 8.94\n",
      "Episode length: 83.60 +/- 8.94\n",
      "Eval num_timesteps=475136, episode_reward=-80.40 +/- 7.12\n",
      "Episode length: 81.40 +/- 7.12\n",
      "Eval num_timesteps=475136, episode_reward=-80.20 +/- 9.03\n",
      "Episode length: 81.20 +/- 9.03\n",
      "Eval num_timesteps=475136, episode_reward=-76.70 +/- 3.63\n",
      "Episode length: 77.70 +/- 3.63\n",
      "Eval num_timesteps=475136, episode_reward=-82.80 +/- 15.01\n",
      "Episode length: 83.80 +/- 15.01\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      41.47145 |       0.00035 |       0.19418\n",
      "     -0.00095 |       0.00000 |      39.99326 |       0.00050 |       0.19257\n",
      "     -0.00155 |       0.00000 |      39.19018 |       0.00051 |       0.19322\n",
      "     -0.00160 |       0.00000 |      38.62375 |       0.00053 |       0.19328\n",
      "     -0.00146 |       0.00000 |      38.22586 |       0.00058 |       0.19210\n",
      "     -0.00228 |       0.00000 |      38.00887 |       0.00065 |       0.19071\n",
      "     -0.00186 |       0.00000 |      37.75239 |       0.00076 |       0.19329\n",
      "     -0.00248 |       0.00000 |      37.56832 |       0.00068 |       0.19348\n",
      "     -0.00256 |       0.00000 |      37.29274 |       0.00057 |       0.19174\n",
      "     -0.00270 |       0.00000 |      37.17378 |       0.00095 |       0.19352\n",
      "Evaluating losses...\n",
      "     -0.00351 |       0.00000 |      36.83265 |       0.00065 |       0.19269\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94            |\n",
      "| EpRewMean       | -93           |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 4229          |\n",
      "| TimeElapsed     | 1.14e+03      |\n",
      "| TimestepsSoFar  | 479232        |\n",
      "| ev_tdlam_before | 0.832         |\n",
      "| loss_ent        | 0.1926943     |\n",
      "| loss_kl         | 0.0006538387  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0035066952 |\n",
      "| loss_vf_loss    | 36.832645     |\n",
      "-----------------------------------\n",
      "********** Iteration 117 ************\n",
      "Eval num_timesteps=479232, episode_reward=-82.30 +/- 7.67\n",
      "Episode length: 83.30 +/- 7.67\n",
      "Eval num_timesteps=479232, episode_reward=-74.80 +/- 6.88\n",
      "Episode length: 75.80 +/- 6.88\n",
      "Eval num_timesteps=479232, episode_reward=-79.40 +/- 5.52\n",
      "Episode length: 80.40 +/- 5.52\n",
      "Eval num_timesteps=479232, episode_reward=-78.90 +/- 5.65\n",
      "Episode length: 79.90 +/- 5.65\n",
      "Eval num_timesteps=479232, episode_reward=-83.60 +/- 10.01\n",
      "Episode length: 84.60 +/- 10.01\n",
      "Eval num_timesteps=479232, episode_reward=-79.50 +/- 7.37\n",
      "Episode length: 80.50 +/- 7.37\n",
      "Eval num_timesteps=479232, episode_reward=-79.70 +/- 6.40\n",
      "Episode length: 80.70 +/- 6.40\n",
      "Eval num_timesteps=479232, episode_reward=-82.20 +/- 7.97\n",
      "Episode length: 83.20 +/- 7.97\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -6.11e-05 |       0.00000 |      40.79524 |       0.00024 |       0.23568\n",
      "     -0.00061 |       0.00000 |      34.35561 |       0.00040 |       0.23216\n",
      "     -0.00105 |       0.00000 |      33.68668 |       0.00045 |       0.23334\n",
      "     -0.00099 |       0.00000 |      33.15479 |       0.00042 |       0.23483\n",
      "     -0.00161 |       0.00000 |      32.75233 |       0.00044 |       0.23329\n",
      "     -0.00163 |       0.00000 |      32.41325 |       0.00043 |       0.23361\n",
      "     -0.00183 |       0.00000 |      32.27377 |       0.00049 |       0.23346\n",
      "     -0.00198 |       0.00000 |      31.87182 |       0.00066 |       0.23073\n",
      "     -0.00214 |       0.00000 |      31.69764 |       0.00051 |       0.23433\n",
      "     -0.00211 |       0.00000 |      31.56394 |       0.00068 |       0.23261\n",
      "Evaluating losses...\n",
      "     -0.00288 |       0.00000 |      31.25135 |       0.00060 |       0.23071\n",
      "-----------------------------------\n",
      "| EpLenMean       | 105           |\n",
      "| EpRewMean       | -104          |\n",
      "| EpThisIter      | 34            |\n",
      "| EpisodesSoFar   | 4263          |\n",
      "| TimeElapsed     | 1.15e+03      |\n",
      "| TimestepsSoFar  | 483328        |\n",
      "| ev_tdlam_before | 0.777         |\n",
      "| loss_ent        | 0.23071177    |\n",
      "| loss_kl         | 0.000601357   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0028798245 |\n",
      "| loss_vf_loss    | 31.251352     |\n",
      "-----------------------------------\n",
      "********** Iteration 118 ************\n",
      "Eval num_timesteps=483328, episode_reward=-91.00 +/- 20.32\n",
      "Episode length: 92.00 +/- 20.32\n",
      "Eval num_timesteps=483328, episode_reward=-81.60 +/- 8.21\n",
      "Episode length: 82.60 +/- 8.21\n",
      "Eval num_timesteps=483328, episode_reward=-81.00 +/- 8.33\n",
      "Episode length: 82.00 +/- 8.33\n",
      "Eval num_timesteps=483328, episode_reward=-77.00 +/- 8.07\n",
      "Episode length: 78.00 +/- 8.07\n",
      "Eval num_timesteps=483328, episode_reward=-82.80 +/- 5.53\n",
      "Episode length: 83.80 +/- 5.53\n",
      "Eval num_timesteps=483328, episode_reward=-78.70 +/- 4.58\n",
      "Episode length: 79.70 +/- 4.58\n",
      "Eval num_timesteps=483328, episode_reward=-82.80 +/- 8.69\n",
      "Episode length: 83.80 +/- 8.69\n",
      "Eval num_timesteps=483328, episode_reward=-80.30 +/- 8.45\n",
      "Episode length: 81.30 +/- 8.45\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00029 |       0.00000 |      49.30503 |       0.00033 |       0.19662\n",
      "     -0.00161 |       0.00000 |      48.25980 |       0.00038 |       0.19573\n",
      "     -0.00191 |       0.00000 |      47.63468 |       0.00045 |       0.19679\n",
      "     -0.00241 |       0.00000 |      47.22095 |       0.00046 |       0.19537\n",
      "     -0.00247 |       0.00000 |      46.84550 |       0.00059 |       0.19608\n",
      "     -0.00284 |       0.00000 |      46.53608 |       0.00055 |       0.19758\n",
      "     -0.00323 |       0.00000 |      46.22454 |       0.00056 |       0.19920\n",
      "     -0.00315 |       0.00000 |      46.02065 |       0.00061 |       0.19815\n",
      "     -0.00339 |       0.00000 |      45.69608 |       0.00067 |       0.19750\n",
      "     -0.00339 |       0.00000 |      45.50340 |       0.00066 |       0.19925\n",
      "Evaluating losses...\n",
      "     -0.00415 |       0.00000 |      45.00754 |       0.00072 |       0.20134\n",
      "-----------------------------------\n",
      "| EpLenMean       | 105           |\n",
      "| EpRewMean       | -104          |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 4304          |\n",
      "| TimeElapsed     | 1.16e+03      |\n",
      "| TimestepsSoFar  | 487424        |\n",
      "| ev_tdlam_before | 0.804         |\n",
      "| loss_ent        | 0.20133722    |\n",
      "| loss_kl         | 0.0007169228  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0041527525 |\n",
      "| loss_vf_loss    | 45.00754      |\n",
      "-----------------------------------\n",
      "********** Iteration 119 ************\n",
      "Eval num_timesteps=487424, episode_reward=-82.80 +/- 4.45\n",
      "Episode length: 83.80 +/- 4.45\n",
      "Eval num_timesteps=487424, episode_reward=-78.60 +/- 11.29\n",
      "Episode length: 79.60 +/- 11.29\n",
      "Eval num_timesteps=487424, episode_reward=-79.30 +/- 5.87\n",
      "Episode length: 80.30 +/- 5.87\n",
      "Eval num_timesteps=487424, episode_reward=-112.50 +/- 69.83\n",
      "Episode length: 113.50 +/- 69.83\n",
      "Eval num_timesteps=487424, episode_reward=-79.40 +/- 8.53\n",
      "Episode length: 80.40 +/- 8.53\n",
      "Eval num_timesteps=487424, episode_reward=-84.10 +/- 6.55\n",
      "Episode length: 85.10 +/- 6.55\n",
      "Eval num_timesteps=487424, episode_reward=-82.90 +/- 7.73\n",
      "Episode length: 83.90 +/- 7.73\n",
      "Eval num_timesteps=487424, episode_reward=-79.80 +/- 9.63\n",
      "Episode length: 80.80 +/- 9.63\n",
      "Eval num_timesteps=487424, episode_reward=-92.70 +/- 26.36\n",
      "Episode length: 93.70 +/- 26.36\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00033 |       0.00000 |      37.19292 |       0.00015 |       0.18729\n",
      "     -0.00115 |       0.00000 |      35.94611 |       0.00029 |       0.18790\n",
      "     -0.00164 |       0.00000 |      35.51587 |       0.00033 |       0.18802\n",
      "     -0.00204 |       0.00000 |      35.20051 |       0.00058 |       0.18762\n",
      "     -0.00246 |       0.00000 |      34.99010 |       0.00049 |       0.18703\n",
      "     -0.00273 |       0.00000 |      34.79258 |       0.00065 |       0.18538\n",
      "     -0.00315 |       0.00000 |      34.64753 |       0.00064 |       0.18688\n",
      "     -0.00289 |       0.00000 |      34.46960 |       0.00080 |       0.18748\n",
      "     -0.00337 |       0.00000 |      34.37465 |       0.00093 |       0.18626\n",
      "     -0.00352 |       0.00000 |      34.26964 |       0.00079 |       0.18592\n",
      "Evaluating losses...\n",
      "     -0.00415 |       0.00000 |      33.98275 |       0.00078 |       0.18685\n",
      "-----------------------------------\n",
      "| EpLenMean       | 102           |\n",
      "| EpRewMean       | -101          |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 4349          |\n",
      "| TimeElapsed     | 1.17e+03      |\n",
      "| TimestepsSoFar  | 491520        |\n",
      "| ev_tdlam_before | 0.859         |\n",
      "| loss_ent        | 0.18685256    |\n",
      "| loss_kl         | 0.00077543064 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0041464576 |\n",
      "| loss_vf_loss    | 33.98275      |\n",
      "-----------------------------------\n",
      "********** Iteration 120 ************\n",
      "Eval num_timesteps=491520, episode_reward=-75.90 +/- 7.93\n",
      "Episode length: 76.90 +/- 7.93\n",
      "Eval num_timesteps=491520, episode_reward=-85.90 +/- 22.41\n",
      "Episode length: 86.90 +/- 22.41\n",
      "Eval num_timesteps=491520, episode_reward=-78.70 +/- 5.20\n",
      "Episode length: 79.70 +/- 5.20\n",
      "Eval num_timesteps=491520, episode_reward=-84.50 +/- 8.73\n",
      "Episode length: 85.50 +/- 8.73\n",
      "Eval num_timesteps=491520, episode_reward=-78.10 +/- 6.80\n",
      "Episode length: 79.10 +/- 6.80\n",
      "Eval num_timesteps=491520, episode_reward=-87.40 +/- 17.39\n",
      "Episode length: 88.40 +/- 17.39\n",
      "Eval num_timesteps=491520, episode_reward=-94.90 +/- 44.30\n",
      "Episode length: 95.90 +/- 44.30\n",
      "Eval num_timesteps=491520, episode_reward=-78.40 +/- 8.13\n",
      "Episode length: 79.40 +/- 8.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00028 |       0.00000 |      52.14478 |       0.00020 |       0.19361\n",
      "     -0.00078 |       0.00000 |      49.74290 |       0.00022 |       0.19180\n",
      "     -0.00141 |       0.00000 |      48.60581 |       0.00038 |       0.19354\n",
      "     -0.00160 |       0.00000 |      47.72766 |       0.00081 |       0.19490\n",
      "     -0.00205 |       0.00000 |      47.24183 |       0.00062 |       0.19328\n",
      "     -0.00208 |       0.00000 |      46.74933 |       0.00071 |       0.19359\n",
      "     -0.00256 |       0.00000 |      46.29031 |       0.00080 |       0.19296\n",
      "     -0.00283 |       0.00000 |      46.01830 |       0.00081 |       0.19350\n",
      "     -0.00263 |       0.00000 |      45.53328 |       0.00081 |       0.19378\n",
      "     -0.00308 |       0.00000 |      45.29284 |       0.00090 |       0.19379\n",
      "Evaluating losses...\n",
      "     -0.00359 |       0.00000 |      44.91090 |       0.00083 |       0.19382\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.7          |\n",
      "| EpRewMean       | -91.7         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 4392          |\n",
      "| TimeElapsed     | 1.17e+03      |\n",
      "| TimestepsSoFar  | 495616        |\n",
      "| ev_tdlam_before | 0.791         |\n",
      "| loss_ent        | 0.19381683    |\n",
      "| loss_kl         | 0.00083000894 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0035877111 |\n",
      "| loss_vf_loss    | 44.910904     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 121 ************\n",
      "Eval num_timesteps=495616, episode_reward=-79.50 +/- 7.68\n",
      "Episode length: 80.50 +/- 7.68\n",
      "Eval num_timesteps=495616, episode_reward=-78.50 +/- 5.87\n",
      "Episode length: 79.50 +/- 5.87\n",
      "Eval num_timesteps=495616, episode_reward=-79.00 +/- 6.08\n",
      "Episode length: 80.00 +/- 6.08\n",
      "Eval num_timesteps=495616, episode_reward=-85.20 +/- 4.12\n",
      "Episode length: 86.20 +/- 4.12\n",
      "Eval num_timesteps=495616, episode_reward=-80.40 +/- 7.71\n",
      "Episode length: 81.40 +/- 7.71\n",
      "Eval num_timesteps=495616, episode_reward=-80.40 +/- 7.51\n",
      "Episode length: 81.40 +/- 7.51\n",
      "Eval num_timesteps=495616, episode_reward=-87.60 +/- 24.98\n",
      "Episode length: 88.60 +/- 24.98\n",
      "Eval num_timesteps=495616, episode_reward=-76.90 +/- 11.40\n",
      "Episode length: 77.90 +/- 11.40\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00040 |       0.00000 |      27.89815 |       0.00036 |       0.18639\n",
      "     -0.00175 |       0.00000 |      26.33686 |       0.00053 |       0.18874\n",
      "     -0.00205 |       0.00000 |      25.60368 |       0.00062 |       0.18933\n",
      "     -0.00257 |       0.00000 |      25.05164 |       0.00079 |       0.19071\n",
      "     -0.00265 |       0.00000 |      24.62307 |       0.00087 |       0.19117\n",
      "     -0.00315 |       0.00000 |      24.31046 |       0.00095 |       0.19168\n",
      "     -0.00308 |       0.00000 |      24.03480 |       0.00111 |       0.19241\n",
      "     -0.00342 |       0.00000 |      23.78899 |       0.00096 |       0.19226\n",
      "     -0.00366 |       0.00000 |      23.56695 |       0.00117 |       0.19272\n",
      "     -0.00368 |       0.00000 |      23.37142 |       0.00111 |       0.19230\n",
      "Evaluating losses...\n",
      "     -0.00410 |       0.00000 |      23.12526 |       0.00146 |       0.19506\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.3          |\n",
      "| EpRewMean       | -91.3         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 4438          |\n",
      "| TimeElapsed     | 1.18e+03      |\n",
      "| TimestepsSoFar  | 499712        |\n",
      "| ev_tdlam_before | 0.894         |\n",
      "| loss_ent        | 0.19505695    |\n",
      "| loss_kl         | 0.0014637095  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0041041384 |\n",
      "| loss_vf_loss    | 23.125256     |\n",
      "-----------------------------------\n",
      "********** Iteration 122 ************\n",
      "Eval num_timesteps=499712, episode_reward=-92.50 +/- 27.33\n",
      "Episode length: 93.50 +/- 27.33\n",
      "Eval num_timesteps=499712, episode_reward=-81.20 +/- 7.95\n",
      "Episode length: 82.20 +/- 7.95\n",
      "Eval num_timesteps=499712, episode_reward=-81.20 +/- 7.08\n",
      "Episode length: 82.20 +/- 7.08\n",
      "Eval num_timesteps=499712, episode_reward=-80.60 +/- 14.39\n",
      "Episode length: 81.60 +/- 14.39\n",
      "Eval num_timesteps=499712, episode_reward=-95.70 +/- 48.11\n",
      "Episode length: 96.70 +/- 48.11\n",
      "Eval num_timesteps=499712, episode_reward=-82.00 +/- 15.55\n",
      "Episode length: 83.00 +/- 15.55\n",
      "Eval num_timesteps=499712, episode_reward=-103.10 +/- 35.92\n",
      "Episode length: 104.10 +/- 35.92\n",
      "Eval num_timesteps=499712, episode_reward=-80.20 +/- 9.52\n",
      "Episode length: 81.20 +/- 9.52\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00017 |       0.00000 |      29.71034 |       0.00025 |       0.18887\n",
      "     -0.00077 |       0.00000 |      28.80652 |       0.00039 |       0.18768\n",
      "     -0.00092 |       0.00000 |      28.35705 |       0.00044 |       0.18856\n",
      "     -0.00130 |       0.00000 |      27.97394 |       0.00044 |       0.18545\n",
      "     -0.00142 |       0.00000 |      27.66057 |       0.00056 |       0.18607\n",
      "     -0.00151 |       0.00000 |      27.49455 |       0.00061 |       0.18555\n",
      "     -0.00146 |       0.00000 |      27.27592 |       0.00052 |       0.18897\n",
      "     -0.00175 |       0.00000 |      27.12186 |       0.00054 |       0.18639\n",
      "     -0.00175 |       0.00000 |      26.95712 |       0.00059 |       0.18722\n",
      "     -0.00194 |       0.00000 |      26.84029 |       0.00049 |       0.18696\n",
      "Evaluating losses...\n",
      "     -0.00250 |       0.00000 |      26.58517 |       0.00076 |       0.18706\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.8          |\n",
      "| EpRewMean       | -85.8         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 4486          |\n",
      "| TimeElapsed     | 1.19e+03      |\n",
      "| TimestepsSoFar  | 503808        |\n",
      "| ev_tdlam_before | 0.892         |\n",
      "| loss_ent        | 0.18705863    |\n",
      "| loss_kl         | 0.00075956987 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002496188  |\n",
      "| loss_vf_loss    | 26.585173     |\n",
      "-----------------------------------\n",
      "********** Iteration 123 ************\n",
      "Eval num_timesteps=503808, episode_reward=-77.20 +/- 7.49\n",
      "Episode length: 78.20 +/- 7.49\n",
      "Eval num_timesteps=503808, episode_reward=-87.30 +/- 39.67\n",
      "Episode length: 88.30 +/- 39.67\n",
      "Eval num_timesteps=503808, episode_reward=-78.20 +/- 7.60\n",
      "Episode length: 79.20 +/- 7.60\n",
      "Eval num_timesteps=503808, episode_reward=-80.40 +/- 7.30\n",
      "Episode length: 81.40 +/- 7.30\n",
      "Eval num_timesteps=503808, episode_reward=-80.90 +/- 14.12\n",
      "Episode length: 81.90 +/- 14.12\n",
      "Eval num_timesteps=503808, episode_reward=-79.70 +/- 6.66\n",
      "Episode length: 80.70 +/- 6.66\n",
      "Eval num_timesteps=503808, episode_reward=-90.90 +/- 35.09\n",
      "Episode length: 91.90 +/- 35.09\n",
      "Eval num_timesteps=503808, episode_reward=-97.80 +/- 54.50\n",
      "Episode length: 98.80 +/- 54.50\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.38e-05 |       0.00000 |      49.43428 |       0.00024 |       0.21006\n",
      "     -0.00069 |       0.00000 |      46.98106 |       0.00034 |       0.21025\n",
      "     -0.00137 |       0.00000 |      46.21209 |       0.00036 |       0.20945\n",
      "     -0.00186 |       0.00000 |      45.70741 |       0.00045 |       0.20968\n",
      "     -0.00166 |       0.00000 |      45.25592 |       0.00040 |       0.21363\n",
      "     -0.00237 |       0.00000 |      44.91642 |       0.00043 |       0.21058\n",
      "     -0.00239 |       0.00000 |      44.70305 |       0.00056 |       0.21033\n",
      "     -0.00245 |       0.00000 |      44.46546 |       0.00050 |       0.21085\n",
      "     -0.00275 |       0.00000 |      44.30734 |       0.00055 |       0.21118\n",
      "     -0.00305 |       0.00000 |      44.04758 |       0.00061 |       0.20961\n",
      "Evaluating losses...\n",
      "     -0.00365 |       0.00000 |      43.66244 |       0.00063 |       0.21104\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.7          |\n",
      "| EpRewMean       | -90.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 4528          |\n",
      "| TimeElapsed     | 1.19e+03      |\n",
      "| TimestepsSoFar  | 507904        |\n",
      "| ev_tdlam_before | 0.785         |\n",
      "| loss_ent        | 0.21104237    |\n",
      "| loss_kl         | 0.0006312339  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036514848 |\n",
      "| loss_vf_loss    | 43.662437     |\n",
      "-----------------------------------\n",
      "********** Iteration 124 ************\n",
      "Eval num_timesteps=507904, episode_reward=-95.40 +/- 40.07\n",
      "Episode length: 96.40 +/- 40.07\n",
      "Eval num_timesteps=507904, episode_reward=-90.90 +/- 34.52\n",
      "Episode length: 91.90 +/- 34.52\n",
      "Eval num_timesteps=507904, episode_reward=-80.30 +/- 7.09\n",
      "Episode length: 81.30 +/- 7.09\n",
      "Eval num_timesteps=507904, episode_reward=-74.20 +/- 8.52\n",
      "Episode length: 75.20 +/- 8.52\n",
      "Eval num_timesteps=507904, episode_reward=-79.60 +/- 5.06\n",
      "Episode length: 80.60 +/- 5.06\n",
      "Eval num_timesteps=507904, episode_reward=-83.30 +/- 6.69\n",
      "Episode length: 84.30 +/- 6.69\n",
      "Eval num_timesteps=507904, episode_reward=-80.00 +/- 11.30\n",
      "Episode length: 81.00 +/- 11.30\n",
      "Eval num_timesteps=507904, episode_reward=-88.70 +/- 25.54\n",
      "Episode length: 89.70 +/- 25.54\n",
      "Eval num_timesteps=507904, episode_reward=-77.70 +/- 4.82\n",
      "Episode length: 78.70 +/- 4.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00052 |       0.00000 |      33.71655 |       0.00020 |       0.20876\n",
      "     -0.00043 |       0.00000 |      28.62864 |       0.00029 |       0.20761\n",
      "     -0.00111 |       0.00000 |      27.97845 |       0.00040 |       0.20944\n",
      "     -0.00130 |       0.00000 |      27.46221 |       0.00055 |       0.21182\n",
      "     -0.00141 |       0.00000 |      27.17621 |       0.00055 |       0.21101\n",
      "     -0.00158 |       0.00000 |      26.93094 |       0.00070 |       0.21162\n",
      "     -0.00220 |       0.00000 |      26.74633 |       0.00051 |       0.21028\n",
      "     -0.00046 |       0.00000 |      26.53712 |       0.00082 |       0.21094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00170 |       0.00000 |      26.41245 |       0.00077 |       0.21065\n",
      "     -0.00202 |       0.00000 |      26.26152 |       0.00095 |       0.21150\n",
      "Evaluating losses...\n",
      "     -0.00344 |       0.00000 |      26.02609 |       0.00074 |       0.21011\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.5          |\n",
      "| EpRewMean       | -95.5         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 4569          |\n",
      "| TimeElapsed     | 1.2e+03       |\n",
      "| TimestepsSoFar  | 512000        |\n",
      "| ev_tdlam_before | 0.809         |\n",
      "| loss_ent        | 0.21011439    |\n",
      "| loss_kl         | 0.0007447926  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034399496 |\n",
      "| loss_vf_loss    | 26.026087     |\n",
      "-----------------------------------\n",
      "********** Iteration 125 ************\n",
      "Eval num_timesteps=512000, episode_reward=-83.20 +/- 12.32\n",
      "Episode length: 84.20 +/- 12.32\n",
      "Eval num_timesteps=512000, episode_reward=-75.20 +/- 5.84\n",
      "Episode length: 76.20 +/- 5.84\n",
      "Eval num_timesteps=512000, episode_reward=-74.40 +/- 7.06\n",
      "Episode length: 75.40 +/- 7.06\n",
      "Eval num_timesteps=512000, episode_reward=-80.30 +/- 5.22\n",
      "Episode length: 81.30 +/- 5.22\n",
      "Eval num_timesteps=512000, episode_reward=-80.30 +/- 12.39\n",
      "Episode length: 81.30 +/- 12.39\n",
      "Eval num_timesteps=512000, episode_reward=-74.80 +/- 7.18\n",
      "Episode length: 75.80 +/- 7.18\n",
      "Eval num_timesteps=512000, episode_reward=-77.70 +/- 8.40\n",
      "Episode length: 78.70 +/- 8.40\n",
      "Eval num_timesteps=512000, episode_reward=-81.30 +/- 9.71\n",
      "Episode length: 82.30 +/- 9.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00035 |       0.00000 |      33.13164 |       0.00023 |       0.20969\n",
      "     -0.00119 |       0.00000 |      30.32025 |       0.00032 |       0.21166\n",
      "     -0.00137 |       0.00000 |      29.69077 |       0.00033 |       0.21371\n",
      "     -0.00180 |       0.00000 |      29.29107 |       0.00046 |       0.21352\n",
      "     -0.00228 |       0.00000 |      28.95308 |       0.00053 |       0.21357\n",
      "     -0.00251 |       0.00000 |      28.64595 |       0.00045 |       0.21358\n",
      "     -0.00290 |       0.00000 |      28.52720 |       0.00047 |       0.21293\n",
      "     -0.00275 |       0.00000 |      28.31988 |       0.00048 |       0.21396\n",
      "     -0.00313 |       0.00000 |      28.17734 |       0.00052 |       0.21295\n",
      "     -0.00338 |       0.00000 |      28.04133 |       0.00050 |       0.21346\n",
      "Evaluating losses...\n",
      "     -0.00351 |       0.00000 |      27.77791 |       0.00074 |       0.21330\n",
      "----------------------------------\n",
      "| EpLenMean       | 97.1         |\n",
      "| EpRewMean       | -96.1        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 4611         |\n",
      "| TimeElapsed     | 1.21e+03     |\n",
      "| TimestepsSoFar  | 516096       |\n",
      "| ev_tdlam_before | 0.86         |\n",
      "| loss_ent        | 0.21329632   |\n",
      "| loss_kl         | 0.0007436316 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.0035144   |\n",
      "| loss_vf_loss    | 27.777906    |\n",
      "----------------------------------\n",
      "********** Iteration 126 ************\n",
      "Eval num_timesteps=516096, episode_reward=-79.20 +/- 6.85\n",
      "Episode length: 80.20 +/- 6.85\n",
      "Eval num_timesteps=516096, episode_reward=-80.30 +/- 6.54\n",
      "Episode length: 81.30 +/- 6.54\n",
      "Eval num_timesteps=516096, episode_reward=-80.20 +/- 6.11\n",
      "Episode length: 81.20 +/- 6.11\n",
      "Eval num_timesteps=516096, episode_reward=-90.70 +/- 38.04\n",
      "Episode length: 91.70 +/- 38.04\n",
      "Eval num_timesteps=516096, episode_reward=-85.90 +/- 20.67\n",
      "Episode length: 86.90 +/- 20.67\n",
      "Eval num_timesteps=516096, episode_reward=-84.90 +/- 15.78\n",
      "Episode length: 85.90 +/- 15.78\n",
      "Eval num_timesteps=516096, episode_reward=-82.70 +/- 10.20\n",
      "Episode length: 83.70 +/- 10.20\n",
      "Eval num_timesteps=516096, episode_reward=-107.60 +/- 62.32\n",
      "Episode length: 108.60 +/- 62.32\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00066 |       0.00000 |      23.38970 |       0.00012 |       0.18230\n",
      "     -0.00072 |       0.00000 |      22.28519 |       0.00034 |       0.18088\n",
      "     -0.00124 |       0.00000 |      21.76337 |       0.00037 |       0.18023\n",
      "     -0.00184 |       0.00000 |      21.42906 |       0.00038 |       0.18110\n",
      "     -0.00216 |       0.00000 |      21.22017 |       0.00049 |       0.18010\n",
      "     -0.00226 |       0.00000 |      21.10452 |       0.00045 |       0.17953\n",
      "     -0.00267 |       0.00000 |      20.88786 |       0.00050 |       0.18077\n",
      "     -0.00264 |       0.00000 |      20.76842 |       0.00050 |       0.18120\n",
      "     -0.00269 |       0.00000 |      20.59384 |       0.00051 |       0.18077\n",
      "     -0.00296 |       0.00000 |      20.48466 |       0.00060 |       0.17997\n",
      "Evaluating losses...\n",
      "     -0.00335 |       0.00000 |      20.28940 |       0.00053 |       0.18248\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.1          |\n",
      "| EpRewMean       | -89.1         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 4659          |\n",
      "| TimeElapsed     | 1.21e+03      |\n",
      "| TimestepsSoFar  | 520192        |\n",
      "| ev_tdlam_before | 0.914         |\n",
      "| loss_ent        | 0.18248184    |\n",
      "| loss_kl         | 0.0005326024  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0033539794 |\n",
      "| loss_vf_loss    | 20.289404     |\n",
      "-----------------------------------\n",
      "********** Iteration 127 ************\n",
      "Eval num_timesteps=520192, episode_reward=-74.50 +/- 8.81\n",
      "Episode length: 75.50 +/- 8.81\n",
      "Eval num_timesteps=520192, episode_reward=-83.90 +/- 13.35\n",
      "Episode length: 84.90 +/- 13.35\n",
      "Eval num_timesteps=520192, episode_reward=-84.30 +/- 14.49\n",
      "Episode length: 85.30 +/- 14.49\n",
      "Eval num_timesteps=520192, episode_reward=-78.20 +/- 7.11\n",
      "Episode length: 79.20 +/- 7.11\n",
      "Eval num_timesteps=520192, episode_reward=-93.20 +/- 25.49\n",
      "Episode length: 94.20 +/- 25.49\n",
      "Eval num_timesteps=520192, episode_reward=-78.50 +/- 11.21\n",
      "Episode length: 79.50 +/- 11.21\n",
      "Eval num_timesteps=520192, episode_reward=-87.40 +/- 18.97\n",
      "Episode length: 88.40 +/- 18.97\n",
      "Eval num_timesteps=520192, episode_reward=-76.00 +/- 7.99\n",
      "Episode length: 77.00 +/- 7.99\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00022 |       0.00000 |      32.00050 |       0.00016 |       0.20204\n",
      "     -0.00106 |       0.00000 |      27.63908 |       0.00025 |       0.19889\n",
      "     -0.00155 |       0.00000 |      26.93805 |       0.00030 |       0.20021\n",
      "     -0.00179 |       0.00000 |      26.54634 |       0.00039 |       0.20022\n",
      "     -0.00230 |       0.00000 |      26.10342 |       0.00041 |       0.20077\n",
      "     -0.00273 |       0.00000 |      25.83608 |       0.00047 |       0.20132\n",
      "     -0.00291 |       0.00000 |      25.63901 |       0.00052 |       0.20160\n",
      "     -0.00254 |       0.00000 |      25.42353 |       0.00072 |       0.20432\n",
      "     -0.00353 |       0.00000 |      25.22991 |       0.00078 |       0.20262\n",
      "     -0.00395 |       0.00000 |      25.08706 |       0.00077 |       0.20210\n",
      "Evaluating losses...\n",
      "     -0.00440 |       0.00000 |      24.85510 |       0.00084 |       0.20232\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.2          |\n",
      "| EpRewMean       | -89.2         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4703          |\n",
      "| TimeElapsed     | 1.22e+03      |\n",
      "| TimestepsSoFar  | 524288        |\n",
      "| ev_tdlam_before | 0.843         |\n",
      "| loss_ent        | 0.20232078    |\n",
      "| loss_kl         | 0.0008363369  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0043952386 |\n",
      "| loss_vf_loss    | 24.855103     |\n",
      "-----------------------------------\n",
      "********** Iteration 128 ************\n",
      "Eval num_timesteps=524288, episode_reward=-85.40 +/- 14.93\n",
      "Episode length: 86.40 +/- 14.93\n",
      "Eval num_timesteps=524288, episode_reward=-84.00 +/- 17.19\n",
      "Episode length: 85.00 +/- 17.19\n",
      "Eval num_timesteps=524288, episode_reward=-99.30 +/- 48.58\n",
      "Episode length: 100.30 +/- 48.58\n",
      "Eval num_timesteps=524288, episode_reward=-80.60 +/- 10.07\n",
      "Episode length: 81.60 +/- 10.07\n",
      "Eval num_timesteps=524288, episode_reward=-79.20 +/- 5.33\n",
      "Episode length: 80.20 +/- 5.33\n",
      "Eval num_timesteps=524288, episode_reward=-71.60 +/- 7.75\n",
      "Episode length: 72.60 +/- 7.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=524288, episode_reward=-82.00 +/- 12.98\n",
      "Episode length: 83.00 +/- 12.98\n",
      "Eval num_timesteps=524288, episode_reward=-76.40 +/- 5.61\n",
      "Episode length: 77.40 +/- 5.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.92e-05 |       0.00000 |      18.31411 |       0.00016 |       0.18531\n",
      "     -0.00055 |       0.00000 |      16.89508 |       0.00027 |       0.18634\n",
      "     -0.00121 |       0.00000 |      16.11813 |       0.00050 |       0.18510\n",
      "     -0.00143 |       0.00000 |      15.64025 |       0.00037 |       0.18441\n",
      "     -0.00150 |       0.00000 |      15.29504 |       0.00043 |       0.18500\n",
      "     -0.00184 |       0.00000 |      14.99867 |       0.00045 |       0.18436\n",
      "     -0.00220 |       0.00000 |      14.77858 |       0.00055 |       0.18435\n",
      "     -0.00221 |       0.00000 |      14.63785 |       0.00051 |       0.18473\n",
      "     -0.00256 |       0.00000 |      14.44244 |       0.00060 |       0.18406\n",
      "     -0.00251 |       0.00000 |      14.31774 |       0.00053 |       0.18274\n",
      "Evaluating losses...\n",
      "     -0.00318 |       0.00000 |      14.18777 |       0.00052 |       0.18419\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.2          |\n",
      "| EpRewMean       | -88.2         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 4750          |\n",
      "| TimeElapsed     | 1.23e+03      |\n",
      "| TimestepsSoFar  | 528384        |\n",
      "| ev_tdlam_before | 0.93          |\n",
      "| loss_ent        | 0.18419456    |\n",
      "| loss_kl         | 0.00052256184 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031798817 |\n",
      "| loss_vf_loss    | 14.187769     |\n",
      "-----------------------------------\n",
      "********** Iteration 129 ************\n",
      "Eval num_timesteps=528384, episode_reward=-76.40 +/- 5.30\n",
      "Episode length: 77.40 +/- 5.30\n",
      "Eval num_timesteps=528384, episode_reward=-83.20 +/- 17.61\n",
      "Episode length: 84.20 +/- 17.61\n",
      "Eval num_timesteps=528384, episode_reward=-79.80 +/- 6.82\n",
      "Episode length: 80.80 +/- 6.82\n",
      "Eval num_timesteps=528384, episode_reward=-78.60 +/- 7.62\n",
      "Episode length: 79.60 +/- 7.62\n",
      "Eval num_timesteps=528384, episode_reward=-83.50 +/- 10.35\n",
      "Episode length: 84.50 +/- 10.35\n",
      "Eval num_timesteps=528384, episode_reward=-81.10 +/- 6.80\n",
      "Episode length: 82.10 +/- 6.80\n",
      "Eval num_timesteps=528384, episode_reward=-76.90 +/- 8.35\n",
      "Episode length: 77.90 +/- 8.35\n",
      "Eval num_timesteps=528384, episode_reward=-82.20 +/- 5.95\n",
      "Episode length: 83.20 +/- 5.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00019 |       0.00000 |      24.25037 |       0.00038 |       0.20553\n",
      "     -0.00139 |       0.00000 |      22.97299 |       0.00060 |       0.20311\n",
      "     -0.00201 |       0.00000 |      22.55991 |       0.00053 |       0.20453\n",
      "     -0.00269 |       0.00000 |      22.26861 |       0.00080 |       0.20527\n",
      "     -0.00256 |       0.00000 |      22.07091 |       0.00078 |       0.20564\n",
      "     -0.00311 |       0.00000 |      21.84247 |       0.00094 |       0.20621\n",
      "     -0.00295 |       0.00000 |      21.73392 |       0.00104 |       0.20447\n",
      "     -0.00350 |       0.00000 |      21.62994 |       0.00086 |       0.20695\n",
      "     -0.00361 |       0.00000 |      21.55570 |       0.00115 |       0.20816\n",
      "     -0.00379 |       0.00000 |      21.42700 |       0.00113 |       0.20614\n",
      "Evaluating losses...\n",
      "     -0.00428 |       0.00000 |      21.30033 |       0.00103 |       0.20809\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.8         |\n",
      "| EpRewMean       | -86.8        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 4795         |\n",
      "| TimeElapsed     | 1.23e+03     |\n",
      "| TimestepsSoFar  | 532480       |\n",
      "| ev_tdlam_before | 0.899        |\n",
      "| loss_ent        | 0.2080854    |\n",
      "| loss_kl         | 0.0010260015 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004280057 |\n",
      "| loss_vf_loss    | 21.300327    |\n",
      "----------------------------------\n",
      "********** Iteration 130 ************\n",
      "Eval num_timesteps=532480, episode_reward=-80.90 +/- 6.76\n",
      "Episode length: 81.90 +/- 6.76\n",
      "Eval num_timesteps=532480, episode_reward=-82.00 +/- 9.25\n",
      "Episode length: 83.00 +/- 9.25\n",
      "Eval num_timesteps=532480, episode_reward=-85.90 +/- 25.75\n",
      "Episode length: 86.90 +/- 25.75\n",
      "Eval num_timesteps=532480, episode_reward=-86.40 +/- 18.85\n",
      "Episode length: 87.40 +/- 18.85\n",
      "Eval num_timesteps=532480, episode_reward=-97.20 +/- 34.57\n",
      "Episode length: 98.20 +/- 34.57\n",
      "Eval num_timesteps=532480, episode_reward=-79.70 +/- 12.94\n",
      "Episode length: 80.70 +/- 12.94\n",
      "Eval num_timesteps=532480, episode_reward=-80.90 +/- 15.55\n",
      "Episode length: 81.90 +/- 15.55\n",
      "Eval num_timesteps=532480, episode_reward=-103.60 +/- 58.30\n",
      "Episode length: 104.60 +/- 58.30\n",
      "Eval num_timesteps=532480, episode_reward=-83.20 +/- 13.31\n",
      "Episode length: 84.20 +/- 13.31\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      43.35186 |       0.00019 |       0.22047\n",
      "     -0.00109 |       0.00000 |      41.48831 |       0.00031 |       0.21998\n",
      "     -0.00189 |       0.00000 |      40.78588 |       0.00048 |       0.21878\n",
      "     -0.00189 |       0.00000 |      40.31826 |       0.00049 |       0.22059\n",
      "     -0.00232 |       0.00000 |      39.92358 |       0.00041 |       0.21882\n",
      "     -0.00249 |       0.00000 |      39.64877 |       0.00056 |       0.22096\n",
      "     -0.00294 |       0.00000 |      39.38274 |       0.00062 |       0.21980\n",
      "     -0.00321 |       0.00000 |      39.09491 |       0.00067 |       0.22114\n",
      "     -0.00331 |       0.00000 |      38.99972 |       0.00069 |       0.22109\n",
      "     -0.00321 |       0.00000 |      38.85059 |       0.00061 |       0.22125\n",
      "Evaluating losses...\n",
      "     -0.00375 |       0.00000 |      38.51109 |       0.00068 |       0.22031\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93            |\n",
      "| EpRewMean       | -92           |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 4836          |\n",
      "| TimeElapsed     | 1.24e+03      |\n",
      "| TimestepsSoFar  | 536576        |\n",
      "| ev_tdlam_before | 0.818         |\n",
      "| loss_ent        | 0.22030623    |\n",
      "| loss_kl         | 0.00067710766 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0037470276 |\n",
      "| loss_vf_loss    | 38.511093     |\n",
      "-----------------------------------\n",
      "********** Iteration 131 ************\n",
      "Eval num_timesteps=536576, episode_reward=-91.20 +/- 29.87\n",
      "Episode length: 92.20 +/- 29.87\n",
      "Eval num_timesteps=536576, episode_reward=-80.60 +/- 9.24\n",
      "Episode length: 81.60 +/- 9.24\n",
      "Eval num_timesteps=536576, episode_reward=-78.70 +/- 6.78\n",
      "Episode length: 79.70 +/- 6.78\n",
      "Eval num_timesteps=536576, episode_reward=-78.60 +/- 5.73\n",
      "Episode length: 79.60 +/- 5.73\n",
      "Eval num_timesteps=536576, episode_reward=-83.40 +/- 6.65\n",
      "Episode length: 84.40 +/- 6.65\n",
      "Eval num_timesteps=536576, episode_reward=-79.40 +/- 6.25\n",
      "Episode length: 80.40 +/- 6.25\n",
      "Eval num_timesteps=536576, episode_reward=-78.10 +/- 7.16\n",
      "Episode length: 79.10 +/- 7.16\n",
      "Eval num_timesteps=536576, episode_reward=-75.20 +/- 3.99\n",
      "Episode length: 76.20 +/- 3.99\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00048 |       0.00000 |      43.23489 |       0.00015 |       0.20592\n",
      "     -0.00088 |       0.00000 |      42.12595 |       0.00026 |       0.20177\n",
      "     -0.00131 |       0.00000 |      41.44724 |       0.00038 |       0.20189\n",
      "     -0.00162 |       0.00000 |      41.06829 |       0.00046 |       0.20442\n",
      "     -0.00203 |       0.00000 |      40.65183 |       0.00042 |       0.20331\n",
      "     -0.00238 |       0.00000 |      40.43453 |       0.00044 |       0.20247\n",
      "     -0.00282 |       0.00000 |      40.12577 |       0.00049 |       0.20486\n",
      "     -0.00283 |       0.00000 |      39.94643 |       0.00052 |       0.20319\n",
      "     -0.00286 |       0.00000 |      39.72985 |       0.00066 |       0.20390\n",
      "     -0.00300 |       0.00000 |      39.55638 |       0.00066 |       0.20419\n",
      "Evaluating losses...\n",
      "     -0.00367 |       0.00000 |      39.22449 |       0.00064 |       0.20328\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.2          |\n",
      "| EpRewMean       | -95.2         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 4878          |\n",
      "| TimeElapsed     | 1.25e+03      |\n",
      "| TimestepsSoFar  | 540672        |\n",
      "| ev_tdlam_before | 0.821         |\n",
      "| loss_ent        | 0.20328218    |\n",
      "| loss_kl         | 0.00064373726 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036674524 |\n",
      "| loss_vf_loss    | 39.224487     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 132 ************\n",
      "Eval num_timesteps=540672, episode_reward=-91.50 +/- 29.83\n",
      "Episode length: 92.50 +/- 29.83\n",
      "Eval num_timesteps=540672, episode_reward=-77.10 +/- 7.15\n",
      "Episode length: 78.10 +/- 7.15\n",
      "Eval num_timesteps=540672, episode_reward=-77.20 +/- 4.28\n",
      "Episode length: 78.20 +/- 4.28\n",
      "Eval num_timesteps=540672, episode_reward=-75.90 +/- 4.37\n",
      "Episode length: 76.90 +/- 4.37\n",
      "Eval num_timesteps=540672, episode_reward=-79.40 +/- 7.67\n",
      "Episode length: 80.40 +/- 7.67\n",
      "Eval num_timesteps=540672, episode_reward=-84.90 +/- 11.99\n",
      "Episode length: 85.90 +/- 11.99\n",
      "Eval num_timesteps=540672, episode_reward=-77.90 +/- 4.46\n",
      "Episode length: 78.90 +/- 4.46\n",
      "Eval num_timesteps=540672, episode_reward=-76.80 +/- 5.44\n",
      "Episode length: 77.80 +/- 5.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00010 |       0.00000 |      36.95457 |       0.00013 |       0.19985\n",
      "     -0.00104 |       0.00000 |      35.82861 |       0.00048 |       0.20033\n",
      "     -0.00093 |       0.00000 |      35.23771 |       0.00050 |       0.19976\n",
      "     -0.00125 |       0.00000 |      34.89101 |       0.00050 |       0.20013\n",
      "     -0.00152 |       0.00000 |      34.60302 |       0.00045 |       0.20127\n",
      "     -0.00196 |       0.00000 |      34.40956 |       0.00048 |       0.20210\n",
      "     -0.00185 |       0.00000 |      34.18976 |       0.00050 |       0.20215\n",
      "     -0.00200 |       0.00000 |      34.02862 |       0.00050 |       0.20251\n",
      "     -0.00203 |       0.00000 |      33.91674 |       0.00056 |       0.20206\n",
      "     -0.00203 |       0.00000 |      33.75115 |       0.00064 |       0.20236\n",
      "Evaluating losses...\n",
      "     -0.00292 |       0.00000 |      33.45763 |       0.00056 |       0.20073\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.6          |\n",
      "| EpRewMean       | -95.6         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4922          |\n",
      "| TimeElapsed     | 1.25e+03      |\n",
      "| TimestepsSoFar  | 544768        |\n",
      "| ev_tdlam_before | 0.848         |\n",
      "| loss_ent        | 0.20073499    |\n",
      "| loss_kl         | 0.00055926543 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002919434  |\n",
      "| loss_vf_loss    | 33.457626     |\n",
      "-----------------------------------\n",
      "********** Iteration 133 ************\n",
      "Eval num_timesteps=544768, episode_reward=-87.40 +/- 12.36\n",
      "Episode length: 88.40 +/- 12.36\n",
      "Eval num_timesteps=544768, episode_reward=-91.70 +/- 43.74\n",
      "Episode length: 92.70 +/- 43.74\n",
      "Eval num_timesteps=544768, episode_reward=-76.40 +/- 9.51\n",
      "Episode length: 77.40 +/- 9.51\n",
      "Eval num_timesteps=544768, episode_reward=-81.60 +/- 12.55\n",
      "Episode length: 82.60 +/- 12.55\n",
      "Eval num_timesteps=544768, episode_reward=-85.50 +/- 10.99\n",
      "Episode length: 86.50 +/- 10.99\n",
      "Eval num_timesteps=544768, episode_reward=-81.10 +/- 8.26\n",
      "Episode length: 82.10 +/- 8.26\n",
      "Eval num_timesteps=544768, episode_reward=-74.70 +/- 5.50\n",
      "Episode length: 75.70 +/- 5.50\n",
      "Eval num_timesteps=544768, episode_reward=-78.30 +/- 9.35\n",
      "Episode length: 79.30 +/- 9.35\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.11e-05 |       0.00000 |      39.41133 |      8.13e-05 |       0.19130\n",
      "     -0.00081 |       0.00000 |      38.42396 |       0.00025 |       0.18812\n",
      "     -0.00136 |       0.00000 |      37.76782 |       0.00032 |       0.18732\n",
      "     -0.00120 |       0.00000 |      37.26539 |       0.00030 |       0.18648\n",
      "     -0.00150 |       0.00000 |      36.88148 |       0.00029 |       0.18848\n",
      "     -0.00202 |       0.00000 |      36.53525 |       0.00031 |       0.18724\n",
      "     -0.00199 |       0.00000 |      36.26266 |       0.00033 |       0.18778\n",
      "     -0.00221 |       0.00000 |      36.01390 |       0.00036 |       0.18719\n",
      "     -0.00250 |       0.00000 |      35.80286 |       0.00038 |       0.18767\n",
      "     -0.00259 |       0.00000 |      35.63911 |       0.00040 |       0.18772\n",
      "Evaluating losses...\n",
      "     -0.00287 |       0.00000 |      35.31613 |       0.00040 |       0.18721\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.8          |\n",
      "| EpRewMean       | -93.8         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4966          |\n",
      "| TimeElapsed     | 1.26e+03      |\n",
      "| TimestepsSoFar  | 548864        |\n",
      "| ev_tdlam_before | 0.845         |\n",
      "| loss_ent        | 0.1872102     |\n",
      "| loss_kl         | 0.00039766298 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002874909  |\n",
      "| loss_vf_loss    | 35.316128     |\n",
      "-----------------------------------\n",
      "********** Iteration 134 ************\n",
      "Eval num_timesteps=548864, episode_reward=-121.40 +/- 126.41\n",
      "Episode length: 122.30 +/- 126.11\n",
      "Eval num_timesteps=548864, episode_reward=-77.70 +/- 4.34\n",
      "Episode length: 78.70 +/- 4.34\n",
      "Eval num_timesteps=548864, episode_reward=-83.20 +/- 13.25\n",
      "Episode length: 84.20 +/- 13.25\n",
      "Eval num_timesteps=548864, episode_reward=-74.70 +/- 6.20\n",
      "Episode length: 75.70 +/- 6.20\n",
      "Eval num_timesteps=548864, episode_reward=-84.20 +/- 14.27\n",
      "Episode length: 85.20 +/- 14.27\n",
      "Eval num_timesteps=548864, episode_reward=-81.80 +/- 9.70\n",
      "Episode length: 82.80 +/- 9.70\n",
      "Eval num_timesteps=548864, episode_reward=-81.70 +/- 6.62\n",
      "Episode length: 82.70 +/- 6.62\n",
      "Eval num_timesteps=548864, episode_reward=-78.10 +/- 10.08\n",
      "Episode length: 79.10 +/- 10.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00027 |       0.00000 |      47.83981 |       0.00022 |       0.19691\n",
      "     -0.00132 |       0.00000 |      45.94195 |       0.00038 |       0.19548\n",
      "     -0.00153 |       0.00000 |      45.11365 |       0.00036 |       0.19725\n",
      "     -0.00207 |       0.00000 |      44.52583 |       0.00044 |       0.19665\n",
      "     -0.00204 |       0.00000 |      44.03136 |       0.00049 |       0.19767\n",
      "     -0.00262 |       0.00000 |      43.69119 |       0.00045 |       0.19808\n",
      "     -0.00260 |       0.00000 |      43.42012 |       0.00041 |       0.19734\n",
      "     -0.00294 |       0.00000 |      43.05814 |       0.00059 |       0.19790\n",
      "     -0.00292 |       0.00000 |      42.80391 |       0.00068 |       0.19858\n",
      "     -0.00328 |       0.00000 |      42.54440 |       0.00058 |       0.19817\n",
      "Evaluating losses...\n",
      "     -0.00348 |       0.00000 |      42.18355 |       0.00070 |       0.19958\n",
      "-----------------------------------\n",
      "| EpLenMean       | 97.5          |\n",
      "| EpRewMean       | -96.5         |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 5006          |\n",
      "| TimeElapsed     | 1.27e+03      |\n",
      "| TimestepsSoFar  | 552960        |\n",
      "| ev_tdlam_before | 0.806         |\n",
      "| loss_ent        | 0.19957747    |\n",
      "| loss_kl         | 0.000696655   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034847262 |\n",
      "| loss_vf_loss    | 42.183548     |\n",
      "-----------------------------------\n",
      "********** Iteration 135 ************\n",
      "Eval num_timesteps=552960, episode_reward=-79.70 +/- 5.81\n",
      "Episode length: 80.70 +/- 5.81\n",
      "Eval num_timesteps=552960, episode_reward=-79.30 +/- 12.00\n",
      "Episode length: 80.30 +/- 12.00\n",
      "Eval num_timesteps=552960, episode_reward=-89.10 +/- 28.17\n",
      "Episode length: 90.10 +/- 28.17\n",
      "Eval num_timesteps=552960, episode_reward=-78.80 +/- 13.59\n",
      "Episode length: 79.80 +/- 13.59\n",
      "Eval num_timesteps=552960, episode_reward=-76.80 +/- 8.35\n",
      "Episode length: 77.80 +/- 8.35\n",
      "Eval num_timesteps=552960, episode_reward=-78.60 +/- 10.87\n",
      "Episode length: 79.60 +/- 10.87\n",
      "Eval num_timesteps=552960, episode_reward=-85.50 +/- 16.68\n",
      "Episode length: 86.50 +/- 16.68\n",
      "Eval num_timesteps=552960, episode_reward=-80.80 +/- 5.21\n",
      "Episode length: 81.80 +/- 5.21\n",
      "Eval num_timesteps=552960, episode_reward=-80.30 +/- 8.49\n",
      "Episode length: 81.30 +/- 8.49\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00011 |       0.00000 |      41.73838 |       0.00016 |       0.21782\n",
      "     -0.00114 |       0.00000 |      39.41319 |       0.00039 |       0.22107\n",
      "     -0.00120 |       0.00000 |      38.96045 |       0.00042 |       0.22142\n",
      "     -0.00199 |       0.00000 |      38.60268 |       0.00040 |       0.21950\n",
      "     -0.00240 |       0.00000 |      38.29380 |       0.00053 |       0.22159\n",
      "     -0.00267 |       0.00000 |      38.02250 |       0.00047 |       0.21966\n",
      "     -0.00264 |       0.00000 |      37.82781 |       0.00055 |       0.22037\n",
      "     -0.00265 |       0.00000 |      37.66942 |       0.00082 |       0.22019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00306 |       0.00000 |      37.51353 |       0.00062 |       0.21848\n",
      "     -0.00319 |       0.00000 |      37.37259 |       0.00076 |       0.22174\n",
      "Evaluating losses...\n",
      "     -0.00362 |       0.00000 |      37.04667 |       0.00066 |       0.21903\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.9          |\n",
      "| EpRewMean       | -97.9         |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 5046          |\n",
      "| TimeElapsed     | 1.27e+03      |\n",
      "| TimestepsSoFar  | 557056        |\n",
      "| ev_tdlam_before | 0.814         |\n",
      "| loss_ent        | 0.21902595    |\n",
      "| loss_kl         | 0.000660024   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036222232 |\n",
      "| loss_vf_loss    | 37.046673     |\n",
      "-----------------------------------\n",
      "********** Iteration 136 ************\n",
      "Eval num_timesteps=557056, episode_reward=-86.80 +/- 21.07\n",
      "Episode length: 87.80 +/- 21.07\n",
      "Eval num_timesteps=557056, episode_reward=-87.50 +/- 16.42\n",
      "Episode length: 88.50 +/- 16.42\n",
      "Eval num_timesteps=557056, episode_reward=-76.80 +/- 6.88\n",
      "Episode length: 77.80 +/- 6.88\n",
      "Eval num_timesteps=557056, episode_reward=-79.50 +/- 7.45\n",
      "Episode length: 80.50 +/- 7.45\n",
      "Eval num_timesteps=557056, episode_reward=-98.20 +/- 38.12\n",
      "Episode length: 99.20 +/- 38.12\n",
      "Eval num_timesteps=557056, episode_reward=-74.40 +/- 7.16\n",
      "Episode length: 75.40 +/- 7.16\n",
      "Eval num_timesteps=557056, episode_reward=-74.30 +/- 6.21\n",
      "Episode length: 75.30 +/- 6.21\n",
      "Eval num_timesteps=557056, episode_reward=-82.60 +/- 4.76\n",
      "Episode length: 83.60 +/- 4.76\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00015 |       0.00000 |      31.50695 |       0.00017 |       0.20082\n",
      "     -0.00104 |       0.00000 |      30.46483 |       0.00025 |       0.20021\n",
      "     -0.00130 |       0.00000 |      30.09056 |       0.00040 |       0.19995\n",
      "     -0.00175 |       0.00000 |      29.81971 |       0.00041 |       0.19904\n",
      "     -0.00196 |       0.00000 |      29.66829 |       0.00037 |       0.19875\n",
      "     -0.00201 |       0.00000 |      29.32957 |       0.00046 |       0.19963\n",
      "     -0.00229 |       0.00000 |      29.21749 |       0.00050 |       0.19814\n",
      "     -0.00246 |       0.00000 |      29.06881 |       0.00047 |       0.19901\n",
      "     -0.00269 |       0.00000 |      28.88108 |       0.00050 |       0.19888\n",
      "     -0.00294 |       0.00000 |      28.91607 |       0.00050 |       0.19798\n",
      "Evaluating losses...\n",
      "     -0.00326 |       0.00000 |      28.58589 |       0.00052 |       0.20012\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.6         |\n",
      "| EpRewMean       | -94.6        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 5092         |\n",
      "| TimeElapsed     | 1.28e+03     |\n",
      "| TimestepsSoFar  | 561152       |\n",
      "| ev_tdlam_before | 0.884        |\n",
      "| loss_ent        | 0.2001189    |\n",
      "| loss_kl         | 0.0005208733 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003263422 |\n",
      "| loss_vf_loss    | 28.585892    |\n",
      "----------------------------------\n",
      "********** Iteration 137 ************\n",
      "Eval num_timesteps=561152, episode_reward=-91.60 +/- 35.90\n",
      "Episode length: 92.60 +/- 35.90\n",
      "Eval num_timesteps=561152, episode_reward=-83.70 +/- 18.47\n",
      "Episode length: 84.70 +/- 18.47\n",
      "Eval num_timesteps=561152, episode_reward=-122.40 +/- 126.00\n",
      "Episode length: 123.30 +/- 125.70\n",
      "Eval num_timesteps=561152, episode_reward=-82.50 +/- 9.93\n",
      "Episode length: 83.50 +/- 9.93\n",
      "Eval num_timesteps=561152, episode_reward=-92.90 +/- 27.93\n",
      "Episode length: 93.90 +/- 27.93\n",
      "Eval num_timesteps=561152, episode_reward=-96.10 +/- 31.28\n",
      "Episode length: 97.10 +/- 31.28\n",
      "Eval num_timesteps=561152, episode_reward=-77.70 +/- 5.93\n",
      "Episode length: 78.70 +/- 5.93\n",
      "Eval num_timesteps=561152, episode_reward=-77.80 +/- 7.14\n",
      "Episode length: 78.80 +/- 7.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00018 |       0.00000 |      26.09284 |       0.00013 |       0.17407\n",
      "     -0.00083 |       0.00000 |      25.12812 |       0.00021 |       0.17190\n",
      "     -0.00131 |       0.00000 |      24.85926 |       0.00029 |       0.17190\n",
      "     -0.00154 |       0.00000 |      24.64544 |       0.00026 |       0.17293\n",
      "     -0.00163 |       0.00000 |      24.49121 |       0.00040 |       0.17245\n",
      "     -0.00172 |       0.00000 |      24.39546 |       0.00045 |       0.17264\n",
      "     -0.00199 |       0.00000 |      24.26663 |       0.00038 |       0.17328\n",
      "     -0.00195 |       0.00000 |      24.09231 |       0.00038 |       0.17301\n",
      "     -0.00212 |       0.00000 |      24.02657 |       0.00039 |       0.17407\n",
      "     -0.00222 |       0.00000 |      23.94857 |       0.00051 |       0.17375\n",
      "Evaluating losses...\n",
      "     -0.00261 |       0.00000 |      23.78754 |       0.00036 |       0.17429\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.5          |\n",
      "| EpRewMean       | -88.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5138          |\n",
      "| TimeElapsed     | 1.29e+03      |\n",
      "| TimestepsSoFar  | 565248        |\n",
      "| ev_tdlam_before | 0.91          |\n",
      "| loss_ent        | 0.17428793    |\n",
      "| loss_kl         | 0.0003641214  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0026116883 |\n",
      "| loss_vf_loss    | 23.78754      |\n",
      "-----------------------------------\n",
      "********** Iteration 138 ************\n",
      "Eval num_timesteps=565248, episode_reward=-92.60 +/- 13.09\n",
      "Episode length: 93.60 +/- 13.09\n",
      "Eval num_timesteps=565248, episode_reward=-76.90 +/- 5.37\n",
      "Episode length: 77.90 +/- 5.37\n",
      "Eval num_timesteps=565248, episode_reward=-77.00 +/- 4.58\n",
      "Episode length: 78.00 +/- 4.58\n",
      "Eval num_timesteps=565248, episode_reward=-86.60 +/- 20.05\n",
      "Episode length: 87.60 +/- 20.05\n",
      "Eval num_timesteps=565248, episode_reward=-73.90 +/- 7.31\n",
      "Episode length: 74.90 +/- 7.31\n",
      "Eval num_timesteps=565248, episode_reward=-79.30 +/- 4.47\n",
      "Episode length: 80.30 +/- 4.47\n",
      "Eval num_timesteps=565248, episode_reward=-83.70 +/- 12.09\n",
      "Episode length: 84.70 +/- 12.09\n",
      "Eval num_timesteps=565248, episode_reward=-81.90 +/- 14.08\n",
      "Episode length: 82.90 +/- 14.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00038 |       0.00000 |      24.07174 |       0.00026 |       0.19574\n",
      "     -0.00113 |       0.00000 |      21.77324 |       0.00041 |       0.19610\n",
      "     -0.00110 |       0.00000 |      21.27555 |       0.00053 |       0.19781\n",
      "     -0.00211 |       0.00000 |      20.98310 |       0.00054 |       0.19817\n",
      "     -0.00211 |       0.00000 |      20.70403 |       0.00054 |       0.19665\n",
      "     -0.00267 |       0.00000 |      20.55667 |       0.00055 |       0.19798\n",
      "     -0.00256 |       0.00000 |      20.32376 |       0.00079 |       0.19935\n",
      "     -0.00236 |       0.00000 |      20.19913 |       0.00074 |       0.19889\n",
      "     -0.00305 |       0.00000 |      20.04523 |       0.00079 |       0.19998\n",
      "     -0.00297 |       0.00000 |      19.92060 |       0.00075 |       0.19966\n",
      "Evaluating losses...\n",
      "     -0.00348 |       0.00000 |      19.74422 |       0.00089 |       0.19671\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.2          |\n",
      "| EpRewMean       | -89.2         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 5182          |\n",
      "| TimeElapsed     | 1.29e+03      |\n",
      "| TimestepsSoFar  | 569344        |\n",
      "| ev_tdlam_before | 0.9           |\n",
      "| loss_ent        | 0.19671452    |\n",
      "| loss_kl         | 0.00088791276 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034808086 |\n",
      "| loss_vf_loss    | 19.744215     |\n",
      "-----------------------------------\n",
      "********** Iteration 139 ************\n",
      "Eval num_timesteps=569344, episode_reward=-76.20 +/- 10.10\n",
      "Episode length: 77.20 +/- 10.10\n",
      "Eval num_timesteps=569344, episode_reward=-78.20 +/- 8.03\n",
      "Episode length: 79.20 +/- 8.03\n",
      "Eval num_timesteps=569344, episode_reward=-97.40 +/- 41.57\n",
      "Episode length: 98.40 +/- 41.57\n",
      "Eval num_timesteps=569344, episode_reward=-77.70 +/- 7.55\n",
      "Episode length: 78.70 +/- 7.55\n",
      "Eval num_timesteps=569344, episode_reward=-80.40 +/- 4.76\n",
      "Episode length: 81.40 +/- 4.76\n",
      "Eval num_timesteps=569344, episode_reward=-83.30 +/- 10.16\n",
      "Episode length: 84.30 +/- 10.16\n",
      "Eval num_timesteps=569344, episode_reward=-85.80 +/- 29.69\n",
      "Episode length: 86.80 +/- 29.69\n",
      "Eval num_timesteps=569344, episode_reward=-80.20 +/- 7.45\n",
      "Episode length: 81.20 +/- 7.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00066 |       0.00000 |      36.38766 |       0.00014 |       0.17690\n",
      "     -0.00036 |       0.00000 |      34.89367 |       0.00018 |       0.17870\n",
      "     -0.00081 |       0.00000 |      34.14669 |       0.00034 |       0.17899\n",
      "     -0.00128 |       0.00000 |      33.69407 |       0.00028 |       0.17743\n",
      "     -0.00152 |       0.00000 |      33.34379 |       0.00032 |       0.17863\n",
      "     -0.00169 |       0.00000 |      33.09655 |       0.00036 |       0.17854\n",
      "     -0.00171 |       0.00000 |      32.84844 |       0.00039 |       0.17840\n",
      "     -0.00208 |       0.00000 |      32.63876 |       0.00038 |       0.17778\n",
      "     -0.00215 |       0.00000 |      32.45194 |       0.00046 |       0.17769\n",
      "     -0.00225 |       0.00000 |      32.23697 |       0.00046 |       0.17779\n",
      "Evaluating losses...\n",
      "     -0.00261 |       0.00000 |      31.96768 |       0.00049 |       0.17819\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.7          |\n",
      "| EpRewMean       | -89.7         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5227          |\n",
      "| TimeElapsed     | 1.3e+03       |\n",
      "| TimestepsSoFar  | 573440        |\n",
      "| ev_tdlam_before | 0.865         |\n",
      "| loss_ent        | 0.17819056    |\n",
      "| loss_kl         | 0.000493782   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0026095985 |\n",
      "| loss_vf_loss    | 31.967684     |\n",
      "-----------------------------------\n",
      "********** Iteration 140 ************\n",
      "Eval num_timesteps=573440, episode_reward=-76.50 +/- 6.39\n",
      "Episode length: 77.50 +/- 6.39\n",
      "Eval num_timesteps=573440, episode_reward=-77.90 +/- 6.93\n",
      "Episode length: 78.90 +/- 6.93\n",
      "Eval num_timesteps=573440, episode_reward=-77.80 +/- 9.33\n",
      "Episode length: 78.80 +/- 9.33\n",
      "Eval num_timesteps=573440, episode_reward=-79.00 +/- 7.97\n",
      "Episode length: 80.00 +/- 7.97\n",
      "Eval num_timesteps=573440, episode_reward=-81.70 +/- 13.86\n",
      "Episode length: 82.70 +/- 13.86\n",
      "Eval num_timesteps=573440, episode_reward=-79.80 +/- 5.76\n",
      "Episode length: 80.80 +/- 5.76\n",
      "Eval num_timesteps=573440, episode_reward=-81.20 +/- 16.74\n",
      "Episode length: 82.20 +/- 16.74\n",
      "Eval num_timesteps=573440, episode_reward=-77.30 +/- 8.22\n",
      "Episode length: 78.30 +/- 8.22\n",
      "Eval num_timesteps=573440, episode_reward=-77.80 +/- 4.71\n",
      "Episode length: 78.80 +/- 4.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -5.20e-06 |       0.00000 |      22.55338 |       0.00015 |       0.16645\n",
      "     -0.00043 |       0.00000 |      21.04257 |       0.00027 |       0.16578\n",
      "     -0.00071 |       0.00000 |      20.38437 |       0.00028 |       0.16763\n",
      "     -0.00087 |       0.00000 |      20.09283 |       0.00046 |       0.16538\n",
      "     -0.00096 |       0.00000 |      19.89122 |       0.00035 |       0.16721\n",
      "     -0.00100 |       0.00000 |      19.70256 |       0.00043 |       0.16661\n",
      "     -0.00151 |       0.00000 |      19.60494 |       0.00040 |       0.16585\n",
      "     -0.00143 |       0.00000 |      19.48842 |       0.00038 |       0.16573\n",
      "     -0.00164 |       0.00000 |      19.46021 |       0.00039 |       0.16579\n",
      "     -0.00182 |       0.00000 |      19.35568 |       0.00042 |       0.16598\n",
      "Evaluating losses...\n",
      "     -0.00230 |       0.00000 |      19.22501 |       0.00043 |       0.16666\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.1          |\n",
      "| EpRewMean       | -87.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 5274          |\n",
      "| TimeElapsed     | 1.3e+03       |\n",
      "| TimestepsSoFar  | 577536        |\n",
      "| ev_tdlam_before | 0.917         |\n",
      "| loss_ent        | 0.1666606     |\n",
      "| loss_kl         | 0.00042904602 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002303944  |\n",
      "| loss_vf_loss    | 19.225014     |\n",
      "-----------------------------------\n",
      "********** Iteration 141 ************\n",
      "Eval num_timesteps=577536, episode_reward=-83.00 +/- 7.72\n",
      "Episode length: 84.00 +/- 7.72\n",
      "Eval num_timesteps=577536, episode_reward=-86.30 +/- 19.93\n",
      "Episode length: 87.30 +/- 19.93\n",
      "Eval num_timesteps=577536, episode_reward=-78.80 +/- 6.24\n",
      "Episode length: 79.80 +/- 6.24\n",
      "Eval num_timesteps=577536, episode_reward=-79.70 +/- 5.66\n",
      "Episode length: 80.70 +/- 5.66\n",
      "Eval num_timesteps=577536, episode_reward=-75.70 +/- 4.73\n",
      "Episode length: 76.70 +/- 4.73\n",
      "Eval num_timesteps=577536, episode_reward=-96.60 +/- 29.02\n",
      "Episode length: 97.60 +/- 29.02\n",
      "Eval num_timesteps=577536, episode_reward=-80.90 +/- 8.70\n",
      "Episode length: 81.90 +/- 8.70\n",
      "Eval num_timesteps=577536, episode_reward=-78.80 +/- 6.16\n",
      "Episode length: 79.80 +/- 6.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00062 |       0.00000 |      23.77308 |       0.00019 |       0.19802\n",
      "     -0.00033 |       0.00000 |      23.09222 |       0.00050 |       0.20159\n",
      "     -0.00068 |       0.00000 |      22.86325 |       0.00039 |       0.19968\n",
      "     -0.00107 |       0.00000 |      22.63049 |       0.00045 |       0.20217\n",
      "     -0.00122 |       0.00000 |      22.51907 |       0.00054 |       0.20209\n",
      "     -0.00140 |       0.00000 |      22.37616 |       0.00050 |       0.20147\n",
      "     -0.00152 |       0.00000 |      22.28668 |       0.00049 |       0.20144\n",
      "     -0.00160 |       0.00000 |      22.20181 |       0.00044 |       0.19962\n",
      "     -0.00124 |       0.00000 |      22.04913 |       0.00063 |       0.20225\n",
      "     -0.00187 |       0.00000 |      22.02050 |       0.00066 |       0.20305\n",
      "Evaluating losses...\n",
      "     -0.00233 |       0.00000 |      21.84995 |       0.00056 |       0.20155\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89            |\n",
      "| EpRewMean       | -88           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5319          |\n",
      "| TimeElapsed     | 1.31e+03      |\n",
      "| TimestepsSoFar  | 581632        |\n",
      "| ev_tdlam_before | 0.913         |\n",
      "| loss_ent        | 0.20155239    |\n",
      "| loss_kl         | 0.0005552911  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0023327926 |\n",
      "| loss_vf_loss    | 21.849955     |\n",
      "-----------------------------------\n",
      "********** Iteration 142 ************\n",
      "Eval num_timesteps=581632, episode_reward=-80.10 +/- 9.00\n",
      "Episode length: 81.10 +/- 9.00\n",
      "Eval num_timesteps=581632, episode_reward=-76.90 +/- 6.83\n",
      "Episode length: 77.90 +/- 6.83\n",
      "Eval num_timesteps=581632, episode_reward=-81.20 +/- 8.03\n",
      "Episode length: 82.20 +/- 8.03\n",
      "Eval num_timesteps=581632, episode_reward=-91.50 +/- 25.35\n",
      "Episode length: 92.50 +/- 25.35\n",
      "Eval num_timesteps=581632, episode_reward=-81.20 +/- 4.28\n",
      "Episode length: 82.20 +/- 4.28\n",
      "Eval num_timesteps=581632, episode_reward=-82.10 +/- 8.46\n",
      "Episode length: 83.10 +/- 8.46\n",
      "Eval num_timesteps=581632, episode_reward=-84.30 +/- 8.49\n",
      "Episode length: 85.30 +/- 8.49\n",
      "Eval num_timesteps=581632, episode_reward=-81.10 +/- 12.77\n",
      "Episode length: 82.10 +/- 12.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -5.67e-05 |       0.00000 |      34.15404 |       0.00014 |       0.20123\n",
      "     -0.00030 |       0.00000 |      31.56260 |       0.00024 |       0.20201\n",
      "     -0.00069 |       0.00000 |      30.71237 |       0.00032 |       0.20095\n",
      "     -0.00130 |       0.00000 |      30.19440 |       0.00025 |       0.20107\n",
      "     -0.00147 |       0.00000 |      29.78200 |       0.00028 |       0.20233\n",
      "     -0.00163 |       0.00000 |      29.48419 |       0.00036 |       0.20193\n",
      "     -0.00172 |       0.00000 |      29.29032 |       0.00034 |       0.20303\n",
      "     -0.00186 |       0.00000 |      29.03200 |       0.00036 |       0.20135\n",
      "     -0.00199 |       0.00000 |      28.82791 |       0.00040 |       0.20240\n",
      "     -0.00218 |       0.00000 |      28.73464 |       0.00036 |       0.20192\n",
      "Evaluating losses...\n",
      "     -0.00255 |       0.00000 |      28.53347 |       0.00042 |       0.20406\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.9          |\n",
      "| EpRewMean       | -92.9         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 5362          |\n",
      "| TimeElapsed     | 1.32e+03      |\n",
      "| TimestepsSoFar  | 585728        |\n",
      "| ev_tdlam_before | 0.873         |\n",
      "| loss_ent        | 0.20405933    |\n",
      "| loss_kl         | 0.0004173248  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025509151 |\n",
      "| loss_vf_loss    | 28.533474     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 143 ************\n",
      "Eval num_timesteps=585728, episode_reward=-77.40 +/- 6.39\n",
      "Episode length: 78.40 +/- 6.39\n",
      "Eval num_timesteps=585728, episode_reward=-90.90 +/- 32.93\n",
      "Episode length: 91.90 +/- 32.93\n",
      "Eval num_timesteps=585728, episode_reward=-77.40 +/- 6.76\n",
      "Episode length: 78.40 +/- 6.76\n",
      "Eval num_timesteps=585728, episode_reward=-83.10 +/- 5.84\n",
      "Episode length: 84.10 +/- 5.84\n",
      "Eval num_timesteps=585728, episode_reward=-81.50 +/- 6.67\n",
      "Episode length: 82.50 +/- 6.67\n",
      "Eval num_timesteps=585728, episode_reward=-81.90 +/- 9.12\n",
      "Episode length: 82.90 +/- 9.12\n",
      "Eval num_timesteps=585728, episode_reward=-76.60 +/- 8.38\n",
      "Episode length: 77.60 +/- 8.38\n",
      "Eval num_timesteps=585728, episode_reward=-92.50 +/- 22.08\n",
      "Episode length: 93.50 +/- 22.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00042 |       0.00000 |      46.50500 |       0.00021 |       0.20058\n",
      "     -0.00097 |       0.00000 |      44.91489 |       0.00042 |       0.20149\n",
      "     -0.00122 |       0.00000 |      44.20586 |       0.00032 |       0.20219\n",
      "     -0.00161 |       0.00000 |      43.55518 |       0.00047 |       0.20054\n",
      "     -0.00196 |       0.00000 |      43.16373 |       0.00044 |       0.20304\n",
      "     -0.00224 |       0.00000 |      42.84614 |       0.00042 |       0.20061\n",
      "     -0.00218 |       0.00000 |      42.60778 |       0.00049 |       0.20228\n",
      "     -0.00239 |       0.00000 |      42.32212 |       0.00039 |       0.20158\n",
      "     -0.00251 |       0.00000 |      42.23454 |       0.00050 |       0.19985\n",
      "     -0.00264 |       0.00000 |      41.87120 |       0.00049 |       0.20112\n",
      "Evaluating losses...\n",
      "     -0.00361 |       0.00000 |      41.70668 |       0.00045 |       0.20138\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.4          |\n",
      "| EpRewMean       | -92.4         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 5406          |\n",
      "| TimeElapsed     | 1.32e+03      |\n",
      "| TimestepsSoFar  | 589824        |\n",
      "| ev_tdlam_before | 0.822         |\n",
      "| loss_ent        | 0.20138408    |\n",
      "| loss_kl         | 0.00044715358 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036099697 |\n",
      "| loss_vf_loss    | 41.706684     |\n",
      "-----------------------------------\n",
      "********** Iteration 144 ************\n",
      "Eval num_timesteps=589824, episode_reward=-82.20 +/- 4.51\n",
      "Episode length: 83.20 +/- 4.51\n",
      "Eval num_timesteps=589824, episode_reward=-81.00 +/- 8.10\n",
      "Episode length: 82.00 +/- 8.10\n",
      "Eval num_timesteps=589824, episode_reward=-88.30 +/- 20.74\n",
      "Episode length: 89.30 +/- 20.74\n",
      "Eval num_timesteps=589824, episode_reward=-77.60 +/- 8.28\n",
      "Episode length: 78.60 +/- 8.28\n",
      "Eval num_timesteps=589824, episode_reward=-77.30 +/- 15.02\n",
      "Episode length: 78.30 +/- 15.02\n",
      "Eval num_timesteps=589824, episode_reward=-83.70 +/- 10.40\n",
      "Episode length: 84.70 +/- 10.40\n",
      "Eval num_timesteps=589824, episode_reward=-84.60 +/- 9.53\n",
      "Episode length: 85.60 +/- 9.53\n",
      "Eval num_timesteps=589824, episode_reward=-83.60 +/- 6.30\n",
      "Episode length: 84.60 +/- 6.30\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00044 |       0.00000 |      40.91091 |       0.00014 |       0.21619\n",
      "     -0.00043 |       0.00000 |      40.04797 |       0.00024 |       0.21459\n",
      "     -0.00036 |       0.00000 |      39.54240 |       0.00033 |       0.21656\n",
      "     -0.00094 |       0.00000 |      39.17973 |       0.00032 |       0.21370\n",
      "     -0.00095 |       0.00000 |      38.83542 |       0.00031 |       0.21481\n",
      "     -0.00145 |       0.00000 |      38.59304 |       0.00032 |       0.21375\n",
      "     -0.00178 |       0.00000 |      38.37035 |       0.00030 |       0.21478\n",
      "     -0.00208 |       0.00000 |      38.21183 |       0.00034 |       0.21562\n",
      "     -0.00194 |       0.00000 |      38.03351 |       0.00040 |       0.21440\n",
      "     -0.00201 |       0.00000 |      37.83659 |       0.00035 |       0.21566\n",
      "Evaluating losses...\n",
      "     -0.00266 |       0.00000 |      37.64868 |       0.00036 |       0.21489\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96.5          |\n",
      "| EpRewMean       | -95.5         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 5448          |\n",
      "| TimeElapsed     | 1.33e+03      |\n",
      "| TimestepsSoFar  | 593920        |\n",
      "| ev_tdlam_before | 0.842         |\n",
      "| loss_ent        | 0.21489292    |\n",
      "| loss_kl         | 0.00035890777 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0026587816 |\n",
      "| loss_vf_loss    | 37.64868      |\n",
      "-----------------------------------\n",
      "********** Iteration 145 ************\n",
      "Eval num_timesteps=593920, episode_reward=-76.90 +/- 7.19\n",
      "Episode length: 77.90 +/- 7.19\n",
      "Eval num_timesteps=593920, episode_reward=-86.60 +/- 18.05\n",
      "Episode length: 87.60 +/- 18.05\n",
      "Eval num_timesteps=593920, episode_reward=-87.30 +/- 31.58\n",
      "Episode length: 88.30 +/- 31.58\n",
      "Eval num_timesteps=593920, episode_reward=-78.40 +/- 8.70\n",
      "Episode length: 79.40 +/- 8.70\n",
      "Eval num_timesteps=593920, episode_reward=-82.10 +/- 8.18\n",
      "Episode length: 83.10 +/- 8.18\n",
      "Eval num_timesteps=593920, episode_reward=-79.90 +/- 4.35\n",
      "Episode length: 80.90 +/- 4.35\n",
      "Eval num_timesteps=593920, episode_reward=-92.60 +/- 45.14\n",
      "Episode length: 93.60 +/- 45.14\n",
      "Eval num_timesteps=593920, episode_reward=-75.50 +/- 6.14\n",
      "Episode length: 76.50 +/- 6.14\n",
      "Eval num_timesteps=593920, episode_reward=-84.90 +/- 23.64\n",
      "Episode length: 85.90 +/- 23.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00024 |       0.00000 |      32.99464 |       0.00021 |       0.17476\n",
      "     -0.00124 |       0.00000 |      31.76269 |       0.00026 |       0.17722\n",
      "     -0.00176 |       0.00000 |      31.19997 |       0.00038 |       0.17567\n",
      "     -0.00165 |       0.00000 |      30.84669 |       0.00032 |       0.17654\n",
      "     -0.00186 |       0.00000 |      30.62308 |       0.00044 |       0.17668\n",
      "     -0.00187 |       0.00000 |      30.36001 |       0.00038 |       0.17563\n",
      "     -0.00210 |       0.00000 |      30.14910 |       0.00036 |       0.17567\n",
      "     -0.00228 |       0.00000 |      30.00276 |       0.00041 |       0.17542\n",
      "     -0.00232 |       0.00000 |      29.91249 |       0.00034 |       0.17601\n",
      "     -0.00246 |       0.00000 |      29.72748 |       0.00038 |       0.17580\n",
      "Evaluating losses...\n",
      "     -0.00262 |       0.00000 |      29.52137 |       0.00059 |       0.17411\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.4          |\n",
      "| EpRewMean       | -91.4         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5494          |\n",
      "| TimeElapsed     | 1.34e+03      |\n",
      "| TimestepsSoFar  | 598016        |\n",
      "| ev_tdlam_before | 0.875         |\n",
      "| loss_ent        | 0.17410688    |\n",
      "| loss_kl         | 0.00059336144 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0026248172 |\n",
      "| loss_vf_loss    | 29.52137      |\n",
      "-----------------------------------\n",
      "********** Iteration 146 ************\n",
      "Eval num_timesteps=598016, episode_reward=-82.10 +/- 8.98\n",
      "Episode length: 83.10 +/- 8.98\n",
      "Eval num_timesteps=598016, episode_reward=-81.50 +/- 3.88\n",
      "Episode length: 82.50 +/- 3.88\n",
      "Eval num_timesteps=598016, episode_reward=-118.10 +/- 127.59\n",
      "Episode length: 119.00 +/- 127.29\n",
      "Eval num_timesteps=598016, episode_reward=-76.10 +/- 6.70\n",
      "Episode length: 77.10 +/- 6.70\n",
      "Eval num_timesteps=598016, episode_reward=-97.70 +/- 49.24\n",
      "Episode length: 98.70 +/- 49.24\n",
      "Eval num_timesteps=598016, episode_reward=-75.00 +/- 8.63\n",
      "Episode length: 76.00 +/- 8.63\n",
      "Eval num_timesteps=598016, episode_reward=-79.70 +/- 8.15\n",
      "Episode length: 80.70 +/- 8.15\n",
      "Eval num_timesteps=598016, episode_reward=-78.90 +/- 5.41\n",
      "Episode length: 79.90 +/- 5.41\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -5.89e-05 |       0.00000 |      37.43129 |       0.00016 |       0.18600\n",
      "     -0.00102 |       0.00000 |      36.54362 |       0.00021 |       0.18669\n",
      "     -0.00130 |       0.00000 |      35.90348 |       0.00023 |       0.18670\n",
      "     -0.00141 |       0.00000 |      35.62313 |       0.00026 |       0.18702\n",
      "     -0.00178 |       0.00000 |      35.24888 |       0.00030 |       0.18583\n",
      "     -0.00178 |       0.00000 |      34.97183 |       0.00037 |       0.18636\n",
      "     -0.00216 |       0.00000 |      34.68827 |       0.00037 |       0.18657\n",
      "     -0.00216 |       0.00000 |      34.50981 |       0.00040 |       0.18730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00240 |       0.00000 |      34.29575 |       0.00032 |       0.18694\n",
      "     -0.00260 |       0.00000 |      34.16397 |       0.00042 |       0.18592\n",
      "Evaluating losses...\n",
      "     -0.00300 |       0.00000 |      33.85868 |       0.00045 |       0.18581\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.8          |\n",
      "| EpRewMean       | -89.8         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5539          |\n",
      "| TimeElapsed     | 1.34e+03      |\n",
      "| TimestepsSoFar  | 602112        |\n",
      "| ev_tdlam_before | 0.856         |\n",
      "| loss_ent        | 0.18580684    |\n",
      "| loss_kl         | 0.0004479684  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0030022275 |\n",
      "| loss_vf_loss    | 33.858677     |\n",
      "-----------------------------------\n",
      "********** Iteration 147 ************\n",
      "Eval num_timesteps=602112, episode_reward=-102.30 +/- 49.66\n",
      "Episode length: 103.30 +/- 49.66\n",
      "Eval num_timesteps=602112, episode_reward=-82.60 +/- 14.02\n",
      "Episode length: 83.60 +/- 14.02\n",
      "Eval num_timesteps=602112, episode_reward=-81.70 +/- 7.46\n",
      "Episode length: 82.70 +/- 7.46\n",
      "Eval num_timesteps=602112, episode_reward=-79.30 +/- 6.80\n",
      "Episode length: 80.30 +/- 6.80\n",
      "Eval num_timesteps=602112, episode_reward=-87.00 +/- 23.44\n",
      "Episode length: 88.00 +/- 23.44\n",
      "Eval num_timesteps=602112, episode_reward=-78.10 +/- 9.52\n",
      "Episode length: 79.10 +/- 9.52\n",
      "Eval num_timesteps=602112, episode_reward=-88.00 +/- 27.76\n",
      "Episode length: 89.00 +/- 27.76\n",
      "Eval num_timesteps=602112, episode_reward=-85.10 +/- 12.63\n",
      "Episode length: 86.10 +/- 12.63\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00015 |       0.00000 |      32.05933 |       0.00012 |       0.18902\n",
      "     -0.00074 |       0.00000 |      31.16885 |       0.00026 |       0.18715\n",
      "     -0.00106 |       0.00000 |      30.55325 |       0.00023 |       0.18642\n",
      "     -0.00147 |       0.00000 |      30.11884 |       0.00027 |       0.18615\n",
      "     -0.00158 |       0.00000 |      29.71065 |       0.00025 |       0.18690\n",
      "     -0.00169 |       0.00000 |      29.47305 |       0.00024 |       0.18765\n",
      "     -0.00182 |       0.00000 |      29.14312 |       0.00024 |       0.18737\n",
      "     -0.00187 |       0.00000 |      29.00349 |       0.00030 |       0.18733\n",
      "     -0.00202 |       0.00000 |      28.84898 |       0.00028 |       0.18644\n",
      "     -0.00222 |       0.00000 |      28.63297 |       0.00028 |       0.18782\n",
      "Evaluating losses...\n",
      "     -0.00256 |       0.00000 |      28.37250 |       0.00031 |       0.18660\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90            |\n",
      "| EpRewMean       | -89           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5584          |\n",
      "| TimeElapsed     | 1.35e+03      |\n",
      "| TimestepsSoFar  | 606208        |\n",
      "| ev_tdlam_before | 0.877         |\n",
      "| loss_ent        | 0.18659963    |\n",
      "| loss_kl         | 0.00031273798 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025593578 |\n",
      "| loss_vf_loss    | 28.372501     |\n",
      "-----------------------------------\n",
      "********** Iteration 148 ************\n",
      "Eval num_timesteps=606208, episode_reward=-79.90 +/- 8.48\n",
      "Episode length: 80.90 +/- 8.48\n",
      "Eval num_timesteps=606208, episode_reward=-79.00 +/- 11.73\n",
      "Episode length: 80.00 +/- 11.73\n",
      "Eval num_timesteps=606208, episode_reward=-79.20 +/- 8.73\n",
      "Episode length: 80.20 +/- 8.73\n",
      "Eval num_timesteps=606208, episode_reward=-82.30 +/- 4.54\n",
      "Episode length: 83.30 +/- 4.54\n",
      "Eval num_timesteps=606208, episode_reward=-77.10 +/- 9.09\n",
      "Episode length: 78.10 +/- 9.09\n",
      "Eval num_timesteps=606208, episode_reward=-77.00 +/- 6.51\n",
      "Episode length: 78.00 +/- 6.51\n",
      "Eval num_timesteps=606208, episode_reward=-77.70 +/- 7.86\n",
      "Episode length: 78.70 +/- 7.86\n",
      "Eval num_timesteps=606208, episode_reward=-80.30 +/- 8.51\n",
      "Episode length: 81.30 +/- 8.51\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00029 |       0.00000 |      32.22734 |       0.00014 |       0.17540\n",
      "     -0.00059 |       0.00000 |      30.85832 |       0.00028 |       0.17688\n",
      "     -0.00097 |       0.00000 |      30.17762 |       0.00021 |       0.17560\n",
      "     -0.00112 |       0.00000 |      29.53171 |       0.00029 |       0.17516\n",
      "     -0.00153 |       0.00000 |      29.13923 |       0.00030 |       0.17546\n",
      "     -0.00138 |       0.00000 |      28.79287 |       0.00035 |       0.17394\n",
      "     -0.00183 |       0.00000 |      28.60575 |       0.00034 |       0.17439\n",
      "     -0.00187 |       0.00000 |      28.38722 |       0.00037 |       0.17374\n",
      "     -0.00193 |       0.00000 |      28.19073 |       0.00040 |       0.17292\n",
      "     -0.00196 |       0.00000 |      28.06295 |       0.00041 |       0.17325\n",
      "Evaluating losses...\n",
      "     -0.00250 |       0.00000 |      27.79008 |       0.00036 |       0.17297\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.3          |\n",
      "| EpRewMean       | -91.3         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5629          |\n",
      "| TimeElapsed     | 1.36e+03      |\n",
      "| TimestepsSoFar  | 610304        |\n",
      "| ev_tdlam_before | 0.877         |\n",
      "| loss_ent        | 0.17296639    |\n",
      "| loss_kl         | 0.0003564002  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025048521 |\n",
      "| loss_vf_loss    | 27.790083     |\n",
      "-----------------------------------\n",
      "********** Iteration 149 ************\n",
      "Eval num_timesteps=610304, episode_reward=-76.00 +/- 3.97\n",
      "Episode length: 77.00 +/- 3.97\n",
      "Eval num_timesteps=610304, episode_reward=-80.90 +/- 8.85\n",
      "Episode length: 81.90 +/- 8.85\n",
      "Eval num_timesteps=610304, episode_reward=-79.70 +/- 4.96\n",
      "Episode length: 80.70 +/- 4.96\n",
      "Eval num_timesteps=610304, episode_reward=-79.80 +/- 8.16\n",
      "Episode length: 80.80 +/- 8.16\n",
      "Eval num_timesteps=610304, episode_reward=-92.20 +/- 42.43\n",
      "Episode length: 93.20 +/- 42.43\n",
      "Eval num_timesteps=610304, episode_reward=-83.00 +/- 6.63\n",
      "Episode length: 84.00 +/- 6.63\n",
      "Eval num_timesteps=610304, episode_reward=-76.60 +/- 8.19\n",
      "Episode length: 77.60 +/- 8.19\n",
      "Eval num_timesteps=610304, episode_reward=-79.00 +/- 11.59\n",
      "Episode length: 80.00 +/- 11.59\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00027 |       0.00000 |      30.08258 |       0.00011 |       0.17646\n",
      "     -0.00019 |       0.00000 |      29.34573 |       0.00026 |       0.17973\n",
      "     -0.00039 |       0.00000 |      28.98292 |       0.00015 |       0.17691\n",
      "     -0.00093 |       0.00000 |      28.72323 |       0.00024 |       0.17677\n",
      "     -0.00105 |       0.00000 |      28.48535 |       0.00025 |       0.17759\n",
      "     -0.00120 |       0.00000 |      28.21170 |       0.00021 |       0.17717\n",
      "     -0.00134 |       0.00000 |      28.04424 |       0.00030 |       0.17715\n",
      "     -0.00139 |       0.00000 |      27.86817 |       0.00024 |       0.17694\n",
      "     -0.00148 |       0.00000 |      27.75137 |       0.00023 |       0.17754\n",
      "     -0.00114 |       0.00000 |      27.67070 |       0.00026 |       0.17661\n",
      "Evaluating losses...\n",
      "     -0.00159 |       0.00000 |      27.39180 |       0.00033 |       0.17710\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.7          |\n",
      "| EpRewMean       | -88.7         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5675          |\n",
      "| TimeElapsed     | 1.36e+03      |\n",
      "| TimestepsSoFar  | 614400        |\n",
      "| ev_tdlam_before | 0.888         |\n",
      "| loss_ent        | 0.177104      |\n",
      "| loss_kl         | 0.00032592597 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015889133 |\n",
      "| loss_vf_loss    | 27.391804     |\n",
      "-----------------------------------\n",
      "********** Iteration 150 ************\n",
      "Eval num_timesteps=614400, episode_reward=-76.80 +/- 4.45\n",
      "Episode length: 77.80 +/- 4.45\n",
      "Eval num_timesteps=614400, episode_reward=-123.00 +/- 126.01\n",
      "Episode length: 123.90 +/- 125.71\n",
      "Eval num_timesteps=614400, episode_reward=-83.60 +/- 9.05\n",
      "Episode length: 84.60 +/- 9.05\n",
      "Eval num_timesteps=614400, episode_reward=-80.00 +/- 6.94\n",
      "Episode length: 81.00 +/- 6.94\n",
      "Eval num_timesteps=614400, episode_reward=-78.50 +/- 6.95\n",
      "Episode length: 79.50 +/- 6.95\n",
      "Eval num_timesteps=614400, episode_reward=-80.20 +/- 7.18\n",
      "Episode length: 81.20 +/- 7.18\n",
      "Eval num_timesteps=614400, episode_reward=-88.60 +/- 13.71\n",
      "Episode length: 89.60 +/- 13.71\n",
      "Eval num_timesteps=614400, episode_reward=-92.00 +/- 28.79\n",
      "Episode length: 93.00 +/- 28.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00018 |       0.00000 |      33.28110 |       0.00011 |       0.17152\n",
      "     -0.00076 |       0.00000 |      32.61338 |       0.00021 |       0.17056\n",
      "     -0.00105 |       0.00000 |      32.24359 |       0.00027 |       0.17234\n",
      "     -0.00125 |       0.00000 |      31.95699 |       0.00021 |       0.17153\n",
      "     -0.00136 |       0.00000 |      31.71258 |       0.00022 |       0.17266\n",
      "     -0.00173 |       0.00000 |      31.56159 |       0.00027 |       0.17258\n",
      "     -0.00168 |       0.00000 |      31.41045 |       0.00026 |       0.17270\n",
      "     -0.00172 |       0.00000 |      31.23510 |       0.00029 |       0.17283\n",
      "     -0.00188 |       0.00000 |      31.05143 |       0.00031 |       0.17312\n",
      "     -0.00195 |       0.00000 |      30.97242 |       0.00029 |       0.17309\n",
      "Evaluating losses...\n",
      "     -0.00244 |       0.00000 |      30.76859 |       0.00033 |       0.17387\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.6          |\n",
      "| EpRewMean       | -87.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5721          |\n",
      "| TimeElapsed     | 1.37e+03      |\n",
      "| TimestepsSoFar  | 618496        |\n",
      "| ev_tdlam_before | 0.879         |\n",
      "| loss_ent        | 0.17387       |\n",
      "| loss_kl         | 0.00033155165 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0024439464 |\n",
      "| loss_vf_loss    | 30.768595     |\n",
      "-----------------------------------\n",
      "********** Iteration 151 ************\n",
      "Eval num_timesteps=618496, episode_reward=-78.70 +/- 6.40\n",
      "Episode length: 79.70 +/- 6.40\n",
      "Eval num_timesteps=618496, episode_reward=-78.60 +/- 5.78\n",
      "Episode length: 79.60 +/- 5.78\n",
      "Eval num_timesteps=618496, episode_reward=-79.60 +/- 8.27\n",
      "Episode length: 80.60 +/- 8.27\n",
      "Eval num_timesteps=618496, episode_reward=-75.50 +/- 6.28\n",
      "Episode length: 76.50 +/- 6.28\n",
      "Eval num_timesteps=618496, episode_reward=-79.50 +/- 6.96\n",
      "Episode length: 80.50 +/- 6.96\n",
      "Eval num_timesteps=618496, episode_reward=-77.40 +/- 11.26\n",
      "Episode length: 78.40 +/- 11.26\n",
      "Eval num_timesteps=618496, episode_reward=-82.00 +/- 6.00\n",
      "Episode length: 83.00 +/- 6.00\n",
      "Eval num_timesteps=618496, episode_reward=-77.90 +/- 12.17\n",
      "Episode length: 78.90 +/- 12.17\n",
      "Eval num_timesteps=618496, episode_reward=-82.90 +/- 18.19\n",
      "Episode length: 83.90 +/- 18.19\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.14e-05 |       0.00000 |      29.47476 |       0.00013 |       0.17024\n",
      "     -0.00063 |       0.00000 |      28.54475 |       0.00028 |       0.16886\n",
      "     -0.00079 |       0.00000 |      28.04942 |       0.00026 |       0.16985\n",
      "     -0.00133 |       0.00000 |      27.69294 |       0.00028 |       0.17010\n",
      "     -0.00148 |       0.00000 |      27.49760 |       0.00037 |       0.16869\n",
      "     -0.00177 |       0.00000 |      27.34245 |       0.00031 |       0.16981\n",
      "     -0.00201 |       0.00000 |      27.12593 |       0.00037 |       0.17038\n",
      "     -0.00203 |       0.00000 |      26.94884 |       0.00038 |       0.17003\n",
      "     -0.00223 |       0.00000 |      26.82020 |       0.00042 |       0.16924\n",
      "     -0.00231 |       0.00000 |      26.67617 |       0.00043 |       0.17040\n",
      "Evaluating losses...\n",
      "     -0.00273 |       0.00000 |      26.49771 |       0.00042 |       0.17014\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.8          |\n",
      "| EpRewMean       | -89.8         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 5765          |\n",
      "| TimeElapsed     | 1.38e+03      |\n",
      "| TimestepsSoFar  | 622592        |\n",
      "| ev_tdlam_before | 0.887         |\n",
      "| loss_ent        | 0.17013742    |\n",
      "| loss_kl         | 0.00042319047 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0027283663 |\n",
      "| loss_vf_loss    | 26.497715     |\n",
      "-----------------------------------\n",
      "********** Iteration 152 ************\n",
      "Eval num_timesteps=622592, episode_reward=-78.80 +/- 11.21\n",
      "Episode length: 79.80 +/- 11.21\n",
      "Eval num_timesteps=622592, episode_reward=-83.80 +/- 20.22\n",
      "Episode length: 84.80 +/- 20.22\n",
      "Eval num_timesteps=622592, episode_reward=-85.90 +/- 20.77\n",
      "Episode length: 86.90 +/- 20.77\n",
      "Eval num_timesteps=622592, episode_reward=-83.10 +/- 15.71\n",
      "Episode length: 84.10 +/- 15.71\n",
      "Eval num_timesteps=622592, episode_reward=-94.80 +/- 46.28\n",
      "Episode length: 95.80 +/- 46.28\n",
      "Eval num_timesteps=622592, episode_reward=-82.70 +/- 10.08\n",
      "Episode length: 83.70 +/- 10.08\n",
      "Eval num_timesteps=622592, episode_reward=-76.80 +/- 6.37\n",
      "Episode length: 77.80 +/- 6.37\n",
      "Eval num_timesteps=622592, episode_reward=-78.10 +/- 7.82\n",
      "Episode length: 79.10 +/- 7.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00023 |       0.00000 |      29.02593 |       0.00015 |       0.17987\n",
      "     -0.00056 |       0.00000 |      27.48776 |       0.00028 |       0.18038\n",
      "     -0.00111 |       0.00000 |      26.77685 |       0.00033 |       0.17948\n",
      "     -0.00123 |       0.00000 |      26.35473 |       0.00035 |       0.18149\n",
      "     -0.00172 |       0.00000 |      25.99990 |       0.00036 |       0.18072\n",
      "     -0.00192 |       0.00000 |      25.77224 |       0.00041 |       0.18248\n",
      "     -0.00215 |       0.00000 |      25.57024 |       0.00050 |       0.18273\n",
      "     -0.00217 |       0.00000 |      25.37851 |       0.00055 |       0.18237\n",
      "     -0.00260 |       0.00000 |      25.22849 |       0.00055 |       0.18271\n",
      "     -0.00276 |       0.00000 |      25.07905 |       0.00068 |       0.18411\n",
      "Evaluating losses...\n",
      "     -0.00334 |       0.00000 |      24.87511 |       0.00070 |       0.18322\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.3          |\n",
      "| EpRewMean       | -91.3         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 5809          |\n",
      "| TimeElapsed     | 1.38e+03      |\n",
      "| TimestepsSoFar  | 626688        |\n",
      "| ev_tdlam_before | 0.891         |\n",
      "| loss_ent        | 0.1832211     |\n",
      "| loss_kl         | 0.000695108   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0033415898 |\n",
      "| loss_vf_loss    | 24.875114     |\n",
      "-----------------------------------\n",
      "********** Iteration 153 ************\n",
      "Eval num_timesteps=626688, episode_reward=-80.70 +/- 7.21\n",
      "Episode length: 81.70 +/- 7.21\n",
      "Eval num_timesteps=626688, episode_reward=-78.60 +/- 5.85\n",
      "Episode length: 79.60 +/- 5.85\n",
      "Eval num_timesteps=626688, episode_reward=-89.20 +/- 22.65\n",
      "Episode length: 90.20 +/- 22.65\n",
      "Eval num_timesteps=626688, episode_reward=-85.60 +/- 15.73\n",
      "Episode length: 86.60 +/- 15.73\n",
      "Eval num_timesteps=626688, episode_reward=-80.70 +/- 11.60\n",
      "Episode length: 81.70 +/- 11.60\n",
      "Eval num_timesteps=626688, episode_reward=-88.10 +/- 19.37\n",
      "Episode length: 89.10 +/- 19.37\n",
      "Eval num_timesteps=626688, episode_reward=-77.80 +/- 9.28\n",
      "Episode length: 78.80 +/- 9.28\n",
      "Eval num_timesteps=626688, episode_reward=-80.90 +/- 5.28\n",
      "Episode length: 81.90 +/- 5.28\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00022 |       0.00000 |      33.75634 |       0.00017 |       0.16730\n",
      "     -0.00077 |       0.00000 |      33.07250 |       0.00029 |       0.16917\n",
      "     -0.00132 |       0.00000 |      32.61229 |       0.00046 |       0.16805\n",
      "     -0.00151 |       0.00000 |      32.27353 |       0.00043 |       0.16792\n",
      "     -0.00179 |       0.00000 |      32.00098 |       0.00047 |       0.16877\n",
      "     -0.00170 |       0.00000 |      31.75313 |       0.00042 |       0.16797\n",
      "     -0.00191 |       0.00000 |      31.58041 |       0.00038 |       0.16884\n",
      "     -0.00198 |       0.00000 |      31.41566 |       0.00046 |       0.16885\n",
      "     -0.00218 |       0.00000 |      31.15440 |       0.00050 |       0.16894\n",
      "     -0.00226 |       0.00000 |      31.01750 |       0.00038 |       0.16872\n",
      "Evaluating losses...\n",
      "     -0.00258 |       0.00000 |      30.80691 |       0.00049 |       0.16952\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.6          |\n",
      "| EpRewMean       | -90.6         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5854          |\n",
      "| TimeElapsed     | 1.39e+03      |\n",
      "| TimestepsSoFar  | 630784        |\n",
      "| ev_tdlam_before | 0.874         |\n",
      "| loss_ent        | 0.1695229     |\n",
      "| loss_kl         | 0.0004887404  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025795412 |\n",
      "| loss_vf_loss    | 30.80691      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 154 ************\n",
      "Eval num_timesteps=630784, episode_reward=-81.80 +/- 16.61\n",
      "Episode length: 82.80 +/- 16.61\n",
      "Eval num_timesteps=630784, episode_reward=-80.10 +/- 7.75\n",
      "Episode length: 81.10 +/- 7.75\n",
      "Eval num_timesteps=630784, episode_reward=-86.60 +/- 17.25\n",
      "Episode length: 87.60 +/- 17.25\n",
      "Eval num_timesteps=630784, episode_reward=-87.80 +/- 21.94\n",
      "Episode length: 88.80 +/- 21.94\n",
      "Eval num_timesteps=630784, episode_reward=-76.80 +/- 6.90\n",
      "Episode length: 77.80 +/- 6.90\n",
      "Eval num_timesteps=630784, episode_reward=-85.60 +/- 22.90\n",
      "Episode length: 86.60 +/- 22.90\n",
      "Eval num_timesteps=630784, episode_reward=-123.20 +/- 125.70\n",
      "Episode length: 124.10 +/- 125.40\n",
      "Eval num_timesteps=630784, episode_reward=-76.90 +/- 7.37\n",
      "Episode length: 77.90 +/- 7.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00013 |       0.00000 |      28.85007 |      7.80e-05 |       0.18624\n",
      "     -0.00069 |       0.00000 |      28.44521 |       0.00019 |       0.18552\n",
      "     -0.00073 |       0.00000 |      28.29776 |       0.00022 |       0.18605\n",
      "     -0.00132 |       0.00000 |      28.07697 |       0.00025 |       0.18787\n",
      "     -0.00101 |       0.00000 |      27.91450 |       0.00029 |       0.18775\n",
      "     -0.00133 |       0.00000 |      27.81243 |       0.00023 |       0.18831\n",
      "     -0.00157 |       0.00000 |      27.69245 |       0.00026 |       0.18838\n",
      "     -0.00157 |       0.00000 |      27.63431 |       0.00024 |       0.18849\n",
      "     -0.00166 |       0.00000 |      27.52988 |       0.00027 |       0.19006\n",
      "     -0.00164 |       0.00000 |      27.44798 |       0.00028 |       0.18850\n",
      "Evaluating losses...\n",
      "     -0.00211 |       0.00000 |      27.37886 |       0.00024 |       0.18817\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.1          |\n",
      "| EpRewMean       | -90.1         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5899          |\n",
      "| TimeElapsed     | 1.4e+03       |\n",
      "| TimestepsSoFar  | 634880        |\n",
      "| ev_tdlam_before | 0.894         |\n",
      "| loss_ent        | 0.18816593    |\n",
      "| loss_kl         | 0.00023571878 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002112192  |\n",
      "| loss_vf_loss    | 27.378864     |\n",
      "-----------------------------------\n",
      "********** Iteration 155 ************\n",
      "Eval num_timesteps=634880, episode_reward=-89.30 +/- 42.09\n",
      "Episode length: 90.30 +/- 42.09\n",
      "Eval num_timesteps=634880, episode_reward=-82.70 +/- 9.88\n",
      "Episode length: 83.70 +/- 9.88\n",
      "Eval num_timesteps=634880, episode_reward=-77.80 +/- 5.10\n",
      "Episode length: 78.80 +/- 5.10\n",
      "Eval num_timesteps=634880, episode_reward=-92.80 +/- 25.27\n",
      "Episode length: 93.80 +/- 25.27\n",
      "Eval num_timesteps=634880, episode_reward=-90.50 +/- 29.67\n",
      "Episode length: 91.50 +/- 29.67\n",
      "Eval num_timesteps=634880, episode_reward=-85.70 +/- 20.14\n",
      "Episode length: 86.70 +/- 20.14\n",
      "Eval num_timesteps=634880, episode_reward=-76.10 +/- 6.64\n",
      "Episode length: 77.10 +/- 6.64\n",
      "Eval num_timesteps=634880, episode_reward=-81.90 +/- 13.25\n",
      "Episode length: 82.90 +/- 13.25\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      35.51170 |      8.35e-05 |       0.17308\n",
      "     -0.00106 |       0.00000 |      35.01335 |       0.00028 |       0.17181\n",
      "     -0.00136 |       0.00000 |      34.63087 |       0.00049 |       0.17304\n",
      "     -0.00162 |       0.00000 |      34.39020 |       0.00044 |       0.17327\n",
      "     -0.00184 |       0.00000 |      34.16081 |       0.00040 |       0.17396\n",
      "     -0.00192 |       0.00000 |      33.97703 |       0.00047 |       0.17418\n",
      "     -0.00193 |       0.00000 |      33.83185 |       0.00046 |       0.17574\n",
      "     -0.00217 |       0.00000 |      33.70171 |       0.00050 |       0.17537\n",
      "     -0.00211 |       0.00000 |      33.58647 |       0.00054 |       0.17531\n",
      "     -0.00239 |       0.00000 |      33.50042 |       0.00055 |       0.17470\n",
      "Evaluating losses...\n",
      "     -0.00258 |       0.00000 |      33.30489 |       0.00050 |       0.17630\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.5          |\n",
      "| EpRewMean       | -89.5         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5944          |\n",
      "| TimeElapsed     | 1.4e+03       |\n",
      "| TimestepsSoFar  | 638976        |\n",
      "| ev_tdlam_before | 0.866         |\n",
      "| loss_ent        | 0.17630304    |\n",
      "| loss_kl         | 0.0004960749  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025836935 |\n",
      "| loss_vf_loss    | 33.304893     |\n",
      "-----------------------------------\n",
      "********** Iteration 156 ************\n",
      "Eval num_timesteps=638976, episode_reward=-95.60 +/- 38.97\n",
      "Episode length: 96.60 +/- 38.97\n",
      "Eval num_timesteps=638976, episode_reward=-77.10 +/- 11.48\n",
      "Episode length: 78.10 +/- 11.48\n",
      "Eval num_timesteps=638976, episode_reward=-76.50 +/- 6.02\n",
      "Episode length: 77.50 +/- 6.02\n",
      "Eval num_timesteps=638976, episode_reward=-78.20 +/- 9.46\n",
      "Episode length: 79.20 +/- 9.46\n",
      "Eval num_timesteps=638976, episode_reward=-78.60 +/- 5.62\n",
      "Episode length: 79.60 +/- 5.62\n",
      "Eval num_timesteps=638976, episode_reward=-77.90 +/- 4.32\n",
      "Episode length: 78.90 +/- 4.32\n",
      "Eval num_timesteps=638976, episode_reward=-82.40 +/- 4.48\n",
      "Episode length: 83.40 +/- 4.48\n",
      "Eval num_timesteps=638976, episode_reward=-82.20 +/- 10.93\n",
      "Episode length: 83.20 +/- 10.93\n",
      "Eval num_timesteps=638976, episode_reward=-81.80 +/- 10.02\n",
      "Episode length: 82.80 +/- 10.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00022 |       0.00000 |      36.60770 |      7.39e-05 |       0.16951\n",
      "     -0.00055 |       0.00000 |      35.84581 |       0.00016 |       0.16980\n",
      "     -0.00072 |       0.00000 |      35.48551 |       0.00018 |       0.16946\n",
      "     -0.00094 |       0.00000 |      35.17738 |       0.00021 |       0.17166\n",
      "     -0.00106 |       0.00000 |      34.90949 |       0.00019 |       0.17109\n",
      "     -0.00128 |       0.00000 |      34.71522 |       0.00017 |       0.17175\n",
      "     -0.00129 |       0.00000 |      34.54834 |       0.00020 |       0.17140\n",
      "     -0.00144 |       0.00000 |      34.38218 |       0.00027 |       0.17203\n",
      "     -0.00156 |       0.00000 |      34.27917 |       0.00023 |       0.17266\n",
      "     -0.00157 |       0.00000 |      34.16217 |       0.00027 |       0.17240\n",
      "Evaluating losses...\n",
      "     -0.00183 |       0.00000 |      33.88772 |       0.00022 |       0.17269\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.2          |\n",
      "| EpRewMean       | -90.2         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 5988          |\n",
      "| TimeElapsed     | 1.41e+03      |\n",
      "| TimestepsSoFar  | 643072        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.17269433    |\n",
      "| loss_kl         | 0.00022314355 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018326063 |\n",
      "| loss_vf_loss    | 33.88772      |\n",
      "-----------------------------------\n",
      "********** Iteration 157 ************\n",
      "Eval num_timesteps=643072, episode_reward=-81.20 +/- 26.36\n",
      "Episode length: 82.20 +/- 26.36\n",
      "Eval num_timesteps=643072, episode_reward=-86.10 +/- 15.85\n",
      "Episode length: 87.10 +/- 15.85\n",
      "Eval num_timesteps=643072, episode_reward=-74.30 +/- 7.03\n",
      "Episode length: 75.30 +/- 7.03\n",
      "Eval num_timesteps=643072, episode_reward=-79.10 +/- 10.62\n",
      "Episode length: 80.10 +/- 10.62\n",
      "Eval num_timesteps=643072, episode_reward=-74.40 +/- 6.79\n",
      "Episode length: 75.40 +/- 6.79\n",
      "Eval num_timesteps=643072, episode_reward=-80.30 +/- 9.40\n",
      "Episode length: 81.30 +/- 9.40\n",
      "Eval num_timesteps=643072, episode_reward=-85.40 +/- 17.22\n",
      "Episode length: 86.40 +/- 17.22\n",
      "Eval num_timesteps=643072, episode_reward=-78.00 +/- 6.40\n",
      "Episode length: 79.00 +/- 6.40\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00039 |       0.00000 |      22.71820 |      5.67e-05 |       0.18123\n",
      "     -0.00037 |       0.00000 |      21.86257 |       0.00017 |       0.18466\n",
      "     -0.00041 |       0.00000 |      21.44950 |       0.00013 |       0.18212\n",
      "     -0.00091 |       0.00000 |      21.26080 |       0.00013 |       0.18241\n",
      "     -0.00107 |       0.00000 |      21.10633 |       0.00016 |       0.18341\n",
      "     -0.00111 |       0.00000 |      20.95899 |       0.00018 |       0.18152\n",
      "     -0.00157 |       0.00000 |      20.85792 |       0.00017 |       0.18215\n",
      "     -0.00166 |       0.00000 |      20.76171 |       0.00017 |       0.18237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00166 |       0.00000 |      20.71074 |       0.00020 |       0.18185\n",
      "     -0.00195 |       0.00000 |      20.60930 |       0.00022 |       0.18274\n",
      "Evaluating losses...\n",
      "     -0.00199 |       0.00000 |      20.46766 |       0.00025 |       0.18387\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.5          |\n",
      "| EpRewMean       | -87.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6035          |\n",
      "| TimeElapsed     | 1.42e+03      |\n",
      "| TimestepsSoFar  | 647168        |\n",
      "| ev_tdlam_before | 0.917         |\n",
      "| loss_ent        | 0.18387341    |\n",
      "| loss_kl         | 0.00025282742 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0019917116 |\n",
      "| loss_vf_loss    | 20.467655     |\n",
      "-----------------------------------\n",
      "********** Iteration 158 ************\n",
      "Eval num_timesteps=647168, episode_reward=-74.90 +/- 6.80\n",
      "Episode length: 75.90 +/- 6.80\n",
      "Eval num_timesteps=647168, episode_reward=-78.00 +/- 8.10\n",
      "Episode length: 79.00 +/- 8.10\n",
      "Eval num_timesteps=647168, episode_reward=-82.60 +/- 14.54\n",
      "Episode length: 83.60 +/- 14.54\n",
      "Eval num_timesteps=647168, episode_reward=-79.00 +/- 10.37\n",
      "Episode length: 80.00 +/- 10.37\n",
      "Eval num_timesteps=647168, episode_reward=-86.90 +/- 19.36\n",
      "Episode length: 87.90 +/- 19.36\n",
      "Eval num_timesteps=647168, episode_reward=-78.30 +/- 6.33\n",
      "Episode length: 79.30 +/- 6.33\n",
      "Eval num_timesteps=647168, episode_reward=-80.00 +/- 6.96\n",
      "Episode length: 81.00 +/- 6.96\n",
      "Eval num_timesteps=647168, episode_reward=-76.20 +/- 6.37\n",
      "Episode length: 77.20 +/- 6.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00033 |       0.00000 |      33.77786 |       0.00020 |       0.20037\n",
      "     -0.00062 |       0.00000 |      30.77962 |       0.00035 |       0.20522\n",
      "     -0.00098 |       0.00000 |      29.94987 |       0.00032 |       0.20229\n",
      "     -0.00144 |       0.00000 |      29.52925 |       0.00051 |       0.20561\n",
      "     -0.00178 |       0.00000 |      29.21008 |       0.00043 |       0.20284\n",
      "     -0.00202 |       0.00000 |      28.97786 |       0.00047 |       0.20467\n",
      "     -0.00220 |       0.00000 |      28.79900 |       0.00041 |       0.20324\n",
      "     -0.00237 |       0.00000 |      28.64678 |       0.00057 |       0.20464\n",
      "     -0.00237 |       0.00000 |      28.47553 |       0.00044 |       0.20199\n",
      "     -0.00226 |       0.00000 |      28.41465 |       0.00053 |       0.20345\n",
      "Evaluating losses...\n",
      "     -0.00314 |       0.00000 |      28.14211 |       0.00056 |       0.20471\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.9          |\n",
      "| EpRewMean       | -89.9         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 6078          |\n",
      "| TimeElapsed     | 1.42e+03      |\n",
      "| TimestepsSoFar  | 651264        |\n",
      "| ev_tdlam_before | 0.856         |\n",
      "| loss_ent        | 0.20470731    |\n",
      "| loss_kl         | 0.00056150067 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0031407645 |\n",
      "| loss_vf_loss    | 28.142113     |\n",
      "-----------------------------------\n",
      "********** Iteration 159 ************\n",
      "Eval num_timesteps=651264, episode_reward=-76.60 +/- 9.04\n",
      "Episode length: 77.60 +/- 9.04\n",
      "Eval num_timesteps=651264, episode_reward=-81.60 +/- 11.69\n",
      "Episode length: 82.60 +/- 11.69\n",
      "Eval num_timesteps=651264, episode_reward=-73.70 +/- 7.73\n",
      "Episode length: 74.70 +/- 7.73\n",
      "Eval num_timesteps=651264, episode_reward=-78.40 +/- 7.09\n",
      "Episode length: 79.40 +/- 7.09\n",
      "Eval num_timesteps=651264, episode_reward=-79.60 +/- 8.49\n",
      "Episode length: 80.60 +/- 8.49\n",
      "Eval num_timesteps=651264, episode_reward=-78.80 +/- 10.16\n",
      "Episode length: 79.80 +/- 10.16\n",
      "Eval num_timesteps=651264, episode_reward=-85.50 +/- 11.56\n",
      "Episode length: 86.50 +/- 11.56\n",
      "Eval num_timesteps=651264, episode_reward=-84.80 +/- 13.65\n",
      "Episode length: 85.80 +/- 13.65\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00011 |       0.00000 |      31.03370 |       0.00014 |       0.20944\n",
      "     -0.00052 |       0.00000 |      28.82875 |       0.00016 |       0.20991\n",
      "     -0.00109 |       0.00000 |      28.47508 |       0.00015 |       0.21021\n",
      "     -0.00100 |       0.00000 |      28.25976 |       0.00017 |       0.21011\n",
      "     -0.00120 |       0.00000 |      27.97436 |       0.00017 |       0.21075\n",
      "     -0.00134 |       0.00000 |      27.87650 |       0.00022 |       0.21053\n",
      "     -0.00144 |       0.00000 |      27.69008 |       0.00017 |       0.21186\n",
      "     -0.00164 |       0.00000 |      27.55343 |       0.00019 |       0.21039\n",
      "     -0.00122 |       0.00000 |      27.45245 |       0.00020 |       0.21078\n",
      "     -0.00167 |       0.00000 |      27.33190 |       0.00020 |       0.21056\n",
      "Evaluating losses...\n",
      "     -0.00210 |       0.00000 |      27.16932 |       0.00017 |       0.21184\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.9          |\n",
      "| EpRewMean       | -89.9         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 6122          |\n",
      "| TimeElapsed     | 1.43e+03      |\n",
      "| TimestepsSoFar  | 655360        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.2118352     |\n",
      "| loss_kl         | 0.00017119414 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0021022754 |\n",
      "| loss_vf_loss    | 27.169323     |\n",
      "-----------------------------------\n",
      "********** Iteration 160 ************\n",
      "Eval num_timesteps=655360, episode_reward=-79.00 +/- 4.86\n",
      "Episode length: 80.00 +/- 4.86\n",
      "Eval num_timesteps=655360, episode_reward=-85.10 +/- 27.91\n",
      "Episode length: 86.10 +/- 27.91\n",
      "Eval num_timesteps=655360, episode_reward=-79.90 +/- 5.63\n",
      "Episode length: 80.90 +/- 5.63\n",
      "Eval num_timesteps=655360, episode_reward=-80.20 +/- 7.57\n",
      "Episode length: 81.20 +/- 7.57\n",
      "Eval num_timesteps=655360, episode_reward=-78.00 +/- 11.38\n",
      "Episode length: 79.00 +/- 11.38\n",
      "Eval num_timesteps=655360, episode_reward=-78.50 +/- 5.63\n",
      "Episode length: 79.50 +/- 5.63\n",
      "Eval num_timesteps=655360, episode_reward=-88.30 +/- 38.49\n",
      "Episode length: 89.30 +/- 38.49\n",
      "Eval num_timesteps=655360, episode_reward=-85.30 +/- 7.18\n",
      "Episode length: 86.30 +/- 7.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00020 |       0.00000 |      30.83135 |       0.00013 |       0.20438\n",
      "     -0.00090 |       0.00000 |      28.69241 |       0.00032 |       0.20579\n",
      "     -0.00154 |       0.00000 |      27.75393 |       0.00034 |       0.20566\n",
      "     -0.00175 |       0.00000 |      27.06954 |       0.00044 |       0.20545\n",
      "     -0.00177 |       0.00000 |      26.62876 |       0.00046 |       0.20423\n",
      "     -0.00219 |       0.00000 |      26.21366 |       0.00060 |       0.20592\n",
      "     -0.00295 |       0.00000 |      25.89038 |       0.00053 |       0.20591\n",
      "     -0.00307 |       0.00000 |      25.64326 |       0.00065 |       0.20609\n",
      "     -0.00320 |       0.00000 |      25.40610 |       0.00061 |       0.20618\n",
      "     -0.00326 |       0.00000 |      25.19408 |       0.00067 |       0.20689\n",
      "Evaluating losses...\n",
      "     -0.00398 |       0.00000 |      24.93034 |       0.00069 |       0.20755\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.8          |\n",
      "| EpRewMean       | -93.8         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 6165          |\n",
      "| TimeElapsed     | 1.44e+03      |\n",
      "| TimestepsSoFar  | 659456        |\n",
      "| ev_tdlam_before | 0.877         |\n",
      "| loss_ent        | 0.20755287    |\n",
      "| loss_kl         | 0.00068764976 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0039763255 |\n",
      "| loss_vf_loss    | 24.93034      |\n",
      "-----------------------------------\n",
      "********** Iteration 161 ************\n",
      "Eval num_timesteps=659456, episode_reward=-80.50 +/- 17.98\n",
      "Episode length: 81.50 +/- 17.98\n",
      "Eval num_timesteps=659456, episode_reward=-84.10 +/- 7.11\n",
      "Episode length: 85.10 +/- 7.11\n",
      "Eval num_timesteps=659456, episode_reward=-78.00 +/- 4.84\n",
      "Episode length: 79.00 +/- 4.84\n",
      "Eval num_timesteps=659456, episode_reward=-82.00 +/- 18.13\n",
      "Episode length: 83.00 +/- 18.13\n",
      "Eval num_timesteps=659456, episode_reward=-74.80 +/- 9.71\n",
      "Episode length: 75.80 +/- 9.71\n",
      "Eval num_timesteps=659456, episode_reward=-82.70 +/- 11.63\n",
      "Episode length: 83.70 +/- 11.63\n",
      "Eval num_timesteps=659456, episode_reward=-82.70 +/- 9.73\n",
      "Episode length: 83.70 +/- 9.73\n",
      "Eval num_timesteps=659456, episode_reward=-82.90 +/- 4.53\n",
      "Episode length: 83.90 +/- 4.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=659456, episode_reward=-85.90 +/- 25.20\n",
      "Episode length: 86.90 +/- 25.20\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      35.89163 |      6.79e-05 |       0.19521\n",
      "     -0.00058 |       0.00000 |      34.95934 |       0.00015 |       0.19713\n",
      "     -0.00096 |       0.00000 |      34.23967 |       0.00018 |       0.19684\n",
      "     -0.00110 |       0.00000 |      33.69006 |       0.00017 |       0.19682\n",
      "     -0.00139 |       0.00000 |      33.32582 |       0.00020 |       0.19705\n",
      "     -0.00157 |       0.00000 |      32.90348 |       0.00023 |       0.19732\n",
      "     -0.00172 |       0.00000 |      32.58852 |       0.00031 |       0.19760\n",
      "     -0.00207 |       0.00000 |      32.33592 |       0.00034 |       0.19912\n",
      "     -0.00208 |       0.00000 |      32.12019 |       0.00027 |       0.19669\n",
      "     -0.00207 |       0.00000 |      31.92309 |       0.00033 |       0.19919\n",
      "Evaluating losses...\n",
      "     -0.00255 |       0.00000 |      31.61018 |       0.00029 |       0.19788\n",
      "-----------------------------------\n",
      "| EpLenMean       | 96            |\n",
      "| EpRewMean       | -95           |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 6209          |\n",
      "| TimeElapsed     | 1.44e+03      |\n",
      "| TimestepsSoFar  | 663552        |\n",
      "| ev_tdlam_before | 0.858         |\n",
      "| loss_ent        | 0.1978832     |\n",
      "| loss_kl         | 0.00028537115 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.002549583  |\n",
      "| loss_vf_loss    | 31.61018      |\n",
      "-----------------------------------\n",
      "********** Iteration 162 ************\n",
      "Eval num_timesteps=663552, episode_reward=-77.80 +/- 10.82\n",
      "Episode length: 78.80 +/- 10.82\n",
      "Eval num_timesteps=663552, episode_reward=-80.50 +/- 12.18\n",
      "Episode length: 81.50 +/- 12.18\n",
      "Eval num_timesteps=663552, episode_reward=-79.20 +/- 8.40\n",
      "Episode length: 80.20 +/- 8.40\n",
      "Eval num_timesteps=663552, episode_reward=-82.90 +/- 8.79\n",
      "Episode length: 83.90 +/- 8.79\n",
      "Eval num_timesteps=663552, episode_reward=-80.10 +/- 11.67\n",
      "Episode length: 81.10 +/- 11.67\n",
      "Eval num_timesteps=663552, episode_reward=-80.60 +/- 6.65\n",
      "Episode length: 81.60 +/- 6.65\n",
      "Eval num_timesteps=663552, episode_reward=-77.60 +/- 3.80\n",
      "Episode length: 78.60 +/- 3.80\n",
      "Eval num_timesteps=663552, episode_reward=-87.00 +/- 17.87\n",
      "Episode length: 88.00 +/- 17.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00012 |       0.00000 |      35.31467 |      8.44e-05 |       0.20413\n",
      "     -0.00096 |       0.00000 |      34.55851 |       0.00018 |       0.20590\n",
      "     -0.00126 |       0.00000 |      34.18698 |       0.00025 |       0.20402\n",
      "     -0.00141 |       0.00000 |      33.75291 |       0.00025 |       0.20631\n",
      "     -0.00172 |       0.00000 |      33.54151 |       0.00022 |       0.20513\n",
      "     -0.00178 |       0.00000 |      33.32477 |       0.00027 |       0.20656\n",
      "     -0.00195 |       0.00000 |      33.17173 |       0.00023 |       0.20512\n",
      "     -0.00214 |       0.00000 |      33.00429 |       0.00028 |       0.20537\n",
      "     -0.00231 |       0.00000 |      32.84550 |       0.00031 |       0.20589\n",
      "     -0.00219 |       0.00000 |      32.72633 |       0.00033 |       0.20581\n",
      "Evaluating losses...\n",
      "     -0.00271 |       0.00000 |      32.47268 |       0.00028 |       0.20520\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.8          |\n",
      "| EpRewMean       | -90.8         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 6253          |\n",
      "| TimeElapsed     | 1.45e+03      |\n",
      "| TimestepsSoFar  | 667648        |\n",
      "| ev_tdlam_before | 0.861         |\n",
      "| loss_ent        | 0.20519651    |\n",
      "| loss_kl         | 0.0002751751  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0027056977 |\n",
      "| loss_vf_loss    | 32.472675     |\n",
      "-----------------------------------\n",
      "********** Iteration 163 ************\n",
      "Eval num_timesteps=667648, episode_reward=-80.80 +/- 11.15\n",
      "Episode length: 81.80 +/- 11.15\n",
      "Eval num_timesteps=667648, episode_reward=-79.10 +/- 8.02\n",
      "Episode length: 80.10 +/- 8.02\n",
      "Eval num_timesteps=667648, episode_reward=-77.10 +/- 11.42\n",
      "Episode length: 78.10 +/- 11.42\n",
      "Eval num_timesteps=667648, episode_reward=-76.50 +/- 7.47\n",
      "Episode length: 77.50 +/- 7.47\n",
      "Eval num_timesteps=667648, episode_reward=-77.50 +/- 10.02\n",
      "Episode length: 78.50 +/- 10.02\n",
      "Eval num_timesteps=667648, episode_reward=-124.30 +/- 63.90\n",
      "Episode length: 125.30 +/- 63.90\n",
      "Eval num_timesteps=667648, episode_reward=-79.40 +/- 15.41\n",
      "Episode length: 80.40 +/- 15.41\n",
      "Eval num_timesteps=667648, episode_reward=-83.10 +/- 8.84\n",
      "Episode length: 84.10 +/- 8.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.01e-06 |       0.00000 |      23.22422 |      5.96e-05 |       0.18421\n",
      "     -0.00059 |       0.00000 |      22.84769 |       0.00016 |       0.18278\n",
      "     -0.00094 |       0.00000 |      22.65561 |       0.00022 |       0.18370\n",
      "     -0.00107 |       0.00000 |      22.53253 |       0.00018 |       0.18369\n",
      "     -0.00112 |       0.00000 |      22.41060 |       0.00025 |       0.18383\n",
      "     -0.00145 |       0.00000 |      22.34067 |       0.00019 |       0.18406\n",
      "     -0.00154 |       0.00000 |      22.26485 |       0.00021 |       0.18381\n",
      "     -0.00149 |       0.00000 |      22.21918 |       0.00023 |       0.18345\n",
      "     -0.00177 |       0.00000 |      22.15157 |       0.00022 |       0.18364\n",
      "     -0.00181 |       0.00000 |      22.08434 |       0.00025 |       0.18376\n",
      "Evaluating losses...\n",
      "     -0.00207 |       0.00000 |      21.94890 |       0.00019 |       0.18357\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.1          |\n",
      "| EpRewMean       | -89.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6300          |\n",
      "| TimeElapsed     | 1.46e+03      |\n",
      "| TimestepsSoFar  | 671744        |\n",
      "| ev_tdlam_before | 0.914         |\n",
      "| loss_ent        | 0.18356965    |\n",
      "| loss_kl         | 0.00018963583 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0020664625 |\n",
      "| loss_vf_loss    | 21.948896     |\n",
      "-----------------------------------\n",
      "********** Iteration 164 ************\n",
      "Eval num_timesteps=671744, episode_reward=-77.20 +/- 6.00\n",
      "Episode length: 78.20 +/- 6.00\n",
      "Eval num_timesteps=671744, episode_reward=-84.20 +/- 9.89\n",
      "Episode length: 85.20 +/- 9.89\n",
      "Eval num_timesteps=671744, episode_reward=-79.40 +/- 5.54\n",
      "Episode length: 80.40 +/- 5.54\n",
      "Eval num_timesteps=671744, episode_reward=-78.80 +/- 6.31\n",
      "Episode length: 79.80 +/- 6.31\n",
      "Eval num_timesteps=671744, episode_reward=-78.50 +/- 16.09\n",
      "Episode length: 79.50 +/- 16.09\n",
      "Eval num_timesteps=671744, episode_reward=-80.00 +/- 7.71\n",
      "Episode length: 81.00 +/- 7.71\n",
      "Eval num_timesteps=671744, episode_reward=-85.90 +/- 13.73\n",
      "Episode length: 86.90 +/- 13.73\n",
      "Eval num_timesteps=671744, episode_reward=-78.90 +/- 10.93\n",
      "Episode length: 79.90 +/- 10.93\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.99e-06 |       0.00000 |      28.30770 |       0.00014 |       0.19467\n",
      "     -0.00036 |       0.00000 |      27.97165 |       0.00018 |       0.19400\n",
      "     -0.00069 |       0.00000 |      27.73493 |       0.00028 |       0.19498\n",
      "     -0.00091 |       0.00000 |      27.64051 |       0.00023 |       0.19627\n",
      "     -0.00116 |       0.00000 |      27.42675 |       0.00021 |       0.19728\n",
      "     -0.00125 |       0.00000 |      27.29823 |       0.00031 |       0.19566\n",
      "     -0.00118 |       0.00000 |      27.18234 |       0.00028 |       0.19707\n",
      "     -0.00148 |       0.00000 |      27.12930 |       0.00028 |       0.19644\n",
      "     -0.00163 |       0.00000 |      27.01085 |       0.00030 |       0.19753\n",
      "     -0.00182 |       0.00000 |      26.91981 |       0.00028 |       0.19742\n",
      "Evaluating losses...\n",
      "     -0.00204 |       0.00000 |      26.72966 |       0.00032 |       0.19793\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.5          |\n",
      "| EpRewMean       | -87.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6347          |\n",
      "| TimeElapsed     | 1.46e+03      |\n",
      "| TimestepsSoFar  | 675840        |\n",
      "| ev_tdlam_before | 0.898         |\n",
      "| loss_ent        | 0.19793375    |\n",
      "| loss_kl         | 0.00032007965 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0020391927 |\n",
      "| loss_vf_loss    | 26.729658     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 165 ************\n",
      "Eval num_timesteps=675840, episode_reward=-80.80 +/- 6.73\n",
      "Episode length: 81.80 +/- 6.73\n",
      "Eval num_timesteps=675840, episode_reward=-81.90 +/- 13.46\n",
      "Episode length: 82.90 +/- 13.46\n",
      "Eval num_timesteps=675840, episode_reward=-77.40 +/- 7.55\n",
      "Episode length: 78.40 +/- 7.55\n",
      "Eval num_timesteps=675840, episode_reward=-75.70 +/- 5.60\n",
      "Episode length: 76.70 +/- 5.60\n",
      "Eval num_timesteps=675840, episode_reward=-86.00 +/- 15.20\n",
      "Episode length: 87.00 +/- 15.20\n",
      "Eval num_timesteps=675840, episode_reward=-77.20 +/- 8.81\n",
      "Episode length: 78.20 +/- 8.81\n",
      "Eval num_timesteps=675840, episode_reward=-85.10 +/- 16.98\n",
      "Episode length: 86.10 +/- 16.98\n",
      "Eval num_timesteps=675840, episode_reward=-79.40 +/- 6.34\n",
      "Episode length: 80.40 +/- 6.34\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.74e-05 |       0.00000 |      26.28915 |       0.00016 |       0.19443\n",
      "     -0.00085 |       0.00000 |      25.83081 |       0.00028 |       0.19390\n",
      "     -0.00140 |       0.00000 |      25.57604 |       0.00031 |       0.19487\n",
      "     -0.00140 |       0.00000 |      25.42895 |       0.00032 |       0.19440\n",
      "     -0.00195 |       0.00000 |      25.27887 |       0.00034 |       0.19533\n",
      "     -0.00196 |       0.00000 |      25.20293 |       0.00034 |       0.19491\n",
      "     -0.00205 |       0.00000 |      25.06843 |       0.00042 |       0.19552\n",
      "     -0.00217 |       0.00000 |      25.00427 |       0.00041 |       0.19492\n",
      "     -0.00228 |       0.00000 |      24.85684 |       0.00046 |       0.19511\n",
      "     -0.00222 |       0.00000 |      24.80917 |       0.00044 |       0.19414\n",
      "Evaluating losses...\n",
      "     -0.00264 |       0.00000 |      24.69132 |       0.00048 |       0.19400\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88            |\n",
      "| EpRewMean       | -87           |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6393          |\n",
      "| TimeElapsed     | 1.47e+03      |\n",
      "| TimestepsSoFar  | 679936        |\n",
      "| ev_tdlam_before | 0.9           |\n",
      "| loss_ent        | 0.1939967     |\n",
      "| loss_kl         | 0.000483652   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0026449384 |\n",
      "| loss_vf_loss    | 24.691319     |\n",
      "-----------------------------------\n",
      "********** Iteration 166 ************\n",
      "Eval num_timesteps=679936, episode_reward=-78.80 +/- 5.53\n",
      "Episode length: 79.80 +/- 5.53\n",
      "Eval num_timesteps=679936, episode_reward=-88.70 +/- 33.91\n",
      "Episode length: 89.70 +/- 33.91\n",
      "Eval num_timesteps=679936, episode_reward=-80.50 +/- 8.43\n",
      "Episode length: 81.50 +/- 8.43\n",
      "Eval num_timesteps=679936, episode_reward=-78.50 +/- 7.54\n",
      "Episode length: 79.50 +/- 7.54\n",
      "Eval num_timesteps=679936, episode_reward=-85.60 +/- 10.52\n",
      "Episode length: 86.60 +/- 10.52\n",
      "Eval num_timesteps=679936, episode_reward=-76.30 +/- 8.43\n",
      "Episode length: 77.30 +/- 8.43\n",
      "Eval num_timesteps=679936, episode_reward=-80.60 +/- 4.61\n",
      "Episode length: 81.60 +/- 4.61\n",
      "Eval num_timesteps=679936, episode_reward=-90.50 +/- 20.97\n",
      "Episode length: 91.50 +/- 20.97\n",
      "Eval num_timesteps=679936, episode_reward=-79.70 +/- 6.44\n",
      "Episode length: 80.70 +/- 6.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.69e-05 |       0.00000 |      49.79420 |       0.00011 |       0.24428\n",
      "     -0.00069 |       0.00000 |      45.53861 |       0.00015 |       0.24655\n",
      "     -0.00133 |       0.00000 |      43.81719 |       0.00027 |       0.24779\n",
      "     -0.00149 |       0.00000 |      42.69984 |       0.00033 |       0.24810\n",
      "     -0.00193 |       0.00000 |      41.94500 |       0.00033 |       0.24757\n",
      "     -0.00224 |       0.00000 |      41.29030 |       0.00032 |       0.24885\n",
      "     -0.00198 |       0.00000 |      40.85331 |       0.00045 |       0.24870\n",
      "     -0.00186 |       0.00000 |      40.56179 |       0.00042 |       0.24719\n",
      "     -0.00254 |       0.00000 |      40.18171 |       0.00031 |       0.24751\n",
      "     -0.00234 |       0.00000 |      39.85585 |       0.00049 |       0.24957\n",
      "Evaluating losses...\n",
      "     -0.00314 |       0.00000 |      39.55762 |       0.00036 |       0.24835\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.5          |\n",
      "| EpRewMean       | -90.5         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 6434          |\n",
      "| TimeElapsed     | 1.48e+03      |\n",
      "| TimestepsSoFar  | 684032        |\n",
      "| ev_tdlam_before | 0.791         |\n",
      "| loss_ent        | 0.24835165    |\n",
      "| loss_kl         | 0.00036428997 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.003137496  |\n",
      "| loss_vf_loss    | 39.55762      |\n",
      "-----------------------------------\n",
      "********** Iteration 167 ************\n",
      "Eval num_timesteps=684032, episode_reward=-83.60 +/- 16.05\n",
      "Episode length: 84.60 +/- 16.05\n",
      "Eval num_timesteps=684032, episode_reward=-84.80 +/- 12.29\n",
      "Episode length: 85.80 +/- 12.29\n",
      "Eval num_timesteps=684032, episode_reward=-83.90 +/- 13.89\n",
      "Episode length: 84.90 +/- 13.89\n",
      "Eval num_timesteps=684032, episode_reward=-76.70 +/- 4.78\n",
      "Episode length: 77.70 +/- 4.78\n",
      "Eval num_timesteps=684032, episode_reward=-73.60 +/- 8.35\n",
      "Episode length: 74.60 +/- 8.35\n",
      "Eval num_timesteps=684032, episode_reward=-86.00 +/- 16.53\n",
      "Episode length: 87.00 +/- 16.53\n",
      "Eval num_timesteps=684032, episode_reward=-81.60 +/- 4.69\n",
      "Episode length: 82.60 +/- 4.69\n",
      "Eval num_timesteps=684032, episode_reward=-88.00 +/- 28.63\n",
      "Episode length: 89.00 +/- 28.63\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.10e-05 |       0.00000 |      51.75562 |       0.00013 |       0.28619\n",
      "     -0.00054 |       0.00000 |      46.99645 |       0.00017 |       0.28697\n",
      "     -0.00094 |       0.00000 |      45.64353 |       0.00024 |       0.28804\n",
      "     -0.00101 |       0.00000 |      45.00037 |       0.00016 |       0.28501\n",
      "     -0.00130 |       0.00000 |      44.59970 |       0.00020 |       0.28535\n",
      "     -0.00120 |       0.00000 |      44.26867 |       0.00027 |       0.28780\n",
      "     -0.00159 |       0.00000 |      44.03849 |       0.00021 |       0.28458\n",
      "     -0.00150 |       0.00000 |      43.81180 |       0.00026 |       0.28543\n",
      "     -0.00170 |       0.00000 |      43.57564 |       0.00020 |       0.28549\n",
      "     -0.00185 |       0.00000 |      43.43442 |       0.00025 |       0.28575\n",
      "Evaluating losses...\n",
      "     -0.00237 |       0.00000 |      43.12092 |       0.00031 |       0.28684\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99.9          |\n",
      "| EpRewMean       | -98.9         |\n",
      "| EpThisIter      | 37            |\n",
      "| EpisodesSoFar   | 6471          |\n",
      "| TimeElapsed     | 1.48e+03      |\n",
      "| TimestepsSoFar  | 688128        |\n",
      "| ev_tdlam_before | 0.783         |\n",
      "| loss_ent        | 0.28683978    |\n",
      "| loss_kl         | 0.0003148243  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0023712707 |\n",
      "| loss_vf_loss    | 43.12092      |\n",
      "-----------------------------------\n",
      "********** Iteration 168 ************\n",
      "Eval num_timesteps=688128, episode_reward=-89.10 +/- 21.77\n",
      "Episode length: 90.10 +/- 21.77\n",
      "Eval num_timesteps=688128, episode_reward=-84.80 +/- 13.73\n",
      "Episode length: 85.80 +/- 13.73\n",
      "Eval num_timesteps=688128, episode_reward=-81.00 +/- 9.63\n",
      "Episode length: 82.00 +/- 9.63\n",
      "Eval num_timesteps=688128, episode_reward=-77.90 +/- 5.52\n",
      "Episode length: 78.90 +/- 5.52\n",
      "Eval num_timesteps=688128, episode_reward=-76.70 +/- 8.60\n",
      "Episode length: 77.70 +/- 8.60\n",
      "Eval num_timesteps=688128, episode_reward=-86.20 +/- 14.39\n",
      "Episode length: 87.20 +/- 14.39\n",
      "Eval num_timesteps=688128, episode_reward=-80.20 +/- 13.27\n",
      "Episode length: 81.20 +/- 13.27\n",
      "Eval num_timesteps=688128, episode_reward=-81.60 +/- 9.41\n",
      "Episode length: 82.60 +/- 9.41\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00023 |       0.00000 |      38.08958 |       0.00018 |       0.19257\n",
      "     -0.00089 |       0.00000 |      37.31524 |       0.00021 |       0.19216\n",
      "     -0.00130 |       0.00000 |      36.68192 |       0.00025 |       0.19180\n",
      "     -0.00143 |       0.00000 |      36.26258 |       0.00030 |       0.19098\n",
      "     -0.00177 |       0.00000 |      35.85441 |       0.00033 |       0.19157\n",
      "     -0.00172 |       0.00000 |      35.51293 |       0.00037 |       0.19198\n",
      "     -0.00187 |       0.00000 |      35.26381 |       0.00033 |       0.19156\n",
      "     -0.00209 |       0.00000 |      35.02169 |       0.00032 |       0.19196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00216 |       0.00000 |      34.76704 |       0.00034 |       0.19171\n",
      "     -0.00217 |       0.00000 |      34.57024 |       0.00033 |       0.19246\n",
      "Evaluating losses...\n",
      "     -0.00255 |       0.00000 |      34.37474 |       0.00031 |       0.19322\n",
      "-----------------------------------\n",
      "| EpLenMean       | 100           |\n",
      "| EpRewMean       | -99.1         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 6516          |\n",
      "| TimeElapsed     | 1.49e+03      |\n",
      "| TimestepsSoFar  | 692224        |\n",
      "| ev_tdlam_before | 0.852         |\n",
      "| loss_ent        | 0.19322094    |\n",
      "| loss_kl         | 0.0003104576  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0025457095 |\n",
      "| loss_vf_loss    | 34.37474      |\n",
      "-----------------------------------\n",
      "********** Iteration 169 ************\n",
      "Eval num_timesteps=692224, episode_reward=-78.70 +/- 8.83\n",
      "Episode length: 79.70 +/- 8.83\n",
      "Eval num_timesteps=692224, episode_reward=-78.60 +/- 11.96\n",
      "Episode length: 79.60 +/- 11.96\n",
      "Eval num_timesteps=692224, episode_reward=-83.40 +/- 17.13\n",
      "Episode length: 84.40 +/- 17.13\n",
      "Eval num_timesteps=692224, episode_reward=-81.80 +/- 7.24\n",
      "Episode length: 82.80 +/- 7.24\n",
      "Eval num_timesteps=692224, episode_reward=-86.40 +/- 17.69\n",
      "Episode length: 87.40 +/- 17.69\n",
      "Eval num_timesteps=692224, episode_reward=-78.30 +/- 8.51\n",
      "Episode length: 79.30 +/- 8.51\n",
      "Eval num_timesteps=692224, episode_reward=-82.30 +/- 7.99\n",
      "Episode length: 83.30 +/- 7.99\n",
      "Eval num_timesteps=692224, episode_reward=-78.20 +/- 12.75\n",
      "Episode length: 79.20 +/- 12.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     7.72e-05 |       0.00000 |      33.65497 |      5.60e-05 |       0.19295\n",
      "     -0.00055 |       0.00000 |      33.17573 |       0.00018 |       0.19462\n",
      "     -0.00090 |       0.00000 |      32.89943 |       0.00015 |       0.19333\n",
      "     -0.00086 |       0.00000 |      32.68991 |       0.00014 |       0.19390\n",
      "     -0.00114 |       0.00000 |      32.54062 |       0.00017 |       0.19391\n",
      "     -0.00121 |       0.00000 |      32.37814 |       0.00019 |       0.19335\n",
      "     -0.00135 |       0.00000 |      32.25975 |       0.00017 |       0.19451\n",
      "     -0.00152 |       0.00000 |      32.12391 |       0.00022 |       0.19447\n",
      "     -0.00152 |       0.00000 |      32.04045 |       0.00019 |       0.19385\n",
      "     -0.00157 |       0.00000 |      31.92924 |       0.00019 |       0.19511\n",
      "Evaluating losses...\n",
      "     -0.00161 |       0.00000 |      31.73412 |       0.00025 |       0.19339\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.8          |\n",
      "| EpRewMean       | -90.8         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6562          |\n",
      "| TimeElapsed     | 1.49e+03      |\n",
      "| TimestepsSoFar  | 696320        |\n",
      "| ev_tdlam_before | 0.871         |\n",
      "| loss_ent        | 0.19338852    |\n",
      "| loss_kl         | 0.00025194994 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016088004 |\n",
      "| loss_vf_loss    | 31.734123     |\n",
      "-----------------------------------\n",
      "********** Iteration 170 ************\n",
      "Eval num_timesteps=696320, episode_reward=-75.00 +/- 8.56\n",
      "Episode length: 76.00 +/- 8.56\n",
      "Eval num_timesteps=696320, episode_reward=-80.90 +/- 5.66\n",
      "Episode length: 81.90 +/- 5.66\n",
      "Eval num_timesteps=696320, episode_reward=-91.00 +/- 29.22\n",
      "Episode length: 92.00 +/- 29.22\n",
      "Eval num_timesteps=696320, episode_reward=-77.70 +/- 5.78\n",
      "Episode length: 78.70 +/- 5.78\n",
      "Eval num_timesteps=696320, episode_reward=-90.90 +/- 24.81\n",
      "Episode length: 91.90 +/- 24.81\n",
      "Eval num_timesteps=696320, episode_reward=-91.70 +/- 27.50\n",
      "Episode length: 92.70 +/- 27.50\n",
      "Eval num_timesteps=696320, episode_reward=-78.50 +/- 6.18\n",
      "Episode length: 79.50 +/- 6.18\n",
      "Eval num_timesteps=696320, episode_reward=-79.40 +/- 5.87\n",
      "Episode length: 80.40 +/- 5.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00021 |       0.00000 |      29.18648 |       0.00013 |       0.19404\n",
      "     -0.00100 |       0.00000 |      27.26160 |       0.00020 |       0.19334\n",
      "     -0.00134 |       0.00000 |      26.40025 |       0.00020 |       0.19363\n",
      "     -0.00162 |       0.00000 |      25.92901 |       0.00029 |       0.19509\n",
      "     -0.00175 |       0.00000 |      25.57116 |       0.00021 |       0.19349\n",
      "     -0.00205 |       0.00000 |      25.34198 |       0.00024 |       0.19497\n",
      "     -0.00218 |       0.00000 |      25.13759 |       0.00022 |       0.19317\n",
      "     -0.00238 |       0.00000 |      25.01289 |       0.00024 |       0.19378\n",
      "     -0.00254 |       0.00000 |      24.85889 |       0.00030 |       0.19447\n",
      "     -0.00264 |       0.00000 |      24.77053 |       0.00028 |       0.19405\n",
      "Evaluating losses...\n",
      "     -0.00297 |       0.00000 |      24.58096 |       0.00026 |       0.19440\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.7          |\n",
      "| EpRewMean       | -86.7         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 6610          |\n",
      "| TimeElapsed     | 1.5e+03       |\n",
      "| TimestepsSoFar  | 700416        |\n",
      "| ev_tdlam_before | 0.884         |\n",
      "| loss_ent        | 0.19440487    |\n",
      "| loss_kl         | 0.0002582236  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0029673746 |\n",
      "| loss_vf_loss    | 24.580963     |\n",
      "-----------------------------------\n",
      "********** Iteration 171 ************\n",
      "Eval num_timesteps=700416, episode_reward=-80.50 +/- 9.50\n",
      "Episode length: 81.50 +/- 9.50\n",
      "Eval num_timesteps=700416, episode_reward=-84.40 +/- 17.78\n",
      "Episode length: 85.40 +/- 17.78\n",
      "Eval num_timesteps=700416, episode_reward=-77.50 +/- 8.50\n",
      "Episode length: 78.50 +/- 8.50\n",
      "Eval num_timesteps=700416, episode_reward=-88.40 +/- 16.84\n",
      "Episode length: 89.40 +/- 16.84\n",
      "Eval num_timesteps=700416, episode_reward=-97.80 +/- 25.72\n",
      "Episode length: 98.80 +/- 25.72\n",
      "Eval num_timesteps=700416, episode_reward=-82.10 +/- 8.97\n",
      "Episode length: 83.10 +/- 8.97\n",
      "Eval num_timesteps=700416, episode_reward=-99.80 +/- 38.29\n",
      "Episode length: 100.80 +/- 38.29\n",
      "Eval num_timesteps=700416, episode_reward=-81.10 +/- 5.20\n",
      "Episode length: 82.10 +/- 5.20\n",
      "Eval num_timesteps=700416, episode_reward=-92.90 +/- 37.79\n",
      "Episode length: 93.90 +/- 37.79\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00041 |       0.00000 |      44.58225 |      8.87e-05 |       0.19170\n",
      "     -0.00039 |       0.00000 |      42.28009 |       0.00011 |       0.19150\n",
      "     -0.00080 |       0.00000 |      41.52988 |       0.00014 |       0.19266\n",
      "     -0.00115 |       0.00000 |      41.08839 |       0.00015 |       0.19333\n",
      "     -0.00138 |       0.00000 |      40.60353 |       0.00016 |       0.19202\n",
      "     -0.00143 |       0.00000 |      40.32708 |       0.00016 |       0.19366\n",
      "     -0.00177 |       0.00000 |      40.05694 |       0.00017 |       0.19347\n",
      "     -0.00196 |       0.00000 |      39.81808 |       0.00016 |       0.19202\n",
      "     -0.00186 |       0.00000 |      39.60677 |       0.00017 |       0.19275\n",
      "     -0.00191 |       0.00000 |      39.39414 |       0.00024 |       0.19272\n",
      "Evaluating losses...\n",
      "     -0.00237 |       0.00000 |      39.13090 |       0.00020 |       0.19209\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.1          |\n",
      "| EpRewMean       | -88.1         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 6653          |\n",
      "| TimeElapsed     | 1.51e+03      |\n",
      "| TimestepsSoFar  | 704512        |\n",
      "| ev_tdlam_before | 0.819         |\n",
      "| loss_ent        | 0.19209078    |\n",
      "| loss_kl         | 0.00020152277 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0023691012 |\n",
      "| loss_vf_loss    | 39.1309       |\n",
      "-----------------------------------\n",
      "********** Iteration 172 ************\n",
      "Eval num_timesteps=704512, episode_reward=-85.60 +/- 22.85\n",
      "Episode length: 86.60 +/- 22.85\n",
      "Eval num_timesteps=704512, episode_reward=-80.80 +/- 10.98\n",
      "Episode length: 81.80 +/- 10.98\n",
      "Eval num_timesteps=704512, episode_reward=-83.80 +/- 13.20\n",
      "Episode length: 84.80 +/- 13.20\n",
      "Eval num_timesteps=704512, episode_reward=-87.30 +/- 18.47\n",
      "Episode length: 88.30 +/- 18.47\n",
      "Eval num_timesteps=704512, episode_reward=-79.50 +/- 8.65\n",
      "Episode length: 80.50 +/- 8.65\n",
      "Eval num_timesteps=704512, episode_reward=-80.60 +/- 5.30\n",
      "Episode length: 81.60 +/- 5.30\n",
      "Eval num_timesteps=704512, episode_reward=-130.70 +/- 126.76\n",
      "Episode length: 131.60 +/- 126.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=704512, episode_reward=-97.50 +/- 42.64\n",
      "Episode length: 98.50 +/- 42.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.76e-05 |       0.00000 |      26.10618 |      5.83e-05 |       0.18295\n",
      "     -0.00047 |       0.00000 |      24.04368 |       0.00018 |       0.18228\n",
      "     -0.00097 |       0.00000 |      23.06094 |       0.00024 |       0.18338\n",
      "     -0.00106 |       0.00000 |      22.42527 |       0.00023 |       0.18277\n",
      "     -0.00117 |       0.00000 |      22.00630 |       0.00022 |       0.18174\n",
      "     -0.00124 |       0.00000 |      21.71053 |       0.00025 |       0.18151\n",
      "     -0.00159 |       0.00000 |      21.44998 |       0.00031 |       0.18235\n",
      "     -0.00158 |       0.00000 |      21.29240 |       0.00026 |       0.18094\n",
      "     -0.00156 |       0.00000 |      21.16466 |       0.00027 |       0.18170\n",
      "     -0.00181 |       0.00000 |      21.01313 |       0.00028 |       0.18134\n",
      "Evaluating losses...\n",
      "     -0.00213 |       0.00000 |      20.88082 |       0.00030 |       0.18137\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.1          |\n",
      "| EpRewMean       | -89.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6700          |\n",
      "| TimeElapsed     | 1.51e+03      |\n",
      "| TimestepsSoFar  | 708608        |\n",
      "| ev_tdlam_before | 0.896         |\n",
      "| loss_ent        | 0.18136857    |\n",
      "| loss_kl         | 0.00030099816 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0021260444 |\n",
      "| loss_vf_loss    | 20.880816     |\n",
      "-----------------------------------\n",
      "********** Iteration 173 ************\n",
      "Eval num_timesteps=708608, episode_reward=-81.50 +/- 13.72\n",
      "Episode length: 82.50 +/- 13.72\n",
      "Eval num_timesteps=708608, episode_reward=-94.30 +/- 25.21\n",
      "Episode length: 95.30 +/- 25.21\n",
      "Eval num_timesteps=708608, episode_reward=-85.10 +/- 8.49\n",
      "Episode length: 86.10 +/- 8.49\n",
      "Eval num_timesteps=708608, episode_reward=-79.40 +/- 9.11\n",
      "Episode length: 80.40 +/- 9.11\n",
      "Eval num_timesteps=708608, episode_reward=-75.30 +/- 9.70\n",
      "Episode length: 76.30 +/- 9.70\n",
      "Eval num_timesteps=708608, episode_reward=-81.00 +/- 8.23\n",
      "Episode length: 82.00 +/- 8.23\n",
      "Eval num_timesteps=708608, episode_reward=-75.80 +/- 9.21\n",
      "Episode length: 76.80 +/- 9.21\n",
      "Eval num_timesteps=708608, episode_reward=-76.50 +/- 6.38\n",
      "Episode length: 77.50 +/- 6.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00028 |       0.00000 |      41.91220 |       0.00015 |       0.19498\n",
      "     -0.00058 |       0.00000 |      40.06145 |       0.00011 |       0.19488\n",
      "     -0.00100 |       0.00000 |      39.13309 |       0.00011 |       0.19620\n",
      "     -0.00122 |       0.00000 |      38.57067 |       0.00011 |       0.19589\n",
      "     -0.00146 |       0.00000 |      38.09366 |       0.00015 |       0.19594\n",
      "     -0.00150 |       0.00000 |      37.71841 |       0.00015 |       0.19575\n",
      "     -0.00185 |       0.00000 |      37.48004 |       0.00016 |       0.19644\n",
      "     -0.00197 |       0.00000 |      37.20547 |       0.00018 |       0.19636\n",
      "     -0.00207 |       0.00000 |      36.99955 |       0.00016 |       0.19733\n",
      "     -0.00210 |       0.00000 |      36.79804 |       0.00017 |       0.19695\n",
      "Evaluating losses...\n",
      "     -0.00247 |       0.00000 |      36.56793 |       0.00015 |       0.19780\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.7          |\n",
      "| EpRewMean       | -91.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 6742          |\n",
      "| TimeElapsed     | 1.52e+03      |\n",
      "| TimestepsSoFar  | 712704        |\n",
      "| ev_tdlam_before | 0.836         |\n",
      "| loss_ent        | 0.19780305    |\n",
      "| loss_kl         | 0.00014870908 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0024681713 |\n",
      "| loss_vf_loss    | 36.56793      |\n",
      "-----------------------------------\n",
      "********** Iteration 174 ************\n",
      "Eval num_timesteps=712704, episode_reward=-77.70 +/- 9.80\n",
      "Episode length: 78.70 +/- 9.80\n",
      "Eval num_timesteps=712704, episode_reward=-84.20 +/- 15.18\n",
      "Episode length: 85.20 +/- 15.18\n",
      "Eval num_timesteps=712704, episode_reward=-77.90 +/- 4.55\n",
      "Episode length: 78.90 +/- 4.55\n",
      "Eval num_timesteps=712704, episode_reward=-87.50 +/- 18.27\n",
      "Episode length: 88.50 +/- 18.27\n",
      "Eval num_timesteps=712704, episode_reward=-77.90 +/- 6.91\n",
      "Episode length: 78.90 +/- 6.91\n",
      "Eval num_timesteps=712704, episode_reward=-77.20 +/- 11.17\n",
      "Episode length: 78.20 +/- 11.17\n",
      "Eval num_timesteps=712704, episode_reward=-79.80 +/- 5.47\n",
      "Episode length: 80.80 +/- 5.47\n",
      "Eval num_timesteps=712704, episode_reward=-79.90 +/- 6.02\n",
      "Episode length: 80.90 +/- 6.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.90e-05 |       0.00000 |      28.49191 |      7.07e-05 |       0.18866\n",
      "     -0.00061 |       0.00000 |      27.88056 |       0.00012 |       0.18677\n",
      "     -0.00078 |       0.00000 |      27.46760 |       0.00012 |       0.18697\n",
      "     -0.00100 |       0.00000 |      27.20417 |       0.00012 |       0.18725\n",
      "     -0.00119 |       0.00000 |      27.01454 |       0.00012 |       0.18776\n",
      "     -0.00137 |       0.00000 |      26.81436 |       0.00011 |       0.18813\n",
      "     -0.00136 |       0.00000 |      26.66683 |       0.00015 |       0.18830\n",
      "     -0.00146 |       0.00000 |      26.58021 |       0.00014 |       0.18758\n",
      "     -0.00133 |       0.00000 |      26.44270 |       0.00016 |       0.18810\n",
      "     -0.00160 |       0.00000 |      26.36666 |       0.00015 |       0.18837\n",
      "Evaluating losses...\n",
      "     -0.00199 |       0.00000 |      26.15320 |       0.00013 |       0.18785\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.2          |\n",
      "| EpRewMean       | -92.2         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6788          |\n",
      "| TimeElapsed     | 1.53e+03      |\n",
      "| TimestepsSoFar  | 716800        |\n",
      "| ev_tdlam_before | 0.893         |\n",
      "| loss_ent        | 0.18785186    |\n",
      "| loss_kl         | 0.0001298498  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0019870303 |\n",
      "| loss_vf_loss    | 26.153204     |\n",
      "-----------------------------------\n",
      "********** Iteration 175 ************\n",
      "Eval num_timesteps=716800, episode_reward=-81.00 +/- 17.39\n",
      "Episode length: 82.00 +/- 17.39\n",
      "Eval num_timesteps=716800, episode_reward=-78.00 +/- 7.29\n",
      "Episode length: 79.00 +/- 7.29\n",
      "Eval num_timesteps=716800, episode_reward=-81.40 +/- 5.48\n",
      "Episode length: 82.40 +/- 5.48\n",
      "Eval num_timesteps=716800, episode_reward=-79.40 +/- 10.65\n",
      "Episode length: 80.40 +/- 10.65\n",
      "Eval num_timesteps=716800, episode_reward=-81.30 +/- 6.96\n",
      "Episode length: 82.30 +/- 6.96\n",
      "Eval num_timesteps=716800, episode_reward=-88.10 +/- 24.30\n",
      "Episode length: 89.10 +/- 24.30\n",
      "Eval num_timesteps=716800, episode_reward=-88.30 +/- 28.04\n",
      "Episode length: 89.30 +/- 28.04\n",
      "Eval num_timesteps=716800, episode_reward=-78.10 +/- 5.39\n",
      "Episode length: 79.10 +/- 5.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00019 |       0.00000 |      27.02229 |      4.94e-05 |       0.18886\n",
      "     -0.00046 |       0.00000 |      26.27028 |      9.10e-05 |       0.18920\n",
      "     -0.00061 |       0.00000 |      25.94329 |      8.29e-05 |       0.18854\n",
      "     -0.00090 |       0.00000 |      25.69999 |      7.41e-05 |       0.18876\n",
      "     -0.00098 |       0.00000 |      25.57754 |       0.00011 |       0.18876\n",
      "     -0.00111 |       0.00000 |      25.37781 |       0.00012 |       0.18873\n",
      "     -0.00126 |       0.00000 |      25.26263 |       0.00013 |       0.18822\n",
      "     -0.00137 |       0.00000 |      25.13758 |       0.00015 |       0.18815\n",
      "     -0.00125 |       0.00000 |      25.01888 |       0.00015 |       0.18826\n",
      "     -0.00156 |       0.00000 |      24.92699 |       0.00019 |       0.18920\n",
      "Evaluating losses...\n",
      "     -0.00184 |       0.00000 |      24.76366 |       0.00015 |       0.18853\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.2          |\n",
      "| EpRewMean       | -89.2         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6834          |\n",
      "| TimeElapsed     | 1.53e+03      |\n",
      "| TimestepsSoFar  | 720896        |\n",
      "| ev_tdlam_before | 0.899         |\n",
      "| loss_ent        | 0.18853149    |\n",
      "| loss_kl         | 0.00015170578 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018371351 |\n",
      "| loss_vf_loss    | 24.763662     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 176 ************\n",
      "Eval num_timesteps=720896, episode_reward=-93.90 +/- 17.69\n",
      "Episode length: 94.90 +/- 17.69\n",
      "Eval num_timesteps=720896, episode_reward=-79.90 +/- 5.63\n",
      "Episode length: 80.90 +/- 5.63\n",
      "Eval num_timesteps=720896, episode_reward=-80.90 +/- 12.83\n",
      "Episode length: 81.90 +/- 12.83\n",
      "Eval num_timesteps=720896, episode_reward=-75.80 +/- 10.59\n",
      "Episode length: 76.80 +/- 10.59\n",
      "Eval num_timesteps=720896, episode_reward=-78.20 +/- 6.23\n",
      "Episode length: 79.20 +/- 6.23\n",
      "Eval num_timesteps=720896, episode_reward=-85.30 +/- 13.70\n",
      "Episode length: 86.30 +/- 13.70\n",
      "Eval num_timesteps=720896, episode_reward=-84.40 +/- 8.25\n",
      "Episode length: 85.40 +/- 8.25\n",
      "Eval num_timesteps=720896, episode_reward=-79.50 +/- 12.92\n",
      "Episode length: 80.50 +/- 12.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -5.60e-05 |       0.00000 |      33.44278 |      8.66e-05 |       0.18189\n",
      "     -0.00031 |       0.00000 |      32.54841 |      8.91e-05 |       0.18331\n",
      "     -0.00059 |       0.00000 |      31.92718 |       0.00023 |       0.18171\n",
      "     -0.00088 |       0.00000 |      31.53901 |       0.00016 |       0.18245\n",
      "     -0.00084 |       0.00000 |      31.20267 |       0.00020 |       0.18138\n",
      "     -0.00102 |       0.00000 |      30.97165 |       0.00017 |       0.18111\n",
      "     -0.00110 |       0.00000 |      30.74393 |       0.00015 |       0.18122\n",
      "     -0.00113 |       0.00000 |      30.57830 |       0.00023 |       0.18098\n",
      "     -0.00121 |       0.00000 |      30.39319 |       0.00016 |       0.18086\n",
      "     -0.00145 |       0.00000 |      30.25571 |       0.00020 |       0.18093\n",
      "Evaluating losses...\n",
      "     -0.00163 |       0.00000 |      30.10721 |       0.00017 |       0.18073\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.5          |\n",
      "| EpRewMean       | -87.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6880          |\n",
      "| TimeElapsed     | 1.54e+03      |\n",
      "| TimestepsSoFar  | 724992        |\n",
      "| ev_tdlam_before | 0.875         |\n",
      "| loss_ent        | 0.18073061    |\n",
      "| loss_kl         | 0.00017477173 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016270292 |\n",
      "| loss_vf_loss    | 30.107212     |\n",
      "-----------------------------------\n",
      "********** Iteration 177 ************\n",
      "Eval num_timesteps=724992, episode_reward=-88.00 +/- 15.21\n",
      "Episode length: 89.00 +/- 15.21\n",
      "Eval num_timesteps=724992, episode_reward=-83.10 +/- 12.19\n",
      "Episode length: 84.10 +/- 12.19\n",
      "Eval num_timesteps=724992, episode_reward=-86.20 +/- 28.83\n",
      "Episode length: 87.20 +/- 28.83\n",
      "Eval num_timesteps=724992, episode_reward=-73.20 +/- 6.16\n",
      "Episode length: 74.20 +/- 6.16\n",
      "Eval num_timesteps=724992, episode_reward=-89.10 +/- 9.24\n",
      "Episode length: 90.10 +/- 9.24\n",
      "Eval num_timesteps=724992, episode_reward=-79.70 +/- 7.51\n",
      "Episode length: 80.70 +/- 7.51\n",
      "Eval num_timesteps=724992, episode_reward=-82.10 +/- 14.80\n",
      "Episode length: 83.10 +/- 14.80\n",
      "Eval num_timesteps=724992, episode_reward=-74.60 +/- 7.02\n",
      "Episode length: 75.60 +/- 7.02\n",
      "Eval num_timesteps=724992, episode_reward=-78.50 +/- 8.13\n",
      "Episode length: 79.50 +/- 8.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      44.16420 |       0.00015 |       0.22087\n",
      "     -0.00122 |       0.00000 |      40.16454 |       0.00019 |       0.21959\n",
      "     -0.00130 |       0.00000 |      39.40222 |       0.00024 |       0.22065\n",
      "     -0.00156 |       0.00000 |      38.83692 |       0.00021 |       0.22035\n",
      "     -0.00161 |       0.00000 |      38.50293 |       0.00021 |       0.22184\n",
      "     -0.00174 |       0.00000 |      38.21922 |       0.00031 |       0.22424\n",
      "     -0.00201 |       0.00000 |      38.03763 |       0.00031 |       0.22246\n",
      "     -0.00200 |       0.00000 |      37.80051 |       0.00028 |       0.22289\n",
      "     -0.00220 |       0.00000 |      37.65872 |       0.00036 |       0.22272\n",
      "     -0.00235 |       0.00000 |      37.49297 |       0.00035 |       0.22316\n",
      "Evaluating losses...\n",
      "     -0.00274 |       0.00000 |      37.28177 |       0.00039 |       0.22481\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 6921          |\n",
      "| TimeElapsed     | 1.55e+03      |\n",
      "| TimestepsSoFar  | 729088        |\n",
      "| ev_tdlam_before | 0.805         |\n",
      "| loss_ent        | 0.22481196    |\n",
      "| loss_kl         | 0.00039095743 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0027374066 |\n",
      "| loss_vf_loss    | 37.281773     |\n",
      "-----------------------------------\n",
      "********** Iteration 178 ************\n",
      "Eval num_timesteps=729088, episode_reward=-88.50 +/- 19.65\n",
      "Episode length: 89.50 +/- 19.65\n",
      "Eval num_timesteps=729088, episode_reward=-94.40 +/- 30.95\n",
      "Episode length: 95.40 +/- 30.95\n",
      "Eval num_timesteps=729088, episode_reward=-98.90 +/- 45.30\n",
      "Episode length: 99.90 +/- 45.30\n",
      "Eval num_timesteps=729088, episode_reward=-81.60 +/- 4.82\n",
      "Episode length: 82.60 +/- 4.82\n",
      "Eval num_timesteps=729088, episode_reward=-80.20 +/- 8.85\n",
      "Episode length: 81.20 +/- 8.85\n",
      "Eval num_timesteps=729088, episode_reward=-77.90 +/- 8.93\n",
      "Episode length: 78.90 +/- 8.93\n",
      "Eval num_timesteps=729088, episode_reward=-76.70 +/- 7.90\n",
      "Episode length: 77.70 +/- 7.90\n",
      "Eval num_timesteps=729088, episode_reward=-82.20 +/- 9.24\n",
      "Episode length: 83.20 +/- 9.24\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -2.99e-05 |       0.00000 |      44.99118 |      5.73e-05 |       0.22104\n",
      "     -0.00092 |       0.00000 |      43.82135 |       0.00013 |       0.22157\n",
      "     -0.00136 |       0.00000 |      43.33798 |       0.00015 |       0.22153\n",
      "     -0.00134 |       0.00000 |      42.96551 |       0.00021 |       0.22259\n",
      "     -0.00147 |       0.00000 |      42.61613 |       0.00017 |       0.22123\n",
      "     -0.00163 |       0.00000 |      42.34556 |       0.00018 |       0.22232\n",
      "     -0.00170 |       0.00000 |      42.09750 |       0.00018 |       0.22227\n",
      "     -0.00183 |       0.00000 |      41.96683 |       0.00019 |       0.22241\n",
      "     -0.00183 |       0.00000 |      41.73126 |       0.00019 |       0.22252\n",
      "     -0.00190 |       0.00000 |      41.58335 |       0.00022 |       0.22275\n",
      "Evaluating losses...\n",
      "     -0.00215 |       0.00000 |      41.30812 |       0.00022 |       0.22344\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.7          |\n",
      "| EpRewMean       | -94.7         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 6964          |\n",
      "| TimeElapsed     | 1.55e+03      |\n",
      "| TimestepsSoFar  | 733184        |\n",
      "| ev_tdlam_before | 0.819         |\n",
      "| loss_ent        | 0.22344273    |\n",
      "| loss_kl         | 0.0002162711  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0021522553 |\n",
      "| loss_vf_loss    | 41.30812      |\n",
      "-----------------------------------\n",
      "********** Iteration 179 ************\n",
      "Eval num_timesteps=733184, episode_reward=-81.80 +/- 3.97\n",
      "Episode length: 82.80 +/- 3.97\n",
      "Eval num_timesteps=733184, episode_reward=-77.10 +/- 9.86\n",
      "Episode length: 78.10 +/- 9.86\n",
      "Eval num_timesteps=733184, episode_reward=-124.30 +/- 125.45\n",
      "Episode length: 125.20 +/- 125.15\n",
      "Eval num_timesteps=733184, episode_reward=-81.60 +/- 8.74\n",
      "Episode length: 82.60 +/- 8.74\n",
      "Eval num_timesteps=733184, episode_reward=-76.60 +/- 4.61\n",
      "Episode length: 77.60 +/- 4.61\n",
      "Eval num_timesteps=733184, episode_reward=-82.50 +/- 9.59\n",
      "Episode length: 83.50 +/- 9.59\n",
      "Eval num_timesteps=733184, episode_reward=-81.00 +/- 8.58\n",
      "Episode length: 82.00 +/- 8.58\n",
      "Eval num_timesteps=733184, episode_reward=-76.50 +/- 6.92\n",
      "Episode length: 77.50 +/- 6.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00012 |       0.00000 |      35.60268 |       0.00010 |       0.25242\n",
      "     -0.00034 |       0.00000 |      33.77722 |       0.00017 |       0.24958\n",
      "     -0.00083 |       0.00000 |      33.42881 |       0.00021 |       0.25007\n",
      "     -0.00051 |       0.00000 |      33.09156 |       0.00023 |       0.25120\n",
      "     -0.00081 |       0.00000 |      32.86502 |       0.00022 |       0.25006\n",
      "     -0.00090 |       0.00000 |      32.61596 |       0.00023 |       0.25010\n",
      "     -0.00103 |       0.00000 |      32.45438 |       0.00022 |       0.24883\n",
      "     -0.00137 |       0.00000 |      32.25494 |       0.00022 |       0.25004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00133 |       0.00000 |      32.11997 |       0.00024 |       0.24916\n",
      "     -0.00161 |       0.00000 |      31.99579 |       0.00025 |       0.24888\n",
      "Evaluating losses...\n",
      "     -0.00184 |       0.00000 |      31.78710 |       0.00020 |       0.25011\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.9          |\n",
      "| EpRewMean       | -93.9         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 7005          |\n",
      "| TimeElapsed     | 1.56e+03      |\n",
      "| TimestepsSoFar  | 737280        |\n",
      "| ev_tdlam_before | 0.86          |\n",
      "| loss_ent        | 0.25011072    |\n",
      "| loss_kl         | 0.00019624681 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018370256 |\n",
      "| loss_vf_loss    | 31.787102     |\n",
      "-----------------------------------\n",
      "********** Iteration 180 ************\n",
      "Eval num_timesteps=737280, episode_reward=-83.40 +/- 9.90\n",
      "Episode length: 84.40 +/- 9.90\n",
      "Eval num_timesteps=737280, episode_reward=-78.10 +/- 9.68\n",
      "Episode length: 79.10 +/- 9.68\n",
      "Eval num_timesteps=737280, episode_reward=-82.40 +/- 4.96\n",
      "Episode length: 83.40 +/- 4.96\n",
      "Eval num_timesteps=737280, episode_reward=-83.50 +/- 10.34\n",
      "Episode length: 84.50 +/- 10.34\n",
      "Eval num_timesteps=737280, episode_reward=-81.60 +/- 10.60\n",
      "Episode length: 82.60 +/- 10.60\n",
      "Eval num_timesteps=737280, episode_reward=-81.50 +/- 10.50\n",
      "Episode length: 82.50 +/- 10.50\n",
      "Eval num_timesteps=737280, episode_reward=-79.70 +/- 6.13\n",
      "Episode length: 80.70 +/- 6.13\n",
      "Eval num_timesteps=737280, episode_reward=-91.00 +/- 32.09\n",
      "Episode length: 92.00 +/- 32.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00045 |       0.00000 |      30.96513 |      6.59e-05 |       0.18657\n",
      "     -0.00027 |       0.00000 |      30.17670 |      8.47e-05 |       0.18782\n",
      "     -0.00051 |       0.00000 |      29.64265 |       0.00013 |       0.18798\n",
      "     -0.00078 |       0.00000 |      29.24721 |       0.00011 |       0.18785\n",
      "     -0.00092 |       0.00000 |      28.91825 |       0.00014 |       0.18805\n",
      "     -0.00093 |       0.00000 |      28.63038 |       0.00017 |       0.18830\n",
      "     -0.00112 |       0.00000 |      28.38390 |       0.00015 |       0.18898\n",
      "     -0.00114 |       0.00000 |      28.16464 |       0.00016 |       0.18837\n",
      "     -0.00122 |       0.00000 |      27.97390 |       0.00016 |       0.18765\n",
      "     -0.00140 |       0.00000 |      27.80381 |       0.00016 |       0.18876\n",
      "Evaluating losses...\n",
      "     -0.00169 |       0.00000 |      27.62869 |       0.00018 |       0.18857\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.8          |\n",
      "| EpRewMean       | -92.8         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 7050          |\n",
      "| TimeElapsed     | 1.57e+03      |\n",
      "| TimestepsSoFar  | 741376        |\n",
      "| ev_tdlam_before | 0.882         |\n",
      "| loss_ent        | 0.18857211    |\n",
      "| loss_kl         | 0.00017890609 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016931334 |\n",
      "| loss_vf_loss    | 27.628687     |\n",
      "-----------------------------------\n",
      "********** Iteration 181 ************\n",
      "Eval num_timesteps=741376, episode_reward=-82.00 +/- 15.03\n",
      "Episode length: 83.00 +/- 15.03\n",
      "Eval num_timesteps=741376, episode_reward=-81.30 +/- 7.01\n",
      "Episode length: 82.30 +/- 7.01\n",
      "Eval num_timesteps=741376, episode_reward=-83.20 +/- 12.61\n",
      "Episode length: 84.20 +/- 12.61\n",
      "Eval num_timesteps=741376, episode_reward=-78.00 +/- 7.48\n",
      "Episode length: 79.00 +/- 7.48\n",
      "Eval num_timesteps=741376, episode_reward=-77.60 +/- 10.37\n",
      "Episode length: 78.60 +/- 10.37\n",
      "Eval num_timesteps=741376, episode_reward=-84.30 +/- 10.33\n",
      "Episode length: 85.30 +/- 10.33\n",
      "Eval num_timesteps=741376, episode_reward=-82.80 +/- 5.36\n",
      "Episode length: 83.80 +/- 5.36\n",
      "Eval num_timesteps=741376, episode_reward=-76.20 +/- 9.72\n",
      "Episode length: 77.20 +/- 9.72\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00054 |       0.00000 |      37.86294 |      6.06e-05 |       0.19542\n",
      "    -8.10e-05 |       0.00000 |      37.24644 |      7.68e-05 |       0.19577\n",
      "     -0.00054 |       0.00000 |      36.83442 |       0.00011 |       0.19651\n",
      "     -0.00078 |       0.00000 |      36.54808 |       0.00011 |       0.19684\n",
      "     -0.00099 |       0.00000 |      36.39225 |       0.00013 |       0.19627\n",
      "     -0.00126 |       0.00000 |      36.20947 |       0.00016 |       0.19712\n",
      "     -0.00121 |       0.00000 |      36.05844 |       0.00015 |       0.19674\n",
      "     -0.00143 |       0.00000 |      35.92402 |       0.00018 |       0.19680\n",
      "     -0.00154 |       0.00000 |      35.79018 |       0.00019 |       0.19641\n",
      "     -0.00180 |       0.00000 |      35.68979 |       0.00017 |       0.19715\n",
      "Evaluating losses...\n",
      "     -0.00202 |       0.00000 |      35.51704 |       0.00022 |       0.19756\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.5          |\n",
      "| EpRewMean       | -89.5         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 7095          |\n",
      "| TimeElapsed     | 1.57e+03      |\n",
      "| TimestepsSoFar  | 745472        |\n",
      "| ev_tdlam_before | 0.855         |\n",
      "| loss_ent        | 0.19756262    |\n",
      "| loss_kl         | 0.00021982467 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0020171567 |\n",
      "| loss_vf_loss    | 35.51704      |\n",
      "-----------------------------------\n",
      "********** Iteration 182 ************\n",
      "Eval num_timesteps=745472, episode_reward=-81.20 +/- 6.54\n",
      "Episode length: 82.20 +/- 6.54\n",
      "Eval num_timesteps=745472, episode_reward=-78.30 +/- 6.50\n",
      "Episode length: 79.30 +/- 6.50\n",
      "Eval num_timesteps=745472, episode_reward=-81.10 +/- 5.22\n",
      "Episode length: 82.10 +/- 5.22\n",
      "Eval num_timesteps=745472, episode_reward=-84.00 +/- 19.51\n",
      "Episode length: 85.00 +/- 19.51\n",
      "Eval num_timesteps=745472, episode_reward=-78.20 +/- 6.52\n",
      "Episode length: 79.20 +/- 6.52\n",
      "Eval num_timesteps=745472, episode_reward=-82.90 +/- 12.19\n",
      "Episode length: 83.90 +/- 12.19\n",
      "Eval num_timesteps=745472, episode_reward=-77.50 +/- 6.34\n",
      "Episode length: 78.50 +/- 6.34\n",
      "Eval num_timesteps=745472, episode_reward=-76.70 +/- 5.69\n",
      "Episode length: 77.70 +/- 5.69\n",
      "Eval num_timesteps=745472, episode_reward=-81.90 +/- 11.31\n",
      "Episode length: 82.90 +/- 11.31\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00038 |       0.00000 |      32.28249 |      5.50e-05 |       0.22198\n",
      "     -0.00049 |       0.00000 |      30.29838 |      8.28e-05 |       0.22336\n",
      "     -0.00084 |       0.00000 |      29.93666 |      9.04e-05 |       0.22375\n",
      "     -0.00096 |       0.00000 |      29.69665 |       0.00011 |       0.22283\n",
      "     -0.00102 |       0.00000 |      29.52291 |       0.00020 |       0.22422\n",
      "     -0.00119 |       0.00000 |      29.32280 |       0.00014 |       0.22370\n",
      "     -0.00122 |       0.00000 |      29.20282 |       0.00018 |       0.22493\n",
      "     -0.00133 |       0.00000 |      29.02186 |       0.00021 |       0.22404\n",
      "     -0.00152 |       0.00000 |      28.95896 |       0.00019 |       0.22426\n",
      "     -0.00152 |       0.00000 |      28.80787 |       0.00024 |       0.22531\n",
      "Evaluating losses...\n",
      "     -0.00188 |       0.00000 |      28.65229 |       0.00020 |       0.22454\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 7137          |\n",
      "| TimeElapsed     | 1.58e+03      |\n",
      "| TimestepsSoFar  | 749568        |\n",
      "| ev_tdlam_before | 0.865         |\n",
      "| loss_ent        | 0.2245359     |\n",
      "| loss_kl         | 0.0002001265  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018758117 |\n",
      "| loss_vf_loss    | 28.652287     |\n",
      "-----------------------------------\n",
      "********** Iteration 183 ************\n",
      "Eval num_timesteps=749568, episode_reward=-82.90 +/- 3.96\n",
      "Episode length: 83.90 +/- 3.96\n",
      "Eval num_timesteps=749568, episode_reward=-85.80 +/- 10.93\n",
      "Episode length: 86.80 +/- 10.93\n",
      "Eval num_timesteps=749568, episode_reward=-86.90 +/- 12.94\n",
      "Episode length: 87.90 +/- 12.94\n",
      "Eval num_timesteps=749568, episode_reward=-81.10 +/- 8.88\n",
      "Episode length: 82.10 +/- 8.88\n",
      "Eval num_timesteps=749568, episode_reward=-78.50 +/- 9.19\n",
      "Episode length: 79.50 +/- 9.19\n",
      "Eval num_timesteps=749568, episode_reward=-81.60 +/- 8.31\n",
      "Episode length: 82.60 +/- 8.31\n",
      "Eval num_timesteps=749568, episode_reward=-83.70 +/- 7.07\n",
      "Episode length: 84.70 +/- 7.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=749568, episode_reward=-83.20 +/- 9.43\n",
      "Episode length: 84.20 +/- 9.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00023 |       0.00000 |      33.59089 |      3.54e-05 |       0.21032\n",
      "    -3.24e-05 |       0.00000 |      32.51509 |      7.76e-05 |       0.20861\n",
      "     -0.00033 |       0.00000 |      32.10011 |      6.36e-05 |       0.20899\n",
      "     -0.00059 |       0.00000 |      31.77599 |      9.15e-05 |       0.20932\n",
      "     -0.00041 |       0.00000 |      31.51797 |      8.33e-05 |       0.20830\n",
      "     -0.00071 |       0.00000 |      31.26426 |      7.90e-05 |       0.20909\n",
      "     -0.00080 |       0.00000 |      31.07109 |      8.80e-05 |       0.20861\n",
      "     -0.00091 |       0.00000 |      30.91648 |      8.55e-05 |       0.20904\n",
      "     -0.00088 |       0.00000 |      30.74599 |      8.94e-05 |       0.20872\n",
      "     -0.00103 |       0.00000 |      30.60154 |      8.38e-05 |       0.20890\n",
      "Evaluating losses...\n",
      "     -0.00119 |       0.00000 |      30.42306 |       0.00012 |       0.20977\n",
      "------------------------------------\n",
      "| EpLenMean       | 94.1           |\n",
      "| EpRewMean       | -93.1          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 7180           |\n",
      "| TimeElapsed     | 1.59e+03       |\n",
      "| TimestepsSoFar  | 753664         |\n",
      "| ev_tdlam_before | 0.875          |\n",
      "| loss_ent        | 0.20977037     |\n",
      "| loss_kl         | 0.000118653676 |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.0011853338  |\n",
      "| loss_vf_loss    | 30.423061      |\n",
      "------------------------------------\n",
      "********** Iteration 184 ************\n",
      "Eval num_timesteps=753664, episode_reward=-87.90 +/- 34.45\n",
      "Episode length: 88.90 +/- 34.45\n",
      "Eval num_timesteps=753664, episode_reward=-79.90 +/- 4.99\n",
      "Episode length: 80.90 +/- 4.99\n",
      "Eval num_timesteps=753664, episode_reward=-76.00 +/- 8.06\n",
      "Episode length: 77.00 +/- 8.06\n",
      "Eval num_timesteps=753664, episode_reward=-84.10 +/- 8.76\n",
      "Episode length: 85.10 +/- 8.76\n",
      "Eval num_timesteps=753664, episode_reward=-98.10 +/- 40.76\n",
      "Episode length: 99.10 +/- 40.76\n",
      "Eval num_timesteps=753664, episode_reward=-78.70 +/- 7.79\n",
      "Episode length: 79.70 +/- 7.79\n",
      "Eval num_timesteps=753664, episode_reward=-80.50 +/- 10.76\n",
      "Episode length: 81.50 +/- 10.76\n",
      "Eval num_timesteps=753664, episode_reward=-85.20 +/- 26.99\n",
      "Episode length: 86.20 +/- 26.99\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00031 |       0.00000 |      29.49755 |      6.33e-05 |       0.18104\n",
      "     -0.00043 |       0.00000 |      29.03029 |      7.84e-05 |       0.18211\n",
      "     -0.00077 |       0.00000 |      28.76133 |      7.20e-05 |       0.18251\n",
      "     -0.00086 |       0.00000 |      28.54252 |       0.00011 |       0.18293\n",
      "     -0.00071 |       0.00000 |      28.36835 |      7.30e-05 |       0.18140\n",
      "     -0.00101 |       0.00000 |      28.21301 |       0.00013 |       0.18303\n",
      "     -0.00117 |       0.00000 |      28.08082 |       0.00011 |       0.18260\n",
      "     -0.00112 |       0.00000 |      27.96582 |       0.00012 |       0.18205\n",
      "     -0.00126 |       0.00000 |      27.89586 |       0.00013 |       0.18225\n",
      "     -0.00137 |       0.00000 |      27.74917 |       0.00012 |       0.18207\n",
      "Evaluating losses...\n",
      "     -0.00161 |       0.00000 |      27.61790 |       0.00013 |       0.18206\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.7          |\n",
      "| EpRewMean       | -94.7         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7226          |\n",
      "| TimeElapsed     | 1.59e+03      |\n",
      "| TimestepsSoFar  | 757760        |\n",
      "| ev_tdlam_before | 0.895         |\n",
      "| loss_ent        | 0.1820565     |\n",
      "| loss_kl         | 0.00013264062 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016096632 |\n",
      "| loss_vf_loss    | 27.617899     |\n",
      "-----------------------------------\n",
      "********** Iteration 185 ************\n",
      "Eval num_timesteps=757760, episode_reward=-77.90 +/- 11.70\n",
      "Episode length: 78.90 +/- 11.70\n",
      "Eval num_timesteps=757760, episode_reward=-85.30 +/- 10.51\n",
      "Episode length: 86.30 +/- 10.51\n",
      "Eval num_timesteps=757760, episode_reward=-86.60 +/- 17.25\n",
      "Episode length: 87.60 +/- 17.25\n",
      "Eval num_timesteps=757760, episode_reward=-81.20 +/- 9.70\n",
      "Episode length: 82.20 +/- 9.70\n",
      "Eval num_timesteps=757760, episode_reward=-77.70 +/- 8.71\n",
      "Episode length: 78.70 +/- 8.71\n",
      "Eval num_timesteps=757760, episode_reward=-79.70 +/- 5.18\n",
      "Episode length: 80.70 +/- 5.18\n",
      "Eval num_timesteps=757760, episode_reward=-79.90 +/- 11.90\n",
      "Episode length: 80.90 +/- 11.90\n",
      "Eval num_timesteps=757760, episode_reward=-95.30 +/- 47.69\n",
      "Episode length: 96.30 +/- 47.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00038 |       0.00000 |      36.19439 |      4.97e-05 |       0.24309\n",
      "     -0.00055 |       0.00000 |      33.41018 |       0.00010 |       0.24424\n",
      "     -0.00102 |       0.00000 |      33.08916 |       0.00013 |       0.24380\n",
      "     -0.00111 |       0.00000 |      32.89437 |       0.00018 |       0.24456\n",
      "     -0.00152 |       0.00000 |      32.70335 |       0.00020 |       0.24535\n",
      "     -0.00177 |       0.00000 |      32.54003 |       0.00028 |       0.24717\n",
      "     -0.00211 |       0.00000 |      32.42802 |       0.00045 |       0.24928\n",
      "     -0.00213 |       0.00000 |      32.29366 |       0.00044 |       0.24895\n",
      "     -0.00238 |       0.00000 |      32.21487 |       0.00059 |       0.24967\n",
      "     -0.00279 |       0.00000 |      32.12932 |       0.00060 |       0.25013\n",
      "Evaluating losses...\n",
      "     -0.00306 |       0.00000 |      31.92154 |       0.00065 |       0.25011\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.7          |\n",
      "| EpRewMean       | -94.7         |\n",
      "| EpThisIter      | 39            |\n",
      "| EpisodesSoFar   | 7265          |\n",
      "| TimeElapsed     | 1.6e+03       |\n",
      "| TimestepsSoFar  | 761856        |\n",
      "| ev_tdlam_before | 0.841         |\n",
      "| loss_ent        | 0.25011423    |\n",
      "| loss_kl         | 0.00064850296 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0030637975 |\n",
      "| loss_vf_loss    | 31.921535     |\n",
      "-----------------------------------\n",
      "********** Iteration 186 ************\n",
      "Eval num_timesteps=761856, episode_reward=-80.80 +/- 8.70\n",
      "Episode length: 81.80 +/- 8.70\n",
      "Eval num_timesteps=761856, episode_reward=-83.00 +/- 11.06\n",
      "Episode length: 84.00 +/- 11.06\n",
      "Eval num_timesteps=761856, episode_reward=-75.30 +/- 9.84\n",
      "Episode length: 76.30 +/- 9.84\n",
      "Eval num_timesteps=761856, episode_reward=-76.50 +/- 8.39\n",
      "Episode length: 77.50 +/- 8.39\n",
      "Eval num_timesteps=761856, episode_reward=-77.10 +/- 5.89\n",
      "Episode length: 78.10 +/- 5.89\n",
      "Eval num_timesteps=761856, episode_reward=-77.20 +/- 6.78\n",
      "Episode length: 78.20 +/- 6.78\n",
      "Eval num_timesteps=761856, episode_reward=-79.00 +/- 4.56\n",
      "Episode length: 80.00 +/- 4.56\n",
      "Eval num_timesteps=761856, episode_reward=-79.70 +/- 7.16\n",
      "Episode length: 80.70 +/- 7.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.79e-05 |       0.00000 |      28.54018 |      3.58e-05 |       0.18794\n",
      "     -0.00016 |       0.00000 |      27.70380 |       0.00025 |       0.18754\n",
      "     -0.00054 |       0.00000 |      27.21870 |       0.00013 |       0.18846\n",
      "     -0.00064 |       0.00000 |      26.91543 |      9.33e-05 |       0.18794\n",
      "     -0.00076 |       0.00000 |      26.64781 |       0.00013 |       0.18791\n",
      "     -0.00098 |       0.00000 |      26.42233 |       0.00012 |       0.18747\n",
      "     -0.00099 |       0.00000 |      26.27344 |       0.00014 |       0.18811\n",
      "     -0.00109 |       0.00000 |      26.15396 |       0.00013 |       0.18798\n",
      "     -0.00117 |       0.00000 |      26.05225 |       0.00016 |       0.18781\n",
      "     -0.00127 |       0.00000 |      25.93826 |       0.00012 |       0.18726\n",
      "Evaluating losses...\n",
      "     -0.00147 |       0.00000 |      25.79859 |       0.00017 |       0.18760\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.6          |\n",
      "| EpRewMean       | -93.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7311          |\n",
      "| TimeElapsed     | 1.61e+03      |\n",
      "| TimestepsSoFar  | 765952        |\n",
      "| ev_tdlam_before | 0.895         |\n",
      "| loss_ent        | 0.18759717    |\n",
      "| loss_kl         | 0.00017264305 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0014733904 |\n",
      "| loss_vf_loss    | 25.798592     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 187 ************\n",
      "Eval num_timesteps=765952, episode_reward=-90.20 +/- 21.09\n",
      "Episode length: 91.20 +/- 21.09\n",
      "Eval num_timesteps=765952, episode_reward=-82.00 +/- 10.85\n",
      "Episode length: 83.00 +/- 10.85\n",
      "Eval num_timesteps=765952, episode_reward=-83.00 +/- 13.14\n",
      "Episode length: 84.00 +/- 13.14\n",
      "Eval num_timesteps=765952, episode_reward=-80.20 +/- 5.25\n",
      "Episode length: 81.20 +/- 5.25\n",
      "Eval num_timesteps=765952, episode_reward=-75.50 +/- 7.58\n",
      "Episode length: 76.50 +/- 7.58\n",
      "Eval num_timesteps=765952, episode_reward=-80.40 +/- 8.26\n",
      "Episode length: 81.40 +/- 8.26\n",
      "Eval num_timesteps=765952, episode_reward=-82.80 +/- 13.87\n",
      "Episode length: 83.80 +/- 13.87\n",
      "Eval num_timesteps=765952, episode_reward=-81.30 +/- 5.85\n",
      "Episode length: 82.30 +/- 5.85\n",
      "Eval num_timesteps=765952, episode_reward=-78.90 +/- 5.37\n",
      "Episode length: 79.90 +/- 5.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00030 |       0.00000 |      52.64968 |      6.52e-05 |       0.21568\n",
      "     -0.00031 |       0.00000 |      51.65563 |       0.00012 |       0.21347\n",
      "     -0.00051 |       0.00000 |      51.12810 |      9.29e-05 |       0.21338\n",
      "     -0.00059 |       0.00000 |      50.74284 |       0.00012 |       0.21410\n",
      "     -0.00083 |       0.00000 |      50.46518 |       0.00012 |       0.21375\n",
      "     -0.00094 |       0.00000 |      50.14466 |       0.00014 |       0.21345\n",
      "     -0.00102 |       0.00000 |      49.92022 |       0.00011 |       0.21485\n",
      "     -0.00124 |       0.00000 |      49.78373 |       0.00014 |       0.21444\n",
      "     -0.00116 |       0.00000 |      49.55611 |       0.00015 |       0.21431\n",
      "     -0.00134 |       0.00000 |      49.42004 |       0.00016 |       0.21411\n",
      "Evaluating losses...\n",
      "     -0.00161 |       0.00000 |      49.20471 |       0.00017 |       0.21389\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.9          |\n",
      "| EpRewMean       | -90.9         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 7354          |\n",
      "| TimeElapsed     | 1.61e+03      |\n",
      "| TimestepsSoFar  | 770048        |\n",
      "| ev_tdlam_before | 0.799         |\n",
      "| loss_ent        | 0.21388505    |\n",
      "| loss_kl         | 0.00017004691 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016086902 |\n",
      "| loss_vf_loss    | 49.20471      |\n",
      "-----------------------------------\n",
      "********** Iteration 188 ************\n",
      "Eval num_timesteps=770048, episode_reward=-83.60 +/- 7.21\n",
      "Episode length: 84.60 +/- 7.21\n",
      "Eval num_timesteps=770048, episode_reward=-84.90 +/- 23.82\n",
      "Episode length: 85.90 +/- 23.82\n",
      "Eval num_timesteps=770048, episode_reward=-79.60 +/- 6.53\n",
      "Episode length: 80.60 +/- 6.53\n",
      "Eval num_timesteps=770048, episode_reward=-80.80 +/- 6.31\n",
      "Episode length: 81.80 +/- 6.31\n",
      "Eval num_timesteps=770048, episode_reward=-77.40 +/- 8.42\n",
      "Episode length: 78.40 +/- 8.42\n",
      "Eval num_timesteps=770048, episode_reward=-84.00 +/- 14.35\n",
      "Episode length: 85.00 +/- 14.35\n",
      "Eval num_timesteps=770048, episode_reward=-83.80 +/- 13.72\n",
      "Episode length: 84.80 +/- 13.72\n",
      "Eval num_timesteps=770048, episode_reward=-79.10 +/- 6.58\n",
      "Episode length: 80.10 +/- 6.58\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00016 |       0.00000 |      24.96259 |      3.61e-05 |       0.19819\n",
      "     -0.00049 |       0.00000 |      24.17693 |      4.71e-05 |       0.19710\n",
      "     -0.00085 |       0.00000 |      23.88011 |      8.46e-05 |       0.19761\n",
      "     -0.00110 |       0.00000 |      23.71745 |      1.00e-04 |       0.19809\n",
      "     -0.00120 |       0.00000 |      23.55564 |       0.00012 |       0.19600\n",
      "     -0.00148 |       0.00000 |      23.44038 |       0.00015 |       0.19796\n",
      "     -0.00143 |       0.00000 |      23.35749 |       0.00013 |       0.19850\n",
      "     -0.00151 |       0.00000 |      23.26392 |       0.00013 |       0.19738\n",
      "     -0.00167 |       0.00000 |      23.17819 |       0.00014 |       0.19764\n",
      "     -0.00185 |       0.00000 |      23.13230 |       0.00014 |       0.19709\n",
      "Evaluating losses...\n",
      "     -0.00212 |       0.00000 |      22.99819 |       0.00013 |       0.19802\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.8          |\n",
      "| EpRewMean       | -90.8         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7400          |\n",
      "| TimeElapsed     | 1.62e+03      |\n",
      "| TimestepsSoFar  | 774144        |\n",
      "| ev_tdlam_before | 0.908         |\n",
      "| loss_ent        | 0.19802496    |\n",
      "| loss_kl         | 0.0001288885  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0021160473 |\n",
      "| loss_vf_loss    | 22.998186     |\n",
      "-----------------------------------\n",
      "********** Iteration 189 ************\n",
      "Eval num_timesteps=774144, episode_reward=-75.30 +/- 8.10\n",
      "Episode length: 76.30 +/- 8.10\n",
      "Eval num_timesteps=774144, episode_reward=-76.50 +/- 4.65\n",
      "Episode length: 77.50 +/- 4.65\n",
      "Eval num_timesteps=774144, episode_reward=-81.40 +/- 9.83\n",
      "Episode length: 82.40 +/- 9.83\n",
      "Eval num_timesteps=774144, episode_reward=-86.40 +/- 30.22\n",
      "Episode length: 87.40 +/- 30.22\n",
      "Eval num_timesteps=774144, episode_reward=-87.00 +/- 11.77\n",
      "Episode length: 88.00 +/- 11.77\n",
      "Eval num_timesteps=774144, episode_reward=-92.30 +/- 22.20\n",
      "Episode length: 93.30 +/- 22.20\n",
      "Eval num_timesteps=774144, episode_reward=-82.80 +/- 11.26\n",
      "Episode length: 83.80 +/- 11.26\n",
      "Eval num_timesteps=774144, episode_reward=-82.20 +/- 18.72\n",
      "Episode length: 83.20 +/- 18.72\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00013 |       0.00000 |      22.48521 |      4.03e-05 |       0.19193\n",
      "     -0.00066 |       0.00000 |      22.04993 |      8.98e-05 |       0.19174\n",
      "     -0.00098 |       0.00000 |      21.80361 |       0.00011 |       0.19183\n",
      "     -0.00100 |       0.00000 |      21.59252 |       0.00012 |       0.19158\n",
      "     -0.00116 |       0.00000 |      21.43773 |       0.00013 |       0.19126\n",
      "     -0.00136 |       0.00000 |      21.30231 |       0.00015 |       0.19241\n",
      "     -0.00136 |       0.00000 |      21.19921 |       0.00016 |       0.19097\n",
      "     -0.00143 |       0.00000 |      21.11381 |       0.00018 |       0.19104\n",
      "     -0.00152 |       0.00000 |      21.02179 |       0.00014 |       0.19149\n",
      "     -0.00161 |       0.00000 |      20.93729 |       0.00014 |       0.19159\n",
      "Evaluating losses...\n",
      "     -0.00162 |       0.00000 |      20.83409 |       0.00023 |       0.19004\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.7          |\n",
      "| EpRewMean       | -87.7         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 7447          |\n",
      "| TimeElapsed     | 1.63e+03      |\n",
      "| TimestepsSoFar  | 778240        |\n",
      "| ev_tdlam_before | 0.92          |\n",
      "| loss_ent        | 0.19004059    |\n",
      "| loss_kl         | 0.00022519799 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016171816 |\n",
      "| loss_vf_loss    | 20.834085     |\n",
      "-----------------------------------\n",
      "********** Iteration 190 ************\n",
      "Eval num_timesteps=778240, episode_reward=-78.40 +/- 6.37\n",
      "Episode length: 79.40 +/- 6.37\n",
      "Eval num_timesteps=778240, episode_reward=-84.00 +/- 7.09\n",
      "Episode length: 85.00 +/- 7.09\n",
      "Eval num_timesteps=778240, episode_reward=-88.00 +/- 39.05\n",
      "Episode length: 89.00 +/- 39.05\n",
      "Eval num_timesteps=778240, episode_reward=-99.10 +/- 69.25\n",
      "Episode length: 100.10 +/- 69.25\n",
      "Eval num_timesteps=778240, episode_reward=-79.40 +/- 5.95\n",
      "Episode length: 80.40 +/- 5.95\n",
      "Eval num_timesteps=778240, episode_reward=-81.10 +/- 10.56\n",
      "Episode length: 82.10 +/- 10.56\n",
      "Eval num_timesteps=778240, episode_reward=-76.50 +/- 8.48\n",
      "Episode length: 77.50 +/- 8.48\n",
      "Eval num_timesteps=778240, episode_reward=-92.70 +/- 36.82\n",
      "Episode length: 93.70 +/- 36.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.89e-05 |       0.00000 |      26.28048 |      5.69e-05 |       0.18173\n",
      "     -0.00050 |       0.00000 |      25.83510 |      9.34e-05 |       0.18251\n",
      "     -0.00066 |       0.00000 |      25.44431 |      9.85e-05 |       0.18191\n",
      "     -0.00097 |       0.00000 |      25.17967 |       0.00012 |       0.18179\n",
      "     -0.00095 |       0.00000 |      24.98368 |       0.00011 |       0.18207\n",
      "     -0.00108 |       0.00000 |      24.79534 |       0.00011 |       0.18178\n",
      "     -0.00112 |       0.00000 |      24.62606 |       0.00011 |       0.18185\n",
      "     -0.00129 |       0.00000 |      24.52158 |       0.00011 |       0.18173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00142 |       0.00000 |      24.40597 |       0.00011 |       0.18176\n",
      "     -0.00129 |       0.00000 |      24.29957 |       0.00011 |       0.18188\n",
      "Evaluating losses...\n",
      "     -0.00164 |       0.00000 |      24.16348 |       0.00013 |       0.18157\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.9          |\n",
      "| EpRewMean       | -86.9         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7493          |\n",
      "| TimeElapsed     | 1.63e+03      |\n",
      "| TimestepsSoFar  | 782336        |\n",
      "| ev_tdlam_before | 0.905         |\n",
      "| loss_ent        | 0.18156932    |\n",
      "| loss_kl         | 0.00013288135 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016368185 |\n",
      "| loss_vf_loss    | 24.163477     |\n",
      "-----------------------------------\n",
      "********** Iteration 191 ************\n",
      "Eval num_timesteps=782336, episode_reward=-79.10 +/- 9.65\n",
      "Episode length: 80.10 +/- 9.65\n",
      "Eval num_timesteps=782336, episode_reward=-80.30 +/- 7.20\n",
      "Episode length: 81.30 +/- 7.20\n",
      "Eval num_timesteps=782336, episode_reward=-77.50 +/- 8.75\n",
      "Episode length: 78.50 +/- 8.75\n",
      "Eval num_timesteps=782336, episode_reward=-82.80 +/- 6.63\n",
      "Episode length: 83.80 +/- 6.63\n",
      "Eval num_timesteps=782336, episode_reward=-85.10 +/- 10.93\n",
      "Episode length: 86.10 +/- 10.93\n",
      "Eval num_timesteps=782336, episode_reward=-78.40 +/- 7.85\n",
      "Episode length: 79.40 +/- 7.85\n",
      "Eval num_timesteps=782336, episode_reward=-83.60 +/- 15.40\n",
      "Episode length: 84.60 +/- 15.40\n",
      "Eval num_timesteps=782336, episode_reward=-82.10 +/- 15.08\n",
      "Episode length: 83.10 +/- 15.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.42e-05 |       0.00000 |      27.29332 |      2.93e-05 |       0.19036\n",
      "     -0.00046 |       0.00000 |      26.33124 |      9.95e-05 |       0.18872\n",
      "     -0.00070 |       0.00000 |      25.81345 |      8.35e-05 |       0.18967\n",
      "     -0.00078 |       0.00000 |      25.39945 |       0.00010 |       0.18913\n",
      "     -0.00106 |       0.00000 |      25.12106 |      9.99e-05 |       0.18876\n",
      "     -0.00097 |       0.00000 |      24.88519 |       0.00012 |       0.18983\n",
      "     -0.00118 |       0.00000 |      24.66994 |       0.00011 |       0.18950\n",
      "     -0.00116 |       0.00000 |      24.53266 |       0.00013 |       0.18902\n",
      "     -0.00120 |       0.00000 |      24.33990 |       0.00013 |       0.18917\n",
      "     -0.00137 |       0.00000 |      24.20750 |       0.00015 |       0.18979\n",
      "Evaluating losses...\n",
      "     -0.00140 |       0.00000 |      24.07867 |       0.00016 |       0.18910\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.4          |\n",
      "| EpRewMean       | -88.4         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7539          |\n",
      "| TimeElapsed     | 1.64e+03      |\n",
      "| TimestepsSoFar  | 786432        |\n",
      "| ev_tdlam_before | 0.897         |\n",
      "| loss_ent        | 0.18910047    |\n",
      "| loss_kl         | 0.00015577125 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0014001664 |\n",
      "| loss_vf_loss    | 24.078672     |\n",
      "-----------------------------------\n",
      "********** Iteration 192 ************\n",
      "Eval num_timesteps=786432, episode_reward=-74.60 +/- 5.78\n",
      "Episode length: 75.60 +/- 5.78\n",
      "Eval num_timesteps=786432, episode_reward=-93.40 +/- 39.56\n",
      "Episode length: 94.40 +/- 39.56\n",
      "Eval num_timesteps=786432, episode_reward=-76.30 +/- 7.67\n",
      "Episode length: 77.30 +/- 7.67\n",
      "Eval num_timesteps=786432, episode_reward=-75.60 +/- 8.64\n",
      "Episode length: 76.60 +/- 8.64\n",
      "Eval num_timesteps=786432, episode_reward=-71.50 +/- 7.53\n",
      "Episode length: 72.50 +/- 7.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=786432, episode_reward=-82.60 +/- 10.96\n",
      "Episode length: 83.60 +/- 10.96\n",
      "Eval num_timesteps=786432, episode_reward=-81.80 +/- 7.25\n",
      "Episode length: 82.80 +/- 7.25\n",
      "Eval num_timesteps=786432, episode_reward=-75.90 +/- 5.05\n",
      "Episode length: 76.90 +/- 5.05\n",
      "Eval num_timesteps=786432, episode_reward=-76.60 +/- 7.61\n",
      "Episode length: 77.60 +/- 7.61\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.61e-06 |       0.00000 |      32.66227 |      3.34e-05 |       0.17934\n",
      "     -0.00029 |       0.00000 |      32.17026 |      6.27e-05 |       0.17893\n",
      "     -0.00068 |       0.00000 |      31.82892 |      7.14e-05 |       0.17923\n",
      "     -0.00085 |       0.00000 |      31.52250 |      8.15e-05 |       0.17943\n",
      "     -0.00082 |       0.00000 |      31.30856 |      7.77e-05 |       0.17816\n",
      "     -0.00092 |       0.00000 |      31.14063 |      9.62e-05 |       0.17865\n",
      "     -0.00099 |       0.00000 |      30.99577 |      8.60e-05 |       0.17859\n",
      "     -0.00108 |       0.00000 |      30.82831 |       0.00011 |       0.17895\n",
      "     -0.00106 |       0.00000 |      30.72750 |       0.00011 |       0.17847\n",
      "     -0.00113 |       0.00000 |      30.58689 |       0.00014 |       0.17939\n",
      "Evaluating losses...\n",
      "     -0.00133 |       0.00000 |      30.45165 |       0.00012 |       0.17834\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.9          |\n",
      "| EpRewMean       | -87.9         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7585          |\n",
      "| TimeElapsed     | 1.65e+03      |\n",
      "| TimestepsSoFar  | 790528        |\n",
      "| ev_tdlam_before | 0.877         |\n",
      "| loss_ent        | 0.17834044    |\n",
      "| loss_kl         | 0.00011998926 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013332157 |\n",
      "| loss_vf_loss    | 30.45165      |\n",
      "-----------------------------------\n",
      "********** Iteration 193 ************\n",
      "Eval num_timesteps=790528, episode_reward=-82.50 +/- 10.92\n",
      "Episode length: 83.50 +/- 10.92\n",
      "Eval num_timesteps=790528, episode_reward=-78.50 +/- 9.17\n",
      "Episode length: 79.50 +/- 9.17\n",
      "Eval num_timesteps=790528, episode_reward=-82.70 +/- 5.06\n",
      "Episode length: 83.70 +/- 5.06\n",
      "Eval num_timesteps=790528, episode_reward=-78.40 +/- 7.16\n",
      "Episode length: 79.40 +/- 7.16\n",
      "Eval num_timesteps=790528, episode_reward=-80.60 +/- 9.05\n",
      "Episode length: 81.60 +/- 9.05\n",
      "Eval num_timesteps=790528, episode_reward=-87.40 +/- 13.47\n",
      "Episode length: 88.40 +/- 13.47\n",
      "Eval num_timesteps=790528, episode_reward=-79.20 +/- 9.25\n",
      "Episode length: 80.20 +/- 9.25\n",
      "Eval num_timesteps=790528, episode_reward=-83.90 +/- 8.43\n",
      "Episode length: 84.90 +/- 8.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.14e-06 |       0.00000 |      29.85156 |      4.30e-05 |       0.22225\n",
      "     -0.00032 |       0.00000 |      25.41443 |      8.01e-05 |       0.22217\n",
      "     -0.00055 |       0.00000 |      24.61191 |      9.55e-05 |       0.22272\n",
      "     -0.00100 |       0.00000 |      24.10185 |      9.17e-05 |       0.22241\n",
      "     -0.00071 |       0.00000 |      23.74010 |       0.00011 |       0.22213\n",
      "     -0.00099 |       0.00000 |      23.44583 |       0.00010 |       0.22208\n",
      "     -0.00117 |       0.00000 |      23.20141 |       0.00011 |       0.22208\n",
      "     -0.00102 |       0.00000 |      23.00159 |       0.00014 |       0.22291\n",
      "     -0.00126 |       0.00000 |      22.82303 |       0.00015 |       0.22266\n",
      "     -0.00120 |       0.00000 |      22.67299 |       0.00014 |       0.22231\n",
      "Evaluating losses...\n",
      "     -0.00175 |       0.00000 |      22.49783 |       0.00013 |       0.22274\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.4          |\n",
      "| EpRewMean       | -90.4         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 7629          |\n",
      "| TimeElapsed     | 1.65e+03      |\n",
      "| TimestepsSoFar  | 794624        |\n",
      "| ev_tdlam_before | 0.858         |\n",
      "| loss_ent        | 0.222743      |\n",
      "| loss_kl         | 0.00013289496 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0017472672 |\n",
      "| loss_vf_loss    | 22.497833     |\n",
      "-----------------------------------\n",
      "********** Iteration 194 ************\n",
      "Eval num_timesteps=794624, episode_reward=-86.20 +/- 10.45\n",
      "Episode length: 87.20 +/- 10.45\n",
      "Eval num_timesteps=794624, episode_reward=-79.00 +/- 6.71\n",
      "Episode length: 80.00 +/- 6.71\n",
      "Eval num_timesteps=794624, episode_reward=-85.50 +/- 25.30\n",
      "Episode length: 86.50 +/- 25.30\n",
      "Eval num_timesteps=794624, episode_reward=-83.50 +/- 7.37\n",
      "Episode length: 84.50 +/- 7.37\n",
      "Eval num_timesteps=794624, episode_reward=-90.20 +/- 32.44\n",
      "Episode length: 91.20 +/- 32.44\n",
      "Eval num_timesteps=794624, episode_reward=-98.60 +/- 58.13\n",
      "Episode length: 99.60 +/- 58.13\n",
      "Eval num_timesteps=794624, episode_reward=-85.20 +/- 9.91\n",
      "Episode length: 86.20 +/- 9.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=794624, episode_reward=-72.60 +/- 8.49\n",
      "Episode length: 73.60 +/- 8.49\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -8.81e-05 |       0.00000 |      30.49960 |      5.13e-05 |       0.18273\n",
      "     -0.00050 |       0.00000 |      29.99010 |      7.92e-05 |       0.18259\n",
      "     -0.00067 |       0.00000 |      29.71490 |      5.86e-05 |       0.18252\n",
      "     -0.00074 |       0.00000 |      29.51040 |      6.21e-05 |       0.18205\n",
      "     -0.00088 |       0.00000 |      29.35060 |      8.81e-05 |       0.18298\n",
      "     -0.00088 |       0.00000 |      29.22084 |      9.73e-05 |       0.18212\n",
      "     -0.00099 |       0.00000 |      29.13793 |      7.88e-05 |       0.18277\n",
      "     -0.00103 |       0.00000 |      29.03695 |      6.88e-05 |       0.18268\n",
      "     -0.00112 |       0.00000 |      28.90564 |      7.88e-05 |       0.18296\n",
      "     -0.00115 |       0.00000 |      28.82027 |      9.41e-05 |       0.18293\n",
      "Evaluating losses...\n",
      "     -0.00135 |       0.00000 |      28.69512 |      9.96e-05 |       0.18239\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.4          |\n",
      "| EpRewMean       | -88.4         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 7676          |\n",
      "| TimeElapsed     | 1.66e+03      |\n",
      "| TimestepsSoFar  | 798720        |\n",
      "| ev_tdlam_before | 0.885         |\n",
      "| loss_ent        | 0.18239097    |\n",
      "| loss_kl         | 9.9643104e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013548135 |\n",
      "| loss_vf_loss    | 28.69512      |\n",
      "-----------------------------------\n",
      "********** Iteration 195 ************\n",
      "Eval num_timesteps=798720, episode_reward=-78.60 +/- 7.28\n",
      "Episode length: 79.60 +/- 7.28\n",
      "Eval num_timesteps=798720, episode_reward=-80.50 +/- 10.42\n",
      "Episode length: 81.50 +/- 10.42\n",
      "Eval num_timesteps=798720, episode_reward=-79.50 +/- 12.99\n",
      "Episode length: 80.50 +/- 12.99\n",
      "Eval num_timesteps=798720, episode_reward=-80.30 +/- 7.71\n",
      "Episode length: 81.30 +/- 7.71\n",
      "Eval num_timesteps=798720, episode_reward=-81.40 +/- 9.24\n",
      "Episode length: 82.40 +/- 9.24\n",
      "Eval num_timesteps=798720, episode_reward=-90.70 +/- 38.46\n",
      "Episode length: 91.70 +/- 38.46\n",
      "Eval num_timesteps=798720, episode_reward=-80.90 +/- 9.10\n",
      "Episode length: 81.90 +/- 9.10\n",
      "Eval num_timesteps=798720, episode_reward=-83.30 +/- 10.41\n",
      "Episode length: 84.30 +/- 10.41\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.90e-05 |       0.00000 |      29.03258 |      2.32e-05 |       0.17933\n",
      "     -0.00026 |       0.00000 |      28.55508 |      7.20e-05 |       0.18087\n",
      "     -0.00043 |       0.00000 |      28.29530 |      5.09e-05 |       0.18133\n",
      "     -0.00056 |       0.00000 |      28.11219 |      4.85e-05 |       0.18070\n",
      "     -0.00073 |       0.00000 |      27.93734 |      7.69e-05 |       0.18092\n",
      "     -0.00085 |       0.00000 |      27.83250 |      5.47e-05 |       0.18083\n",
      "     -0.00089 |       0.00000 |      27.69647 |      8.68e-05 |       0.18128\n",
      "     -0.00100 |       0.00000 |      27.59396 |      7.30e-05 |       0.18155\n",
      "     -0.00096 |       0.00000 |      27.51815 |      8.67e-05 |       0.18083\n",
      "     -0.00114 |       0.00000 |      27.44125 |      9.63e-05 |       0.18142\n",
      "Evaluating losses...\n",
      "     -0.00108 |       0.00000 |      27.32295 |      9.31e-05 |       0.18068\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.2          |\n",
      "| EpRewMean       | -87.2         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7722          |\n",
      "| TimeElapsed     | 1.67e+03      |\n",
      "| TimestepsSoFar  | 802816        |\n",
      "| ev_tdlam_before | 0.889         |\n",
      "| loss_ent        | 0.18068247    |\n",
      "| loss_kl         | 9.3135604e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010846569 |\n",
      "| loss_vf_loss    | 27.322954     |\n",
      "-----------------------------------\n",
      "********** Iteration 196 ************\n",
      "Eval num_timesteps=802816, episode_reward=-99.50 +/- 39.65\n",
      "Episode length: 100.50 +/- 39.65\n",
      "Eval num_timesteps=802816, episode_reward=-77.90 +/- 7.89\n",
      "Episode length: 78.90 +/- 7.89\n",
      "Eval num_timesteps=802816, episode_reward=-78.20 +/- 12.50\n",
      "Episode length: 79.20 +/- 12.50\n",
      "Eval num_timesteps=802816, episode_reward=-81.80 +/- 9.59\n",
      "Episode length: 82.80 +/- 9.59\n",
      "Eval num_timesteps=802816, episode_reward=-78.40 +/- 8.33\n",
      "Episode length: 79.40 +/- 8.33\n",
      "Eval num_timesteps=802816, episode_reward=-78.20 +/- 4.28\n",
      "Episode length: 79.20 +/- 4.28\n",
      "Eval num_timesteps=802816, episode_reward=-74.50 +/- 7.13\n",
      "Episode length: 75.50 +/- 7.13\n",
      "Eval num_timesteps=802816, episode_reward=-87.20 +/- 27.18\n",
      "Episode length: 88.20 +/- 27.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     7.07e-05 |       0.00000 |      34.15831 |      2.81e-05 |       0.20499\n",
      "     -0.00038 |       0.00000 |      32.59087 |      7.01e-05 |       0.20423\n",
      "     -0.00067 |       0.00000 |      31.68411 |      6.95e-05 |       0.20444\n",
      "     -0.00092 |       0.00000 |      31.10513 |       0.00010 |       0.20511\n",
      "     -0.00101 |       0.00000 |      30.69558 |       0.00010 |       0.20499\n",
      "     -0.00088 |       0.00000 |      30.37796 |      9.37e-05 |       0.20558\n",
      "     -0.00104 |       0.00000 |      30.15279 |       0.00010 |       0.20499\n",
      "     -0.00111 |       0.00000 |      29.95427 |       0.00012 |       0.20573\n",
      "     -0.00108 |       0.00000 |      29.78361 |       0.00014 |       0.20537\n",
      "     -0.00119 |       0.00000 |      29.66265 |       0.00010 |       0.20475\n",
      "Evaluating losses...\n",
      "     -0.00146 |       0.00000 |      29.50448 |       0.00011 |       0.20575\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.9          |\n",
      "| EpRewMean       | -88.9         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 7765          |\n",
      "| TimeElapsed     | 1.67e+03      |\n",
      "| TimestepsSoFar  | 806912        |\n",
      "| ev_tdlam_before | 0.865         |\n",
      "| loss_ent        | 0.20574574    |\n",
      "| loss_kl         | 0.00011244765 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.001455473  |\n",
      "| loss_vf_loss    | 29.504482     |\n",
      "-----------------------------------\n",
      "********** Iteration 197 ************\n",
      "Eval num_timesteps=806912, episode_reward=-80.00 +/- 5.74\n",
      "Episode length: 81.00 +/- 5.74\n",
      "Eval num_timesteps=806912, episode_reward=-88.90 +/- 19.78\n",
      "Episode length: 89.90 +/- 19.78\n",
      "Eval num_timesteps=806912, episode_reward=-83.70 +/- 15.60\n",
      "Episode length: 84.70 +/- 15.60\n",
      "Eval num_timesteps=806912, episode_reward=-80.30 +/- 14.55\n",
      "Episode length: 81.30 +/- 14.55\n",
      "Eval num_timesteps=806912, episode_reward=-94.40 +/- 33.03\n",
      "Episode length: 95.40 +/- 33.03\n",
      "Eval num_timesteps=806912, episode_reward=-129.10 +/- 124.38\n",
      "Episode length: 130.00 +/- 124.08\n",
      "Eval num_timesteps=806912, episode_reward=-80.80 +/- 5.15\n",
      "Episode length: 81.80 +/- 5.15\n",
      "Eval num_timesteps=806912, episode_reward=-90.80 +/- 44.11\n",
      "Episode length: 91.80 +/- 44.11\n",
      "Eval num_timesteps=806912, episode_reward=-80.20 +/- 8.77\n",
      "Episode length: 81.20 +/- 8.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.34e-06 |       0.00000 |      39.77987 |      3.02e-05 |       0.20013\n",
      "     -0.00049 |       0.00000 |      38.67757 |      5.06e-05 |       0.19989\n",
      "     -0.00086 |       0.00000 |      38.21592 |      9.34e-05 |       0.19973\n",
      "     -0.00105 |       0.00000 |      37.82585 |      9.78e-05 |       0.20099\n",
      "     -0.00122 |       0.00000 |      37.52107 |       0.00012 |       0.20125\n",
      "     -0.00133 |       0.00000 |      37.27242 |       0.00013 |       0.20095\n",
      "     -0.00137 |       0.00000 |      37.05817 |       0.00015 |       0.20108\n",
      "     -0.00153 |       0.00000 |      36.87119 |       0.00015 |       0.20168\n",
      "     -0.00158 |       0.00000 |      36.70163 |       0.00017 |       0.20177\n",
      "     -0.00171 |       0.00000 |      36.51727 |       0.00015 |       0.20127\n",
      "Evaluating losses...\n",
      "     -0.00186 |       0.00000 |      36.32556 |       0.00015 |       0.20186\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 7810          |\n",
      "| TimeElapsed     | 1.68e+03      |\n",
      "| TimestepsSoFar  | 811008        |\n",
      "| ev_tdlam_before | 0.843         |\n",
      "| loss_ent        | 0.2018555     |\n",
      "| loss_kl         | 0.00015151082 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018637409 |\n",
      "| loss_vf_loss    | 36.32556      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 198 ************\n",
      "Eval num_timesteps=811008, episode_reward=-74.80 +/- 6.34\n",
      "Episode length: 75.80 +/- 6.34\n",
      "Eval num_timesteps=811008, episode_reward=-81.90 +/- 10.89\n",
      "Episode length: 82.90 +/- 10.89\n",
      "Eval num_timesteps=811008, episode_reward=-78.00 +/- 8.80\n",
      "Episode length: 79.00 +/- 8.80\n",
      "Eval num_timesteps=811008, episode_reward=-82.80 +/- 3.94\n",
      "Episode length: 83.80 +/- 3.94\n",
      "Eval num_timesteps=811008, episode_reward=-94.00 +/- 25.60\n",
      "Episode length: 95.00 +/- 25.60\n",
      "Eval num_timesteps=811008, episode_reward=-77.70 +/- 9.12\n",
      "Episode length: 78.70 +/- 9.12\n",
      "Eval num_timesteps=811008, episode_reward=-77.20 +/- 7.57\n",
      "Episode length: 78.20 +/- 7.57\n",
      "Eval num_timesteps=811008, episode_reward=-80.10 +/- 15.06\n",
      "Episode length: 81.10 +/- 15.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.13e-05 |       0.00000 |      33.75390 |      3.12e-05 |       0.19801\n",
      "     -0.00052 |       0.00000 |      31.84686 |      7.30e-05 |       0.19799\n",
      "     -0.00062 |       0.00000 |      31.12528 |      5.36e-05 |       0.19831\n",
      "     -0.00081 |       0.00000 |      30.72251 |      7.89e-05 |       0.19795\n",
      "     -0.00088 |       0.00000 |      30.49989 |      8.00e-05 |       0.19825\n",
      "     -0.00090 |       0.00000 |      30.29567 |      7.23e-05 |       0.19801\n",
      "     -0.00095 |       0.00000 |      30.11177 |      8.16e-05 |       0.19840\n",
      "     -0.00112 |       0.00000 |      29.94953 |      7.79e-05 |       0.19829\n",
      "     -0.00107 |       0.00000 |      29.81477 |      7.57e-05 |       0.19886\n",
      "     -0.00119 |       0.00000 |      29.73312 |      8.46e-05 |       0.19849\n",
      "Evaluating losses...\n",
      "     -0.00139 |       0.00000 |      29.55888 |      9.84e-05 |       0.19805\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.5          |\n",
      "| EpRewMean       | -91.5         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 7854          |\n",
      "| TimeElapsed     | 1.69e+03      |\n",
      "| TimestepsSoFar  | 815104        |\n",
      "| ev_tdlam_before | 0.866         |\n",
      "| loss_ent        | 0.19805321    |\n",
      "| loss_kl         | 9.844878e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013894693 |\n",
      "| loss_vf_loss    | 29.558878     |\n",
      "-----------------------------------\n",
      "********** Iteration 199 ************\n",
      "Eval num_timesteps=815104, episode_reward=-76.40 +/- 7.16\n",
      "Episode length: 77.40 +/- 7.16\n",
      "Eval num_timesteps=815104, episode_reward=-85.30 +/- 12.98\n",
      "Episode length: 86.30 +/- 12.98\n",
      "Eval num_timesteps=815104, episode_reward=-82.00 +/- 18.54\n",
      "Episode length: 83.00 +/- 18.54\n",
      "Eval num_timesteps=815104, episode_reward=-80.40 +/- 4.43\n",
      "Episode length: 81.40 +/- 4.43\n",
      "Eval num_timesteps=815104, episode_reward=-81.60 +/- 6.95\n",
      "Episode length: 82.60 +/- 6.95\n",
      "Eval num_timesteps=815104, episode_reward=-88.20 +/- 22.99\n",
      "Episode length: 89.20 +/- 22.99\n",
      "Eval num_timesteps=815104, episode_reward=-85.20 +/- 19.04\n",
      "Episode length: 86.20 +/- 19.04\n",
      "Eval num_timesteps=815104, episode_reward=-81.20 +/- 13.75\n",
      "Episode length: 82.20 +/- 13.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00013 |       0.00000 |      37.01228 |      4.21e-05 |       0.18489\n",
      "     -0.00045 |       0.00000 |      36.28050 |      4.59e-05 |       0.18470\n",
      "     -0.00075 |       0.00000 |      35.82380 |      5.72e-05 |       0.18458\n",
      "     -0.00089 |       0.00000 |      35.49012 |      7.09e-05 |       0.18472\n",
      "     -0.00094 |       0.00000 |      35.23874 |      7.62e-05 |       0.18477\n",
      "     -0.00102 |       0.00000 |      35.01704 |      9.04e-05 |       0.18549\n",
      "     -0.00117 |       0.00000 |      34.82772 |       0.00011 |       0.18501\n",
      "     -0.00116 |       0.00000 |      34.65720 |      7.58e-05 |       0.18496\n",
      "     -0.00131 |       0.00000 |      34.51706 |      7.90e-05 |       0.18498\n",
      "     -0.00125 |       0.00000 |      34.41260 |       0.00012 |       0.18570\n",
      "Evaluating losses...\n",
      "     -0.00139 |       0.00000 |      34.21565 |       0.00012 |       0.18513\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.6          |\n",
      "| EpRewMean       | -89.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7900          |\n",
      "| TimeElapsed     | 1.69e+03      |\n",
      "| TimestepsSoFar  | 819200        |\n",
      "| ev_tdlam_before | 0.854         |\n",
      "| loss_ent        | 0.18513408    |\n",
      "| loss_kl         | 0.00011858914 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013858143 |\n",
      "| loss_vf_loss    | 34.215652     |\n",
      "-----------------------------------\n",
      "********** Iteration 200 ************\n",
      "Eval num_timesteps=819200, episode_reward=-99.90 +/- 42.72\n",
      "Episode length: 100.90 +/- 42.72\n",
      "Eval num_timesteps=819200, episode_reward=-93.60 +/- 35.97\n",
      "Episode length: 94.60 +/- 35.97\n",
      "Eval num_timesteps=819200, episode_reward=-92.80 +/- 28.18\n",
      "Episode length: 93.80 +/- 28.18\n",
      "Eval num_timesteps=819200, episode_reward=-79.30 +/- 6.86\n",
      "Episode length: 80.30 +/- 6.86\n",
      "Eval num_timesteps=819200, episode_reward=-75.40 +/- 8.48\n",
      "Episode length: 76.40 +/- 8.48\n",
      "Eval num_timesteps=819200, episode_reward=-75.90 +/- 8.44\n",
      "Episode length: 76.90 +/- 8.44\n",
      "Eval num_timesteps=819200, episode_reward=-84.60 +/- 19.20\n",
      "Episode length: 85.60 +/- 19.20\n",
      "Eval num_timesteps=819200, episode_reward=-76.50 +/- 8.90\n",
      "Episode length: 77.50 +/- 8.90\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.86e-05 |       0.00000 |      24.36433 |      9.08e-05 |       0.17272\n",
      "     -0.00031 |       0.00000 |      23.66253 |      9.96e-05 |       0.17360\n",
      "     -0.00042 |       0.00000 |      23.36234 |      7.13e-05 |       0.17364\n",
      "     -0.00054 |       0.00000 |      23.12758 |      7.58e-05 |       0.17350\n",
      "     -0.00059 |       0.00000 |      22.97421 |      6.68e-05 |       0.17374\n",
      "     -0.00072 |       0.00000 |      22.82754 |      9.25e-05 |       0.17320\n",
      "     -0.00079 |       0.00000 |      22.72313 |      7.48e-05 |       0.17360\n",
      "     -0.00081 |       0.00000 |      22.60140 |      9.41e-05 |       0.17339\n",
      "     -0.00079 |       0.00000 |      22.50492 |      8.02e-05 |       0.17322\n",
      "     -0.00087 |       0.00000 |      22.43937 |       0.00011 |       0.17256\n",
      "Evaluating losses...\n",
      "     -0.00108 |       0.00000 |      22.31912 |      7.85e-05 |       0.17337\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.3          |\n",
      "| EpRewMean       | -85.3         |\n",
      "| EpThisIter      | 49            |\n",
      "| EpisodesSoFar   | 7949          |\n",
      "| TimeElapsed     | 1.7e+03       |\n",
      "| TimestepsSoFar  | 823296        |\n",
      "| ev_tdlam_before | 0.909         |\n",
      "| loss_ent        | 0.17337027    |\n",
      "| loss_kl         | 7.846677e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010823852 |\n",
      "| loss_vf_loss    | 22.319117     |\n",
      "-----------------------------------\n",
      "********** Iteration 201 ************\n",
      "Eval num_timesteps=823296, episode_reward=-81.50 +/- 8.41\n",
      "Episode length: 82.50 +/- 8.41\n",
      "Eval num_timesteps=823296, episode_reward=-78.90 +/- 7.30\n",
      "Episode length: 79.90 +/- 7.30\n",
      "Eval num_timesteps=823296, episode_reward=-79.00 +/- 7.73\n",
      "Episode length: 80.00 +/- 7.73\n",
      "Eval num_timesteps=823296, episode_reward=-75.60 +/- 10.90\n",
      "Episode length: 76.60 +/- 10.90\n",
      "Eval num_timesteps=823296, episode_reward=-82.80 +/- 10.26\n",
      "Episode length: 83.80 +/- 10.26\n",
      "Eval num_timesteps=823296, episode_reward=-82.00 +/- 11.17\n",
      "Episode length: 83.00 +/- 11.17\n",
      "Eval num_timesteps=823296, episode_reward=-84.00 +/- 24.65\n",
      "Episode length: 85.00 +/- 24.65\n",
      "Eval num_timesteps=823296, episode_reward=-78.10 +/- 4.57\n",
      "Episode length: 79.10 +/- 4.57\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.81e-05 |       0.00000 |      27.84708 |      3.10e-05 |       0.19559\n",
      "     -0.00032 |       0.00000 |      26.87534 |      4.93e-05 |       0.19505\n",
      "     -0.00048 |       0.00000 |      26.33642 |      6.29e-05 |       0.19522\n",
      "     -0.00067 |       0.00000 |      26.04743 |      6.42e-05 |       0.19495\n",
      "     -0.00079 |       0.00000 |      25.81013 |      7.41e-05 |       0.19529\n",
      "     -0.00088 |       0.00000 |      25.64032 |      8.20e-05 |       0.19553\n",
      "     -0.00086 |       0.00000 |      25.49822 |      7.85e-05 |       0.19565\n",
      "     -0.00105 |       0.00000 |      25.39439 |      7.87e-05 |       0.19582\n",
      "     -0.00096 |       0.00000 |      25.30581 |      7.64e-05 |       0.19585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00113 |       0.00000 |      25.19521 |      8.02e-05 |       0.19585\n",
      "Evaluating losses...\n",
      "     -0.00136 |       0.00000 |      25.08315 |      8.60e-05 |       0.19578\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.5          |\n",
      "| EpRewMean       | -85.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 7995          |\n",
      "| TimeElapsed     | 1.7e+03       |\n",
      "| TimestepsSoFar  | 827392        |\n",
      "| ev_tdlam_before | 0.895         |\n",
      "| loss_ent        | 0.19578248    |\n",
      "| loss_kl         | 8.601357e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013582108 |\n",
      "| loss_vf_loss    | 25.083145     |\n",
      "-----------------------------------\n",
      "********** Iteration 202 ************\n",
      "Eval num_timesteps=827392, episode_reward=-83.30 +/- 23.32\n",
      "Episode length: 84.30 +/- 23.32\n",
      "Eval num_timesteps=827392, episode_reward=-76.90 +/- 8.43\n",
      "Episode length: 77.90 +/- 8.43\n",
      "Eval num_timesteps=827392, episode_reward=-81.90 +/- 10.43\n",
      "Episode length: 82.90 +/- 10.43\n",
      "Eval num_timesteps=827392, episode_reward=-88.10 +/- 19.10\n",
      "Episode length: 89.10 +/- 19.10\n",
      "Eval num_timesteps=827392, episode_reward=-82.50 +/- 12.42\n",
      "Episode length: 83.50 +/- 12.42\n",
      "Eval num_timesteps=827392, episode_reward=-78.50 +/- 5.97\n",
      "Episode length: 79.50 +/- 5.97\n",
      "Eval num_timesteps=827392, episode_reward=-82.50 +/- 7.49\n",
      "Episode length: 83.50 +/- 7.49\n",
      "Eval num_timesteps=827392, episode_reward=-93.30 +/- 34.57\n",
      "Episode length: 94.30 +/- 34.57\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -2.90e-05 |       0.00000 |      38.63991 |      2.65e-05 |       0.18643\n",
      "     -0.00041 |       0.00000 |      37.27354 |      4.29e-05 |       0.18652\n",
      "     -0.00068 |       0.00000 |      36.36074 |      4.52e-05 |       0.18662\n",
      "     -0.00088 |       0.00000 |      35.66754 |      4.49e-05 |       0.18661\n",
      "     -0.00093 |       0.00000 |      35.12880 |      4.46e-05 |       0.18658\n",
      "     -0.00109 |       0.00000 |      34.72519 |      6.13e-05 |       0.18619\n",
      "     -0.00118 |       0.00000 |      34.34541 |      6.84e-05 |       0.18666\n",
      "     -0.00108 |       0.00000 |      34.06808 |      6.62e-05 |       0.18606\n",
      "     -0.00126 |       0.00000 |      33.80472 |      7.79e-05 |       0.18647\n",
      "     -0.00127 |       0.00000 |      33.58508 |      6.88e-05 |       0.18620\n",
      "Evaluating losses...\n",
      "     -0.00143 |       0.00000 |      33.38431 |      6.37e-05 |       0.18655\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.7          |\n",
      "| EpRewMean       | -87.7         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 8040          |\n",
      "| TimeElapsed     | 1.71e+03      |\n",
      "| TimestepsSoFar  | 831488        |\n",
      "| ev_tdlam_before | 0.846         |\n",
      "| loss_ent        | 0.18655317    |\n",
      "| loss_kl         | 6.370318e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0014325643 |\n",
      "| loss_vf_loss    | 33.38431      |\n",
      "-----------------------------------\n",
      "********** Iteration 203 ************\n",
      "Eval num_timesteps=831488, episode_reward=-93.00 +/- 34.87\n",
      "Episode length: 94.00 +/- 34.87\n",
      "Eval num_timesteps=831488, episode_reward=-78.20 +/- 10.11\n",
      "Episode length: 79.20 +/- 10.11\n",
      "Eval num_timesteps=831488, episode_reward=-96.30 +/- 50.30\n",
      "Episode length: 97.30 +/- 50.30\n",
      "Eval num_timesteps=831488, episode_reward=-80.20 +/- 9.45\n",
      "Episode length: 81.20 +/- 9.45\n",
      "Eval num_timesteps=831488, episode_reward=-85.50 +/- 12.37\n",
      "Episode length: 86.50 +/- 12.37\n",
      "Eval num_timesteps=831488, episode_reward=-81.10 +/- 8.14\n",
      "Episode length: 82.10 +/- 8.14\n",
      "Eval num_timesteps=831488, episode_reward=-80.90 +/- 8.06\n",
      "Episode length: 81.90 +/- 8.06\n",
      "Eval num_timesteps=831488, episode_reward=-73.50 +/- 9.07\n",
      "Episode length: 74.50 +/- 9.07\n",
      "Eval num_timesteps=831488, episode_reward=-96.50 +/- 39.71\n",
      "Episode length: 97.50 +/- 39.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.98e-05 |       0.00000 |      38.40552 |      2.33e-05 |       0.19293\n",
      "     -0.00028 |       0.00000 |      37.24590 |      3.48e-05 |       0.19323\n",
      "     -0.00032 |       0.00000 |      36.54132 |      4.26e-05 |       0.19355\n",
      "     -0.00055 |       0.00000 |      36.03991 |      4.66e-05 |       0.19331\n",
      "     -0.00053 |       0.00000 |      35.68858 |      4.94e-05 |       0.19287\n",
      "     -0.00068 |       0.00000 |      35.37324 |      6.14e-05 |       0.19332\n",
      "     -0.00058 |       0.00000 |      35.12960 |      5.98e-05 |       0.19328\n",
      "     -0.00085 |       0.00000 |      34.94346 |      6.77e-05 |       0.19364\n",
      "     -0.00088 |       0.00000 |      34.72658 |      6.48e-05 |       0.19359\n",
      "     -0.00092 |       0.00000 |      34.55961 |      6.23e-05 |       0.19353\n",
      "Evaluating losses...\n",
      "     -0.00120 |       0.00000 |      34.38523 |      4.69e-05 |       0.19346\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92            |\n",
      "| EpRewMean       | -91           |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 8084          |\n",
      "| TimeElapsed     | 1.72e+03      |\n",
      "| TimestepsSoFar  | 835584        |\n",
      "| ev_tdlam_before | 0.845         |\n",
      "| loss_ent        | 0.19345808    |\n",
      "| loss_kl         | 4.691442e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0011959111 |\n",
      "| loss_vf_loss    | 34.385227     |\n",
      "-----------------------------------\n",
      "********** Iteration 204 ************\n",
      "Eval num_timesteps=835584, episode_reward=-84.50 +/- 10.20\n",
      "Episode length: 85.50 +/- 10.20\n",
      "Eval num_timesteps=835584, episode_reward=-78.90 +/- 10.47\n",
      "Episode length: 79.90 +/- 10.47\n",
      "Eval num_timesteps=835584, episode_reward=-78.40 +/- 5.99\n",
      "Episode length: 79.40 +/- 5.99\n",
      "Eval num_timesteps=835584, episode_reward=-82.60 +/- 6.48\n",
      "Episode length: 83.60 +/- 6.48\n",
      "Eval num_timesteps=835584, episode_reward=-81.30 +/- 7.55\n",
      "Episode length: 82.30 +/- 7.55\n",
      "Eval num_timesteps=835584, episode_reward=-77.10 +/- 10.12\n",
      "Episode length: 78.10 +/- 10.12\n",
      "Eval num_timesteps=835584, episode_reward=-84.10 +/- 13.49\n",
      "Episode length: 85.10 +/- 13.49\n",
      "Eval num_timesteps=835584, episode_reward=-82.50 +/- 12.06\n",
      "Episode length: 83.50 +/- 12.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00031 |       0.00000 |      42.46108 |      3.75e-05 |       0.20633\n",
      "     -0.00052 |       0.00000 |      41.94455 |      5.56e-05 |       0.20752\n",
      "     -0.00083 |       0.00000 |      41.60454 |      5.99e-05 |       0.20740\n",
      "     -0.00087 |       0.00000 |      41.31424 |      6.84e-05 |       0.20730\n",
      "     -0.00106 |       0.00000 |      41.08199 |      8.26e-05 |       0.20713\n",
      "     -0.00107 |       0.00000 |      40.80803 |      6.79e-05 |       0.20803\n",
      "     -0.00111 |       0.00000 |      40.63423 |      7.11e-05 |       0.20687\n",
      "     -0.00128 |       0.00000 |      40.43195 |      7.45e-05 |       0.20787\n",
      "     -0.00126 |       0.00000 |      40.25095 |      9.45e-05 |       0.20797\n",
      "     -0.00129 |       0.00000 |      40.09174 |      8.71e-05 |       0.20756\n",
      "Evaluating losses...\n",
      "     -0.00157 |       0.00000 |      39.92601 |      7.38e-05 |       0.20706\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.8          |\n",
      "| EpRewMean       | -90.8         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 8129          |\n",
      "| TimeElapsed     | 1.72e+03      |\n",
      "| TimestepsSoFar  | 839680        |\n",
      "| ev_tdlam_before | 0.829         |\n",
      "| loss_ent        | 0.20705715    |\n",
      "| loss_kl         | 7.375587e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015697967 |\n",
      "| loss_vf_loss    | 39.926006     |\n",
      "-----------------------------------\n",
      "********** Iteration 205 ************\n",
      "Eval num_timesteps=839680, episode_reward=-77.60 +/- 7.06\n",
      "Episode length: 78.60 +/- 7.06\n",
      "Eval num_timesteps=839680, episode_reward=-93.90 +/- 42.60\n",
      "Episode length: 94.90 +/- 42.60\n",
      "Eval num_timesteps=839680, episode_reward=-84.80 +/- 21.35\n",
      "Episode length: 85.80 +/- 21.35\n",
      "Eval num_timesteps=839680, episode_reward=-96.60 +/- 73.09\n",
      "Episode length: 97.60 +/- 73.09\n",
      "Eval num_timesteps=839680, episode_reward=-74.50 +/- 7.89\n",
      "Episode length: 75.50 +/- 7.89\n",
      "Eval num_timesteps=839680, episode_reward=-78.50 +/- 6.31\n",
      "Episode length: 79.50 +/- 6.31\n",
      "Eval num_timesteps=839680, episode_reward=-85.10 +/- 21.46\n",
      "Episode length: 86.10 +/- 21.46\n",
      "Eval num_timesteps=839680, episode_reward=-76.60 +/- 10.84\n",
      "Episode length: 77.60 +/- 10.84\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.56e-05 |       0.00000 |      33.19708 |      2.15e-05 |       0.19430\n",
      "     -0.00067 |       0.00000 |      32.14586 |      7.00e-05 |       0.19554\n",
      "     -0.00084 |       0.00000 |      31.61902 |      8.35e-05 |       0.19567\n",
      "     -0.00097 |       0.00000 |      31.30832 |      8.45e-05 |       0.19612\n",
      "     -0.00101 |       0.00000 |      31.05972 |       0.00011 |       0.19607\n",
      "     -0.00113 |       0.00000 |      30.88098 |       0.00011 |       0.19628\n",
      "     -0.00121 |       0.00000 |      30.74255 |       0.00011 |       0.19620\n",
      "     -0.00131 |       0.00000 |      30.61852 |       0.00011 |       0.19625\n",
      "     -0.00128 |       0.00000 |      30.49660 |       0.00012 |       0.19631\n",
      "     -0.00143 |       0.00000 |      30.41622 |       0.00012 |       0.19636\n",
      "Evaluating losses...\n",
      "     -0.00156 |       0.00000 |      30.28696 |       0.00013 |       0.19682\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.5          |\n",
      "| EpRewMean       | -89.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 8175          |\n",
      "| TimeElapsed     | 1.73e+03      |\n",
      "| TimestepsSoFar  | 843776        |\n",
      "| ev_tdlam_before | 0.869         |\n",
      "| loss_ent        | 0.19682173    |\n",
      "| loss_kl         | 0.00012828573 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015591262 |\n",
      "| loss_vf_loss    | 30.286964     |\n",
      "-----------------------------------\n",
      "********** Iteration 206 ************\n",
      "Eval num_timesteps=843776, episode_reward=-94.40 +/- 35.63\n",
      "Episode length: 95.40 +/- 35.63\n",
      "Eval num_timesteps=843776, episode_reward=-85.30 +/- 12.81\n",
      "Episode length: 86.30 +/- 12.81\n",
      "Eval num_timesteps=843776, episode_reward=-88.80 +/- 19.00\n",
      "Episode length: 89.80 +/- 19.00\n",
      "Eval num_timesteps=843776, episode_reward=-75.60 +/- 8.39\n",
      "Episode length: 76.60 +/- 8.39\n",
      "Eval num_timesteps=843776, episode_reward=-78.90 +/- 13.60\n",
      "Episode length: 79.90 +/- 13.60\n",
      "Eval num_timesteps=843776, episode_reward=-87.60 +/- 28.30\n",
      "Episode length: 88.60 +/- 28.30\n",
      "Eval num_timesteps=843776, episode_reward=-80.30 +/- 8.34\n",
      "Episode length: 81.30 +/- 8.34\n",
      "Eval num_timesteps=843776, episode_reward=-80.50 +/- 5.48\n",
      "Episode length: 81.50 +/- 5.48\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -9.81e-05 |       0.00000 |      34.93827 |      4.66e-05 |       0.22753\n",
      "     -0.00049 |       0.00000 |      32.12670 |      9.50e-05 |       0.22825\n",
      "     -0.00073 |       0.00000 |      31.51982 |      9.89e-05 |       0.22765\n",
      "     -0.00084 |       0.00000 |      31.25026 |       0.00013 |       0.22767\n",
      "     -0.00078 |       0.00000 |      31.08587 |       0.00011 |       0.22764\n",
      "     -0.00097 |       0.00000 |      30.90918 |       0.00012 |       0.22779\n",
      "     -0.00104 |       0.00000 |      30.79503 |       0.00013 |       0.22735\n",
      "     -0.00102 |       0.00000 |      30.68502 |       0.00013 |       0.22769\n",
      "     -0.00109 |       0.00000 |      30.60111 |       0.00011 |       0.22739\n",
      "     -0.00113 |       0.00000 |      30.50505 |       0.00013 |       0.22766\n",
      "Evaluating losses...\n",
      "     -0.00119 |       0.00000 |      30.39716 |       0.00011 |       0.22786\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.7          |\n",
      "| EpRewMean       | -90.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 8217          |\n",
      "| TimeElapsed     | 1.74e+03      |\n",
      "| TimestepsSoFar  | 847872        |\n",
      "| ev_tdlam_before | 0.857         |\n",
      "| loss_ent        | 0.22785975    |\n",
      "| loss_kl         | 0.00010830288 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.001185456  |\n",
      "| loss_vf_loss    | 30.397158     |\n",
      "-----------------------------------\n",
      "********** Iteration 207 ************\n",
      "Eval num_timesteps=847872, episode_reward=-91.60 +/- 27.03\n",
      "Episode length: 92.60 +/- 27.03\n",
      "Eval num_timesteps=847872, episode_reward=-80.50 +/- 9.70\n",
      "Episode length: 81.50 +/- 9.70\n",
      "Eval num_timesteps=847872, episode_reward=-72.20 +/- 6.31\n",
      "Episode length: 73.20 +/- 6.31\n",
      "Eval num_timesteps=847872, episode_reward=-87.20 +/- 16.72\n",
      "Episode length: 88.20 +/- 16.72\n",
      "Eval num_timesteps=847872, episode_reward=-83.90 +/- 20.41\n",
      "Episode length: 84.90 +/- 20.41\n",
      "Eval num_timesteps=847872, episode_reward=-82.40 +/- 13.03\n",
      "Episode length: 83.40 +/- 13.03\n",
      "Eval num_timesteps=847872, episode_reward=-81.40 +/- 9.33\n",
      "Episode length: 82.40 +/- 9.33\n",
      "Eval num_timesteps=847872, episode_reward=-87.50 +/- 11.47\n",
      "Episode length: 88.50 +/- 11.47\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     6.56e-05 |       0.00000 |      44.17024 |      5.17e-05 |       0.29975\n",
      "     -0.00034 |       0.00000 |      39.11563 |      7.43e-05 |       0.30048\n",
      "     -0.00063 |       0.00000 |      38.21652 |       0.00011 |       0.30038\n",
      "     -0.00085 |       0.00000 |      37.71976 |       0.00013 |       0.30107\n",
      "     -0.00090 |       0.00000 |      37.39634 |       0.00011 |       0.30055\n",
      "     -0.00095 |       0.00000 |      37.12271 |       0.00014 |       0.30048\n",
      "     -0.00116 |       0.00000 |      36.90576 |       0.00017 |       0.30111\n",
      "     -0.00124 |       0.00000 |      36.71678 |       0.00018 |       0.30077\n",
      "     -0.00125 |       0.00000 |      36.53893 |       0.00018 |       0.30137\n",
      "     -0.00136 |       0.00000 |      36.38839 |       0.00019 |       0.30073\n",
      "Evaluating losses...\n",
      "     -0.00153 |       0.00000 |      36.22055 |       0.00016 |       0.30038\n",
      "-----------------------------------\n",
      "| EpLenMean       | 102           |\n",
      "| EpRewMean       | -101          |\n",
      "| EpThisIter      | 34            |\n",
      "| EpisodesSoFar   | 8251          |\n",
      "| TimeElapsed     | 1.74e+03      |\n",
      "| TimestepsSoFar  | 851968        |\n",
      "| ev_tdlam_before | 0.828         |\n",
      "| loss_ent        | 0.30038273    |\n",
      "| loss_kl         | 0.00016147469 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015324401 |\n",
      "| loss_vf_loss    | 36.22055      |\n",
      "-----------------------------------\n",
      "********** Iteration 208 ************\n",
      "Eval num_timesteps=851968, episode_reward=-83.40 +/- 13.34\n",
      "Episode length: 84.40 +/- 13.34\n",
      "Eval num_timesteps=851968, episode_reward=-82.40 +/- 9.62\n",
      "Episode length: 83.40 +/- 9.62\n",
      "Eval num_timesteps=851968, episode_reward=-82.10 +/- 14.34\n",
      "Episode length: 83.10 +/- 14.34\n",
      "Eval num_timesteps=851968, episode_reward=-79.80 +/- 9.92\n",
      "Episode length: 80.80 +/- 9.92\n",
      "Eval num_timesteps=851968, episode_reward=-78.00 +/- 6.88\n",
      "Episode length: 79.00 +/- 6.88\n",
      "Eval num_timesteps=851968, episode_reward=-83.80 +/- 12.50\n",
      "Episode length: 84.80 +/- 12.50\n",
      "Eval num_timesteps=851968, episode_reward=-86.40 +/- 18.94\n",
      "Episode length: 87.40 +/- 18.94\n",
      "Eval num_timesteps=851968, episode_reward=-79.30 +/- 10.27\n",
      "Episode length: 80.30 +/- 10.27\n",
      "Eval num_timesteps=851968, episode_reward=-77.20 +/- 5.56\n",
      "Episode length: 78.20 +/- 5.56\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.67e-05 |       0.00000 |      27.99302 |      2.38e-05 |       0.17974\n",
      "     -0.00042 |       0.00000 |      27.16651 |      5.77e-05 |       0.17994\n",
      "     -0.00049 |       0.00000 |      26.71246 |      3.83e-05 |       0.17970\n",
      "     -0.00059 |       0.00000 |      26.40660 |      4.69e-05 |       0.17967\n",
      "     -0.00066 |       0.00000 |      26.17209 |      6.07e-05 |       0.17960\n",
      "     -0.00070 |       0.00000 |      25.96676 |      6.16e-05 |       0.17945\n",
      "     -0.00077 |       0.00000 |      25.78490 |      5.62e-05 |       0.17954\n",
      "     -0.00080 |       0.00000 |      25.62906 |      6.51e-05 |       0.17912\n",
      "     -0.00083 |       0.00000 |      25.49079 |      6.70e-05 |       0.17939\n",
      "     -0.00088 |       0.00000 |      25.37605 |      7.94e-05 |       0.17888\n",
      "Evaluating losses...\n",
      "     -0.00102 |       0.00000 |      25.24446 |      6.48e-05 |       0.17916\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.1          |\n",
      "| EpRewMean       | -97.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 8298          |\n",
      "| TimeElapsed     | 1.75e+03      |\n",
      "| TimestepsSoFar  | 856064        |\n",
      "| ev_tdlam_before | 0.892         |\n",
      "| loss_ent        | 0.17915913    |\n",
      "| loss_kl         | 6.478325e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010196341 |\n",
      "| loss_vf_loss    | 25.244465     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 209 ************\n",
      "Eval num_timesteps=856064, episode_reward=-82.20 +/- 23.35\n",
      "Episode length: 83.20 +/- 23.35\n",
      "Eval num_timesteps=856064, episode_reward=-88.60 +/- 25.85\n",
      "Episode length: 89.60 +/- 25.85\n",
      "Eval num_timesteps=856064, episode_reward=-74.30 +/- 6.05\n",
      "Episode length: 75.30 +/- 6.05\n",
      "Eval num_timesteps=856064, episode_reward=-77.00 +/- 7.73\n",
      "Episode length: 78.00 +/- 7.73\n",
      "Eval num_timesteps=856064, episode_reward=-79.70 +/- 5.78\n",
      "Episode length: 80.70 +/- 5.78\n",
      "Eval num_timesteps=856064, episode_reward=-81.30 +/- 8.65\n",
      "Episode length: 82.30 +/- 8.65\n",
      "Eval num_timesteps=856064, episode_reward=-80.80 +/- 6.66\n",
      "Episode length: 81.80 +/- 6.66\n",
      "Eval num_timesteps=856064, episode_reward=-81.00 +/- 7.22\n",
      "Episode length: 82.00 +/- 7.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00011 |       0.00000 |      20.06735 |      2.52e-05 |       0.18047\n",
      "     -0.00018 |       0.00000 |      19.24236 |      3.39e-05 |       0.18079\n",
      "     -0.00029 |       0.00000 |      18.92186 |      3.91e-05 |       0.18041\n",
      "     -0.00036 |       0.00000 |      18.71008 |      3.56e-05 |       0.18081\n",
      "     -0.00042 |       0.00000 |      18.56534 |      3.69e-05 |       0.18085\n",
      "     -0.00055 |       0.00000 |      18.44037 |      3.83e-05 |       0.18063\n",
      "     -0.00063 |       0.00000 |      18.32085 |      4.49e-05 |       0.18055\n",
      "     -0.00070 |       0.00000 |      18.21922 |      4.33e-05 |       0.18093\n",
      "     -0.00071 |       0.00000 |      18.11976 |      4.50e-05 |       0.18056\n",
      "     -0.00077 |       0.00000 |      18.03732 |      5.77e-05 |       0.18044\n",
      "Evaluating losses...\n",
      "     -0.00080 |       0.00000 |      17.95155 |      7.18e-05 |       0.18008\n",
      "-----------------------------------\n",
      "| EpLenMean       | 85.2          |\n",
      "| EpRewMean       | -84.2         |\n",
      "| EpThisIter      | 49            |\n",
      "| EpisodesSoFar   | 8347          |\n",
      "| TimeElapsed     | 1.76e+03      |\n",
      "| TimestepsSoFar  | 860160        |\n",
      "| ev_tdlam_before | 0.929         |\n",
      "| loss_ent        | 0.18007636    |\n",
      "| loss_kl         | 7.178645e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0007966709 |\n",
      "| loss_vf_loss    | 17.951548     |\n",
      "-----------------------------------\n",
      "********** Iteration 210 ************\n",
      "Eval num_timesteps=860160, episode_reward=-84.10 +/- 9.37\n",
      "Episode length: 85.10 +/- 9.37\n",
      "Eval num_timesteps=860160, episode_reward=-79.40 +/- 9.44\n",
      "Episode length: 80.40 +/- 9.44\n",
      "Eval num_timesteps=860160, episode_reward=-79.90 +/- 9.05\n",
      "Episode length: 80.90 +/- 9.05\n",
      "Eval num_timesteps=860160, episode_reward=-89.10 +/- 23.56\n",
      "Episode length: 90.10 +/- 23.56\n",
      "Eval num_timesteps=860160, episode_reward=-82.80 +/- 7.28\n",
      "Episode length: 83.80 +/- 7.28\n",
      "Eval num_timesteps=860160, episode_reward=-73.70 +/- 9.40\n",
      "Episode length: 74.70 +/- 9.40\n",
      "Eval num_timesteps=860160, episode_reward=-92.30 +/- 21.78\n",
      "Episode length: 93.30 +/- 21.78\n",
      "Eval num_timesteps=860160, episode_reward=-82.90 +/- 10.99\n",
      "Episode length: 83.90 +/- 10.99\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.26e-05 |       0.00000 |      39.62496 |      1.11e-05 |       0.19742\n",
      "     -0.00024 |       0.00000 |      39.13789 |      2.71e-05 |       0.19708\n",
      "     -0.00039 |       0.00000 |      38.83204 |      2.55e-05 |       0.19705\n",
      "     -0.00050 |       0.00000 |      38.61524 |      3.40e-05 |       0.19715\n",
      "     -0.00055 |       0.00000 |      38.43695 |      4.34e-05 |       0.19706\n",
      "     -0.00068 |       0.00000 |      38.26292 |      3.92e-05 |       0.19713\n",
      "     -0.00075 |       0.00000 |      38.12115 |      4.08e-05 |       0.19699\n",
      "     -0.00076 |       0.00000 |      38.00916 |      4.85e-05 |       0.19738\n",
      "     -0.00081 |       0.00000 |      37.88154 |      5.13e-05 |       0.19656\n",
      "     -0.00088 |       0.00000 |      37.76493 |      5.50e-05 |       0.19694\n",
      "Evaluating losses...\n",
      "     -0.00102 |       0.00000 |      37.63022 |      4.99e-05 |       0.19703\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.1          |\n",
      "| EpRewMean       | -85.1         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 8393          |\n",
      "| TimeElapsed     | 1.76e+03      |\n",
      "| TimestepsSoFar  | 864256        |\n",
      "| ev_tdlam_before | 0.848         |\n",
      "| loss_ent        | 0.1970312     |\n",
      "| loss_kl         | 4.9904316e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010230986 |\n",
      "| loss_vf_loss    | 37.630222     |\n",
      "-----------------------------------\n",
      "********** Iteration 211 ************\n",
      "Eval num_timesteps=864256, episode_reward=-82.60 +/- 19.76\n",
      "Episode length: 83.60 +/- 19.76\n",
      "Eval num_timesteps=864256, episode_reward=-104.30 +/- 74.43\n",
      "Episode length: 105.30 +/- 74.43\n",
      "Eval num_timesteps=864256, episode_reward=-85.10 +/- 20.12\n",
      "Episode length: 86.10 +/- 20.12\n",
      "Eval num_timesteps=864256, episode_reward=-83.50 +/- 13.67\n",
      "Episode length: 84.50 +/- 13.67\n",
      "Eval num_timesteps=864256, episode_reward=-81.60 +/- 6.33\n",
      "Episode length: 82.60 +/- 6.33\n",
      "Eval num_timesteps=864256, episode_reward=-80.60 +/- 11.67\n",
      "Episode length: 81.60 +/- 11.67\n",
      "Eval num_timesteps=864256, episode_reward=-79.80 +/- 13.53\n",
      "Episode length: 80.80 +/- 13.53\n",
      "Eval num_timesteps=864256, episode_reward=-102.10 +/- 40.38\n",
      "Episode length: 103.10 +/- 40.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.50e-05 |       0.00000 |      34.38681 |      1.54e-05 |       0.18970\n",
      "     -0.00029 |       0.00000 |      33.74884 |      4.98e-05 |       0.18953\n",
      "     -0.00048 |       0.00000 |      33.25156 |      4.27e-05 |       0.19001\n",
      "     -0.00055 |       0.00000 |      32.87225 |      3.88e-05 |       0.18984\n",
      "     -0.00065 |       0.00000 |      32.56244 |      5.93e-05 |       0.19003\n",
      "     -0.00075 |       0.00000 |      32.27611 |      4.53e-05 |       0.18995\n",
      "     -0.00081 |       0.00000 |      32.04464 |      4.94e-05 |       0.19013\n",
      "     -0.00089 |       0.00000 |      31.81871 |      5.88e-05 |       0.19037\n",
      "     -0.00090 |       0.00000 |      31.64131 |      6.03e-05 |       0.19015\n",
      "     -0.00092 |       0.00000 |      31.45890 |      6.42e-05 |       0.19016\n",
      "Evaluating losses...\n",
      "     -0.00108 |       0.00000 |      31.30472 |      6.28e-05 |       0.19055\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.3          |\n",
      "| EpRewMean       | -89.3         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 8438          |\n",
      "| TimeElapsed     | 1.77e+03      |\n",
      "| TimestepsSoFar  | 868352        |\n",
      "| ev_tdlam_before | 0.869         |\n",
      "| loss_ent        | 0.19055054    |\n",
      "| loss_kl         | 6.28102e-05   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010818065 |\n",
      "| loss_vf_loss    | 31.304718     |\n",
      "-----------------------------------\n",
      "********** Iteration 212 ************\n",
      "Eval num_timesteps=868352, episode_reward=-79.90 +/- 3.83\n",
      "Episode length: 80.90 +/- 3.83\n",
      "Eval num_timesteps=868352, episode_reward=-73.60 +/- 8.75\n",
      "Episode length: 74.60 +/- 8.75\n",
      "Eval num_timesteps=868352, episode_reward=-89.30 +/- 35.11\n",
      "Episode length: 90.30 +/- 35.11\n",
      "Eval num_timesteps=868352, episode_reward=-78.70 +/- 8.08\n",
      "Episode length: 79.70 +/- 8.08\n",
      "Eval num_timesteps=868352, episode_reward=-81.10 +/- 18.66\n",
      "Episode length: 82.10 +/- 18.66\n",
      "Eval num_timesteps=868352, episode_reward=-76.80 +/- 7.82\n",
      "Episode length: 77.80 +/- 7.82\n",
      "Eval num_timesteps=868352, episode_reward=-85.50 +/- 21.42\n",
      "Episode length: 86.50 +/- 21.42\n",
      "Eval num_timesteps=868352, episode_reward=-81.90 +/- 4.74\n",
      "Episode length: 82.90 +/- 4.74\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     6.13e-05 |       0.00000 |      33.94574 |      2.00e-05 |       0.18075\n",
      "     -0.00030 |       0.00000 |      33.45933 |      2.24e-05 |       0.18065\n",
      "     -0.00046 |       0.00000 |      33.12527 |      1.93e-05 |       0.18052\n",
      "     -0.00057 |       0.00000 |      32.88440 |      2.65e-05 |       0.18062\n",
      "     -0.00055 |       0.00000 |      32.67062 |      3.72e-05 |       0.18079\n",
      "     -0.00068 |       0.00000 |      32.48053 |      3.40e-05 |       0.18040\n",
      "     -0.00074 |       0.00000 |      32.32642 |      3.51e-05 |       0.18088\n",
      "     -0.00084 |       0.00000 |      32.18224 |      3.76e-05 |       0.18061\n",
      "     -0.00084 |       0.00000 |      32.04607 |      4.60e-05 |       0.18075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00093 |       0.00000 |      31.92765 |      3.82e-05 |       0.18079\n",
      "Evaluating losses...\n",
      "     -0.00104 |       0.00000 |      31.80495 |      4.48e-05 |       0.18080\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.7          |\n",
      "| EpRewMean       | -89.7         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 8484          |\n",
      "| TimeElapsed     | 1.78e+03      |\n",
      "| TimestepsSoFar  | 872448        |\n",
      "| ev_tdlam_before | 0.869         |\n",
      "| loss_ent        | 0.18079735    |\n",
      "| loss_kl         | 4.48372e-05   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010397257 |\n",
      "| loss_vf_loss    | 31.80495      |\n",
      "-----------------------------------\n",
      "********** Iteration 213 ************\n",
      "Eval num_timesteps=872448, episode_reward=-80.70 +/- 19.36\n",
      "Episode length: 81.70 +/- 19.36\n",
      "Eval num_timesteps=872448, episode_reward=-83.20 +/- 12.98\n",
      "Episode length: 84.20 +/- 12.98\n",
      "Eval num_timesteps=872448, episode_reward=-85.40 +/- 23.68\n",
      "Episode length: 86.40 +/- 23.68\n",
      "Eval num_timesteps=872448, episode_reward=-78.10 +/- 11.53\n",
      "Episode length: 79.10 +/- 11.53\n",
      "Eval num_timesteps=872448, episode_reward=-76.40 +/- 7.19\n",
      "Episode length: 77.40 +/- 7.19\n",
      "Eval num_timesteps=872448, episode_reward=-84.10 +/- 23.18\n",
      "Episode length: 85.10 +/- 23.18\n",
      "Eval num_timesteps=872448, episode_reward=-73.40 +/- 8.01\n",
      "Episode length: 74.40 +/- 8.01\n",
      "Eval num_timesteps=872448, episode_reward=-78.10 +/- 7.54\n",
      "Episode length: 79.10 +/- 7.54\n",
      "Eval num_timesteps=872448, episode_reward=-84.50 +/- 19.09\n",
      "Episode length: 85.50 +/- 19.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00013 |       0.00000 |      30.06600 |      1.94e-05 |       0.19019\n",
      "     -0.00042 |       0.00000 |      29.39657 |      2.68e-05 |       0.19059\n",
      "     -0.00050 |       0.00000 |      28.95883 |      3.99e-05 |       0.19034\n",
      "     -0.00059 |       0.00000 |      28.63342 |      4.35e-05 |       0.19089\n",
      "     -0.00064 |       0.00000 |      28.38518 |      3.35e-05 |       0.19067\n",
      "     -0.00074 |       0.00000 |      28.17848 |      3.70e-05 |       0.19067\n",
      "     -0.00077 |       0.00000 |      27.98609 |      4.21e-05 |       0.19036\n",
      "     -0.00073 |       0.00000 |      27.83182 |      4.31e-05 |       0.19102\n",
      "     -0.00085 |       0.00000 |      27.70433 |      4.88e-05 |       0.19049\n",
      "     -0.00086 |       0.00000 |      27.57185 |      4.26e-05 |       0.19075\n",
      "Evaluating losses...\n",
      "     -0.00108 |       0.00000 |      27.45241 |      4.33e-05 |       0.19042\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.3          |\n",
      "| EpRewMean       | -88.3         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 8529          |\n",
      "| TimeElapsed     | 1.78e+03      |\n",
      "| TimestepsSoFar  | 876544        |\n",
      "| ev_tdlam_before | 0.884         |\n",
      "| loss_ent        | 0.19042389    |\n",
      "| loss_kl         | 4.329846e-05  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010799787 |\n",
      "| loss_vf_loss    | 27.452406     |\n",
      "-----------------------------------\n",
      "********** Iteration 214 ************\n",
      "Eval num_timesteps=876544, episode_reward=-77.90 +/- 9.15\n",
      "Episode length: 78.90 +/- 9.15\n",
      "Eval num_timesteps=876544, episode_reward=-78.80 +/- 10.02\n",
      "Episode length: 79.80 +/- 10.02\n",
      "Eval num_timesteps=876544, episode_reward=-79.60 +/- 9.25\n",
      "Episode length: 80.60 +/- 9.25\n",
      "Eval num_timesteps=876544, episode_reward=-91.00 +/- 28.02\n",
      "Episode length: 92.00 +/- 28.02\n",
      "Eval num_timesteps=876544, episode_reward=-80.90 +/- 13.17\n",
      "Episode length: 81.90 +/- 13.17\n",
      "Eval num_timesteps=876544, episode_reward=-94.70 +/- 37.21\n",
      "Episode length: 95.70 +/- 37.21\n",
      "Eval num_timesteps=876544, episode_reward=-85.40 +/- 27.28\n",
      "Episode length: 86.40 +/- 27.28\n",
      "Eval num_timesteps=876544, episode_reward=-80.10 +/- 8.35\n",
      "Episode length: 81.10 +/- 8.35\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00019 |       0.00000 |      24.42062 |      9.93e-06 |       0.17878\n",
      "    -9.87e-05 |       0.00000 |      23.53254 |      1.87e-05 |       0.17849\n",
      "     -0.00028 |       0.00000 |      23.06986 |      2.08e-05 |       0.17896\n",
      "     -0.00035 |       0.00000 |      22.74567 |      2.99e-05 |       0.17865\n",
      "     -0.00046 |       0.00000 |      22.51458 |      2.54e-05 |       0.17877\n",
      "     -0.00044 |       0.00000 |      22.30353 |      2.10e-05 |       0.17838\n",
      "     -0.00053 |       0.00000 |      22.14692 |      2.29e-05 |       0.17868\n",
      "     -0.00063 |       0.00000 |      21.99178 |      2.59e-05 |       0.17913\n",
      "     -0.00071 |       0.00000 |      21.89058 |      2.98e-05 |       0.17889\n",
      "     -0.00076 |       0.00000 |      21.77002 |      2.91e-05 |       0.17916\n",
      "Evaluating losses...\n",
      "     -0.00083 |       0.00000 |      21.67411 |      3.85e-05 |       0.17890\n",
      "------------------------------------\n",
      "| EpLenMean       | 88.4           |\n",
      "| EpRewMean       | -87.4          |\n",
      "| EpThisIter      | 48             |\n",
      "| EpisodesSoFar   | 8577           |\n",
      "| TimeElapsed     | 1.79e+03       |\n",
      "| TimestepsSoFar  | 880640         |\n",
      "| ev_tdlam_before | 0.906          |\n",
      "| loss_ent        | 0.17890158     |\n",
      "| loss_kl         | 3.8545968e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00083287945 |\n",
      "| loss_vf_loss    | 21.67411       |\n",
      "------------------------------------\n",
      "********** Iteration 215 ************\n",
      "Eval num_timesteps=880640, episode_reward=-78.50 +/- 13.00\n",
      "Episode length: 79.50 +/- 13.00\n",
      "Eval num_timesteps=880640, episode_reward=-75.20 +/- 9.66\n",
      "Episode length: 76.20 +/- 9.66\n",
      "Eval num_timesteps=880640, episode_reward=-74.80 +/- 8.61\n",
      "Episode length: 75.80 +/- 8.61\n",
      "Eval num_timesteps=880640, episode_reward=-77.00 +/- 4.92\n",
      "Episode length: 78.00 +/- 4.92\n",
      "Eval num_timesteps=880640, episode_reward=-78.20 +/- 8.22\n",
      "Episode length: 79.20 +/- 8.22\n",
      "Eval num_timesteps=880640, episode_reward=-81.30 +/- 10.73\n",
      "Episode length: 82.30 +/- 10.73\n",
      "Eval num_timesteps=880640, episode_reward=-77.40 +/- 9.30\n",
      "Episode length: 78.40 +/- 9.30\n",
      "Eval num_timesteps=880640, episode_reward=-80.70 +/- 6.39\n",
      "Episode length: 81.70 +/- 6.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.83e-05 |       0.00000 |      48.42835 |      1.36e-05 |       0.22704\n",
      "     -0.00027 |       0.00000 |      43.76104 |      1.81e-05 |       0.22712\n",
      "     -0.00041 |       0.00000 |      42.65927 |      2.22e-05 |       0.22702\n",
      "     -0.00057 |       0.00000 |      42.16543 |      3.48e-05 |       0.22717\n",
      "     -0.00063 |       0.00000 |      41.87234 |      3.46e-05 |       0.22755\n",
      "     -0.00072 |       0.00000 |      41.65348 |      4.62e-05 |       0.22704\n",
      "     -0.00082 |       0.00000 |      41.48601 |      3.93e-05 |       0.22736\n",
      "     -0.00085 |       0.00000 |      41.31079 |      4.38e-05 |       0.22743\n",
      "     -0.00087 |       0.00000 |      41.18919 |      4.68e-05 |       0.22776\n",
      "     -0.00093 |       0.00000 |      41.03330 |      5.07e-05 |       0.22707\n",
      "Evaluating losses...\n",
      "     -0.00104 |       0.00000 |      40.90004 |      4.60e-05 |       0.22769\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.3          |\n",
      "| EpRewMean       | -94.3         |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 8615          |\n",
      "| TimeElapsed     | 1.8e+03       |\n",
      "| TimestepsSoFar  | 884736        |\n",
      "| ev_tdlam_before | 0.799         |\n",
      "| loss_ent        | 0.22768909    |\n",
      "| loss_kl         | 4.5969173e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010426211 |\n",
      "| loss_vf_loss    | 40.900036     |\n",
      "-----------------------------------\n",
      "********** Iteration 216 ************\n",
      "Eval num_timesteps=884736, episode_reward=-78.80 +/- 6.45\n",
      "Episode length: 79.80 +/- 6.45\n",
      "Eval num_timesteps=884736, episode_reward=-79.40 +/- 9.67\n",
      "Episode length: 80.40 +/- 9.67\n",
      "Eval num_timesteps=884736, episode_reward=-77.50 +/- 5.35\n",
      "Episode length: 78.50 +/- 5.35\n",
      "Eval num_timesteps=884736, episode_reward=-78.40 +/- 7.57\n",
      "Episode length: 79.40 +/- 7.57\n",
      "Eval num_timesteps=884736, episode_reward=-84.10 +/- 8.47\n",
      "Episode length: 85.10 +/- 8.47\n",
      "Eval num_timesteps=884736, episode_reward=-73.50 +/- 6.62\n",
      "Episode length: 74.50 +/- 6.62\n",
      "Eval num_timesteps=884736, episode_reward=-85.30 +/- 14.11\n",
      "Episode length: 86.30 +/- 14.11\n",
      "Eval num_timesteps=884736, episode_reward=-81.40 +/- 9.97\n",
      "Episode length: 82.40 +/- 9.97\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00015 |       0.00000 |      26.73502 |      5.03e-06 |       0.18766\n",
      "    -2.38e-05 |       0.00000 |      25.93671 |      1.57e-05 |       0.18769\n",
      "     -0.00018 |       0.00000 |      25.46445 |      1.90e-05 |       0.18788\n",
      "     -0.00031 |       0.00000 |      25.15795 |      1.70e-05 |       0.18760\n",
      "     -0.00042 |       0.00000 |      24.92784 |      2.00e-05 |       0.18810\n",
      "     -0.00044 |       0.00000 |      24.72597 |      2.53e-05 |       0.18793\n",
      "     -0.00046 |       0.00000 |      24.58242 |      2.21e-05 |       0.18743\n",
      "     -0.00054 |       0.00000 |      24.45052 |      2.72e-05 |       0.18797\n",
      "     -0.00063 |       0.00000 |      24.34412 |      2.25e-05 |       0.18801\n",
      "     -0.00069 |       0.00000 |      24.24826 |      2.78e-05 |       0.18779\n",
      "Evaluating losses...\n",
      "     -0.00073 |       0.00000 |      24.16125 |      3.57e-05 |       0.18819\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.5          |\n",
      "| EpRewMean       | -93.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 8662          |\n",
      "| TimeElapsed     | 1.81e+03      |\n",
      "| TimestepsSoFar  | 888832        |\n",
      "| ev_tdlam_before | 0.897         |\n",
      "| loss_ent        | 0.18818757    |\n",
      "| loss_kl         | 3.5684345e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0007255259 |\n",
      "| loss_vf_loss    | 24.161253     |\n",
      "-----------------------------------\n",
      "********** Iteration 217 ************\n",
      "Eval num_timesteps=888832, episode_reward=-78.90 +/- 6.09\n",
      "Episode length: 79.90 +/- 6.09\n",
      "Eval num_timesteps=888832, episode_reward=-81.40 +/- 5.73\n",
      "Episode length: 82.40 +/- 5.73\n",
      "Eval num_timesteps=888832, episode_reward=-74.30 +/- 8.08\n",
      "Episode length: 75.30 +/- 8.08\n",
      "Eval num_timesteps=888832, episode_reward=-83.80 +/- 9.71\n",
      "Episode length: 84.80 +/- 9.71\n",
      "Eval num_timesteps=888832, episode_reward=-79.60 +/- 7.05\n",
      "Episode length: 80.60 +/- 7.05\n",
      "Eval num_timesteps=888832, episode_reward=-77.00 +/- 7.32\n",
      "Episode length: 78.00 +/- 7.32\n",
      "Eval num_timesteps=888832, episode_reward=-80.20 +/- 10.75\n",
      "Episode length: 81.20 +/- 10.75\n",
      "Eval num_timesteps=888832, episode_reward=-89.10 +/- 20.27\n",
      "Episode length: 90.10 +/- 20.27\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.22e-06 |       0.00000 |      28.08216 |      7.60e-06 |       0.18747\n",
      "     -0.00016 |       0.00000 |      27.72969 |      2.30e-05 |       0.18734\n",
      "     -0.00031 |       0.00000 |      27.47952 |      1.53e-05 |       0.18701\n",
      "     -0.00037 |       0.00000 |      27.28827 |      2.25e-05 |       0.18758\n",
      "     -0.00052 |       0.00000 |      27.14392 |      2.30e-05 |       0.18750\n",
      "     -0.00050 |       0.00000 |      27.03114 |      2.07e-05 |       0.18749\n",
      "     -0.00056 |       0.00000 |      26.93129 |      2.36e-05 |       0.18739\n",
      "     -0.00063 |       0.00000 |      26.83065 |      3.23e-05 |       0.18680\n",
      "     -0.00061 |       0.00000 |      26.75514 |      2.69e-05 |       0.18752\n",
      "     -0.00066 |       0.00000 |      26.67572 |      2.62e-05 |       0.18704\n",
      "Evaluating losses...\n",
      "     -0.00068 |       0.00000 |      26.57554 |      2.42e-05 |       0.18723\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91            |\n",
      "| EpRewMean       | -90           |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 8708          |\n",
      "| TimeElapsed     | 1.81e+03      |\n",
      "| TimestepsSoFar  | 892928        |\n",
      "| ev_tdlam_before | 0.894         |\n",
      "| loss_ent        | 0.18723115    |\n",
      "| loss_kl         | 2.4233694e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0006777701 |\n",
      "| loss_vf_loss    | 26.575542     |\n",
      "-----------------------------------\n",
      "********** Iteration 218 ************\n",
      "Eval num_timesteps=892928, episode_reward=-76.90 +/- 7.83\n",
      "Episode length: 77.90 +/- 7.83\n",
      "Eval num_timesteps=892928, episode_reward=-93.50 +/- 36.42\n",
      "Episode length: 94.50 +/- 36.42\n",
      "Eval num_timesteps=892928, episode_reward=-78.20 +/- 11.62\n",
      "Episode length: 79.20 +/- 11.62\n",
      "Eval num_timesteps=892928, episode_reward=-78.30 +/- 7.82\n",
      "Episode length: 79.30 +/- 7.82\n",
      "Eval num_timesteps=892928, episode_reward=-78.10 +/- 11.21\n",
      "Episode length: 79.10 +/- 11.21\n",
      "Eval num_timesteps=892928, episode_reward=-69.70 +/- 3.74\n",
      "Episode length: 70.70 +/- 3.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=892928, episode_reward=-107.10 +/- 58.95\n",
      "Episode length: 108.10 +/- 58.95\n",
      "Eval num_timesteps=892928, episode_reward=-83.20 +/- 11.62\n",
      "Episode length: 84.20 +/- 11.62\n",
      "Eval num_timesteps=892928, episode_reward=-88.80 +/- 35.67\n",
      "Episode length: 89.80 +/- 35.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.82e-05 |       0.00000 |      35.71898 |      3.96e-06 |       0.18649\n",
      "     -0.00016 |       0.00000 |      35.46721 |      1.43e-05 |       0.18636\n",
      "     -0.00024 |       0.00000 |      35.23559 |      1.17e-05 |       0.18632\n",
      "     -0.00024 |       0.00000 |      35.05812 |      1.48e-05 |       0.18639\n",
      "     -0.00033 |       0.00000 |      34.86208 |      1.65e-05 |       0.18646\n",
      "     -0.00034 |       0.00000 |      34.71482 |      1.51e-05 |       0.18643\n",
      "     -0.00043 |       0.00000 |      34.57069 |      1.67e-05 |       0.18633\n",
      "     -0.00046 |       0.00000 |      34.43621 |      1.61e-05 |       0.18664\n",
      "     -0.00049 |       0.00000 |      34.31618 |      1.81e-05 |       0.18689\n",
      "     -0.00045 |       0.00000 |      34.19973 |      1.46e-05 |       0.18681\n",
      "Evaluating losses...\n",
      "     -0.00066 |       0.00000 |      34.06787 |      1.26e-05 |       0.18672\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.2          |\n",
      "| EpRewMean       | -87.2         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 8755          |\n",
      "| TimeElapsed     | 3.35e+03      |\n",
      "| TimestepsSoFar  | 897024        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.18671912    |\n",
      "| loss_kl         | 1.2631368e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0006633345 |\n",
      "| loss_vf_loss    | 34.06787      |\n",
      "-----------------------------------\n",
      "********** Iteration 219 ************\n",
      "Eval num_timesteps=897024, episode_reward=-84.30 +/- 15.12\n",
      "Episode length: 85.30 +/- 15.12\n",
      "Eval num_timesteps=897024, episode_reward=-82.60 +/- 13.63\n",
      "Episode length: 83.60 +/- 13.63\n",
      "Eval num_timesteps=897024, episode_reward=-82.40 +/- 12.08\n",
      "Episode length: 83.40 +/- 12.08\n",
      "Eval num_timesteps=897024, episode_reward=-94.10 +/- 53.48\n",
      "Episode length: 95.10 +/- 53.48\n",
      "Eval num_timesteps=897024, episode_reward=-75.40 +/- 8.83\n",
      "Episode length: 76.40 +/- 8.83\n",
      "Eval num_timesteps=897024, episode_reward=-75.70 +/- 5.60\n",
      "Episode length: 76.70 +/- 5.60\n",
      "Eval num_timesteps=897024, episode_reward=-103.60 +/- 72.77\n",
      "Episode length: 104.60 +/- 72.77\n",
      "Eval num_timesteps=897024, episode_reward=-78.40 +/- 8.33\n",
      "Episode length: 79.40 +/- 8.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00015 |       0.00000 |      30.58863 |      8.39e-06 |       0.21830\n",
      "    -8.52e-05 |       0.00000 |      29.47739 |      1.41e-05 |       0.21860\n",
      "     -0.00031 |       0.00000 |      29.06602 |      2.20e-05 |       0.21858\n",
      "     -0.00030 |       0.00000 |      28.85191 |      2.43e-05 |       0.21921\n",
      "     -0.00016 |       0.00000 |      28.70684 |      2.99e-05 |       0.21876\n",
      "     -0.00044 |       0.00000 |      28.59546 |      2.51e-05 |       0.21867\n",
      "     -0.00054 |       0.00000 |      28.49740 |      2.26e-05 |       0.21909\n",
      "     -0.00058 |       0.00000 |      28.40983 |      2.56e-05 |       0.21856\n",
      "     -0.00068 |       0.00000 |      28.31455 |      2.83e-05 |       0.21882\n",
      "     -0.00056 |       0.00000 |      28.24361 |      2.89e-05 |       0.21878\n",
      "Evaluating losses...\n",
      "     -0.00081 |       0.00000 |      28.15516 |      2.72e-05 |       0.21853\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89            |\n",
      "| EpRewMean       | -88           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 8800          |\n",
      "| TimeElapsed     | 3.36e+03      |\n",
      "| TimestepsSoFar  | 901120        |\n",
      "| ev_tdlam_before | 0.879         |\n",
      "| loss_ent        | 0.21852653    |\n",
      "| loss_kl         | 2.7231954e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0008120651 |\n",
      "| loss_vf_loss    | 28.155163     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 220 ************\n",
      "Eval num_timesteps=901120, episode_reward=-88.40 +/- 36.13\n",
      "Episode length: 89.40 +/- 36.13\n",
      "Eval num_timesteps=901120, episode_reward=-81.50 +/- 5.12\n",
      "Episode length: 82.50 +/- 5.12\n",
      "Eval num_timesteps=901120, episode_reward=-81.10 +/- 17.60\n",
      "Episode length: 82.10 +/- 17.60\n",
      "Eval num_timesteps=901120, episode_reward=-77.90 +/- 5.49\n",
      "Episode length: 78.90 +/- 5.49\n",
      "Eval num_timesteps=901120, episode_reward=-81.90 +/- 7.49\n",
      "Episode length: 82.90 +/- 7.49\n",
      "Eval num_timesteps=901120, episode_reward=-94.60 +/- 45.91\n",
      "Episode length: 95.60 +/- 45.91\n",
      "Eval num_timesteps=901120, episode_reward=-77.80 +/- 5.34\n",
      "Episode length: 78.80 +/- 5.34\n",
      "Eval num_timesteps=901120, episode_reward=-85.00 +/- 8.12\n",
      "Episode length: 86.00 +/- 8.12\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     7.83e-05 |       0.00000 |      17.15173 |      7.34e-06 |       0.18283\n",
      "    -3.42e-05 |       0.00000 |      16.89716 |      1.86e-05 |       0.18262\n",
      "     -0.00012 |       0.00000 |      16.71782 |      8.00e-06 |       0.18224\n",
      "     -0.00012 |       0.00000 |      16.58985 |      9.77e-06 |       0.18223\n",
      "     -0.00019 |       0.00000 |      16.48168 |      1.11e-05 |       0.18231\n",
      "     -0.00023 |       0.00000 |      16.38715 |      1.62e-05 |       0.18260\n",
      "     -0.00016 |       0.00000 |      16.31163 |      1.81e-05 |       0.18236\n",
      "     -0.00030 |       0.00000 |      16.24683 |      1.36e-05 |       0.18240\n",
      "     -0.00030 |       0.00000 |      16.17680 |      1.53e-05 |       0.18246\n",
      "     -0.00035 |       0.00000 |      16.11661 |      1.80e-05 |       0.18248\n",
      "Evaluating losses...\n",
      "     -0.00049 |       0.00000 |      16.05681 |      1.64e-05 |       0.18240\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.6          |\n",
      "| EpRewMean       | -86.6         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 8848          |\n",
      "| TimeElapsed     | 3.37e+03      |\n",
      "| TimestepsSoFar  | 905216        |\n",
      "| ev_tdlam_before | 0.937         |\n",
      "| loss_ent        | 0.18239868    |\n",
      "| loss_kl         | 1.6448796e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0004929056 |\n",
      "| loss_vf_loss    | 16.05681      |\n",
      "-----------------------------------\n",
      "********** Iteration 221 ************\n",
      "Eval num_timesteps=905216, episode_reward=-90.30 +/- 15.28\n",
      "Episode length: 91.30 +/- 15.28\n",
      "Eval num_timesteps=905216, episode_reward=-78.80 +/- 8.52\n",
      "Episode length: 79.80 +/- 8.52\n",
      "Eval num_timesteps=905216, episode_reward=-91.50 +/- 33.57\n",
      "Episode length: 92.50 +/- 33.57\n",
      "Eval num_timesteps=905216, episode_reward=-74.90 +/- 7.49\n",
      "Episode length: 75.90 +/- 7.49\n",
      "Eval num_timesteps=905216, episode_reward=-84.10 +/- 4.21\n",
      "Episode length: 85.10 +/- 4.21\n",
      "Eval num_timesteps=905216, episode_reward=-95.60 +/- 37.70\n",
      "Episode length: 96.60 +/- 37.70\n",
      "Eval num_timesteps=905216, episode_reward=-80.20 +/- 12.29\n",
      "Episode length: 81.20 +/- 12.29\n",
      "Eval num_timesteps=905216, episode_reward=-84.60 +/- 8.46\n",
      "Episode length: 85.60 +/- 8.46\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.91e-05 |       0.00000 |      47.17768 |      9.05e-06 |       0.21346\n",
      "     -0.00023 |       0.00000 |      45.48839 |      1.87e-05 |       0.21414\n",
      "     -0.00035 |       0.00000 |      44.75620 |      1.61e-05 |       0.21376\n",
      "     -0.00046 |       0.00000 |      44.33663 |      1.91e-05 |       0.21404\n",
      "     -0.00047 |       0.00000 |      44.02945 |      2.21e-05 |       0.21374\n",
      "     -0.00048 |       0.00000 |      43.80582 |      2.13e-05 |       0.21378\n",
      "     -0.00057 |       0.00000 |      43.58589 |      1.96e-05 |       0.21358\n",
      "     -0.00057 |       0.00000 |      43.42883 |      2.28e-05 |       0.21383\n",
      "     -0.00060 |       0.00000 |      43.26315 |      1.94e-05 |       0.21389\n",
      "     -0.00063 |       0.00000 |      43.11464 |      2.02e-05 |       0.21352\n",
      "Evaluating losses...\n",
      "     -0.00071 |       0.00000 |      42.96312 |      2.41e-05 |       0.21404\n",
      "------------------------------------\n",
      "| EpLenMean       | 88.9           |\n",
      "| EpRewMean       | -87.9          |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 8892           |\n",
      "| TimeElapsed     | 3.38e+03       |\n",
      "| TimestepsSoFar  | 909312         |\n",
      "| ev_tdlam_before | 0.819          |\n",
      "| loss_ent        | 0.2140357      |\n",
      "| loss_kl         | 2.4119188e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00071153336 |\n",
      "| loss_vf_loss    | 42.963116      |\n",
      "------------------------------------\n",
      "********** Iteration 222 ************\n",
      "Eval num_timesteps=909312, episode_reward=-126.20 +/- 126.70\n",
      "Episode length: 127.10 +/- 126.40\n",
      "Eval num_timesteps=909312, episode_reward=-79.10 +/- 10.35\n",
      "Episode length: 80.10 +/- 10.35\n",
      "Eval num_timesteps=909312, episode_reward=-84.90 +/- 21.20\n",
      "Episode length: 85.90 +/- 21.20\n",
      "Eval num_timesteps=909312, episode_reward=-81.90 +/- 9.93\n",
      "Episode length: 82.90 +/- 9.93\n",
      "Eval num_timesteps=909312, episode_reward=-75.60 +/- 6.23\n",
      "Episode length: 76.60 +/- 6.23\n",
      "Eval num_timesteps=909312, episode_reward=-75.80 +/- 9.56\n",
      "Episode length: 76.80 +/- 9.56\n",
      "Eval num_timesteps=909312, episode_reward=-87.30 +/- 11.25\n",
      "Episode length: 88.30 +/- 11.25\n",
      "Eval num_timesteps=909312, episode_reward=-78.70 +/- 10.38\n",
      "Episode length: 79.70 +/- 10.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00014 |       0.00000 |      32.43257 |      1.11e-05 |       0.19554\n",
      "    -8.05e-05 |       0.00000 |      31.93542 |      1.43e-05 |       0.19499\n",
      "     -0.00030 |       0.00000 |      31.53590 |      1.13e-05 |       0.19519\n",
      "     -0.00032 |       0.00000 |      31.26444 |      2.25e-05 |       0.19546\n",
      "     -0.00041 |       0.00000 |      31.02975 |      1.74e-05 |       0.19520\n",
      "     -0.00051 |       0.00000 |      30.86862 |      1.89e-05 |       0.19523\n",
      "     -0.00050 |       0.00000 |      30.70835 |      2.15e-05 |       0.19524\n",
      "     -0.00059 |       0.00000 |      30.58249 |      2.14e-05 |       0.19530\n",
      "     -0.00061 |       0.00000 |      30.46819 |      2.07e-05 |       0.19523\n",
      "     -0.00061 |       0.00000 |      30.36910 |      2.81e-05 |       0.19558\n",
      "Evaluating losses...\n",
      "     -0.00068 |       0.00000 |      30.27307 |      2.07e-05 |       0.19511\n",
      "------------------------------------\n",
      "| EpLenMean       | 91             |\n",
      "| EpRewMean       | -90            |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 8937           |\n",
      "| TimeElapsed     | 3.39e+03       |\n",
      "| TimestepsSoFar  | 913408         |\n",
      "| ev_tdlam_before | 0.868          |\n",
      "| loss_ent        | 0.19511457     |\n",
      "| loss_kl         | 2.066994e-05   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00068446074 |\n",
      "| loss_vf_loss    | 30.273073      |\n",
      "------------------------------------\n",
      "********** Iteration 223 ************\n",
      "Eval num_timesteps=913408, episode_reward=-78.40 +/- 13.65\n",
      "Episode length: 79.40 +/- 13.65\n",
      "Eval num_timesteps=913408, episode_reward=-80.10 +/- 13.41\n",
      "Episode length: 81.10 +/- 13.41\n",
      "Eval num_timesteps=913408, episode_reward=-75.10 +/- 5.89\n",
      "Episode length: 76.10 +/- 5.89\n",
      "Eval num_timesteps=913408, episode_reward=-77.30 +/- 8.27\n",
      "Episode length: 78.30 +/- 8.27\n",
      "Eval num_timesteps=913408, episode_reward=-100.90 +/- 28.07\n",
      "Episode length: 101.90 +/- 28.07\n",
      "Eval num_timesteps=913408, episode_reward=-77.10 +/- 6.99\n",
      "Episode length: 78.10 +/- 6.99\n",
      "Eval num_timesteps=913408, episode_reward=-78.60 +/- 6.62\n",
      "Episode length: 79.60 +/- 6.62\n",
      "Eval num_timesteps=913408, episode_reward=-80.40 +/- 9.57\n",
      "Episode length: 81.40 +/- 9.57\n",
      "Eval num_timesteps=913408, episode_reward=-86.10 +/- 18.84\n",
      "Episode length: 87.10 +/- 18.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00011 |       0.00000 |      40.76255 |      5.91e-06 |       0.19563\n",
      "    -1.89e-05 |       0.00000 |      40.38683 |      1.03e-05 |       0.19543\n",
      "     -0.00018 |       0.00000 |      40.17890 |      9.63e-06 |       0.19544\n",
      "     -0.00021 |       0.00000 |      39.99390 |      9.90e-06 |       0.19572\n",
      "     -0.00031 |       0.00000 |      39.86787 |      1.37e-05 |       0.19554\n",
      "     -0.00033 |       0.00000 |      39.76411 |      1.19e-05 |       0.19565\n",
      "     -0.00041 |       0.00000 |      39.65853 |      1.30e-05 |       0.19576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00042 |       0.00000 |      39.56407 |      1.47e-05 |       0.19556\n",
      "     -0.00045 |       0.00000 |      39.47782 |      1.33e-05 |       0.19563\n",
      "     -0.00052 |       0.00000 |      39.42355 |      1.43e-05 |       0.19567\n",
      "Evaluating losses...\n",
      "     -0.00059 |       0.00000 |      39.31932 |      1.37e-05 |       0.19574\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.1           |\n",
      "| EpRewMean       | -89.1          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 8982           |\n",
      "| TimeElapsed     | 3.4e+03        |\n",
      "| TimestepsSoFar  | 917504         |\n",
      "| ev_tdlam_before | 0.837          |\n",
      "| loss_ent        | 0.19573545     |\n",
      "| loss_kl         | 1.3705184e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00059078477 |\n",
      "| loss_vf_loss    | 39.319317      |\n",
      "------------------------------------\n",
      "********** Iteration 224 ************\n",
      "Eval num_timesteps=917504, episode_reward=-122.60 +/- 125.89\n",
      "Episode length: 123.50 +/- 125.59\n",
      "Eval num_timesteps=917504, episode_reward=-86.60 +/- 13.17\n",
      "Episode length: 87.60 +/- 13.17\n",
      "Eval num_timesteps=917504, episode_reward=-85.10 +/- 14.67\n",
      "Episode length: 86.10 +/- 14.67\n",
      "Eval num_timesteps=917504, episode_reward=-79.80 +/- 7.39\n",
      "Episode length: 80.80 +/- 7.39\n",
      "Eval num_timesteps=917504, episode_reward=-91.40 +/- 23.85\n",
      "Episode length: 92.40 +/- 23.85\n",
      "Eval num_timesteps=917504, episode_reward=-78.30 +/- 8.50\n",
      "Episode length: 79.30 +/- 8.50\n",
      "Eval num_timesteps=917504, episode_reward=-79.60 +/- 8.96\n",
      "Episode length: 80.60 +/- 8.96\n",
      "Eval num_timesteps=917504, episode_reward=-76.70 +/- 9.21\n",
      "Episode length: 77.70 +/- 9.21\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.98e-06 |       0.00000 |      43.36855 |      5.00e-06 |       0.20215\n",
      "     -0.00019 |       0.00000 |      41.84670 |      8.76e-06 |       0.20177\n",
      "     -0.00029 |       0.00000 |      41.00838 |      1.09e-05 |       0.20175\n",
      "     -0.00039 |       0.00000 |      40.38530 |      1.41e-05 |       0.20179\n",
      "     -0.00042 |       0.00000 |      39.99191 |      1.71e-05 |       0.20177\n",
      "     -0.00044 |       0.00000 |      39.69455 |      1.71e-05 |       0.20179\n",
      "     -0.00049 |       0.00000 |      39.42009 |      1.70e-05 |       0.20151\n",
      "     -0.00055 |       0.00000 |      39.22944 |      1.87e-05 |       0.20171\n",
      "     -0.00056 |       0.00000 |      39.05767 |      2.05e-05 |       0.20170\n",
      "     -0.00061 |       0.00000 |      38.89653 |      1.94e-05 |       0.20192\n",
      "Evaluating losses...\n",
      "     -0.00070 |       0.00000 |      38.76665 |      1.90e-05 |       0.20163\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.4           |\n",
      "| EpRewMean       | -91.4          |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 9026           |\n",
      "| TimeElapsed     | 3.4e+03        |\n",
      "| TimestepsSoFar  | 921600         |\n",
      "| ev_tdlam_before | 0.824          |\n",
      "| loss_ent        | 0.20162669     |\n",
      "| loss_kl         | 1.8987219e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00069850485 |\n",
      "| loss_vf_loss    | 38.766647      |\n",
      "------------------------------------\n",
      "********** Iteration 225 ************\n",
      "Eval num_timesteps=921600, episode_reward=-81.60 +/- 16.98\n",
      "Episode length: 82.60 +/- 16.98\n",
      "Eval num_timesteps=921600, episode_reward=-80.50 +/- 6.30\n",
      "Episode length: 81.50 +/- 6.30\n",
      "Eval num_timesteps=921600, episode_reward=-80.10 +/- 9.46\n",
      "Episode length: 81.10 +/- 9.46\n",
      "Eval num_timesteps=921600, episode_reward=-77.10 +/- 6.67\n",
      "Episode length: 78.10 +/- 6.67\n",
      "Eval num_timesteps=921600, episode_reward=-76.40 +/- 10.35\n",
      "Episode length: 77.40 +/- 10.35\n",
      "Eval num_timesteps=921600, episode_reward=-74.00 +/- 6.75\n",
      "Episode length: 75.00 +/- 6.75\n",
      "Eval num_timesteps=921600, episode_reward=-89.00 +/- 35.88\n",
      "Episode length: 90.00 +/- 35.88\n",
      "Eval num_timesteps=921600, episode_reward=-75.80 +/- 3.82\n",
      "Episode length: 76.80 +/- 3.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     6.94e-06 |       0.00000 |      26.98268 |      6.99e-06 |       0.18448\n",
      "     -0.00012 |       0.00000 |      26.57621 |      1.16e-05 |       0.18441\n",
      "     -0.00022 |       0.00000 |      26.31507 |      1.23e-05 |       0.18440\n",
      "     -0.00027 |       0.00000 |      26.13190 |      1.50e-05 |       0.18423\n",
      "     -0.00031 |       0.00000 |      26.01043 |      1.39e-05 |       0.18405\n",
      "     -0.00036 |       0.00000 |      25.90965 |      1.60e-05 |       0.18408\n",
      "     -0.00037 |       0.00000 |      25.83295 |      1.40e-05 |       0.18403\n",
      "     -0.00039 |       0.00000 |      25.76704 |      1.43e-05 |       0.18392\n",
      "     -0.00042 |       0.00000 |      25.71563 |      1.71e-05 |       0.18396\n",
      "     -0.00046 |       0.00000 |      25.65579 |      1.77e-05 |       0.18413\n",
      "Evaluating losses...\n",
      "     -0.00053 |       0.00000 |      25.59217 |      1.78e-05 |       0.18420\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.8           |\n",
      "| EpRewMean       | -89.8          |\n",
      "| EpThisIter      | 47             |\n",
      "| EpisodesSoFar   | 9073           |\n",
      "| TimeElapsed     | 3.41e+03       |\n",
      "| TimestepsSoFar  | 925696         |\n",
      "| ev_tdlam_before | 0.899          |\n",
      "| loss_ent        | 0.18420264     |\n",
      "| loss_kl         | 1.7782788e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00052716467 |\n",
      "| loss_vf_loss    | 25.592173      |\n",
      "------------------------------------\n",
      "********** Iteration 226 ************\n",
      "Eval num_timesteps=925696, episode_reward=-79.00 +/- 8.38\n",
      "Episode length: 80.00 +/- 8.38\n",
      "Eval num_timesteps=925696, episode_reward=-76.90 +/- 8.37\n",
      "Episode length: 77.90 +/- 8.37\n",
      "Eval num_timesteps=925696, episode_reward=-75.50 +/- 7.41\n",
      "Episode length: 76.50 +/- 7.41\n",
      "Eval num_timesteps=925696, episode_reward=-78.20 +/- 9.40\n",
      "Episode length: 79.20 +/- 9.40\n",
      "Eval num_timesteps=925696, episode_reward=-74.50 +/- 6.96\n",
      "Episode length: 75.50 +/- 6.96\n",
      "Eval num_timesteps=925696, episode_reward=-79.90 +/- 7.93\n",
      "Episode length: 80.90 +/- 7.93\n",
      "Eval num_timesteps=925696, episode_reward=-76.30 +/- 8.03\n",
      "Episode length: 77.30 +/- 8.03\n",
      "Eval num_timesteps=925696, episode_reward=-80.30 +/- 9.75\n",
      "Episode length: 81.30 +/- 9.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -2.88e-05 |       0.00000 |      43.20283 |      5.80e-06 |       0.24832\n",
      "     -0.00014 |       0.00000 |      40.30693 |      9.98e-06 |       0.24820\n",
      "     -0.00018 |       0.00000 |      38.81355 |      1.07e-05 |       0.24827\n",
      "     -0.00035 |       0.00000 |      37.84316 |      1.34e-05 |       0.24815\n",
      "     -0.00029 |       0.00000 |      37.13801 |      1.51e-05 |       0.24830\n",
      "     -0.00035 |       0.00000 |      36.57921 |      1.48e-05 |       0.24860\n",
      "     -0.00040 |       0.00000 |      36.16002 |      1.50e-05 |       0.24828\n",
      "     -0.00041 |       0.00000 |      35.80476 |      1.55e-05 |       0.24868\n",
      "     -0.00047 |       0.00000 |      35.52950 |      1.85e-05 |       0.24851\n",
      "     -0.00049 |       0.00000 |      35.26956 |      1.90e-05 |       0.24863\n",
      "Evaluating losses...\n",
      "     -0.00061 |       0.00000 |      35.11832 |      2.13e-05 |       0.24875\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.5          |\n",
      "| EpRewMean       | -94.5         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 9114          |\n",
      "| TimeElapsed     | 3.42e+03      |\n",
      "| TimestepsSoFar  | 929792        |\n",
      "| ev_tdlam_before | 0.837         |\n",
      "| loss_ent        | 0.24875148    |\n",
      "| loss_kl         | 2.1345295e-05 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0006085832 |\n",
      "| loss_vf_loss    | 35.118317     |\n",
      "-----------------------------------\n",
      "********** Iteration 227 ************\n",
      "Eval num_timesteps=929792, episode_reward=-78.50 +/- 12.07\n",
      "Episode length: 79.50 +/- 12.07\n",
      "Eval num_timesteps=929792, episode_reward=-73.90 +/- 7.12\n",
      "Episode length: 74.90 +/- 7.12\n",
      "Eval num_timesteps=929792, episode_reward=-84.90 +/- 19.89\n",
      "Episode length: 85.90 +/- 19.89\n",
      "Eval num_timesteps=929792, episode_reward=-87.20 +/- 19.31\n",
      "Episode length: 88.20 +/- 19.31\n",
      "Eval num_timesteps=929792, episode_reward=-87.90 +/- 36.92\n",
      "Episode length: 88.90 +/- 36.92\n",
      "Eval num_timesteps=929792, episode_reward=-81.00 +/- 11.98\n",
      "Episode length: 82.00 +/- 11.98\n",
      "Eval num_timesteps=929792, episode_reward=-76.40 +/- 7.31\n",
      "Episode length: 77.40 +/- 7.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=929792, episode_reward=-78.60 +/- 11.37\n",
      "Episode length: 79.60 +/- 11.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.95e-05 |       0.00000 |      44.60352 |      6.29e-06 |       0.19992\n",
      "     -0.00012 |       0.00000 |      44.23099 |      6.18e-06 |       0.19973\n",
      "     -0.00015 |       0.00000 |      43.96143 |      6.63e-06 |       0.19997\n",
      "     -0.00026 |       0.00000 |      43.74295 |      1.02e-05 |       0.19993\n",
      "     -0.00030 |       0.00000 |      43.55494 |      1.01e-05 |       0.20006\n",
      "     -0.00035 |       0.00000 |      43.38611 |      1.21e-05 |       0.19996\n",
      "     -0.00039 |       0.00000 |      43.22236 |      1.20e-05 |       0.19977\n",
      "     -0.00037 |       0.00000 |      43.09534 |      1.49e-05 |       0.20008\n",
      "     -0.00048 |       0.00000 |      42.95512 |      1.38e-05 |       0.19999\n",
      "     -0.00052 |       0.00000 |      42.84435 |      1.28e-05 |       0.19999\n",
      "Evaluating losses...\n",
      "     -0.00058 |       0.00000 |      42.74108 |      1.16e-05 |       0.19990\n",
      "------------------------------------\n",
      "| EpLenMean       | 96             |\n",
      "| EpRewMean       | -95            |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 9157           |\n",
      "| TimeElapsed     | 3.42e+03       |\n",
      "| TimestepsSoFar  | 933888         |\n",
      "| ev_tdlam_before | 0.822          |\n",
      "| loss_ent        | 0.1999001      |\n",
      "| loss_kl         | 1.1554227e-05  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00057580695 |\n",
      "| loss_vf_loss    | 42.74108       |\n",
      "------------------------------------\n",
      "********** Iteration 228 ************\n",
      "Eval num_timesteps=933888, episode_reward=-81.90 +/- 13.21\n",
      "Episode length: 82.90 +/- 13.21\n",
      "Eval num_timesteps=933888, episode_reward=-73.90 +/- 6.55\n",
      "Episode length: 74.90 +/- 6.55\n",
      "Eval num_timesteps=933888, episode_reward=-77.90 +/- 9.36\n",
      "Episode length: 78.90 +/- 9.36\n",
      "Eval num_timesteps=933888, episode_reward=-81.00 +/- 11.76\n",
      "Episode length: 82.00 +/- 11.76\n",
      "Eval num_timesteps=933888, episode_reward=-72.90 +/- 6.46\n",
      "Episode length: 73.90 +/- 6.46\n",
      "Eval num_timesteps=933888, episode_reward=-96.90 +/- 21.93\n",
      "Episode length: 97.90 +/- 21.93\n",
      "Eval num_timesteps=933888, episode_reward=-77.40 +/- 4.27\n",
      "Episode length: 78.40 +/- 4.27\n",
      "Eval num_timesteps=933888, episode_reward=-85.90 +/- 9.92\n",
      "Episode length: 86.90 +/- 9.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.98e-05 |       0.00000 |      41.14130 |      2.78e-06 |       0.19775\n",
      "     -0.00010 |       0.00000 |      40.28125 |      5.00e-06 |       0.19758\n",
      "     -0.00019 |       0.00000 |      39.74961 |      7.22e-06 |       0.19782\n",
      "     -0.00025 |       0.00000 |      39.34788 |      1.16e-05 |       0.19759\n",
      "     -0.00032 |       0.00000 |      39.05144 |      9.26e-06 |       0.19762\n",
      "     -0.00036 |       0.00000 |      38.83819 |      9.66e-06 |       0.19788\n",
      "     -0.00036 |       0.00000 |      38.64125 |      1.00e-05 |       0.19760\n",
      "     -0.00043 |       0.00000 |      38.49824 |      1.22e-05 |       0.19773\n",
      "     -0.00046 |       0.00000 |      38.37578 |      1.26e-05 |       0.19789\n",
      "     -0.00043 |       0.00000 |      38.26271 |      1.15e-05 |       0.19780\n",
      "Evaluating losses...\n",
      "     -0.00052 |       0.00000 |      38.18092 |      9.27e-06 |       0.19783\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.5          |\n",
      "| EpRewMean       | -91.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 9203          |\n",
      "| TimeElapsed     | 3.43e+03      |\n",
      "| TimestepsSoFar  | 937984        |\n",
      "| ev_tdlam_before | 0.844         |\n",
      "| loss_ent        | 0.19782634    |\n",
      "| loss_kl         | 9.266449e-06  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0005162413 |\n",
      "| loss_vf_loss    | 38.180923     |\n",
      "-----------------------------------\n",
      "********** Iteration 229 ************\n",
      "Eval num_timesteps=937984, episode_reward=-80.60 +/- 11.03\n",
      "Episode length: 81.60 +/- 11.03\n",
      "Eval num_timesteps=937984, episode_reward=-83.50 +/- 15.96\n",
      "Episode length: 84.50 +/- 15.96\n",
      "Eval num_timesteps=937984, episode_reward=-77.10 +/- 11.49\n",
      "Episode length: 78.10 +/- 11.49\n",
      "Eval num_timesteps=937984, episode_reward=-103.00 +/- 75.53\n",
      "Episode length: 104.00 +/- 75.53\n",
      "Eval num_timesteps=937984, episode_reward=-75.40 +/- 6.28\n",
      "Episode length: 76.40 +/- 6.28\n",
      "Eval num_timesteps=937984, episode_reward=-76.30 +/- 9.10\n",
      "Episode length: 77.30 +/- 9.10\n",
      "Eval num_timesteps=937984, episode_reward=-81.50 +/- 9.71\n",
      "Episode length: 82.50 +/- 9.71\n",
      "Eval num_timesteps=937984, episode_reward=-92.30 +/- 41.30\n",
      "Episode length: 93.30 +/- 41.30\n",
      "Eval num_timesteps=937984, episode_reward=-78.30 +/- 12.22\n",
      "Episode length: 79.30 +/- 12.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     7.18e-05 |       0.00000 |      23.80654 |      4.96e-06 |       0.18272\n",
      "    -5.97e-05 |       0.00000 |      23.52489 |      6.27e-06 |       0.18288\n",
      "     -0.00013 |       0.00000 |      23.34215 |      5.54e-06 |       0.18286\n",
      "     -0.00019 |       0.00000 |      23.18343 |      5.70e-06 |       0.18271\n",
      "     -0.00023 |       0.00000 |      23.04267 |      6.40e-06 |       0.18259\n",
      "     -0.00029 |       0.00000 |      22.91478 |      6.90e-06 |       0.18259\n",
      "     -0.00028 |       0.00000 |      22.79496 |      8.37e-06 |       0.18245\n",
      "     -0.00032 |       0.00000 |      22.69098 |      8.63e-06 |       0.18231\n",
      "     -0.00033 |       0.00000 |      22.59036 |      7.73e-06 |       0.18248\n",
      "     -0.00032 |       0.00000 |      22.49730 |      1.22e-05 |       0.18224\n",
      "Evaluating losses...\n",
      "     -0.00041 |       0.00000 |      22.42974 |      9.65e-06 |       0.18211\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.2          |\n",
      "| EpRewMean       | -87.2         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 9249          |\n",
      "| TimeElapsed     | 3.44e+03      |\n",
      "| TimestepsSoFar  | 942080        |\n",
      "| ev_tdlam_before | 0.909         |\n",
      "| loss_ent        | 0.18211217    |\n",
      "| loss_kl         | 9.648257e-06  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0004101966 |\n",
      "| loss_vf_loss    | 22.42974      |\n",
      "-----------------------------------\n",
      "********** Iteration 230 ************\n",
      "Eval num_timesteps=942080, episode_reward=-82.70 +/- 11.94\n",
      "Episode length: 83.70 +/- 11.94\n",
      "Eval num_timesteps=942080, episode_reward=-77.60 +/- 7.10\n",
      "Episode length: 78.60 +/- 7.10\n",
      "Eval num_timesteps=942080, episode_reward=-77.20 +/- 7.48\n",
      "Episode length: 78.20 +/- 7.48\n",
      "Eval num_timesteps=942080, episode_reward=-88.10 +/- 37.87\n",
      "Episode length: 89.10 +/- 37.87\n",
      "Eval num_timesteps=942080, episode_reward=-77.00 +/- 7.42\n",
      "Episode length: 78.00 +/- 7.42\n",
      "Eval num_timesteps=942080, episode_reward=-72.00 +/- 7.94\n",
      "Episode length: 73.00 +/- 7.94\n",
      "Eval num_timesteps=942080, episode_reward=-84.60 +/- 9.15\n",
      "Episode length: 85.60 +/- 9.15\n",
      "Eval num_timesteps=942080, episode_reward=-82.50 +/- 6.70\n",
      "Episode length: 83.50 +/- 6.70\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.25e-05 |       0.00000 |      23.00155 |      2.94e-06 |       0.18579\n",
      "     -0.00011 |       0.00000 |      22.74969 |      9.29e-06 |       0.18575\n",
      "     -0.00014 |       0.00000 |      22.59485 |      5.32e-06 |       0.18579\n",
      "     -0.00024 |       0.00000 |      22.48811 |      5.40e-06 |       0.18606\n",
      "     -0.00025 |       0.00000 |      22.40106 |      6.89e-06 |       0.18594\n",
      "     -0.00032 |       0.00000 |      22.32116 |      5.61e-06 |       0.18583\n",
      "     -0.00033 |       0.00000 |      22.25911 |      7.72e-06 |       0.18598\n",
      "     -0.00038 |       0.00000 |      22.20510 |      7.57e-06 |       0.18596\n",
      "     -0.00039 |       0.00000 |      22.14211 |      7.07e-06 |       0.18602\n",
      "     -0.00039 |       0.00000 |      22.09513 |      7.56e-06 |       0.18592\n",
      "Evaluating losses...\n",
      "     -0.00049 |       0.00000 |      22.04303 |      7.44e-06 |       0.18590\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.7          |\n",
      "| EpRewMean       | -85.7         |\n",
      "| EpThisIter      | 49            |\n",
      "| EpisodesSoFar   | 9298          |\n",
      "| TimeElapsed     | 3.44e+03      |\n",
      "| TimestepsSoFar  | 946176        |\n",
      "| ev_tdlam_before | 0.916         |\n",
      "| loss_ent        | 0.18590237    |\n",
      "| loss_kl         | 7.437696e-06  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0004910274 |\n",
      "| loss_vf_loss    | 22.043028     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 231 ************\n",
      "Eval num_timesteps=946176, episode_reward=-75.30 +/- 8.39\n",
      "Episode length: 76.30 +/- 8.39\n",
      "Eval num_timesteps=946176, episode_reward=-72.60 +/- 6.30\n",
      "Episode length: 73.60 +/- 6.30\n",
      "Eval num_timesteps=946176, episode_reward=-81.70 +/- 8.61\n",
      "Episode length: 82.70 +/- 8.61\n",
      "Eval num_timesteps=946176, episode_reward=-87.00 +/- 19.16\n",
      "Episode length: 88.00 +/- 19.16\n",
      "Eval num_timesteps=946176, episode_reward=-73.80 +/- 7.55\n",
      "Episode length: 74.80 +/- 7.55\n",
      "Eval num_timesteps=946176, episode_reward=-119.50 +/- 127.15\n",
      "Episode length: 120.40 +/- 126.85\n",
      "Eval num_timesteps=946176, episode_reward=-87.30 +/- 23.72\n",
      "Episode length: 88.30 +/- 23.72\n",
      "Eval num_timesteps=946176, episode_reward=-81.70 +/- 8.43\n",
      "Episode length: 82.70 +/- 8.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     5.43e-05 |       0.00000 |      32.65344 |      1.53e-06 |       0.19955\n",
      "    -6.71e-05 |       0.00000 |      32.48978 |      2.79e-06 |       0.19952\n",
      "     -0.00011 |       0.00000 |      32.34725 |      4.03e-06 |       0.19952\n",
      "     -0.00016 |       0.00000 |      32.22305 |      3.98e-06 |       0.19947\n",
      "     -0.00020 |       0.00000 |      32.11180 |      4.26e-06 |       0.19945\n",
      "     -0.00023 |       0.00000 |      32.01553 |      5.26e-06 |       0.19945\n",
      "     -0.00024 |       0.00000 |      31.93005 |      6.28e-06 |       0.19942\n",
      "     -0.00025 |       0.00000 |      31.83886 |      5.06e-06 |       0.19943\n",
      "     -0.00027 |       0.00000 |      31.75612 |      5.89e-06 |       0.19931\n",
      "     -0.00028 |       0.00000 |      31.68310 |      6.64e-06 |       0.19944\n",
      "Evaluating losses...\n",
      "     -0.00036 |       0.00000 |      31.61549 |      5.98e-06 |       0.19934\n",
      "------------------------------------\n",
      "| EpLenMean       | 87.8           |\n",
      "| EpRewMean       | -86.8          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 9344           |\n",
      "| TimeElapsed     | 3.45e+03       |\n",
      "| TimestepsSoFar  | 950272         |\n",
      "| ev_tdlam_before | 0.871          |\n",
      "| loss_ent        | 0.19933859     |\n",
      "| loss_kl         | 5.9781974e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00036100578 |\n",
      "| loss_vf_loss    | 31.615486      |\n",
      "------------------------------------\n",
      "********** Iteration 232 ************\n",
      "Eval num_timesteps=950272, episode_reward=-72.30 +/- 7.89\n",
      "Episode length: 73.30 +/- 7.89\n",
      "Eval num_timesteps=950272, episode_reward=-79.10 +/- 9.19\n",
      "Episode length: 80.10 +/- 9.19\n",
      "Eval num_timesteps=950272, episode_reward=-81.80 +/- 14.63\n",
      "Episode length: 82.80 +/- 14.63\n",
      "Eval num_timesteps=950272, episode_reward=-79.30 +/- 12.04\n",
      "Episode length: 80.30 +/- 12.04\n",
      "Eval num_timesteps=950272, episode_reward=-81.20 +/- 12.12\n",
      "Episode length: 82.20 +/- 12.12\n",
      "Eval num_timesteps=950272, episode_reward=-75.00 +/- 8.64\n",
      "Episode length: 76.00 +/- 8.64\n",
      "Eval num_timesteps=950272, episode_reward=-84.00 +/- 7.43\n",
      "Episode length: 85.00 +/- 7.43\n",
      "Eval num_timesteps=950272, episode_reward=-85.60 +/- 18.82\n",
      "Episode length: 86.60 +/- 18.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.95e-05 |       0.00000 |      36.39193 |      3.19e-06 |       0.23451\n",
      "     -0.00014 |       0.00000 |      33.44502 |      6.37e-06 |       0.23452\n",
      "     -0.00011 |       0.00000 |      32.20650 |      5.56e-06 |       0.23467\n",
      "     -0.00015 |       0.00000 |      31.52691 |      4.99e-06 |       0.23444\n",
      "     -0.00019 |       0.00000 |      31.10011 |      5.76e-06 |       0.23443\n",
      "     -0.00022 |       0.00000 |      30.79526 |      5.18e-06 |       0.23436\n",
      "     -0.00024 |       0.00000 |      30.56903 |      5.36e-06 |       0.23462\n",
      "     -0.00026 |       0.00000 |      30.39173 |      6.98e-06 |       0.23446\n",
      "     -0.00030 |       0.00000 |      30.25688 |      5.74e-06 |       0.23448\n",
      "     -0.00031 |       0.00000 |      30.14349 |      5.73e-06 |       0.23445\n",
      "Evaluating losses...\n",
      "     -0.00038 |       0.00000 |      30.07056 |      5.49e-06 |       0.23444\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.8           |\n",
      "| EpRewMean       | -91.8          |\n",
      "| EpThisIter      | 41             |\n",
      "| EpisodesSoFar   | 9385           |\n",
      "| TimeElapsed     | 3.45e+03       |\n",
      "| TimestepsSoFar  | 954368         |\n",
      "| ev_tdlam_before | 0.872          |\n",
      "| loss_ent        | 0.23443644     |\n",
      "| loss_kl         | 5.4883685e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00037666992 |\n",
      "| loss_vf_loss    | 30.070562      |\n",
      "------------------------------------\n",
      "********** Iteration 233 ************\n",
      "Eval num_timesteps=954368, episode_reward=-78.30 +/- 5.95\n",
      "Episode length: 79.30 +/- 5.95\n",
      "Eval num_timesteps=954368, episode_reward=-71.30 +/- 8.36\n",
      "Episode length: 72.30 +/- 8.36\n",
      "Eval num_timesteps=954368, episode_reward=-82.90 +/- 11.26\n",
      "Episode length: 83.90 +/- 11.26\n",
      "Eval num_timesteps=954368, episode_reward=-85.40 +/- 25.02\n",
      "Episode length: 86.40 +/- 25.02\n",
      "Eval num_timesteps=954368, episode_reward=-90.20 +/- 33.90\n",
      "Episode length: 91.20 +/- 33.90\n",
      "Eval num_timesteps=954368, episode_reward=-78.70 +/- 3.47\n",
      "Episode length: 79.70 +/- 3.47\n",
      "Eval num_timesteps=954368, episode_reward=-75.80 +/- 6.34\n",
      "Episode length: 76.80 +/- 6.34\n",
      "Eval num_timesteps=954368, episode_reward=-78.70 +/- 10.68\n",
      "Episode length: 79.70 +/- 10.68\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.77e-05 |       0.00000 |      43.30976 |      1.87e-06 |       0.20579\n",
      "     -0.00011 |       0.00000 |      42.61723 |      3.25e-06 |       0.20563\n",
      "     -0.00014 |       0.00000 |      42.24144 |      4.38e-06 |       0.20571\n",
      "     -0.00018 |       0.00000 |      41.97142 |      4.55e-06 |       0.20554\n",
      "     -0.00020 |       0.00000 |      41.78901 |      5.29e-06 |       0.20558\n",
      "     -0.00022 |       0.00000 |      41.64635 |      5.50e-06 |       0.20552\n",
      "     -0.00023 |       0.00000 |      41.52071 |      5.82e-06 |       0.20556\n",
      "     -0.00027 |       0.00000 |      41.42670 |      5.70e-06 |       0.20553\n",
      "     -0.00030 |       0.00000 |      41.34265 |      6.45e-06 |       0.20554\n",
      "     -0.00030 |       0.00000 |      41.27483 |      5.70e-06 |       0.20563\n",
      "Evaluating losses...\n",
      "     -0.00038 |       0.00000 |      41.21444 |      5.11e-06 |       0.20551\n",
      "------------------------------------\n",
      "| EpLenMean       | 97             |\n",
      "| EpRewMean       | -96            |\n",
      "| EpThisIter      | 42             |\n",
      "| EpisodesSoFar   | 9427           |\n",
      "| TimeElapsed     | 3.46e+03       |\n",
      "| TimestepsSoFar  | 958464         |\n",
      "| ev_tdlam_before | 0.823          |\n",
      "| loss_ent        | 0.20550679     |\n",
      "| loss_kl         | 5.1134257e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00037762802 |\n",
      "| loss_vf_loss    | 41.21444       |\n",
      "------------------------------------\n",
      "********** Iteration 234 ************\n",
      "Eval num_timesteps=958464, episode_reward=-81.70 +/- 7.76\n",
      "Episode length: 82.70 +/- 7.76\n",
      "Eval num_timesteps=958464, episode_reward=-82.10 +/- 9.27\n",
      "Episode length: 83.10 +/- 9.27\n",
      "Eval num_timesteps=958464, episode_reward=-84.90 +/- 9.05\n",
      "Episode length: 85.90 +/- 9.05\n",
      "Eval num_timesteps=958464, episode_reward=-80.00 +/- 14.67\n",
      "Episode length: 81.00 +/- 14.67\n",
      "Eval num_timesteps=958464, episode_reward=-91.30 +/- 34.77\n",
      "Episode length: 92.30 +/- 34.77\n",
      "Eval num_timesteps=958464, episode_reward=-84.80 +/- 25.34\n",
      "Episode length: 85.80 +/- 25.34\n",
      "Eval num_timesteps=958464, episode_reward=-75.30 +/- 7.28\n",
      "Episode length: 76.30 +/- 7.28\n",
      "Eval num_timesteps=958464, episode_reward=-80.80 +/- 7.32\n",
      "Episode length: 81.80 +/- 7.32\n",
      "Eval num_timesteps=958464, episode_reward=-124.50 +/- 125.39\n",
      "Episode length: 125.40 +/- 125.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -2.37e-05 |       0.00000 |      53.93019 |      1.91e-06 |       0.24031\n",
      "     -0.00016 |       0.00000 |      52.37817 |      3.97e-06 |       0.24039\n",
      "     -0.00020 |       0.00000 |      51.55843 |      4.36e-06 |       0.24015\n",
      "     -0.00022 |       0.00000 |      51.03732 |      5.14e-06 |       0.24022\n",
      "     -0.00026 |       0.00000 |      50.69545 |      6.08e-06 |       0.24016\n",
      "     -0.00028 |       0.00000 |      50.44322 |      5.90e-06 |       0.24032\n",
      "     -0.00029 |       0.00000 |      50.21965 |      6.64e-06 |       0.24009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00030 |       0.00000 |      50.05715 |      6.32e-06 |       0.24033\n",
      "     -0.00031 |       0.00000 |      49.91877 |      6.77e-06 |       0.24024\n",
      "     -0.00032 |       0.00000 |      49.79886 |      6.56e-06 |       0.24018\n",
      "Evaluating losses...\n",
      "     -0.00036 |       0.00000 |      49.71864 |      7.24e-06 |       0.24039\n",
      "------------------------------------\n",
      "| EpLenMean       | 103            |\n",
      "| EpRewMean       | -102           |\n",
      "| EpThisIter      | 37             |\n",
      "| EpisodesSoFar   | 9464           |\n",
      "| TimeElapsed     | 3.47e+03       |\n",
      "| TimestepsSoFar  | 962560         |\n",
      "| ev_tdlam_before | 0.807          |\n",
      "| loss_ent        | 0.240393       |\n",
      "| loss_kl         | 7.2413477e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00035530515 |\n",
      "| loss_vf_loss    | 49.718643      |\n",
      "------------------------------------\n",
      "********** Iteration 235 ************\n",
      "Eval num_timesteps=962560, episode_reward=-82.10 +/- 12.70\n",
      "Episode length: 83.10 +/- 12.70\n",
      "Eval num_timesteps=962560, episode_reward=-84.50 +/- 23.89\n",
      "Episode length: 85.50 +/- 23.89\n",
      "Eval num_timesteps=962560, episode_reward=-79.30 +/- 6.05\n",
      "Episode length: 80.30 +/- 6.05\n",
      "Eval num_timesteps=962560, episode_reward=-79.70 +/- 9.00\n",
      "Episode length: 80.70 +/- 9.00\n",
      "Eval num_timesteps=962560, episode_reward=-85.00 +/- 7.48\n",
      "Episode length: 86.00 +/- 7.48\n",
      "Eval num_timesteps=962560, episode_reward=-88.80 +/- 12.46\n",
      "Episode length: 89.80 +/- 12.46\n",
      "Eval num_timesteps=962560, episode_reward=-83.50 +/- 12.22\n",
      "Episode length: 84.50 +/- 12.22\n",
      "Eval num_timesteps=962560, episode_reward=-76.60 +/- 6.77\n",
      "Episode length: 77.60 +/- 6.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.69e-05 |       0.00000 |      42.41747 |      3.03e-06 |       0.23118\n",
      "    -7.08e-05 |       0.00000 |      39.82564 |      2.04e-06 |       0.23089\n",
      "     -0.00012 |       0.00000 |      38.44170 |      2.92e-06 |       0.23110\n",
      "     -0.00018 |       0.00000 |      37.57927 |      4.92e-06 |       0.23133\n",
      "     -0.00022 |       0.00000 |      36.97275 |      6.05e-06 |       0.23144\n",
      "     -0.00021 |       0.00000 |      36.51999 |      5.96e-06 |       0.23129\n",
      "     -0.00025 |       0.00000 |      36.16663 |      5.57e-06 |       0.23117\n",
      "     -0.00028 |       0.00000 |      35.88607 |      6.43e-06 |       0.23116\n",
      "     -0.00028 |       0.00000 |      35.64981 |      6.73e-06 |       0.23127\n",
      "     -0.00030 |       0.00000 |      35.45275 |      6.86e-06 |       0.23115\n",
      "Evaluating losses...\n",
      "     -0.00034 |       0.00000 |      35.34547 |      6.83e-06 |       0.23101\n",
      "------------------------------------\n",
      "| EpLenMean       | 100            |\n",
      "| EpRewMean       | -99.5          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 9507           |\n",
      "| TimeElapsed     | 3.47e+03       |\n",
      "| TimestepsSoFar  | 966656         |\n",
      "| ev_tdlam_before | 0.836          |\n",
      "| loss_ent        | 0.23101482     |\n",
      "| loss_kl         | 6.8324953e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00033975544 |\n",
      "| loss_vf_loss    | 35.34547       |\n",
      "------------------------------------\n",
      "********** Iteration 236 ************\n",
      "Eval num_timesteps=966656, episode_reward=-74.30 +/- 7.82\n",
      "Episode length: 75.30 +/- 7.82\n",
      "Eval num_timesteps=966656, episode_reward=-79.60 +/- 4.27\n",
      "Episode length: 80.60 +/- 4.27\n",
      "Eval num_timesteps=966656, episode_reward=-80.80 +/- 11.30\n",
      "Episode length: 81.80 +/- 11.30\n",
      "Eval num_timesteps=966656, episode_reward=-84.10 +/- 10.15\n",
      "Episode length: 85.10 +/- 10.15\n",
      "Eval num_timesteps=966656, episode_reward=-79.20 +/- 10.11\n",
      "Episode length: 80.20 +/- 10.11\n",
      "Eval num_timesteps=966656, episode_reward=-90.70 +/- 19.50\n",
      "Episode length: 91.70 +/- 19.50\n",
      "Eval num_timesteps=966656, episode_reward=-74.40 +/- 8.13\n",
      "Episode length: 75.40 +/- 8.13\n",
      "Eval num_timesteps=966656, episode_reward=-76.30 +/- 9.95\n",
      "Episode length: 77.30 +/- 9.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.41e-05 |       0.00000 |      33.74410 |      1.13e-06 |       0.19211\n",
      "    -7.22e-05 |       0.00000 |      33.27612 |      1.10e-06 |       0.19206\n",
      "     -0.00011 |       0.00000 |      32.95205 |      1.77e-06 |       0.19194\n",
      "     -0.00013 |       0.00000 |      32.70949 |      2.05e-06 |       0.19201\n",
      "     -0.00015 |       0.00000 |      32.51530 |      1.78e-06 |       0.19191\n",
      "     -0.00018 |       0.00000 |      32.35519 |      1.59e-06 |       0.19195\n",
      "     -0.00019 |       0.00000 |      32.21703 |      2.19e-06 |       0.19194\n",
      "     -0.00019 |       0.00000 |      32.09389 |      2.42e-06 |       0.19194\n",
      "     -0.00020 |       0.00000 |      31.98936 |      2.04e-06 |       0.19191\n",
      "     -0.00022 |       0.00000 |      31.89542 |      2.31e-06 |       0.19187\n",
      "Evaluating losses...\n",
      "     -0.00027 |       0.00000 |      31.82865 |      2.21e-06 |       0.19179\n",
      "------------------------------------\n",
      "| EpLenMean       | 96             |\n",
      "| EpRewMean       | -95            |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 9553           |\n",
      "| TimeElapsed     | 3.48e+03       |\n",
      "| TimestepsSoFar  | 970752         |\n",
      "| ev_tdlam_before | 0.875          |\n",
      "| loss_ent        | 0.19178617     |\n",
      "| loss_kl         | 2.2133659e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00026605884 |\n",
      "| loss_vf_loss    | 31.828651      |\n",
      "------------------------------------\n",
      "********** Iteration 237 ************\n",
      "Eval num_timesteps=970752, episode_reward=-85.40 +/- 16.65\n",
      "Episode length: 86.40 +/- 16.65\n",
      "Eval num_timesteps=970752, episode_reward=-79.70 +/- 5.92\n",
      "Episode length: 80.70 +/- 5.92\n",
      "Eval num_timesteps=970752, episode_reward=-77.50 +/- 6.80\n",
      "Episode length: 78.50 +/- 6.80\n",
      "Eval num_timesteps=970752, episode_reward=-85.70 +/- 10.22\n",
      "Episode length: 86.70 +/- 10.22\n",
      "Eval num_timesteps=970752, episode_reward=-74.00 +/- 6.00\n",
      "Episode length: 75.00 +/- 6.00\n",
      "Eval num_timesteps=970752, episode_reward=-84.80 +/- 25.86\n",
      "Episode length: 85.80 +/- 25.86\n",
      "Eval num_timesteps=970752, episode_reward=-76.60 +/- 6.74\n",
      "Episode length: 77.60 +/- 6.74\n",
      "Eval num_timesteps=970752, episode_reward=-75.90 +/- 7.23\n",
      "Episode length: 76.90 +/- 7.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     8.15e-06 |       0.00000 |      47.10019 |      7.18e-07 |       0.22678\n",
      "    -9.41e-05 |       0.00000 |      46.95367 |      1.89e-06 |       0.22684\n",
      "     -0.00011 |       0.00000 |      46.84388 |      2.16e-06 |       0.22691\n",
      "     -0.00014 |       0.00000 |      46.75785 |      1.87e-06 |       0.22681\n",
      "     -0.00015 |       0.00000 |      46.67507 |      2.23e-06 |       0.22694\n",
      "     -0.00015 |       0.00000 |      46.60840 |      2.12e-06 |       0.22682\n",
      "     -0.00018 |       0.00000 |      46.54047 |      2.25e-06 |       0.22684\n",
      "     -0.00018 |       0.00000 |      46.48098 |      2.59e-06 |       0.22677\n",
      "     -0.00021 |       0.00000 |      46.42501 |      2.42e-06 |       0.22677\n",
      "     -0.00022 |       0.00000 |      46.37565 |      2.55e-06 |       0.22682\n",
      "Evaluating losses...\n",
      "     -0.00021 |       0.00000 |      46.32731 |      2.50e-06 |       0.22658\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.6           |\n",
      "| EpRewMean       | -91.6          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 9596           |\n",
      "| TimeElapsed     | 3.49e+03       |\n",
      "| TimestepsSoFar  | 974848         |\n",
      "| ev_tdlam_before | 0.834          |\n",
      "| loss_ent        | 0.22658397     |\n",
      "| loss_kl         | 2.4995034e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00021035923 |\n",
      "| loss_vf_loss    | 46.32731       |\n",
      "------------------------------------\n",
      "********** Iteration 238 ************\n",
      "Eval num_timesteps=974848, episode_reward=-82.50 +/- 9.98\n",
      "Episode length: 83.50 +/- 9.98\n",
      "Eval num_timesteps=974848, episode_reward=-82.90 +/- 16.82\n",
      "Episode length: 83.90 +/- 16.82\n",
      "Eval num_timesteps=974848, episode_reward=-81.80 +/- 23.92\n",
      "Episode length: 82.80 +/- 23.92\n",
      "Eval num_timesteps=974848, episode_reward=-77.10 +/- 7.41\n",
      "Episode length: 78.10 +/- 7.41\n",
      "Eval num_timesteps=974848, episode_reward=-80.10 +/- 24.15\n",
      "Episode length: 81.10 +/- 24.15\n",
      "Eval num_timesteps=974848, episode_reward=-83.30 +/- 15.25\n",
      "Episode length: 84.30 +/- 15.25\n",
      "Eval num_timesteps=974848, episode_reward=-79.20 +/- 9.25\n",
      "Episode length: 80.20 +/- 9.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=974848, episode_reward=-77.80 +/- 9.14\n",
      "Episode length: 78.80 +/- 9.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -4.35e-05 |       0.00000 |      48.53344 |      5.12e-07 |       0.19142\n",
      "     -0.00012 |       0.00000 |      47.85929 |      1.90e-06 |       0.19163\n",
      "     -0.00019 |       0.00000 |      47.27733 |      2.72e-06 |       0.19166\n",
      "     -0.00019 |       0.00000 |      46.78163 |      2.73e-06 |       0.19170\n",
      "     -0.00022 |       0.00000 |      46.35685 |      3.33e-06 |       0.19167\n",
      "     -0.00022 |       0.00000 |      45.98158 |      3.31e-06 |       0.19171\n",
      "     -0.00024 |       0.00000 |      45.64834 |      3.05e-06 |       0.19172\n",
      "     -0.00026 |       0.00000 |      45.35695 |      3.31e-06 |       0.19170\n",
      "     -0.00026 |       0.00000 |      45.09705 |      4.04e-06 |       0.19181\n",
      "     -0.00026 |       0.00000 |      44.85057 |      3.82e-06 |       0.19167\n",
      "Evaluating losses...\n",
      "     -0.00030 |       0.00000 |      44.72264 |      3.96e-06 |       0.19180\n",
      "------------------------------------\n",
      "| EpLenMean       | 95.2           |\n",
      "| EpRewMean       | -94.2          |\n",
      "| EpThisIter      | 42             |\n",
      "| EpisodesSoFar   | 9638           |\n",
      "| TimeElapsed     | 3.49e+03       |\n",
      "| TimestepsSoFar  | 978944         |\n",
      "| ev_tdlam_before | 0.81           |\n",
      "| loss_ent        | 0.1917978      |\n",
      "| loss_kl         | 3.962128e-06   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00029654108 |\n",
      "| loss_vf_loss    | 44.722637      |\n",
      "------------------------------------\n",
      "********** Iteration 239 ************\n",
      "Eval num_timesteps=978944, episode_reward=-77.60 +/- 7.06\n",
      "Episode length: 78.60 +/- 7.06\n",
      "Eval num_timesteps=978944, episode_reward=-76.40 +/- 6.95\n",
      "Episode length: 77.40 +/- 6.95\n",
      "Eval num_timesteps=978944, episode_reward=-79.40 +/- 7.94\n",
      "Episode length: 80.40 +/- 7.94\n",
      "Eval num_timesteps=978944, episode_reward=-87.80 +/- 31.88\n",
      "Episode length: 88.80 +/- 31.88\n",
      "Eval num_timesteps=978944, episode_reward=-75.00 +/- 6.24\n",
      "Episode length: 76.00 +/- 6.24\n",
      "Eval num_timesteps=978944, episode_reward=-77.80 +/- 10.08\n",
      "Episode length: 78.80 +/- 10.08\n",
      "Eval num_timesteps=978944, episode_reward=-80.40 +/- 10.29\n",
      "Episode length: 81.40 +/- 10.29\n",
      "Eval num_timesteps=978944, episode_reward=-77.00 +/- 6.37\n",
      "Episode length: 78.00 +/- 6.37\n",
      "Eval num_timesteps=978944, episode_reward=-75.90 +/- 7.31\n",
      "Episode length: 76.90 +/- 7.31\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     3.45e-05 |       0.00000 |      38.21253 |      5.38e-07 |       0.21316\n",
      "    -6.20e-05 |       0.00000 |      37.45829 |      5.60e-07 |       0.21321\n",
      "    -9.56e-05 |       0.00000 |      36.81511 |      7.29e-07 |       0.21312\n",
      "    -9.07e-05 |       0.00000 |      36.30590 |      1.13e-06 |       0.21300\n",
      "     -0.00011 |       0.00000 |      35.85508 |      8.10e-07 |       0.21316\n",
      "     -0.00013 |       0.00000 |      35.47230 |      7.99e-07 |       0.21308\n",
      "     -0.00014 |       0.00000 |      35.14211 |      1.11e-06 |       0.21305\n",
      "     -0.00015 |       0.00000 |      34.84855 |      1.13e-06 |       0.21302\n",
      "     -0.00015 |       0.00000 |      34.58722 |      9.75e-07 |       0.21307\n",
      "     -0.00015 |       0.00000 |      34.35054 |      9.75e-07 |       0.21310\n",
      "Evaluating losses...\n",
      "     -0.00018 |       0.00000 |      34.22814 |      1.18e-06 |       0.21299\n",
      "------------------------------------\n",
      "| EpLenMean       | 93.6           |\n",
      "| EpRewMean       | -92.6          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 9683           |\n",
      "| TimeElapsed     | 3.5e+03        |\n",
      "| TimestepsSoFar  | 983040         |\n",
      "| ev_tdlam_before | 0.864          |\n",
      "| loss_ent        | 0.21298589     |\n",
      "| loss_kl         | 1.182617e-06   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00017618667 |\n",
      "| loss_vf_loss    | 34.22814       |\n",
      "------------------------------------\n",
      "********** Iteration 240 ************\n",
      "Eval num_timesteps=983040, episode_reward=-77.20 +/- 9.01\n",
      "Episode length: 78.20 +/- 9.01\n",
      "Eval num_timesteps=983040, episode_reward=-77.30 +/- 9.20\n",
      "Episode length: 78.30 +/- 9.20\n",
      "Eval num_timesteps=983040, episode_reward=-78.10 +/- 5.84\n",
      "Episode length: 79.10 +/- 5.84\n",
      "Eval num_timesteps=983040, episode_reward=-77.50 +/- 6.95\n",
      "Episode length: 78.50 +/- 6.95\n",
      "Eval num_timesteps=983040, episode_reward=-87.80 +/- 30.13\n",
      "Episode length: 88.80 +/- 30.13\n",
      "Eval num_timesteps=983040, episode_reward=-78.20 +/- 8.35\n",
      "Episode length: 79.20 +/- 8.35\n",
      "Eval num_timesteps=983040, episode_reward=-85.20 +/- 15.35\n",
      "Episode length: 86.20 +/- 15.35\n",
      "Eval num_timesteps=983040, episode_reward=-76.50 +/- 9.82\n",
      "Episode length: 77.50 +/- 9.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -7.89e-06 |       0.00000 |      30.40563 |      3.85e-07 |       0.18704\n",
      "    -6.86e-05 |       0.00000 |      30.32013 |      7.03e-07 |       0.18686\n",
      "    -8.60e-05 |       0.00000 |      30.24975 |      8.69e-07 |       0.18682\n",
      "    -9.69e-05 |       0.00000 |      30.18736 |      9.73e-07 |       0.18680\n",
      "     -0.00011 |       0.00000 |      30.13208 |      1.03e-06 |       0.18681\n",
      "     -0.00012 |       0.00000 |      30.08027 |      1.04e-06 |       0.18682\n",
      "     -0.00013 |       0.00000 |      30.03340 |      1.19e-06 |       0.18680\n",
      "     -0.00013 |       0.00000 |      29.99051 |      1.25e-06 |       0.18678\n",
      "     -0.00014 |       0.00000 |      29.94546 |      1.29e-06 |       0.18679\n",
      "     -0.00014 |       0.00000 |      29.90479 |      1.49e-06 |       0.18678\n",
      "Evaluating losses...\n",
      "     -0.00017 |       0.00000 |      29.87696 |      1.47e-06 |       0.18677\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.6           |\n",
      "| EpRewMean       | -91.6          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 9728           |\n",
      "| TimeElapsed     | 3.51e+03       |\n",
      "| TimestepsSoFar  | 987136         |\n",
      "| ev_tdlam_before | 0.886          |\n",
      "| loss_ent        | 0.18677276     |\n",
      "| loss_kl         | 1.4668349e-06  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00016897463 |\n",
      "| loss_vf_loss    | 29.87696       |\n",
      "------------------------------------\n",
      "********** Iteration 241 ************\n",
      "Eval num_timesteps=987136, episode_reward=-79.60 +/- 5.75\n",
      "Episode length: 80.60 +/- 5.75\n",
      "Eval num_timesteps=987136, episode_reward=-100.60 +/- 55.15\n",
      "Episode length: 101.60 +/- 55.15\n",
      "Eval num_timesteps=987136, episode_reward=-85.70 +/- 11.57\n",
      "Episode length: 86.70 +/- 11.57\n",
      "Eval num_timesteps=987136, episode_reward=-93.80 +/- 36.74\n",
      "Episode length: 94.80 +/- 36.74\n",
      "Eval num_timesteps=987136, episode_reward=-72.90 +/- 8.55\n",
      "Episode length: 73.90 +/- 8.55\n",
      "Eval num_timesteps=987136, episode_reward=-78.10 +/- 8.40\n",
      "Episode length: 79.10 +/- 8.40\n",
      "Eval num_timesteps=987136, episode_reward=-86.60 +/- 22.22\n",
      "Episode length: 87.60 +/- 22.22\n",
      "Eval num_timesteps=987136, episode_reward=-93.90 +/- 22.91\n",
      "Episode length: 94.90 +/- 22.91\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     1.84e-05 |       0.00000 |      47.71040 |      1.70e-07 |       0.20730\n",
      "    -2.53e-05 |       0.00000 |      47.61134 |      2.63e-07 |       0.20739\n",
      "    -2.16e-05 |       0.00000 |      47.51846 |      2.44e-07 |       0.20733\n",
      "    -5.49e-05 |       0.00000 |      47.44147 |      2.90e-07 |       0.20739\n",
      "    -5.89e-05 |       0.00000 |      47.36797 |      3.85e-07 |       0.20734\n",
      "    -7.40e-05 |       0.00000 |      47.30077 |      2.69e-07 |       0.20734\n",
      "    -7.46e-05 |       0.00000 |      47.24305 |      3.08e-07 |       0.20738\n",
      "    -8.21e-05 |       0.00000 |      47.18527 |      3.56e-07 |       0.20737\n",
      "    -8.11e-05 |       0.00000 |      47.13337 |      3.26e-07 |       0.20738\n",
      "    -9.74e-05 |       0.00000 |      47.08287 |      4.43e-07 |       0.20734\n",
      "Evaluating losses...\n",
      "    -9.08e-05 |       0.00000 |      47.05127 |      6.06e-07 |       0.20726\n",
      "------------------------------------\n",
      "| EpLenMean       | 91.3           |\n",
      "| EpRewMean       | -90.3          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 9771           |\n",
      "| TimeElapsed     | 3.51e+03       |\n",
      "| TimestepsSoFar  | 991232         |\n",
      "| ev_tdlam_before | 0.818          |\n",
      "| loss_ent        | 0.20726499     |\n",
      "| loss_kl         | 6.0572506e-07  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -9.0789516e-05 |\n",
      "| loss_vf_loss    | 47.05127       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 242 ************\n",
      "Eval num_timesteps=991232, episode_reward=-78.80 +/- 10.24\n",
      "Episode length: 79.80 +/- 10.24\n",
      "Eval num_timesteps=991232, episode_reward=-82.20 +/- 11.25\n",
      "Episode length: 83.20 +/- 11.25\n",
      "Eval num_timesteps=991232, episode_reward=-78.80 +/- 14.86\n",
      "Episode length: 79.80 +/- 14.86\n",
      "Eval num_timesteps=991232, episode_reward=-79.10 +/- 10.46\n",
      "Episode length: 80.10 +/- 10.46\n",
      "Eval num_timesteps=991232, episode_reward=-80.50 +/- 13.65\n",
      "Episode length: 81.50 +/- 13.65\n",
      "Eval num_timesteps=991232, episode_reward=-76.90 +/- 9.12\n",
      "Episode length: 77.90 +/- 9.12\n",
      "Eval num_timesteps=991232, episode_reward=-78.20 +/- 5.95\n",
      "Episode length: 79.20 +/- 5.95\n",
      "Eval num_timesteps=991232, episode_reward=-77.40 +/- 9.35\n",
      "Episode length: 78.40 +/- 9.35\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.32e-06 |       0.00000 |      35.20561 |      3.86e-08 |       0.18344\n",
      "    -5.45e-06 |       0.00000 |      35.09093 |      6.95e-08 |       0.18346\n",
      "    -1.47e-05 |       0.00000 |      34.99244 |      1.14e-07 |       0.18341\n",
      "    -2.11e-05 |       0.00000 |      34.90240 |      1.47e-07 |       0.18342\n",
      "    -2.95e-05 |       0.00000 |      34.82552 |      1.63e-07 |       0.18343\n",
      "    -3.42e-05 |       0.00000 |      34.75793 |      2.04e-07 |       0.18343\n",
      "    -3.64e-05 |       0.00000 |      34.69582 |      2.22e-07 |       0.18342\n",
      "    -4.29e-05 |       0.00000 |      34.63904 |      2.12e-07 |       0.18343\n",
      "    -4.34e-05 |       0.00000 |      34.58793 |      2.13e-07 |       0.18342\n",
      "    -4.43e-05 |       0.00000 |      34.54356 |      2.40e-07 |       0.18343\n",
      "Evaluating losses...\n",
      "    -6.06e-05 |       0.00000 |      34.51340 |      2.63e-07 |       0.18343\n",
      "------------------------------------\n",
      "| EpLenMean       | 93.3           |\n",
      "| EpRewMean       | -92.3          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 9816           |\n",
      "| TimeElapsed     | 3.52e+03       |\n",
      "| TimestepsSoFar  | 995328         |\n",
      "| ev_tdlam_before | 0.868          |\n",
      "| loss_ent        | 0.18343046     |\n",
      "| loss_kl         | 2.6336662e-07  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -6.0647726e-05 |\n",
      "| loss_vf_loss    | 34.513405      |\n",
      "------------------------------------\n",
      "********** Iteration 243 ************\n",
      "Eval num_timesteps=995328, episode_reward=-82.10 +/- 8.70\n",
      "Episode length: 83.10 +/- 8.70\n",
      "Eval num_timesteps=995328, episode_reward=-91.40 +/- 27.62\n",
      "Episode length: 92.40 +/- 27.62\n",
      "Eval num_timesteps=995328, episode_reward=-77.40 +/- 12.78\n",
      "Episode length: 78.40 +/- 12.78\n",
      "Eval num_timesteps=995328, episode_reward=-84.30 +/- 8.06\n",
      "Episode length: 85.30 +/- 8.06\n",
      "Eval num_timesteps=995328, episode_reward=-85.00 +/- 6.88\n",
      "Episode length: 86.00 +/- 6.88\n",
      "Eval num_timesteps=995328, episode_reward=-79.20 +/- 10.39\n",
      "Episode length: 80.20 +/- 10.39\n",
      "Eval num_timesteps=995328, episode_reward=-82.40 +/- 7.55\n",
      "Episode length: 83.40 +/- 7.55\n",
      "Eval num_timesteps=995328, episode_reward=-81.60 +/- 9.83\n",
      "Episode length: 82.60 +/- 9.83\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -7.80e-06 |       0.00000 |      38.49840 |      3.09e-08 |       0.20352\n",
      "    -1.51e-05 |       0.00000 |      38.44209 |      3.92e-08 |       0.20353\n",
      "    -2.18e-05 |       0.00000 |      38.38779 |      5.45e-08 |       0.20351\n",
      "    -2.10e-05 |       0.00000 |      38.33511 |      4.65e-08 |       0.20353\n",
      "    -2.47e-05 |       0.00000 |      38.28572 |      4.57e-08 |       0.20352\n",
      "    -2.96e-05 |       0.00000 |      38.23694 |      5.23e-08 |       0.20353\n",
      "    -2.92e-05 |       0.00000 |      38.19058 |      5.47e-08 |       0.20353\n",
      "    -3.35e-05 |       0.00000 |      38.14546 |      5.47e-08 |       0.20352\n",
      "    -3.28e-05 |       0.00000 |      38.10235 |      5.29e-08 |       0.20352\n",
      "    -3.49e-05 |       0.00000 |      38.05993 |      6.46e-08 |       0.20352\n",
      "Evaluating losses...\n",
      "    -4.45e-05 |       0.00000 |      38.03746 |      6.05e-08 |       0.20354\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92            |\n",
      "| EpRewMean       | -91           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 9861          |\n",
      "| TimeElapsed     | 3.53e+03      |\n",
      "| TimestepsSoFar  | 999424        |\n",
      "| ev_tdlam_before | 0.865         |\n",
      "| loss_ent        | 0.2035394     |\n",
      "| loss_kl         | 6.046575e-08  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -4.451495e-05 |\n",
      "| loss_vf_loss    | 38.037457     |\n",
      "-----------------------------------\n",
      "********** Iteration 244 ************\n",
      "Eval num_timesteps=999424, episode_reward=-82.50 +/- 9.75\n",
      "Episode length: 83.50 +/- 9.75\n",
      "Eval num_timesteps=999424, episode_reward=-79.30 +/- 9.49\n",
      "Episode length: 80.30 +/- 9.49\n",
      "Eval num_timesteps=999424, episode_reward=-78.20 +/- 6.46\n",
      "Episode length: 79.20 +/- 6.46\n",
      "Eval num_timesteps=999424, episode_reward=-104.60 +/- 37.41\n",
      "Episode length: 105.60 +/- 37.41\n",
      "Eval num_timesteps=999424, episode_reward=-80.90 +/- 12.14\n",
      "Episode length: 81.90 +/- 12.14\n",
      "Eval num_timesteps=999424, episode_reward=-79.70 +/- 12.40\n",
      "Episode length: 80.70 +/- 12.40\n",
      "Eval num_timesteps=999424, episode_reward=-79.00 +/- 7.21\n",
      "Episode length: 80.00 +/- 7.21\n",
      "Eval num_timesteps=999424, episode_reward=-81.00 +/- 6.86\n",
      "Episode length: 82.00 +/- 6.86\n",
      "Eval num_timesteps=999424, episode_reward=-82.90 +/- 13.70\n",
      "Episode length: 83.90 +/- 13.70\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -6.01e-07 |       0.00000 |      36.53935 |      6.59e-10 |       0.18941\n",
      "    -1.52e-06 |       0.00000 |      36.53568 |     -6.93e-10 |       0.18941\n",
      "    -1.60e-06 |       0.00000 |      36.53185 |     -1.50e-10 |       0.18941\n",
      "    -1.98e-06 |       0.00000 |      36.52810 |     -3.49e-10 |       0.18941\n",
      "    -2.02e-06 |       0.00000 |      36.52446 |      4.88e-10 |       0.18941\n",
      "    -2.38e-06 |       0.00000 |      36.52069 |      6.27e-10 |       0.18941\n",
      "    -2.60e-06 |       0.00000 |      36.51706 |      2.18e-10 |       0.18941\n",
      "    -2.81e-06 |       0.00000 |      36.51342 |     -4.14e-11 |       0.18941\n",
      "    -2.97e-06 |       0.00000 |      36.50980 |     -1.77e-10 |       0.18941\n",
      "    -3.20e-06 |       0.00000 |      36.50621 |     -9.39e-10 |       0.18941\n",
      "Evaluating losses...\n",
      "    -3.70e-06 |       0.00000 |      36.50413 |     -6.12e-10 |       0.18941\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.5           |\n",
      "| EpRewMean       | -89.5          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 9906           |\n",
      "| TimeElapsed     | 3.53e+03       |\n",
      "| TimestepsSoFar  | 1003520        |\n",
      "| ev_tdlam_before | 0.862          |\n",
      "| loss_ent        | 0.1894069      |\n",
      "| loss_kl         | -6.121514e-10  |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -3.6987476e-06 |\n",
      "| loss_vf_loss    | 36.50413       |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "dnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "dnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_acrobot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 722\n",
    "EVAL_FREQ = 500\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"bnn_acrobot\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.seed(SEED)\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "bnn = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<AcrobotEnv<Acrobot-v1>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fbef8c6afd0>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=0, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -3.71e-05 |       0.00000 |     262.05954 |      9.22e-05 |       1.09852\n",
      "     -0.00056 |       0.00000 |     249.26990 |       0.00029 |       1.09833\n",
      "     -0.00069 |       0.00000 |     234.36386 |       0.00070 |       1.09792\n",
      "     -0.00126 |       0.00000 |     217.47513 |       0.00108 |       1.09754\n",
      "     -0.00183 |       0.00000 |     199.10580 |       0.00202 |       1.09660\n",
      "     -0.00235 |       0.00000 |     179.78699 |       0.00347 |       1.09518\n",
      "     -0.00272 |       0.00000 |     160.18877 |       0.00422 |       1.09443\n",
      "     -0.00319 |       0.00000 |     140.58618 |       0.00540 |       1.09327\n",
      "     -0.00361 |       0.00000 |     121.35806 |       0.00548 |       1.09322\n",
      "     -0.00371 |       0.00000 |     103.16393 |       0.00525 |       1.09340\n",
      "Evaluating losses...\n",
      "     -0.00409 |       0.00000 |      94.37483 |       0.00535 |       1.09331\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 8            |\n",
      "| TimeElapsed     | 44           |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | -0.000174    |\n",
      "| loss_ent        | 1.0933065    |\n",
      "| loss_kl         | 0.005345198  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004089619 |\n",
      "| loss_vf_loss    | 94.37483     |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=4096, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -1.56e-05 |       0.00000 |     204.08644 |      8.64e-05 |       1.09452\n",
      "      0.00025 |       0.00000 |     159.32629 |       0.00011 |       1.09490\n",
      "     -0.00042 |       0.00000 |     122.40974 |       0.00018 |       1.09511\n",
      "     -0.00026 |       0.00000 |      92.93335 |       0.00027 |       1.09560\n",
      "     -0.00105 |       0.00000 |      69.88924 |       0.00041 |       1.09538\n",
      "     -0.00105 |       0.00000 |      52.28814 |       0.00068 |       1.09581\n",
      "     -0.00139 |       0.00000 |      39.09489 |       0.00099 |       1.09581\n",
      "     -0.00216 |       0.00000 |      29.67936 |       0.00115 |       1.09585\n",
      "     -0.00194 |       0.00000 |      22.84839 |       0.00154 |       1.09618\n",
      "     -0.00229 |       0.00000 |      18.12847 |       0.00189 |       1.09560\n",
      "Evaluating losses...\n",
      "     -0.00245 |       0.00000 |      16.24667 |       0.00179 |       1.09583\n",
      "-----------------------------------\n",
      "| EpLenMean       | 500           |\n",
      "| EpRewMean       | -500          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 16            |\n",
      "| TimeElapsed     | 85.9          |\n",
      "| TimestepsSoFar  | 8192          |\n",
      "| ev_tdlam_before | 0.055         |\n",
      "| loss_ent        | 1.0958313     |\n",
      "| loss_kl         | 0.0017881305  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0024450105 |\n",
      "| loss_vf_loss    | 16.246672     |\n",
      "-----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00039 |       0.00000 |     157.82458 |       0.00032 |       1.09455\n",
      "     -0.00255 |       0.00000 |     116.94200 |       0.00100 |       1.09357\n",
      "     -0.00414 |       0.00000 |      88.50742 |       0.00233 |       1.09158\n",
      "     -0.00482 |       0.00000 |      68.75917 |       0.00400 |       1.08823\n",
      "     -0.00530 |       0.00000 |      54.79278 |       0.00530 |       1.08622\n",
      "     -0.00665 |       0.00000 |      44.69633 |       0.00649 |       1.08485\n",
      "     -0.00671 |       0.00000 |      37.44170 |       0.00789 |       1.08304\n",
      "     -0.00675 |       0.00000 |      32.27378 |       0.00785 |       1.08358\n",
      "     -0.00723 |       0.00000 |      28.62693 |       0.00920 |       1.08148\n",
      "     -0.00677 |       0.00000 |      25.93301 |       0.00926 |       1.08181\n",
      "Evaluating losses...\n",
      "     -0.00789 |       0.00000 |      24.98059 |       0.00949 |       1.08128\n",
      "----------------------------------\n",
      "| EpLenMean       | 500          |\n",
      "| EpRewMean       | -500         |\n",
      "| EpThisIter      | 8            |\n",
      "| EpisodesSoFar   | 24           |\n",
      "| TimeElapsed     | 127          |\n",
      "| TimestepsSoFar  | 12288        |\n",
      "| ev_tdlam_before | 0.241        |\n",
      "| loss_ent        | 1.0812819    |\n",
      "| loss_kl         | 0.009487283  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.007888878 |\n",
      "| loss_vf_loss    | 24.980585    |\n",
      "----------------------------------\n",
      "********** Iteration 3 ************\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00069 |       0.00000 |     138.99931 |       0.00082 |       1.07441\n",
      "     -0.00297 |       0.00000 |     109.27160 |       0.00290 |       1.06704\n",
      "     -0.00598 |       0.00000 |      88.47849 |       0.00440 |       1.06253\n",
      "     -0.00437 |       0.00000 |      73.58745 |       0.00462 |       1.06190\n",
      "     -0.00525 |       0.00000 |      63.11242 |       0.00482 |       1.06095\n",
      "     -0.00574 |       0.00000 |      55.08356 |       0.00527 |       1.05965\n",
      "     -0.00594 |       0.00000 |      49.57607 |       0.00553 |       1.05832\n",
      "     -0.00602 |       0.00000 |      45.30999 |       0.00549 |       1.05809\n",
      "     -0.00625 |       0.00000 |      41.65629 |       0.00603 |       1.05639\n",
      "     -0.00654 |       0.00000 |      39.36731 |       0.00658 |       1.05596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "     -0.00631 |       0.00000 |      38.12122 |       0.00666 |       1.05631\n",
      "-----------------------------------\n",
      "| EpLenMean       | 497           |\n",
      "| EpRewMean       | -497          |\n",
      "| EpThisIter      | 8             |\n",
      "| EpisodesSoFar   | 32            |\n",
      "| TimeElapsed     | 169           |\n",
      "| TimestepsSoFar  | 16384         |\n",
      "| ev_tdlam_before | 0.285         |\n",
      "| loss_ent        | 1.056307      |\n",
      "| loss_kl         | 0.006660423   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0063062054 |\n",
      "| loss_vf_loss    | 38.121216     |\n",
      "-----------------------------------\n",
      "********** Iteration 4 ************\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-485.40 +/- 43.80\n",
      "Episode length: 485.50 +/- 43.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00157 |       0.00000 |     121.27160 |       0.00038 |       1.04403\n",
      "     -0.00041 |       0.00000 |      97.39468 |       0.00131 |       1.03814\n",
      "     -0.00252 |       0.00000 |      81.31188 |       0.00181 |       1.03521\n",
      "     -0.00128 |       0.00000 |      70.28903 |       0.00394 |       1.02905\n",
      "     -0.00171 |       0.00000 |      62.28879 |       0.00384 |       1.03148\n",
      "     -0.00232 |       0.00000 |      55.67385 |       0.00519 |       1.02800\n",
      "     -0.00201 |       0.00000 |      50.52274 |       0.00529 |       1.02469\n",
      "     -0.00305 |       0.00000 |      46.24690 |       0.00647 |       1.02307\n",
      "     -0.00265 |       0.00000 |      42.03926 |       0.00649 |       1.02496\n",
      "     -0.00371 |       0.00000 |      38.99596 |       0.00638 |       1.02513\n",
      "Evaluating losses...\n",
      "     -0.00361 |       0.00000 |      37.53605 |       0.00688 |       1.02410\n",
      "-----------------------------------\n",
      "| EpLenMean       | 483           |\n",
      "| EpRewMean       | -483          |\n",
      "| EpThisIter      | 10            |\n",
      "| EpisodesSoFar   | 42            |\n",
      "| TimeElapsed     | 209           |\n",
      "| TimestepsSoFar  | 20480         |\n",
      "| ev_tdlam_before | 0.35          |\n",
      "| loss_ent        | 1.0241028     |\n",
      "| loss_kl         | 0.006879732   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036099865 |\n",
      "| loss_vf_loss    | 37.536053     |\n",
      "-----------------------------------\n",
      "********** Iteration 5 ************\n",
      "Eval num_timesteps=20480, episode_reward=-104.10 +/- 45.48\n",
      "Episode length: 105.10 +/- 45.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20480, episode_reward=-88.80 +/- 21.40\n",
      "Episode length: 89.80 +/- 21.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20480, episode_reward=-76.20 +/- 10.70\n",
      "Episode length: 77.20 +/- 10.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20480, episode_reward=-98.80 +/- 34.19\n",
      "Episode length: 99.80 +/- 34.19\n",
      "Eval num_timesteps=20480, episode_reward=-78.80 +/- 12.42\n",
      "Episode length: 79.80 +/- 12.42\n",
      "Eval num_timesteps=20480, episode_reward=-81.30 +/- 13.78\n",
      "Episode length: 82.30 +/- 13.78\n",
      "Eval num_timesteps=20480, episode_reward=-88.90 +/- 16.43\n",
      "Episode length: 89.90 +/- 16.43\n",
      "Eval num_timesteps=20480, episode_reward=-95.00 +/- 45.51\n",
      "Episode length: 96.00 +/- 45.51\n",
      "Eval num_timesteps=20480, episode_reward=-96.20 +/- 30.06\n",
      "Episode length: 97.20 +/- 30.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00080 |       0.00000 |      97.56407 |       0.00085 |       1.02900\n",
      "     -0.00191 |       0.00000 |      82.75021 |       0.00228 |       1.01807\n",
      "     -0.00528 |       0.00000 |      73.74960 |       0.00617 |       1.00118\n",
      "     -0.00512 |       0.00000 |      66.75391 |       0.00845 |       0.99362\n",
      "     -0.00623 |       0.00000 |      61.84520 |       0.01001 |       0.98805\n",
      "     -0.00689 |       0.00000 |      57.80541 |       0.01064 |       0.98685\n",
      "     -0.00686 |       0.00000 |      54.63770 |       0.00989 |       0.98863\n",
      "     -0.00707 |       0.00000 |      52.41764 |       0.01048 |       0.98747\n",
      "     -0.00692 |       0.00000 |      50.37112 |       0.01030 |       0.98878\n",
      "     -0.00739 |       0.00000 |      48.77203 |       0.01150 |       0.98449\n",
      "Evaluating losses...\n",
      "     -0.00760 |       0.00000 |      48.39186 |       0.01164 |       0.98373\n",
      "----------------------------------\n",
      "| EpLenMean       | 437          |\n",
      "| EpRewMean       | -437         |\n",
      "| EpThisIter      | 14           |\n",
      "| EpisodesSoFar   | 56           |\n",
      "| TimeElapsed     | 221          |\n",
      "| TimestepsSoFar  | 24576        |\n",
      "| ev_tdlam_before | 0.456        |\n",
      "| loss_ent        | 0.9837281    |\n",
      "| loss_kl         | 0.011641033  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.007601599 |\n",
      "| loss_vf_loss    | 48.39186     |\n",
      "----------------------------------\n",
      "********** Iteration 6 ************\n",
      "Eval num_timesteps=24576, episode_reward=-77.00 +/- 8.82\n",
      "Episode length: 78.00 +/- 8.82\n",
      "Eval num_timesteps=24576, episode_reward=-97.40 +/- 48.04\n",
      "Episode length: 98.40 +/- 48.04\n",
      "Eval num_timesteps=24576, episode_reward=-84.90 +/- 23.05\n",
      "Episode length: 85.90 +/- 23.05\n",
      "Eval num_timesteps=24576, episode_reward=-80.50 +/- 8.38\n",
      "Episode length: 81.50 +/- 8.38\n",
      "Eval num_timesteps=24576, episode_reward=-83.50 +/- 10.02\n",
      "Episode length: 84.50 +/- 10.02\n",
      "Eval num_timesteps=24576, episode_reward=-85.20 +/- 11.81\n",
      "Episode length: 86.20 +/- 11.81\n",
      "Eval num_timesteps=24576, episode_reward=-78.60 +/- 8.08\n",
      "Episode length: 79.60 +/- 8.08\n",
      "Eval num_timesteps=24576, episode_reward=-93.80 +/- 57.76\n",
      "Episode length: 94.80 +/- 57.76\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00025 |       0.00000 |      79.64684 |       0.00069 |       0.95637\n",
      "     -0.00320 |       0.00000 |      70.78482 |       0.00229 |       0.93065\n",
      "     -0.00364 |       0.00000 |      65.08485 |       0.00393 |       0.92017\n",
      "     -0.00531 |       0.00000 |      62.10475 |       0.00477 |       0.91524\n",
      "     -0.00510 |       0.00000 |      59.89355 |       0.00608 |       0.90878\n",
      "     -0.00557 |       0.00000 |      58.27438 |       0.00702 |       0.90480\n",
      "     -0.00596 |       0.00000 |      56.50684 |       0.00670 |       0.90749\n",
      "     -0.00600 |       0.00000 |      55.34635 |       0.00774 |       0.90430\n",
      "     -0.00706 |       0.00000 |      54.31412 |       0.00669 |       0.91027\n",
      "     -0.00837 |       0.00000 |      53.79571 |       0.00663 |       0.91100\n",
      "Evaluating losses...\n",
      "     -0.00726 |       0.00000 |      53.35655 |       0.00779 |       0.90439\n",
      "-----------------------------------\n",
      "| EpLenMean       | 392           |\n",
      "| EpRewMean       | -392          |\n",
      "| EpThisIter      | 17            |\n",
      "| EpisodesSoFar   | 73            |\n",
      "| TimeElapsed     | 232           |\n",
      "| TimestepsSoFar  | 28672         |\n",
      "| ev_tdlam_before | 0.58          |\n",
      "| loss_ent        | 0.90439034    |\n",
      "| loss_kl         | 0.0077926638  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0072614932 |\n",
      "| loss_vf_loss    | 53.35655      |\n",
      "-----------------------------------\n",
      "********** Iteration 7 ************\n",
      "Eval num_timesteps=28672, episode_reward=-105.30 +/- 55.53\n",
      "Episode length: 106.30 +/- 55.53\n",
      "Eval num_timesteps=28672, episode_reward=-88.00 +/- 21.42\n",
      "Episode length: 89.00 +/- 21.42\n",
      "Eval num_timesteps=28672, episode_reward=-85.40 +/- 17.44\n",
      "Episode length: 86.40 +/- 17.44\n",
      "Eval num_timesteps=28672, episode_reward=-105.00 +/- 54.31\n",
      "Episode length: 106.00 +/- 54.31\n",
      "Eval num_timesteps=28672, episode_reward=-73.20 +/- 2.93\n",
      "Episode length: 74.20 +/- 2.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28672, episode_reward=-87.30 +/- 29.09\n",
      "Episode length: 88.30 +/- 29.09\n",
      "Eval num_timesteps=28672, episode_reward=-90.00 +/- 21.32\n",
      "Episode length: 91.00 +/- 21.32\n",
      "Eval num_timesteps=28672, episode_reward=-80.00 +/- 8.88\n",
      "Episode length: 81.00 +/- 8.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00039 |       0.00000 |      83.17271 |       0.00118 |       0.90169\n",
      "     -0.00221 |       0.00000 |      78.79562 |       0.00296 |       0.88580\n",
      "     -0.00307 |       0.00000 |      76.72425 |       0.00498 |       0.87657\n",
      "     -0.00342 |       0.00000 |      74.59624 |       0.00530 |       0.87921\n",
      "     -0.00450 |       0.00000 |      72.96309 |       0.00507 |       0.87821\n",
      "     -0.00460 |       0.00000 |      72.21340 |       0.00490 |       0.87826\n",
      "     -0.00439 |       0.00000 |      71.47022 |       0.00550 |       0.87787\n",
      "     -0.00462 |       0.00000 |      70.42245 |       0.00558 |       0.87854\n",
      "     -0.00513 |       0.00000 |      69.94061 |       0.00604 |       0.87814\n",
      "     -0.00696 |       0.00000 |      69.29454 |       0.00428 |       0.88065\n",
      "Evaluating losses...\n",
      "     -0.00689 |       0.00000 |      68.47959 |       0.00512 |       0.87989\n",
      "-----------------------------------\n",
      "| EpLenMean       | 340           |\n",
      "| EpRewMean       | -339          |\n",
      "| EpThisIter      | 23            |\n",
      "| EpisodesSoFar   | 96            |\n",
      "| TimeElapsed     | 244           |\n",
      "| TimestepsSoFar  | 32768         |\n",
      "| ev_tdlam_before | 0.62          |\n",
      "| loss_ent        | 0.87989056    |\n",
      "| loss_kl         | 0.0051243803  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0068885693 |\n",
      "| loss_vf_loss    | 68.47959      |\n",
      "-----------------------------------\n",
      "********** Iteration 8 ************\n",
      "Eval num_timesteps=32768, episode_reward=-86.60 +/- 16.98\n",
      "Episode length: 87.60 +/- 16.98\n",
      "Eval num_timesteps=32768, episode_reward=-84.70 +/- 10.84\n",
      "Episode length: 85.70 +/- 10.84\n",
      "Eval num_timesteps=32768, episode_reward=-78.50 +/- 10.08\n",
      "Episode length: 79.50 +/- 10.08\n",
      "Eval num_timesteps=32768, episode_reward=-83.80 +/- 14.01\n",
      "Episode length: 84.80 +/- 14.01\n",
      "Eval num_timesteps=32768, episode_reward=-81.20 +/- 7.85\n",
      "Episode length: 82.20 +/- 7.85\n",
      "Eval num_timesteps=32768, episode_reward=-76.10 +/- 5.52\n",
      "Episode length: 77.10 +/- 5.52\n",
      "Eval num_timesteps=32768, episode_reward=-81.20 +/- 8.52\n",
      "Episode length: 82.20 +/- 8.52\n",
      "Eval num_timesteps=32768, episode_reward=-91.10 +/- 41.48\n",
      "Episode length: 92.10 +/- 41.48\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00180 |       0.00000 |      82.48787 |       0.00103 |       0.88985\n",
      "     -0.00291 |       0.00000 |      81.26926 |       0.00370 |       0.86923\n",
      "     -0.00284 |       0.00000 |      79.82246 |       0.00505 |       0.86077\n",
      "     -0.00449 |       0.00000 |      78.93828 |       0.00439 |       0.86436\n",
      "     -0.00367 |       0.00000 |      78.57138 |       0.00556 |       0.85678\n",
      "     -0.00283 |       0.00000 |      77.64224 |       0.00513 |       0.85781\n",
      "     -0.00386 |       0.00000 |      77.50607 |       0.00603 |       0.85435\n",
      "     -0.00492 |       0.00000 |      76.98289 |       0.00528 |       0.85686\n",
      "     -0.00513 |       0.00000 |      76.83358 |       0.00573 |       0.85579\n",
      "     -0.00466 |       0.00000 |      76.00999 |       0.00625 |       0.84981\n",
      "Evaluating losses...\n",
      "     -0.00418 |       0.00000 |      75.48151 |       0.00609 |       0.85302\n",
      "----------------------------------\n",
      "| EpLenMean       | 253          |\n",
      "| EpRewMean       | -252         |\n",
      "| EpThisIter      | 27           |\n",
      "| EpisodesSoFar   | 123          |\n",
      "| TimeElapsed     | 255          |\n",
      "| TimestepsSoFar  | 36864        |\n",
      "| ev_tdlam_before | 0.699        |\n",
      "| loss_ent        | 0.8530166    |\n",
      "| loss_kl         | 0.0060860673 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.004175227 |\n",
      "| loss_vf_loss    | 75.481514    |\n",
      "----------------------------------\n",
      "********** Iteration 9 ************\n",
      "Eval num_timesteps=36864, episode_reward=-82.00 +/- 16.82\n",
      "Episode length: 83.00 +/- 16.82\n",
      "Eval num_timesteps=36864, episode_reward=-77.70 +/- 7.72\n",
      "Episode length: 78.70 +/- 7.72\n",
      "Eval num_timesteps=36864, episode_reward=-78.10 +/- 11.89\n",
      "Episode length: 79.10 +/- 11.89\n",
      "Eval num_timesteps=36864, episode_reward=-79.90 +/- 16.31\n",
      "Episode length: 80.90 +/- 16.31\n",
      "Eval num_timesteps=36864, episode_reward=-78.80 +/- 8.28\n",
      "Episode length: 79.80 +/- 8.28\n",
      "Eval num_timesteps=36864, episode_reward=-86.80 +/- 19.67\n",
      "Episode length: 87.80 +/- 19.67\n",
      "Eval num_timesteps=36864, episode_reward=-84.00 +/- 18.78\n",
      "Episode length: 85.00 +/- 18.78\n",
      "Eval num_timesteps=36864, episode_reward=-85.70 +/- 22.19\n",
      "Episode length: 86.70 +/- 22.19\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00088 |       0.00000 |      86.57620 |       0.00096 |       0.83550\n",
      "     -0.00175 |       0.00000 |      85.67790 |       0.00145 |       0.83330\n",
      "     -0.00313 |       0.00000 |      85.77411 |       0.00291 |       0.81775\n",
      "     -0.00415 |       0.00000 |      85.16139 |       0.00451 |       0.81735\n",
      "     -0.00328 |       0.00000 |      84.74320 |       0.00481 |       0.81263\n",
      "     -0.00456 |       0.00000 |      84.17878 |       0.00511 |       0.81374\n",
      "     -0.00474 |       0.00000 |      84.72407 |       0.00504 |       0.80973\n",
      "     -0.00421 |       0.00000 |      83.41221 |       0.00355 |       0.80969\n",
      "     -0.00510 |       0.00000 |      83.70627 |       0.00443 |       0.80852\n",
      "     -0.00442 |       0.00000 |      83.56494 |       0.00462 |       0.81043\n",
      "Evaluating losses...\n",
      "     -0.00566 |       0.00000 |      83.52135 |       0.00488 |       0.80501\n",
      "----------------------------------\n",
      "| EpLenMean       | 184          |\n",
      "| EpRewMean       | -183         |\n",
      "| EpThisIter      | 26           |\n",
      "| EpisodesSoFar   | 149          |\n",
      "| TimeElapsed     | 265          |\n",
      "| TimestepsSoFar  | 40960        |\n",
      "| ev_tdlam_before | 0.696        |\n",
      "| loss_ent        | 0.80501324   |\n",
      "| loss_kl         | 0.004883081  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005661251 |\n",
      "| loss_vf_loss    | 83.52135     |\n",
      "----------------------------------\n",
      "********** Iteration 10 ************\n",
      "Eval num_timesteps=40960, episode_reward=-83.00 +/- 12.90\n",
      "Episode length: 84.00 +/- 12.90\n",
      "Eval num_timesteps=40960, episode_reward=-75.50 +/- 17.78\n",
      "Episode length: 76.50 +/- 17.78\n",
      "Eval num_timesteps=40960, episode_reward=-72.80 +/- 6.49\n",
      "Episode length: 73.80 +/- 6.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40960, episode_reward=-76.10 +/- 5.56\n",
      "Episode length: 77.10 +/- 5.56\n",
      "Eval num_timesteps=40960, episode_reward=-80.20 +/- 17.83\n",
      "Episode length: 81.20 +/- 17.83\n",
      "Eval num_timesteps=40960, episode_reward=-75.40 +/- 9.51\n",
      "Episode length: 76.40 +/- 9.51\n",
      "Eval num_timesteps=40960, episode_reward=-78.40 +/- 9.46\n",
      "Episode length: 79.40 +/- 9.46\n",
      "Eval num_timesteps=40960, episode_reward=-75.90 +/- 8.75\n",
      "Episode length: 76.90 +/- 8.75\n",
      "Eval num_timesteps=40960, episode_reward=-85.00 +/- 23.10\n",
      "Episode length: 86.00 +/- 23.10\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00013 |       0.00000 |      90.05264 |       0.00089 |       0.80869\n",
      "     -0.00119 |       0.00000 |      89.14881 |       0.00228 |       0.79568\n",
      "     -0.00234 |       0.00000 |      88.70552 |       0.00354 |       0.79093\n",
      "     -0.00157 |       0.00000 |      88.10176 |       0.00313 |       0.79501\n",
      "     -0.00202 |       0.00000 |      87.60804 |       0.00370 |       0.78988\n",
      "     -0.00170 |       0.00000 |      87.15279 |       0.00426 |       0.78899\n",
      "     -0.00311 |       0.00000 |      86.52062 |       0.00391 |       0.79274\n",
      "     -0.00275 |       0.00000 |      86.66196 |       0.00431 |       0.78937\n",
      "     -0.00380 |       0.00000 |      86.27552 |       0.00328 |       0.79392\n",
      "     -0.00315 |       0.00000 |      84.84195 |       0.00399 |       0.79022\n",
      "Evaluating losses...\n",
      "     -0.00350 |       0.00000 |      85.42234 |       0.00426 |       0.78484\n",
      "-----------------------------------\n",
      "| EpLenMean       | 156           |\n",
      "| EpRewMean       | -155          |\n",
      "| EpThisIter      | 29            |\n",
      "| EpisodesSoFar   | 178           |\n",
      "| TimeElapsed     | 276           |\n",
      "| TimestepsSoFar  | 45056         |\n",
      "| ev_tdlam_before | 0.705         |\n",
      "| loss_ent        | 0.7848355     |\n",
      "| loss_kl         | 0.0042618     |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0034951987 |\n",
      "| loss_vf_loss    | 85.42234      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 11 ************\n",
      "Eval num_timesteps=45056, episode_reward=-80.80 +/- 6.76\n",
      "Episode length: 81.80 +/- 6.76\n",
      "Eval num_timesteps=45056, episode_reward=-77.60 +/- 11.45\n",
      "Episode length: 78.60 +/- 11.45\n",
      "Eval num_timesteps=45056, episode_reward=-92.80 +/- 38.64\n",
      "Episode length: 93.80 +/- 38.64\n",
      "Eval num_timesteps=45056, episode_reward=-77.50 +/- 7.47\n",
      "Episode length: 78.50 +/- 7.47\n",
      "Eval num_timesteps=45056, episode_reward=-74.70 +/- 8.19\n",
      "Episode length: 75.70 +/- 8.19\n",
      "Eval num_timesteps=45056, episode_reward=-104.50 +/- 30.44\n",
      "Episode length: 105.50 +/- 30.44\n",
      "Eval num_timesteps=45056, episode_reward=-78.30 +/- 9.48\n",
      "Episode length: 79.30 +/- 9.48\n",
      "Eval num_timesteps=45056, episode_reward=-82.70 +/- 15.22\n",
      "Episode length: 83.70 +/- 15.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00080 |       0.00000 |      94.92720 |       0.00123 |       0.76141\n",
      "     -0.00123 |       0.00000 |      93.04791 |       0.00211 |       0.74423\n",
      "     -0.00149 |       0.00000 |      92.41670 |       0.00267 |       0.74456\n",
      "     -0.00342 |       0.00000 |      91.88734 |       0.00300 |       0.74355\n",
      "     -0.00328 |       0.00000 |      92.16227 |       0.00297 |       0.74517\n",
      "     -0.00323 |       0.00000 |      90.68303 |       0.00373 |       0.73818\n",
      "     -0.00372 |       0.00000 |      91.06652 |       0.00356 |       0.74014\n",
      "     -0.00227 |       0.00000 |      90.80509 |       0.00396 |       0.73797\n",
      "     -0.00331 |       0.00000 |      90.26772 |       0.00415 |       0.73827\n",
      "     -0.00255 |       0.00000 |      90.77119 |       0.00453 |       0.73352\n",
      "Evaluating losses...\n",
      "     -0.00367 |       0.00000 |      89.84720 |       0.00396 |       0.73730\n",
      "-----------------------------------\n",
      "| EpLenMean       | 148           |\n",
      "| EpRewMean       | -147          |\n",
      "| EpThisIter      | 28            |\n",
      "| EpisodesSoFar   | 206           |\n",
      "| TimeElapsed     | 286           |\n",
      "| TimestepsSoFar  | 49152         |\n",
      "| ev_tdlam_before | 0.673         |\n",
      "| loss_ent        | 0.7373043     |\n",
      "| loss_kl         | 0.0039600176  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0036714962 |\n",
      "| loss_vf_loss    | 89.8472       |\n",
      "-----------------------------------\n",
      "********** Iteration 12 ************\n",
      "Eval num_timesteps=49152, episode_reward=-84.60 +/- 12.18\n",
      "Episode length: 85.60 +/- 12.18\n",
      "Eval num_timesteps=49152, episode_reward=-82.80 +/- 13.28\n",
      "Episode length: 83.80 +/- 13.28\n",
      "Eval num_timesteps=49152, episode_reward=-76.00 +/- 7.51\n",
      "Episode length: 77.00 +/- 7.51\n",
      "Eval num_timesteps=49152, episode_reward=-78.40 +/- 8.22\n",
      "Episode length: 79.40 +/- 8.22\n",
      "Eval num_timesteps=49152, episode_reward=-80.50 +/- 9.60\n",
      "Episode length: 81.50 +/- 9.60\n",
      "Eval num_timesteps=49152, episode_reward=-88.00 +/- 24.31\n",
      "Episode length: 89.00 +/- 24.31\n",
      "Eval num_timesteps=49152, episode_reward=-109.90 +/- 75.56\n",
      "Episode length: 110.90 +/- 75.56\n",
      "Eval num_timesteps=49152, episode_reward=-88.40 +/- 12.67\n",
      "Episode length: 89.40 +/- 12.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00042 |       0.00000 |      97.31756 |       0.00145 |       0.71725\n",
      "     -0.00174 |       0.00000 |      96.75567 |       0.00311 |       0.69447\n",
      "     -0.00426 |       0.00000 |      95.76913 |       0.00455 |       0.68145\n",
      "     -0.00374 |       0.00000 |      95.71946 |       0.00451 |       0.68193\n",
      "     -0.00254 |       0.00000 |      95.18282 |       0.00546 |       0.67400\n",
      "     -0.00284 |       0.00000 |      94.94884 |       0.00496 |       0.67634\n",
      "     -0.00430 |       0.00000 |      94.47734 |       0.00491 |       0.67799\n",
      "     -0.00378 |       0.00000 |      94.16797 |       0.00570 |       0.67346\n",
      "     -0.00469 |       0.00000 |      93.86362 |       0.00520 |       0.67673\n",
      "     -0.00291 |       0.00000 |      93.54382 |       0.00523 |       0.67660\n",
      "Evaluating losses...\n",
      "     -0.00298 |       0.00000 |      93.72691 |       0.00560 |       0.67429\n",
      "-----------------------------------\n",
      "| EpLenMean       | 145           |\n",
      "| EpRewMean       | -144          |\n",
      "| EpThisIter      | 29            |\n",
      "| EpisodesSoFar   | 235           |\n",
      "| TimeElapsed     | 297           |\n",
      "| TimestepsSoFar  | 53248         |\n",
      "| ev_tdlam_before | 0.668         |\n",
      "| loss_ent        | 0.6742857     |\n",
      "| loss_kl         | 0.0056029484  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0029770846 |\n",
      "| loss_vf_loss    | 93.72691      |\n",
      "-----------------------------------\n",
      "********** Iteration 13 ************\n",
      "Eval num_timesteps=53248, episode_reward=-87.50 +/- 30.47\n",
      "Episode length: 88.50 +/- 30.47\n",
      "Eval num_timesteps=53248, episode_reward=-78.50 +/- 9.00\n",
      "Episode length: 79.50 +/- 9.00\n",
      "Eval num_timesteps=53248, episode_reward=-81.50 +/- 6.39\n",
      "Episode length: 82.50 +/- 6.39\n",
      "Eval num_timesteps=53248, episode_reward=-81.30 +/- 18.23\n",
      "Episode length: 82.30 +/- 18.23\n",
      "Eval num_timesteps=53248, episode_reward=-98.50 +/- 34.30\n",
      "Episode length: 99.50 +/- 34.30\n",
      "Eval num_timesteps=53248, episode_reward=-87.90 +/- 13.22\n",
      "Episode length: 88.90 +/- 13.22\n",
      "Eval num_timesteps=53248, episode_reward=-77.10 +/- 5.17\n",
      "Episode length: 78.10 +/- 5.17\n",
      "Eval num_timesteps=53248, episode_reward=-83.50 +/- 11.30\n",
      "Episode length: 84.50 +/- 11.30\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00085 |       0.00000 |      91.87141 |       0.00103 |       0.69435\n",
      "      0.00100 |       0.00000 |      90.12328 |       0.00147 |       0.67516\n",
      "     -0.00136 |       0.00000 |      89.08266 |       0.00192 |       0.67083\n",
      "     -0.00172 |       0.00000 |      88.81725 |       0.00217 |       0.66760\n",
      "     -0.00287 |       0.00000 |      88.55315 |       0.00233 |       0.66330\n",
      "     -0.00077 |       0.00000 |      88.18620 |       0.00250 |       0.66215\n",
      "     -0.00180 |       0.00000 |      87.59266 |       0.00254 |       0.66980\n",
      "     -0.00209 |       0.00000 |      87.35455 |       0.00257 |       0.67152\n",
      "     -0.00109 |       0.00000 |      86.59069 |       0.00258 |       0.67081\n",
      "     -0.00196 |       0.00000 |      86.61571 |       0.00279 |       0.67151\n",
      "Evaluating losses...\n",
      "     -0.00327 |       0.00000 |      86.26449 |       0.00281 |       0.66906\n",
      "----------------------------------\n",
      "| EpLenMean       | 133          |\n",
      "| EpRewMean       | -132         |\n",
      "| EpThisIter      | 35           |\n",
      "| EpisodesSoFar   | 270          |\n",
      "| TimeElapsed     | 308          |\n",
      "| TimestepsSoFar  | 57344        |\n",
      "| ev_tdlam_before | 0.724        |\n",
      "| loss_ent        | 0.66906106   |\n",
      "| loss_kl         | 0.0028145702 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003265486 |\n",
      "| loss_vf_loss    | 86.26449     |\n",
      "----------------------------------\n",
      "********** Iteration 14 ************\n",
      "Eval num_timesteps=57344, episode_reward=-81.90 +/- 6.36\n",
      "Episode length: 82.90 +/- 6.36\n",
      "Eval num_timesteps=57344, episode_reward=-78.50 +/- 5.12\n",
      "Episode length: 79.50 +/- 5.12\n",
      "Eval num_timesteps=57344, episode_reward=-110.40 +/- 38.94\n",
      "Episode length: 111.40 +/- 38.94\n",
      "Eval num_timesteps=57344, episode_reward=-81.70 +/- 10.41\n",
      "Episode length: 82.70 +/- 10.41\n",
      "Eval num_timesteps=57344, episode_reward=-89.20 +/- 21.91\n",
      "Episode length: 90.20 +/- 21.91\n",
      "Eval num_timesteps=57344, episode_reward=-82.60 +/- 7.45\n",
      "Episode length: 83.60 +/- 7.45\n",
      "Eval num_timesteps=57344, episode_reward=-83.00 +/- 8.19\n",
      "Episode length: 84.00 +/- 8.19\n",
      "Eval num_timesteps=57344, episode_reward=-81.00 +/- 12.43\n",
      "Episode length: 82.00 +/- 12.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00030 |       0.00000 |      84.54249 |       0.00099 |       0.66084\n",
      "     -0.00138 |       0.00000 |      83.48975 |       0.00126 |       0.65744\n",
      "     -0.00253 |       0.00000 |      82.94714 |       0.00178 |       0.65003\n",
      "     -0.00217 |       0.00000 |      82.91927 |       0.00222 |       0.64451\n",
      "     -0.00351 |       0.00000 |      82.31820 |       0.00292 |       0.63818\n",
      "     -0.00349 |       0.00000 |      81.69706 |       0.00293 |       0.64232\n",
      "     -0.00595 |       0.00000 |      81.75100 |       0.00347 |       0.63938\n",
      "     -0.00410 |       0.00000 |      81.56528 |       0.00374 |       0.63907\n",
      "     -0.00405 |       0.00000 |      81.18955 |       0.00375 |       0.64089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00497 |       0.00000 |      81.06096 |       0.00415 |       0.63277\n",
      "Evaluating losses...\n",
      "     -0.00361 |       0.00000 |      80.54766 |       0.00410 |       0.63895\n",
      "----------------------------------\n",
      "| EpLenMean       | 128          |\n",
      "| EpRewMean       | -127         |\n",
      "| EpThisIter      | 34           |\n",
      "| EpisodesSoFar   | 304          |\n",
      "| TimeElapsed     | 319          |\n",
      "| TimestepsSoFar  | 61440        |\n",
      "| ev_tdlam_before | 0.73         |\n",
      "| loss_ent        | 0.6389545    |\n",
      "| loss_kl         | 0.004104594  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.003610588 |\n",
      "| loss_vf_loss    | 80.54766     |\n",
      "----------------------------------\n",
      "********** Iteration 15 ************\n",
      "Eval num_timesteps=61440, episode_reward=-93.60 +/- 26.76\n",
      "Episode length: 94.60 +/- 26.76\n",
      "Eval num_timesteps=61440, episode_reward=-89.40 +/- 18.70\n",
      "Episode length: 90.40 +/- 18.70\n",
      "Eval num_timesteps=61440, episode_reward=-76.50 +/- 7.79\n",
      "Episode length: 77.50 +/- 7.79\n",
      "Eval num_timesteps=61440, episode_reward=-78.10 +/- 12.45\n",
      "Episode length: 79.10 +/- 12.45\n",
      "Eval num_timesteps=61440, episode_reward=-83.60 +/- 14.89\n",
      "Episode length: 84.60 +/- 14.89\n",
      "Eval num_timesteps=61440, episode_reward=-82.40 +/- 8.40\n",
      "Episode length: 83.40 +/- 8.40\n",
      "Eval num_timesteps=61440, episode_reward=-100.30 +/- 60.55\n",
      "Episode length: 101.30 +/- 60.55\n",
      "Eval num_timesteps=61440, episode_reward=-81.00 +/- 6.93\n",
      "Episode length: 82.00 +/- 6.93\n",
      "Eval num_timesteps=61440, episode_reward=-81.60 +/- 10.22\n",
      "Episode length: 82.60 +/- 10.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00049 |       0.00000 |      83.08237 |       0.00112 |       0.62880\n",
      "     -0.00215 |       0.00000 |      82.71801 |       0.00195 |       0.60659\n",
      "     -0.00200 |       0.00000 |      82.38110 |       0.00343 |       0.58997\n",
      "     -0.00266 |       0.00000 |      82.15022 |       0.00439 |       0.58288\n",
      "     -0.00308 |       0.00000 |      81.82784 |       0.00428 |       0.58565\n",
      "     -0.00162 |       0.00000 |      81.97273 |       0.00511 |       0.57901\n",
      "     -0.00236 |       0.00000 |      81.87090 |       0.00422 |       0.58665\n",
      "     -0.00240 |       0.00000 |      81.90257 |       0.00433 |       0.58670\n",
      "     -0.00482 |       0.00000 |      82.21085 |       0.00423 |       0.58834\n",
      "     -0.00456 |       0.00000 |      80.81093 |       0.00413 |       0.58926\n",
      "Evaluating losses...\n",
      "     -0.00294 |       0.00000 |      81.07139 |       0.00412 |       0.58915\n",
      "----------------------------------\n",
      "| EpLenMean       | 121          |\n",
      "| EpRewMean       | -120         |\n",
      "| EpThisIter      | 33           |\n",
      "| EpisodesSoFar   | 337          |\n",
      "| TimeElapsed     | 330          |\n",
      "| TimestepsSoFar  | 65536        |\n",
      "| ev_tdlam_before | 0.72         |\n",
      "| loss_ent        | 0.58915293   |\n",
      "| loss_kl         | 0.0041246405 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.002939769 |\n",
      "| loss_vf_loss    | 81.07139     |\n",
      "----------------------------------\n",
      "********** Iteration 16 ************\n",
      "Eval num_timesteps=65536, episode_reward=-83.60 +/- 13.12\n",
      "Episode length: 84.60 +/- 13.12\n",
      "Eval num_timesteps=65536, episode_reward=-82.40 +/- 8.01\n",
      "Episode length: 83.40 +/- 8.01\n",
      "Eval num_timesteps=65536, episode_reward=-83.20 +/- 12.33\n",
      "Episode length: 84.20 +/- 12.33\n",
      "Eval num_timesteps=65536, episode_reward=-94.50 +/- 28.38\n",
      "Episode length: 95.50 +/- 28.38\n",
      "Eval num_timesteps=65536, episode_reward=-81.00 +/- 9.58\n",
      "Episode length: 82.00 +/- 9.58\n",
      "Eval num_timesteps=65536, episode_reward=-82.40 +/- 7.55\n",
      "Episode length: 83.40 +/- 7.55\n",
      "Eval num_timesteps=65536, episode_reward=-77.90 +/- 5.92\n",
      "Episode length: 78.90 +/- 5.92\n",
      "Eval num_timesteps=65536, episode_reward=-82.30 +/- 10.31\n",
      "Episode length: 83.30 +/- 10.31\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00018 |       0.00000 |      84.86452 |       0.00125 |       0.54937\n",
      "     -0.00044 |       0.00000 |      82.25043 |       0.00203 |       0.54185\n",
      "     -0.00216 |       0.00000 |      81.37908 |       0.00227 |       0.53731\n",
      "     -0.00261 |       0.00000 |      80.50425 |       0.00286 |       0.53040\n",
      "     -0.00276 |       0.00000 |      80.40627 |       0.00316 |       0.52962\n",
      "     -0.00235 |       0.00000 |      80.23071 |       0.00286 |       0.52789\n",
      "     -0.00330 |       0.00000 |      79.51084 |       0.00262 |       0.53452\n",
      "     -0.00326 |       0.00000 |      79.42949 |       0.00283 |       0.53206\n",
      "     -0.00176 |       0.00000 |      79.29124 |       0.00301 |       0.53059\n",
      "     -0.00348 |       0.00000 |      78.86639 |       0.00347 |       0.52469\n",
      "Evaluating losses...\n",
      "     -0.00194 |       0.00000 |      78.52348 |       0.00307 |       0.52958\n",
      "-----------------------------------\n",
      "| EpLenMean       | 127           |\n",
      "| EpRewMean       | -126          |\n",
      "| EpThisIter      | 29            |\n",
      "| EpisodesSoFar   | 366           |\n",
      "| TimeElapsed     | 340           |\n",
      "| TimestepsSoFar  | 69632         |\n",
      "| ev_tdlam_before | 0.666         |\n",
      "| loss_ent        | 0.5295808     |\n",
      "| loss_kl         | 0.0030680348  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0019363277 |\n",
      "| loss_vf_loss    | 78.52348      |\n",
      "-----------------------------------\n",
      "********** Iteration 17 ************\n",
      "Eval num_timesteps=69632, episode_reward=-83.50 +/- 6.59\n",
      "Episode length: 84.50 +/- 6.59\n",
      "Eval num_timesteps=69632, episode_reward=-82.60 +/- 9.09\n",
      "Episode length: 83.60 +/- 9.09\n",
      "Eval num_timesteps=69632, episode_reward=-87.40 +/- 15.74\n",
      "Episode length: 88.40 +/- 15.74\n",
      "Eval num_timesteps=69632, episode_reward=-79.70 +/- 7.94\n",
      "Episode length: 80.70 +/- 7.94\n",
      "Eval num_timesteps=69632, episode_reward=-88.20 +/- 18.74\n",
      "Episode length: 89.20 +/- 18.74\n",
      "Eval num_timesteps=69632, episode_reward=-78.00 +/- 8.17\n",
      "Episode length: 79.00 +/- 8.17\n",
      "Eval num_timesteps=69632, episode_reward=-77.80 +/- 7.96\n",
      "Episode length: 78.80 +/- 7.96\n",
      "Eval num_timesteps=69632, episode_reward=-80.60 +/- 7.00\n",
      "Episode length: 81.60 +/- 7.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00080 |       0.00000 |      84.17860 |       0.00112 |       0.55034\n",
      "     -0.00112 |       0.00000 |      81.88062 |       0.00152 |       0.53768\n",
      "     -0.00261 |       0.00000 |      80.60783 |       0.00220 |       0.52647\n",
      "     -0.00257 |       0.00000 |      80.53593 |       0.00298 |       0.51970\n",
      "     -0.00299 |       0.00000 |      79.91731 |       0.00333 |       0.51582\n",
      "     -0.00311 |       0.00000 |      79.20438 |       0.00379 |       0.51206\n",
      "     -0.00482 |       0.00000 |      79.27781 |       0.00398 |       0.51099\n",
      "     -0.00354 |       0.00000 |      78.79154 |       0.00396 |       0.51472\n",
      "     -0.00552 |       0.00000 |      78.56382 |       0.00402 |       0.51437\n",
      "     -0.00376 |       0.00000 |      78.40001 |       0.00425 |       0.51125\n",
      "Evaluating losses...\n",
      "     -0.00553 |       0.00000 |      77.75050 |       0.00448 |       0.50976\n",
      "----------------------------------\n",
      "| EpLenMean       | 122          |\n",
      "| EpRewMean       | -121         |\n",
      "| EpThisIter      | 38           |\n",
      "| EpisodesSoFar   | 404          |\n",
      "| TimeElapsed     | 350          |\n",
      "| TimestepsSoFar  | 73728        |\n",
      "| ev_tdlam_before | 0.731        |\n",
      "| loss_ent        | 0.5097611    |\n",
      "| loss_kl         | 0.0044791694 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.005534794 |\n",
      "| loss_vf_loss    | 77.7505      |\n",
      "----------------------------------\n",
      "********** Iteration 18 ************\n",
      "Eval num_timesteps=73728, episode_reward=-83.90 +/- 8.37\n",
      "Episode length: 84.90 +/- 8.37\n",
      "Eval num_timesteps=73728, episode_reward=-87.20 +/- 20.22\n",
      "Episode length: 88.20 +/- 20.22\n",
      "Eval num_timesteps=73728, episode_reward=-86.70 +/- 14.61\n",
      "Episode length: 87.70 +/- 14.61\n",
      "Eval num_timesteps=73728, episode_reward=-83.30 +/- 8.12\n",
      "Episode length: 84.30 +/- 8.12\n",
      "Eval num_timesteps=73728, episode_reward=-80.20 +/- 9.77\n",
      "Episode length: 81.20 +/- 9.77\n",
      "Eval num_timesteps=73728, episode_reward=-125.20 +/- 125.06\n",
      "Episode length: 126.10 +/- 124.76\n",
      "Eval num_timesteps=73728, episode_reward=-83.30 +/- 14.63\n",
      "Episode length: 84.30 +/- 14.63\n",
      "Eval num_timesteps=73728, episode_reward=-88.70 +/- 18.96\n",
      "Episode length: 89.70 +/- 18.96\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00052 |       0.00000 |      81.85799 |       0.00113 |       0.49442\n",
      "     -0.00068 |       0.00000 |      80.45061 |       0.00165 |       0.48259\n",
      "     -0.00168 |       0.00000 |      80.46700 |       0.00206 |       0.47740\n",
      "     -0.00172 |       0.00000 |      79.96433 |       0.00245 |       0.47507\n",
      "     -0.00142 |       0.00000 |      79.86555 |       0.00330 |       0.46586\n",
      "     -0.00245 |       0.00000 |      79.18313 |       0.00325 |       0.46608\n",
      "     -0.00126 |       0.00000 |      79.56374 |       0.00312 |       0.46715\n",
      "     -0.00223 |       0.00000 |      79.25246 |       0.00359 |       0.46322\n",
      "     -0.00133 |       0.00000 |      79.46743 |       0.00323 |       0.46739\n",
      "     -0.00241 |       0.00000 |      79.10728 |       0.00340 |       0.46774\n",
      "Evaluating losses...\n",
      "     -0.00285 |       0.00000 |      78.59560 |       0.00341 |       0.46876\n",
      "-----------------------------------\n",
      "| EpLenMean       | 114           |\n",
      "| EpRewMean       | -113          |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 444           |\n",
      "| TimeElapsed     | 361           |\n",
      "| TimestepsSoFar  | 77824         |\n",
      "| ev_tdlam_before | 0.732         |\n",
      "| loss_ent        | 0.46875536    |\n",
      "| loss_kl         | 0.003410019   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0028481996 |\n",
      "| loss_vf_loss    | 78.595604     |\n",
      "-----------------------------------\n",
      "********** Iteration 19 ************\n",
      "Eval num_timesteps=77824, episode_reward=-83.50 +/- 8.36\n",
      "Episode length: 84.50 +/- 8.36\n",
      "Eval num_timesteps=77824, episode_reward=-87.00 +/- 17.02\n",
      "Episode length: 88.00 +/- 17.02\n",
      "Eval num_timesteps=77824, episode_reward=-83.80 +/- 13.86\n",
      "Episode length: 84.80 +/- 13.86\n",
      "Eval num_timesteps=77824, episode_reward=-82.50 +/- 11.18\n",
      "Episode length: 83.50 +/- 11.18\n",
      "Eval num_timesteps=77824, episode_reward=-80.00 +/- 7.64\n",
      "Episode length: 81.00 +/- 7.64\n",
      "Eval num_timesteps=77824, episode_reward=-80.90 +/- 6.20\n",
      "Episode length: 81.90 +/- 6.20\n",
      "Eval num_timesteps=77824, episode_reward=-81.60 +/- 17.59\n",
      "Episode length: 82.60 +/- 17.59\n",
      "Eval num_timesteps=77824, episode_reward=-80.20 +/- 4.83\n",
      "Episode length: 81.20 +/- 4.83\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00021 |       0.00000 |      66.22737 |       0.00114 |       0.47111\n",
      "     -0.00133 |       0.00000 |      64.23471 |       0.00148 |       0.45750\n",
      "     -0.00022 |       0.00000 |      63.30228 |       0.00195 |       0.45462\n",
      "     -0.00187 |       0.00000 |      63.26397 |       0.00203 |       0.45424\n",
      "     -0.00218 |       0.00000 |      62.52575 |       0.00234 |       0.45327\n",
      "     -0.00258 |       0.00000 |      62.68541 |       0.00250 |       0.45241\n",
      "     -0.00220 |       0.00000 |      61.88013 |       0.00281 |       0.44889\n",
      "     -0.00257 |       0.00000 |      61.62581 |       0.00267 |       0.45018\n",
      "     -0.00216 |       0.00000 |      61.50319 |       0.00282 |       0.44878\n",
      "     -0.00153 |       0.00000 |      61.28186 |       0.00295 |       0.45036\n",
      "Evaluating losses...\n",
      "     -0.00281 |       0.00000 |      61.24948 |       0.00293 |       0.44954\n",
      "----------------------------------\n",
      "| EpLenMean       | 102          |\n",
      "| EpRewMean       | -101         |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 487          |\n",
      "| TimeElapsed     | 372          |\n",
      "| TimestepsSoFar  | 81920        |\n",
      "| ev_tdlam_before | 0.789        |\n",
      "| loss_ent        | 0.4495358    |\n",
      "| loss_kl         | 0.0029349593 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.002807592 |\n",
      "| loss_vf_loss    | 61.24948     |\n",
      "----------------------------------\n",
      "********** Iteration 20 ************\n",
      "Eval num_timesteps=81920, episode_reward=-89.90 +/- 30.93\n",
      "Episode length: 90.90 +/- 30.93\n",
      "Eval num_timesteps=81920, episode_reward=-81.00 +/- 6.51\n",
      "Episode length: 82.00 +/- 6.51\n",
      "Eval num_timesteps=81920, episode_reward=-80.20 +/- 4.19\n",
      "Episode length: 81.20 +/- 4.19\n",
      "Eval num_timesteps=81920, episode_reward=-84.50 +/- 15.70\n",
      "Episode length: 85.50 +/- 15.70\n",
      "Eval num_timesteps=81920, episode_reward=-102.70 +/- 52.76\n",
      "Episode length: 103.70 +/- 52.76\n",
      "Eval num_timesteps=81920, episode_reward=-83.50 +/- 6.68\n",
      "Episode length: 84.50 +/- 6.68\n",
      "Eval num_timesteps=81920, episode_reward=-83.80 +/- 14.43\n",
      "Episode length: 84.80 +/- 14.43\n",
      "Eval num_timesteps=81920, episode_reward=-90.00 +/- 23.49\n",
      "Episode length: 91.00 +/- 23.49\n",
      "Eval num_timesteps=81920, episode_reward=-88.20 +/- 26.06\n",
      "Episode length: 89.20 +/- 26.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00083 |       0.00000 |      79.26513 |       0.00117 |       0.42051\n",
      "     -0.00066 |       0.00000 |      78.68168 |       0.00135 |       0.41549\n",
      "     -0.00104 |       0.00000 |      77.93317 |       0.00190 |       0.40600\n",
      "     -0.00123 |       0.00000 |      77.48196 |       0.00171 |       0.40733\n",
      "     -0.00035 |       0.00000 |      77.15916 |       0.00219 |       0.40090\n",
      "    -7.44e-05 |       0.00000 |      76.76099 |       0.00236 |       0.39906\n",
      "     -0.00214 |       0.00000 |      76.61723 |       0.00197 |       0.40391\n",
      "     -0.00129 |       0.00000 |      76.86438 |       0.00210 |       0.40221\n",
      "     -0.00230 |       0.00000 |      76.39484 |       0.00196 |       0.40419\n",
      "     -0.00107 |       0.00000 |      76.31486 |       0.00194 |       0.40638\n",
      "Evaluating losses...\n",
      "     -0.00135 |       0.00000 |      76.17694 |       0.00191 |       0.40578\n",
      "----------------------------------\n",
      "| EpLenMean       | 102          |\n",
      "| EpRewMean       | -101         |\n",
      "| EpThisIter      | 37           |\n",
      "| EpisodesSoFar   | 524          |\n",
      "| TimeElapsed     | 383          |\n",
      "| TimestepsSoFar  | 86016        |\n",
      "| ev_tdlam_before | 0.696        |\n",
      "| loss_ent        | 0.40578067   |\n",
      "| loss_kl         | 0.0019117707 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001352008 |\n",
      "| loss_vf_loss    | 76.17694     |\n",
      "----------------------------------\n",
      "********** Iteration 21 ************\n",
      "Eval num_timesteps=86016, episode_reward=-77.00 +/- 5.59\n",
      "Episode length: 78.00 +/- 5.59\n",
      "Eval num_timesteps=86016, episode_reward=-82.50 +/- 8.37\n",
      "Episode length: 83.50 +/- 8.37\n",
      "Eval num_timesteps=86016, episode_reward=-79.20 +/- 8.18\n",
      "Episode length: 80.20 +/- 8.18\n",
      "Eval num_timesteps=86016, episode_reward=-82.40 +/- 6.09\n",
      "Episode length: 83.40 +/- 6.09\n",
      "Eval num_timesteps=86016, episode_reward=-80.70 +/- 6.31\n",
      "Episode length: 81.70 +/- 6.31\n",
      "Eval num_timesteps=86016, episode_reward=-81.30 +/- 7.47\n",
      "Episode length: 82.30 +/- 7.47\n",
      "Eval num_timesteps=86016, episode_reward=-78.50 +/- 7.23\n",
      "Episode length: 79.50 +/- 7.23\n",
      "Eval num_timesteps=86016, episode_reward=-85.70 +/- 4.38\n",
      "Episode length: 86.70 +/- 4.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00083 |       0.00000 |      77.86711 |       0.00087 |       0.38641\n",
      "     -0.00042 |       0.00000 |      76.41428 |       0.00120 |       0.39306\n",
      "     -0.00140 |       0.00000 |      75.45236 |       0.00145 |       0.39115\n",
      "     -0.00205 |       0.00000 |      74.67589 |       0.00191 |       0.39066\n",
      "     -0.00218 |       0.00000 |      74.76629 |       0.00265 |       0.39204\n",
      "     -0.00189 |       0.00000 |      74.33428 |       0.00280 |       0.39029\n",
      "     -0.00301 |       0.00000 |      74.04101 |       0.00263 |       0.38552\n",
      "     -0.00248 |       0.00000 |      73.74825 |       0.00302 |       0.38875\n",
      "     -0.00336 |       0.00000 |      74.09103 |       0.00317 |       0.38796\n",
      "     -0.00286 |       0.00000 |      73.59267 |       0.00328 |       0.38599\n",
      "Evaluating losses...\n",
      "     -0.00273 |       0.00000 |      73.30237 |       0.00373 |       0.38647\n",
      "-----------------------------------\n",
      "| EpLenMean       | 106           |\n",
      "| EpRewMean       | -105          |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 562           |\n",
      "| TimeElapsed     | 393           |\n",
      "| TimestepsSoFar  | 90112         |\n",
      "| ev_tdlam_before | 0.69          |\n",
      "| loss_ent        | 0.38646525    |\n",
      "| loss_kl         | 0.0037295218  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0027262978 |\n",
      "| loss_vf_loss    | 73.30237      |\n",
      "-----------------------------------\n",
      "********** Iteration 22 ************\n",
      "Eval num_timesteps=90112, episode_reward=-82.10 +/- 10.07\n",
      "Episode length: 83.10 +/- 10.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=90112, episode_reward=-86.70 +/- 9.07\n",
      "Episode length: 87.70 +/- 9.07\n",
      "Eval num_timesteps=90112, episode_reward=-82.10 +/- 5.63\n",
      "Episode length: 83.10 +/- 5.63\n",
      "Eval num_timesteps=90112, episode_reward=-76.10 +/- 7.87\n",
      "Episode length: 77.10 +/- 7.87\n",
      "Eval num_timesteps=90112, episode_reward=-86.10 +/- 6.52\n",
      "Episode length: 87.10 +/- 6.52\n",
      "Eval num_timesteps=90112, episode_reward=-82.80 +/- 9.22\n",
      "Episode length: 83.80 +/- 9.22\n",
      "Eval num_timesteps=90112, episode_reward=-83.30 +/- 6.34\n",
      "Episode length: 84.30 +/- 6.34\n",
      "Eval num_timesteps=90112, episode_reward=-92.70 +/- 29.46\n",
      "Episode length: 93.70 +/- 29.46\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00145 |       0.00000 |      71.04189 |       0.00095 |       0.39084\n",
      "      0.00046 |       0.00000 |      70.59669 |       0.00116 |       0.38601\n",
      "     -0.00037 |       0.00000 |      70.16421 |       0.00127 |       0.38067\n",
      "     -0.00126 |       0.00000 |      69.55226 |       0.00161 |       0.37642\n",
      "     -0.00063 |       0.00000 |      69.72806 |       0.00161 |       0.37909\n",
      "      0.00068 |       0.00000 |      69.54120 |       0.00176 |       0.37941\n",
      "     -0.00073 |       0.00000 |      68.92851 |       0.00164 |       0.38280\n",
      "     -0.00131 |       0.00000 |      69.12572 |       0.00178 |       0.37859\n",
      "     -0.00102 |       0.00000 |      68.82378 |       0.00180 |       0.37919\n",
      "     -0.00179 |       0.00000 |      68.44233 |       0.00193 |       0.37856\n",
      "Evaluating losses...\n",
      "     -0.00109 |       0.00000 |      68.45941 |       0.00170 |       0.38073\n",
      "-----------------------------------\n",
      "| EpLenMean       | 110           |\n",
      "| EpRewMean       | -109          |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 600           |\n",
      "| TimeElapsed     | 404           |\n",
      "| TimestepsSoFar  | 94208         |\n",
      "| ev_tdlam_before | 0.715         |\n",
      "| loss_ent        | 0.38072807    |\n",
      "| loss_kl         | 0.0016968737  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010927387 |\n",
      "| loss_vf_loss    | 68.45941      |\n",
      "-----------------------------------\n",
      "********** Iteration 23 ************\n",
      "Eval num_timesteps=94208, episode_reward=-78.90 +/- 7.52\n",
      "Episode length: 79.90 +/- 7.52\n",
      "Eval num_timesteps=94208, episode_reward=-88.90 +/- 20.04\n",
      "Episode length: 89.90 +/- 20.04\n",
      "Eval num_timesteps=94208, episode_reward=-80.60 +/- 8.22\n",
      "Episode length: 81.60 +/- 8.22\n",
      "Eval num_timesteps=94208, episode_reward=-79.40 +/- 6.50\n",
      "Episode length: 80.40 +/- 6.50\n",
      "Eval num_timesteps=94208, episode_reward=-81.40 +/- 10.31\n",
      "Episode length: 82.40 +/- 10.31\n",
      "Eval num_timesteps=94208, episode_reward=-98.80 +/- 29.51\n",
      "Episode length: 99.80 +/- 29.51\n",
      "Eval num_timesteps=94208, episode_reward=-84.80 +/- 9.84\n",
      "Episode length: 85.80 +/- 9.84\n",
      "Eval num_timesteps=94208, episode_reward=-83.70 +/- 6.65\n",
      "Episode length: 84.70 +/- 6.65\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00171 |       0.00000 |      74.85445 |       0.00105 |       0.39145\n",
      "     -0.00167 |       0.00000 |      73.96479 |       0.00120 |       0.38695\n",
      "     -0.00028 |       0.00000 |      73.82212 |       0.00173 |       0.38440\n",
      "     -0.00089 |       0.00000 |      73.53103 |       0.00218 |       0.38178\n",
      "     -0.00119 |       0.00000 |      73.11223 |       0.00272 |       0.38088\n",
      "     -0.00202 |       0.00000 |      73.32970 |       0.00262 |       0.38177\n",
      "     -0.00087 |       0.00000 |      72.80904 |       0.00293 |       0.38655\n",
      "     -0.00123 |       0.00000 |      72.82877 |       0.00310 |       0.38505\n",
      "     -0.00274 |       0.00000 |      72.29465 |       0.00337 |       0.38493\n",
      "     -0.00179 |       0.00000 |      72.29185 |       0.00322 |       0.38446\n",
      "Evaluating losses...\n",
      "     -0.00170 |       0.00000 |      72.23460 |       0.00378 |       0.38101\n",
      "-----------------------------------\n",
      "| EpLenMean       | 103           |\n",
      "| EpRewMean       | -102          |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 641           |\n",
      "| TimeElapsed     | 415           |\n",
      "| TimestepsSoFar  | 98304         |\n",
      "| ev_tdlam_before | 0.714         |\n",
      "| loss_ent        | 0.38101357    |\n",
      "| loss_kl         | 0.0037784316  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0017048384 |\n",
      "| loss_vf_loss    | 72.234604     |\n",
      "-----------------------------------\n",
      "********** Iteration 24 ************\n",
      "Eval num_timesteps=98304, episode_reward=-81.20 +/- 7.24\n",
      "Episode length: 82.20 +/- 7.24\n",
      "Eval num_timesteps=98304, episode_reward=-83.10 +/- 5.63\n",
      "Episode length: 84.10 +/- 5.63\n",
      "Eval num_timesteps=98304, episode_reward=-83.60 +/- 6.41\n",
      "Episode length: 84.60 +/- 6.41\n",
      "Eval num_timesteps=98304, episode_reward=-103.90 +/- 39.46\n",
      "Episode length: 104.90 +/- 39.46\n",
      "Eval num_timesteps=98304, episode_reward=-80.40 +/- 8.09\n",
      "Episode length: 81.40 +/- 8.09\n",
      "Eval num_timesteps=98304, episode_reward=-81.30 +/- 10.35\n",
      "Episode length: 82.30 +/- 10.35\n",
      "Eval num_timesteps=98304, episode_reward=-90.40 +/- 17.77\n",
      "Episode length: 91.40 +/- 17.77\n",
      "Eval num_timesteps=98304, episode_reward=-105.70 +/- 44.81\n",
      "Episode length: 106.70 +/- 44.81\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     2.21e-05 |       0.00000 |      70.85001 |       0.00107 |       0.35718\n",
      "     6.42e-05 |       0.00000 |      70.52654 |       0.00138 |       0.34753\n",
      "     -0.00036 |       0.00000 |      70.08415 |       0.00213 |       0.33833\n",
      "     -0.00201 |       0.00000 |      69.83281 |       0.00221 |       0.33778\n",
      "     -0.00020 |       0.00000 |      69.42477 |       0.00219 |       0.33696\n",
      "     -0.00113 |       0.00000 |      69.62519 |       0.00226 |       0.33772\n",
      "     -0.00151 |       0.00000 |      69.22249 |       0.00259 |       0.33392\n",
      "     -0.00122 |       0.00000 |      69.04511 |       0.00226 |       0.33797\n",
      "     -0.00131 |       0.00000 |      68.46005 |       0.00253 |       0.33499\n",
      "     -0.00109 |       0.00000 |      68.86369 |       0.00274 |       0.33292\n",
      "Evaluating losses...\n",
      "     -0.00119 |       0.00000 |      68.58646 |       0.00277 |       0.33329\n",
      "----------------------------------\n",
      "| EpLenMean       | 104          |\n",
      "| EpRewMean       | -103         |\n",
      "| EpThisIter      | 40           |\n",
      "| EpisodesSoFar   | 681          |\n",
      "| TimeElapsed     | 426          |\n",
      "| TimestepsSoFar  | 102400       |\n",
      "| ev_tdlam_before | 0.718        |\n",
      "| loss_ent        | 0.33328667   |\n",
      "| loss_kl         | 0.0027726877 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001191142 |\n",
      "| loss_vf_loss    | 68.58646     |\n",
      "----------------------------------\n",
      "********** Iteration 25 ************\n",
      "Eval num_timesteps=102400, episode_reward=-80.40 +/- 8.25\n",
      "Episode length: 81.40 +/- 8.25\n",
      "Eval num_timesteps=102400, episode_reward=-86.60 +/- 6.65\n",
      "Episode length: 87.60 +/- 6.65\n",
      "Eval num_timesteps=102400, episode_reward=-88.40 +/- 21.51\n",
      "Episode length: 89.40 +/- 21.51\n",
      "Eval num_timesteps=102400, episode_reward=-80.70 +/- 5.18\n",
      "Episode length: 81.70 +/- 5.18\n",
      "Eval num_timesteps=102400, episode_reward=-87.00 +/- 9.94\n",
      "Episode length: 88.00 +/- 9.94\n",
      "Eval num_timesteps=102400, episode_reward=-78.30 +/- 11.43\n",
      "Episode length: 79.30 +/- 11.43\n",
      "Eval num_timesteps=102400, episode_reward=-87.00 +/- 25.03\n",
      "Episode length: 88.00 +/- 25.03\n",
      "Eval num_timesteps=102400, episode_reward=-82.40 +/- 7.84\n",
      "Episode length: 83.40 +/- 7.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00184 |       0.00000 |      59.55914 |       0.00100 |       0.33647\n",
      "      0.00064 |       0.00000 |      59.37141 |       0.00098 |       0.33993\n",
      "     -0.00075 |       0.00000 |      59.25770 |       0.00120 |       0.33715\n",
      "      0.00025 |       0.00000 |      58.55589 |       0.00135 |       0.33590\n",
      "      0.00030 |       0.00000 |      58.50385 |       0.00147 |       0.33626\n",
      "     -0.00059 |       0.00000 |      58.30634 |       0.00173 |       0.33184\n",
      "     -0.00021 |       0.00000 |      58.05425 |       0.00178 |       0.33271\n",
      "     -0.00227 |       0.00000 |      58.05472 |       0.00174 |       0.33544\n",
      "     -0.00098 |       0.00000 |      57.72111 |       0.00186 |       0.33361\n",
      "     -0.00072 |       0.00000 |      57.67990 |       0.00193 |       0.33177\n",
      "Evaluating losses...\n",
      "     -0.00145 |       0.00000 |      57.45121 |       0.00190 |       0.33094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| EpLenMean       | 100          |\n",
      "| EpRewMean       | -99.1        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 724          |\n",
      "| TimeElapsed     | 436          |\n",
      "| TimestepsSoFar  | 106496       |\n",
      "| ev_tdlam_before | 0.776        |\n",
      "| loss_ent        | 0.3309409    |\n",
      "| loss_kl         | 0.0019029016 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.00144755  |\n",
      "| loss_vf_loss    | 57.451214    |\n",
      "----------------------------------\n",
      "********** Iteration 26 ************\n",
      "Eval num_timesteps=106496, episode_reward=-80.80 +/- 10.26\n",
      "Episode length: 81.80 +/- 10.26\n",
      "Eval num_timesteps=106496, episode_reward=-82.30 +/- 13.91\n",
      "Episode length: 83.30 +/- 13.91\n",
      "Eval num_timesteps=106496, episode_reward=-87.00 +/- 18.19\n",
      "Episode length: 88.00 +/- 18.19\n",
      "Eval num_timesteps=106496, episode_reward=-87.90 +/- 8.48\n",
      "Episode length: 88.90 +/- 8.48\n",
      "Eval num_timesteps=106496, episode_reward=-84.30 +/- 12.82\n",
      "Episode length: 85.30 +/- 12.82\n",
      "Eval num_timesteps=106496, episode_reward=-80.00 +/- 4.22\n",
      "Episode length: 81.00 +/- 4.22\n",
      "Eval num_timesteps=106496, episode_reward=-79.50 +/- 6.55\n",
      "Episode length: 80.50 +/- 6.55\n",
      "Eval num_timesteps=106496, episode_reward=-82.30 +/- 5.08\n",
      "Episode length: 83.30 +/- 5.08\n",
      "Eval num_timesteps=106496, episode_reward=-97.00 +/- 29.68\n",
      "Episode length: 98.00 +/- 29.68\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00019 |       0.00000 |      68.26743 |       0.00101 |       0.31450\n",
      "      0.00012 |       0.00000 |      67.44952 |       0.00150 |       0.31435\n",
      "     -0.00095 |       0.00000 |      67.38092 |       0.00207 |       0.31272\n",
      "     -0.00124 |       0.00000 |      66.98806 |       0.00176 |       0.31485\n",
      "     -0.00098 |       0.00000 |      66.52147 |       0.00217 |       0.31199\n",
      "     -0.00128 |       0.00000 |      66.46282 |       0.00226 |       0.30971\n",
      "     -0.00175 |       0.00000 |      66.38700 |       0.00210 |       0.30880\n",
      "     -0.00165 |       0.00000 |      66.19052 |       0.00217 |       0.30928\n",
      "     -0.00186 |       0.00000 |      66.26456 |       0.00217 |       0.30921\n",
      "     -0.00102 |       0.00000 |      65.90076 |       0.00231 |       0.30534\n",
      "Evaluating losses...\n",
      "     -0.00238 |       0.00000 |      66.06533 |       0.00233 |       0.30367\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99.3          |\n",
      "| EpRewMean       | -98.3         |\n",
      "| EpThisIter      | 40            |\n",
      "| EpisodesSoFar   | 764           |\n",
      "| TimeElapsed     | 447           |\n",
      "| TimestepsSoFar  | 110592        |\n",
      "| ev_tdlam_before | 0.714         |\n",
      "| loss_ent        | 0.3036679     |\n",
      "| loss_kl         | 0.0023300247  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0023776505 |\n",
      "| loss_vf_loss    | 66.06533      |\n",
      "-----------------------------------\n",
      "********** Iteration 27 ************\n",
      "Eval num_timesteps=110592, episode_reward=-82.40 +/- 4.05\n",
      "Episode length: 83.40 +/- 4.05\n",
      "Eval num_timesteps=110592, episode_reward=-91.60 +/- 27.03\n",
      "Episode length: 92.60 +/- 27.03\n",
      "Eval num_timesteps=110592, episode_reward=-78.50 +/- 6.56\n",
      "Episode length: 79.50 +/- 6.56\n",
      "Eval num_timesteps=110592, episode_reward=-85.50 +/- 10.41\n",
      "Episode length: 86.50 +/- 10.41\n",
      "Eval num_timesteps=110592, episode_reward=-82.80 +/- 7.48\n",
      "Episode length: 83.80 +/- 7.48\n",
      "Eval num_timesteps=110592, episode_reward=-87.90 +/- 13.41\n",
      "Episode length: 88.90 +/- 13.41\n",
      "Eval num_timesteps=110592, episode_reward=-94.00 +/- 43.84\n",
      "Episode length: 95.00 +/- 43.84\n",
      "Eval num_timesteps=110592, episode_reward=-87.10 +/- 17.84\n",
      "Episode length: 88.10 +/- 17.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00086 |       0.00000 |      60.22390 |       0.00085 |       0.31960\n",
      "     -0.00052 |       0.00000 |      58.60395 |       0.00096 |       0.31659\n",
      "     -0.00018 |       0.00000 |      57.85310 |       0.00106 |       0.31463\n",
      "     -0.00021 |       0.00000 |      57.12319 |       0.00111 |       0.31338\n",
      "     -0.00081 |       0.00000 |      56.92680 |       0.00123 |       0.30980\n",
      "     -0.00061 |       0.00000 |      56.40313 |       0.00130 |       0.31268\n",
      "     -0.00090 |       0.00000 |      56.23557 |       0.00133 |       0.31148\n",
      "     -0.00029 |       0.00000 |      55.66204 |       0.00136 |       0.31165\n",
      "     -0.00075 |       0.00000 |      55.67759 |       0.00140 |       0.31230\n",
      "     -0.00052 |       0.00000 |      55.47941 |       0.00168 |       0.31184\n",
      "Evaluating losses...\n",
      "     -0.00161 |       0.00000 |      55.46597 |       0.00160 |       0.31339\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.8          |\n",
      "| EpRewMean       | -93.8         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 811           |\n",
      "| TimeElapsed     | 458           |\n",
      "| TimestepsSoFar  | 114688        |\n",
      "| ev_tdlam_before | 0.782         |\n",
      "| loss_ent        | 0.31339243    |\n",
      "| loss_kl         | 0.001598817   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016099815 |\n",
      "| loss_vf_loss    | 55.46597      |\n",
      "-----------------------------------\n",
      "********** Iteration 28 ************\n",
      "Eval num_timesteps=114688, episode_reward=-88.20 +/- 8.07\n",
      "Episode length: 89.20 +/- 8.07\n",
      "Eval num_timesteps=114688, episode_reward=-80.20 +/- 6.90\n",
      "Episode length: 81.20 +/- 6.90\n",
      "Eval num_timesteps=114688, episode_reward=-90.90 +/- 23.65\n",
      "Episode length: 91.90 +/- 23.65\n",
      "Eval num_timesteps=114688, episode_reward=-100.50 +/- 32.59\n",
      "Episode length: 101.50 +/- 32.59\n",
      "Eval num_timesteps=114688, episode_reward=-81.90 +/- 6.67\n",
      "Episode length: 82.90 +/- 6.67\n",
      "Eval num_timesteps=114688, episode_reward=-81.30 +/- 4.22\n",
      "Episode length: 82.30 +/- 4.22\n",
      "Eval num_timesteps=114688, episode_reward=-94.30 +/- 30.21\n",
      "Episode length: 95.30 +/- 30.21\n",
      "Eval num_timesteps=114688, episode_reward=-82.50 +/- 8.03\n",
      "Episode length: 83.50 +/- 8.03\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     4.14e-05 |       0.00000 |      69.44257 |       0.00089 |       0.30105\n",
      "     -0.00104 |       0.00000 |      68.55247 |       0.00112 |       0.30630\n",
      "     -0.00087 |       0.00000 |      68.13885 |       0.00140 |       0.30864\n",
      "     -0.00084 |       0.00000 |      68.00182 |       0.00160 |       0.30849\n",
      "     -0.00115 |       0.00000 |      67.37418 |       0.00176 |       0.30708\n",
      "     -0.00190 |       0.00000 |      67.43684 |       0.00198 |       0.30233\n",
      "     -0.00114 |       0.00000 |      67.09676 |       0.00209 |       0.30545\n",
      "     -0.00188 |       0.00000 |      66.99305 |       0.00221 |       0.30788\n",
      "     -0.00097 |       0.00000 |      66.76723 |       0.00217 |       0.30452\n",
      "     -0.00271 |       0.00000 |      66.63892 |       0.00235 |       0.30391\n",
      "Evaluating losses...\n",
      "     -0.00205 |       0.00000 |      66.66844 |       0.00234 |       0.30354\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.3         |\n",
      "| EpRewMean       | -94.3        |\n",
      "| EpThisIter      | 40           |\n",
      "| EpisodesSoFar   | 851          |\n",
      "| TimeElapsed     | 468          |\n",
      "| TimestepsSoFar  | 118784       |\n",
      "| ev_tdlam_before | 0.704        |\n",
      "| loss_ent        | 0.30353642   |\n",
      "| loss_kl         | 0.0023355908 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.002045154 |\n",
      "| loss_vf_loss    | 66.66844     |\n",
      "----------------------------------\n",
      "********** Iteration 29 ************\n",
      "Eval num_timesteps=118784, episode_reward=-81.20 +/- 3.66\n",
      "Episode length: 82.20 +/- 3.66\n",
      "Eval num_timesteps=118784, episode_reward=-84.80 +/- 8.33\n",
      "Episode length: 85.80 +/- 8.33\n",
      "Eval num_timesteps=118784, episode_reward=-90.50 +/- 13.93\n",
      "Episode length: 91.50 +/- 13.93\n",
      "Eval num_timesteps=118784, episode_reward=-83.80 +/- 5.49\n",
      "Episode length: 84.80 +/- 5.49\n",
      "Eval num_timesteps=118784, episode_reward=-80.30 +/- 10.63\n",
      "Episode length: 81.30 +/- 10.63\n",
      "Eval num_timesteps=118784, episode_reward=-86.30 +/- 16.59\n",
      "Episode length: 87.30 +/- 16.59\n",
      "Eval num_timesteps=118784, episode_reward=-82.20 +/- 7.82\n",
      "Episode length: 83.20 +/- 7.82\n",
      "Eval num_timesteps=118784, episode_reward=-84.80 +/- 27.23\n",
      "Episode length: 85.80 +/- 27.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00077 |       0.00000 |      57.75657 |       0.00099 |       0.31976\n",
      "      0.00071 |       0.00000 |      57.33231 |       0.00108 |       0.32268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00052 |       0.00000 |      57.05937 |       0.00120 |       0.31890\n",
      "     -0.00064 |       0.00000 |      56.84552 |       0.00126 |       0.32043\n",
      "    -5.97e-07 |       0.00000 |      56.85276 |       0.00136 |       0.31896\n",
      "     -0.00074 |       0.00000 |      56.49215 |       0.00150 |       0.31721\n",
      "     -0.00046 |       0.00000 |      56.36475 |       0.00161 |       0.31454\n",
      "     -0.00071 |       0.00000 |      56.33963 |       0.00171 |       0.31577\n",
      "     -0.00053 |       0.00000 |      56.07438 |       0.00190 |       0.31930\n",
      "     -0.00082 |       0.00000 |      55.81591 |       0.00169 |       0.31851\n",
      "Evaluating losses...\n",
      "     -0.00188 |       0.00000 |      55.82423 |       0.00172 |       0.31567\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.8          |\n",
      "| EpRewMean       | -94.8         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 894           |\n",
      "| TimeElapsed     | 479           |\n",
      "| TimestepsSoFar  | 122880        |\n",
      "| ev_tdlam_before | 0.77          |\n",
      "| loss_ent        | 0.31566566    |\n",
      "| loss_kl         | 0.0017234697  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018776993 |\n",
      "| loss_vf_loss    | 55.824226     |\n",
      "-----------------------------------\n",
      "********** Iteration 30 ************\n",
      "Eval num_timesteps=122880, episode_reward=-84.70 +/- 10.85\n",
      "Episode length: 85.70 +/- 10.85\n",
      "Eval num_timesteps=122880, episode_reward=-82.20 +/- 3.97\n",
      "Episode length: 83.20 +/- 3.97\n",
      "Eval num_timesteps=122880, episode_reward=-84.30 +/- 10.52\n",
      "Episode length: 85.30 +/- 10.52\n",
      "Eval num_timesteps=122880, episode_reward=-80.90 +/- 4.18\n",
      "Episode length: 81.90 +/- 4.18\n",
      "Eval num_timesteps=122880, episode_reward=-81.20 +/- 5.13\n",
      "Episode length: 82.20 +/- 5.13\n",
      "Eval num_timesteps=122880, episode_reward=-89.40 +/- 16.91\n",
      "Episode length: 90.40 +/- 16.91\n",
      "Eval num_timesteps=122880, episode_reward=-94.90 +/- 23.97\n",
      "Episode length: 95.90 +/- 23.97\n",
      "Eval num_timesteps=122880, episode_reward=-81.00 +/- 12.28\n",
      "Episode length: 82.00 +/- 12.28\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00070 |       0.00000 |      52.17445 |       0.00095 |       0.31238\n",
      "     -0.00128 |       0.00000 |      51.68225 |       0.00132 |       0.30741\n",
      "     -0.00030 |       0.00000 |      51.92961 |       0.00158 |       0.30169\n",
      "     -0.00014 |       0.00000 |      51.55925 |       0.00191 |       0.29728\n",
      "     -0.00064 |       0.00000 |      51.48975 |       0.00227 |       0.29256\n",
      "     -0.00022 |       0.00000 |      51.08701 |       0.00237 |       0.29228\n",
      "     -0.00192 |       0.00000 |      51.29393 |       0.00252 |       0.29269\n",
      "     -0.00080 |       0.00000 |      50.94687 |       0.00224 |       0.29398\n",
      "     -0.00160 |       0.00000 |      50.98636 |       0.00243 |       0.29509\n",
      "     -0.00054 |       0.00000 |      50.85299 |       0.00255 |       0.29250\n",
      "Evaluating losses...\n",
      "     -0.00198 |       0.00000 |      50.69885 |       0.00277 |       0.29197\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.4         |\n",
      "| EpRewMean       | -92.4        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 938          |\n",
      "| TimeElapsed     | 489          |\n",
      "| TimestepsSoFar  | 126976       |\n",
      "| ev_tdlam_before | 0.792        |\n",
      "| loss_ent        | 0.29197416   |\n",
      "| loss_kl         | 0.0027705533 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001977532 |\n",
      "| loss_vf_loss    | 50.698853    |\n",
      "----------------------------------\n",
      "********** Iteration 31 ************\n",
      "Eval num_timesteps=126976, episode_reward=-85.40 +/- 13.99\n",
      "Episode length: 86.40 +/- 13.99\n",
      "Eval num_timesteps=126976, episode_reward=-86.00 +/- 13.51\n",
      "Episode length: 87.00 +/- 13.51\n",
      "Eval num_timesteps=126976, episode_reward=-95.00 +/- 16.71\n",
      "Episode length: 96.00 +/- 16.71\n",
      "Eval num_timesteps=126976, episode_reward=-84.20 +/- 6.13\n",
      "Episode length: 85.20 +/- 6.13\n",
      "Eval num_timesteps=126976, episode_reward=-77.60 +/- 6.76\n",
      "Episode length: 78.60 +/- 6.76\n",
      "Eval num_timesteps=126976, episode_reward=-87.70 +/- 25.90\n",
      "Episode length: 88.70 +/- 25.90\n",
      "Eval num_timesteps=126976, episode_reward=-84.40 +/- 9.40\n",
      "Episode length: 85.40 +/- 9.40\n",
      "Eval num_timesteps=126976, episode_reward=-87.10 +/- 10.47\n",
      "Episode length: 88.10 +/- 10.47\n",
      "Eval num_timesteps=126976, episode_reward=-84.60 +/- 8.11\n",
      "Episode length: 85.60 +/- 8.11\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00039 |       0.00000 |      58.80104 |       0.00103 |       0.28617\n",
      "      0.00010 |       0.00000 |      58.76602 |       0.00121 |       0.27878\n",
      "      0.00023 |       0.00000 |      58.58362 |       0.00179 |       0.27369\n",
      "      0.00028 |       0.00000 |      58.36839 |       0.00197 |       0.27166\n",
      "     -0.00171 |       0.00000 |      58.30554 |       0.00187 |       0.27269\n",
      "     -0.00104 |       0.00000 |      57.82840 |       0.00188 |       0.27406\n",
      "     -0.00086 |       0.00000 |      58.07444 |       0.00204 |       0.27024\n",
      "     -0.00103 |       0.00000 |      57.47418 |       0.00206 |       0.27091\n",
      "     -0.00064 |       0.00000 |      57.55483 |       0.00214 |       0.26834\n",
      "     -0.00185 |       0.00000 |      57.58726 |       0.00249 |       0.26546\n",
      "Evaluating losses...\n",
      "     -0.00185 |       0.00000 |      57.51126 |       0.00263 |       0.26524\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.1          |\n",
      "| EpRewMean       | -92.1         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 981           |\n",
      "| TimeElapsed     | 501           |\n",
      "| TimestepsSoFar  | 131072        |\n",
      "| ev_tdlam_before | 0.756         |\n",
      "| loss_ent        | 0.26523662    |\n",
      "| loss_kl         | 0.0026347623  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018517245 |\n",
      "| loss_vf_loss    | 57.511265     |\n",
      "-----------------------------------\n",
      "********** Iteration 32 ************\n",
      "Eval num_timesteps=131072, episode_reward=-85.70 +/- 11.37\n",
      "Episode length: 86.70 +/- 11.37\n",
      "Eval num_timesteps=131072, episode_reward=-89.50 +/- 24.38\n",
      "Episode length: 90.50 +/- 24.38\n",
      "Eval num_timesteps=131072, episode_reward=-82.50 +/- 5.35\n",
      "Episode length: 83.50 +/- 5.35\n",
      "Eval num_timesteps=131072, episode_reward=-82.70 +/- 7.48\n",
      "Episode length: 83.70 +/- 7.48\n",
      "Eval num_timesteps=131072, episode_reward=-84.80 +/- 17.10\n",
      "Episode length: 85.80 +/- 17.10\n",
      "Eval num_timesteps=131072, episode_reward=-80.60 +/- 5.26\n",
      "Episode length: 81.60 +/- 5.26\n",
      "Eval num_timesteps=131072, episode_reward=-87.10 +/- 13.20\n",
      "Episode length: 88.10 +/- 13.20\n",
      "Eval num_timesteps=131072, episode_reward=-79.00 +/- 6.83\n",
      "Episode length: 80.00 +/- 6.83\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     9.84e-05 |       0.00000 |      51.19003 |       0.00096 |       0.25790\n",
      "      0.00015 |       0.00000 |      51.17257 |       0.00123 |       0.25923\n",
      "     -0.00046 |       0.00000 |      51.01034 |       0.00116 |       0.25632\n",
      "     -0.00073 |       0.00000 |      50.67318 |       0.00156 |       0.25577\n",
      "      0.00013 |       0.00000 |      50.50509 |       0.00157 |       0.25303\n",
      "     -0.00029 |       0.00000 |      50.24313 |       0.00164 |       0.25311\n",
      "     -0.00161 |       0.00000 |      50.19351 |       0.00186 |       0.25482\n",
      "     -0.00036 |       0.00000 |      50.28312 |       0.00174 |       0.25218\n",
      "     -0.00064 |       0.00000 |      50.18277 |       0.00191 |       0.25481\n",
      "     -0.00132 |       0.00000 |      50.03217 |       0.00182 |       0.25205\n",
      "Evaluating losses...\n",
      "     -0.00169 |       0.00000 |      49.87866 |       0.00184 |       0.25217\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95            |\n",
      "| EpRewMean       | -94           |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 1025          |\n",
      "| TimeElapsed     | 513           |\n",
      "| TimestepsSoFar  | 135168        |\n",
      "| ev_tdlam_before | 0.785         |\n",
      "| loss_ent        | 0.25217164    |\n",
      "| loss_kl         | 0.0018353069  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0016928252 |\n",
      "| loss_vf_loss    | 49.87866      |\n",
      "-----------------------------------\n",
      "********** Iteration 33 ************\n",
      "Eval num_timesteps=135168, episode_reward=-78.30 +/- 7.14\n",
      "Episode length: 79.30 +/- 7.14\n",
      "Eval num_timesteps=135168, episode_reward=-88.10 +/- 16.02\n",
      "Episode length: 89.10 +/- 16.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=135168, episode_reward=-81.10 +/- 7.27\n",
      "Episode length: 82.10 +/- 7.27\n",
      "Eval num_timesteps=135168, episode_reward=-84.60 +/- 10.79\n",
      "Episode length: 85.60 +/- 10.79\n",
      "Eval num_timesteps=135168, episode_reward=-89.70 +/- 10.34\n",
      "Episode length: 90.70 +/- 10.34\n",
      "Eval num_timesteps=135168, episode_reward=-80.50 +/- 5.61\n",
      "Episode length: 81.50 +/- 5.61\n",
      "Eval num_timesteps=135168, episode_reward=-90.60 +/- 32.78\n",
      "Episode length: 91.60 +/- 32.78\n",
      "Eval num_timesteps=135168, episode_reward=-91.10 +/- 23.17\n",
      "Episode length: 92.10 +/- 23.17\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00109 |       0.00000 |      56.78931 |       0.00107 |       0.25430\n",
      "      0.00060 |       0.00000 |      56.49261 |       0.00109 |       0.25874\n",
      "     -0.00151 |       0.00000 |      56.14546 |       0.00129 |       0.26072\n",
      "      0.00034 |       0.00000 |      56.62185 |       0.00147 |       0.26518\n",
      "     -0.00024 |       0.00000 |      56.13193 |       0.00141 |       0.26591\n",
      "      0.00046 |       0.00000 |      55.89885 |       0.00153 |       0.26601\n",
      "     -0.00094 |       0.00000 |      55.95119 |       0.00139 |       0.26144\n",
      "     -0.00088 |       0.00000 |      55.53268 |       0.00154 |       0.25959\n",
      "     -0.00146 |       0.00000 |      55.58175 |       0.00155 |       0.26168\n",
      "     -0.00088 |       0.00000 |      55.49533 |       0.00171 |       0.25999\n",
      "Evaluating losses...\n",
      "     -0.00049 |       0.00000 |      55.52062 |       0.00176 |       0.26029\n",
      "------------------------------------\n",
      "| EpLenMean       | 95.1           |\n",
      "| EpRewMean       | -94.1          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 1068           |\n",
      "| TimeElapsed     | 525            |\n",
      "| TimestepsSoFar  | 139264         |\n",
      "| ev_tdlam_before | 0.76           |\n",
      "| loss_ent        | 0.26028663     |\n",
      "| loss_kl         | 0.0017556164   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00048931746 |\n",
      "| loss_vf_loss    | 55.520622      |\n",
      "------------------------------------\n",
      "********** Iteration 34 ************\n",
      "Eval num_timesteps=139264, episode_reward=-80.90 +/- 6.61\n",
      "Episode length: 81.90 +/- 6.61\n",
      "Eval num_timesteps=139264, episode_reward=-89.70 +/- 18.04\n",
      "Episode length: 90.70 +/- 18.04\n",
      "Eval num_timesteps=139264, episode_reward=-82.70 +/- 14.39\n",
      "Episode length: 83.70 +/- 14.39\n",
      "Eval num_timesteps=139264, episode_reward=-85.30 +/- 4.45\n",
      "Episode length: 86.30 +/- 4.45\n",
      "Eval num_timesteps=139264, episode_reward=-82.90 +/- 11.87\n",
      "Episode length: 83.90 +/- 11.87\n",
      "Eval num_timesteps=139264, episode_reward=-80.10 +/- 10.10\n",
      "Episode length: 81.10 +/- 10.10\n",
      "Eval num_timesteps=139264, episode_reward=-80.90 +/- 8.99\n",
      "Episode length: 81.90 +/- 8.99\n",
      "Eval num_timesteps=139264, episode_reward=-81.50 +/- 14.71\n",
      "Episode length: 82.50 +/- 14.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00102 |       0.00000 |      51.69646 |       0.00112 |       0.26136\n",
      "    -6.70e-05 |       0.00000 |      50.94930 |       0.00143 |       0.25396\n",
      "     -0.00063 |       0.00000 |      50.91039 |       0.00137 |       0.25239\n",
      "      0.00101 |       0.00000 |      50.75057 |       0.00189 |       0.24673\n",
      "     -0.00071 |       0.00000 |      50.27457 |       0.00166 |       0.24860\n",
      "      0.00020 |       0.00000 |      50.30132 |       0.00180 |       0.25013\n",
      "     -0.00083 |       0.00000 |      50.21859 |       0.00182 |       0.24940\n",
      "     -0.00045 |       0.00000 |      49.99918 |       0.00161 |       0.25075\n",
      "     -0.00014 |       0.00000 |      49.93162 |       0.00180 |       0.24837\n",
      "     -0.00142 |       0.00000 |      49.79182 |       0.00166 |       0.24917\n",
      "Evaluating losses...\n",
      "     -0.00013 |       0.00000 |      49.53341 |       0.00170 |       0.24696\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.5           |\n",
      "| EpRewMean       | -91.5          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 1114           |\n",
      "| TimeElapsed     | 537            |\n",
      "| TimestepsSoFar  | 143360         |\n",
      "| ev_tdlam_before | 0.784          |\n",
      "| loss_ent        | 0.24696043     |\n",
      "| loss_kl         | 0.0017036749   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00013372186 |\n",
      "| loss_vf_loss    | 49.533405      |\n",
      "------------------------------------\n",
      "********** Iteration 35 ************\n",
      "Eval num_timesteps=143360, episode_reward=-92.10 +/- 22.74\n",
      "Episode length: 93.10 +/- 22.74\n",
      "Eval num_timesteps=143360, episode_reward=-81.50 +/- 4.32\n",
      "Episode length: 82.50 +/- 4.32\n",
      "Eval num_timesteps=143360, episode_reward=-81.50 +/- 7.53\n",
      "Episode length: 82.50 +/- 7.53\n",
      "Eval num_timesteps=143360, episode_reward=-89.20 +/- 21.49\n",
      "Episode length: 90.20 +/- 21.49\n",
      "Eval num_timesteps=143360, episode_reward=-86.90 +/- 12.36\n",
      "Episode length: 87.90 +/- 12.36\n",
      "Eval num_timesteps=143360, episode_reward=-84.80 +/- 6.10\n",
      "Episode length: 85.80 +/- 6.10\n",
      "Eval num_timesteps=143360, episode_reward=-84.70 +/- 6.53\n",
      "Episode length: 85.70 +/- 6.53\n",
      "Eval num_timesteps=143360, episode_reward=-80.60 +/- 6.55\n",
      "Episode length: 81.60 +/- 6.55\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00068 |       0.00000 |      51.54948 |       0.00115 |       0.25567\n",
      "      0.00054 |       0.00000 |      51.05042 |       0.00103 |       0.25210\n",
      "     -0.00023 |       0.00000 |      50.64324 |       0.00126 |       0.24769\n",
      "     -0.00062 |       0.00000 |      50.42639 |       0.00158 |       0.24482\n",
      "      0.00055 |       0.00000 |      50.03548 |       0.00153 |       0.24455\n",
      "     -0.00071 |       0.00000 |      49.82243 |       0.00174 |       0.24357\n",
      "     -0.00025 |       0.00000 |      49.63388 |       0.00172 |       0.24340\n",
      "     1.26e-05 |       0.00000 |      49.44230 |       0.00168 |       0.24144\n",
      "     -0.00036 |       0.00000 |      49.13879 |       0.00181 |       0.24109\n",
      "     -0.00058 |       0.00000 |      49.02566 |       0.00164 |       0.24129\n",
      "Evaluating losses...\n",
      "     -0.00054 |       0.00000 |      49.20689 |       0.00173 |       0.24297\n",
      "------------------------------------\n",
      "| EpLenMean       | 88.3           |\n",
      "| EpRewMean       | -87.3          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 1160           |\n",
      "| TimeElapsed     | 551            |\n",
      "| TimestepsSoFar  | 147456         |\n",
      "| ev_tdlam_before | 0.788          |\n",
      "| loss_ent        | 0.24297456     |\n",
      "| loss_kl         | 0.0017341495   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00053847645 |\n",
      "| loss_vf_loss    | 49.206894      |\n",
      "------------------------------------\n",
      "********** Iteration 36 ************\n",
      "Eval num_timesteps=147456, episode_reward=-85.10 +/- 7.87\n",
      "Episode length: 86.10 +/- 7.87\n",
      "Eval num_timesteps=147456, episode_reward=-80.80 +/- 5.67\n",
      "Episode length: 81.80 +/- 5.67\n",
      "Eval num_timesteps=147456, episode_reward=-84.10 +/- 9.15\n",
      "Episode length: 85.10 +/- 9.15\n",
      "Eval num_timesteps=147456, episode_reward=-79.20 +/- 6.49\n",
      "Episode length: 80.20 +/- 6.49\n",
      "Eval num_timesteps=147456, episode_reward=-111.20 +/- 45.82\n",
      "Episode length: 112.20 +/- 45.82\n",
      "Eval num_timesteps=147456, episode_reward=-78.00 +/- 6.00\n",
      "Episode length: 79.00 +/- 6.00\n",
      "Eval num_timesteps=147456, episode_reward=-88.60 +/- 25.04\n",
      "Episode length: 89.60 +/- 25.04\n",
      "Eval num_timesteps=147456, episode_reward=-81.40 +/- 7.91\n",
      "Episode length: 82.40 +/- 7.91\n",
      "Eval num_timesteps=147456, episode_reward=-79.90 +/- 5.87\n",
      "Episode length: 80.90 +/- 5.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00037 |       0.00000 |      61.00762 |       0.00111 |       0.23005\n",
      "      0.00086 |       0.00000 |      60.06900 |       0.00116 |       0.22978\n",
      "    -3.32e-05 |       0.00000 |      59.58874 |       0.00136 |       0.23353\n",
      "     -0.00043 |       0.00000 |      58.98965 |       0.00143 |       0.23400\n",
      "     1.18e-05 |       0.00000 |      58.67399 |       0.00161 |       0.23215\n",
      "     -0.00024 |       0.00000 |      58.61479 |       0.00165 |       0.23086\n",
      "     -0.00111 |       0.00000 |      58.41641 |       0.00158 |       0.23150\n",
      "     -0.00097 |       0.00000 |      58.04899 |       0.00173 |       0.23201\n",
      "     -0.00032 |       0.00000 |      58.12546 |       0.00178 |       0.23020\n",
      "     -0.00039 |       0.00000 |      57.91755 |       0.00177 |       0.22987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "     -0.00048 |       0.00000 |      57.89071 |       0.00186 |       0.23000\n",
      "------------------------------------\n",
      "| EpLenMean       | 93.5           |\n",
      "| EpRewMean       | -92.5          |\n",
      "| EpThisIter      | 40             |\n",
      "| EpisodesSoFar   | 1200           |\n",
      "| TimeElapsed     | 564            |\n",
      "| TimestepsSoFar  | 151552         |\n",
      "| ev_tdlam_before | 0.724          |\n",
      "| loss_ent        | 0.23000449     |\n",
      "| loss_kl         | 0.0018646942   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00047500466 |\n",
      "| loss_vf_loss    | 57.89071       |\n",
      "------------------------------------\n",
      "********** Iteration 37 ************\n",
      "Eval num_timesteps=151552, episode_reward=-76.80 +/- 5.04\n",
      "Episode length: 77.80 +/- 5.04\n",
      "Eval num_timesteps=151552, episode_reward=-80.00 +/- 5.55\n",
      "Episode length: 81.00 +/- 5.55\n",
      "Eval num_timesteps=151552, episode_reward=-82.20 +/- 8.51\n",
      "Episode length: 83.20 +/- 8.51\n",
      "Eval num_timesteps=151552, episode_reward=-90.60 +/- 31.81\n",
      "Episode length: 91.60 +/- 31.81\n",
      "Eval num_timesteps=151552, episode_reward=-91.80 +/- 40.50\n",
      "Episode length: 92.80 +/- 40.50\n",
      "Eval num_timesteps=151552, episode_reward=-89.10 +/- 12.76\n",
      "Episode length: 90.10 +/- 12.76\n",
      "Eval num_timesteps=151552, episode_reward=-82.60 +/- 8.51\n",
      "Episode length: 83.60 +/- 8.51\n",
      "Eval num_timesteps=151552, episode_reward=-84.60 +/- 5.12\n",
      "Episode length: 85.60 +/- 5.12\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00012 |       0.00000 |      52.07210 |       0.00102 |       0.22504\n",
      "      0.00064 |       0.00000 |      50.46013 |       0.00114 |       0.22380\n",
      "     -0.00044 |       0.00000 |      50.45227 |       0.00120 |       0.22347\n",
      "     -0.00042 |       0.00000 |      50.09985 |       0.00117 |       0.22357\n",
      "     -0.00017 |       0.00000 |      49.73152 |       0.00135 |       0.22235\n",
      "     -0.00069 |       0.00000 |      49.53637 |       0.00139 |       0.22368\n",
      "     -0.00074 |       0.00000 |      49.28254 |       0.00136 |       0.22307\n",
      "    -7.40e-05 |       0.00000 |      48.90373 |       0.00161 |       0.22281\n",
      "     -0.00058 |       0.00000 |      48.78354 |       0.00159 |       0.22273\n",
      "     -0.00032 |       0.00000 |      48.75856 |       0.00167 |       0.22214\n",
      "Evaluating losses...\n",
      "     -0.00130 |       0.00000 |      48.52528 |       0.00158 |       0.22080\n",
      "----------------------------------\n",
      "| EpLenMean       | 95           |\n",
      "| EpRewMean       | -94          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 1244         |\n",
      "| TimeElapsed     | 576          |\n",
      "| TimestepsSoFar  | 155648       |\n",
      "| ev_tdlam_before | 0.774        |\n",
      "| loss_ent        | 0.22079733   |\n",
      "| loss_kl         | 0.0015817239 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001298164 |\n",
      "| loss_vf_loss    | 48.525276    |\n",
      "----------------------------------\n",
      "********** Iteration 38 ************\n",
      "Eval num_timesteps=155648, episode_reward=-87.30 +/- 17.41\n",
      "Episode length: 88.30 +/- 17.41\n",
      "Eval num_timesteps=155648, episode_reward=-80.00 +/- 7.14\n",
      "Episode length: 81.00 +/- 7.14\n",
      "Eval num_timesteps=155648, episode_reward=-83.00 +/- 7.31\n",
      "Episode length: 84.00 +/- 7.31\n",
      "Eval num_timesteps=155648, episode_reward=-84.70 +/- 10.03\n",
      "Episode length: 85.70 +/- 10.03\n",
      "Eval num_timesteps=155648, episode_reward=-83.40 +/- 10.44\n",
      "Episode length: 84.40 +/- 10.44\n",
      "Eval num_timesteps=155648, episode_reward=-81.50 +/- 4.84\n",
      "Episode length: 82.50 +/- 4.84\n",
      "Eval num_timesteps=155648, episode_reward=-77.70 +/- 10.01\n",
      "Episode length: 78.70 +/- 10.01\n",
      "Eval num_timesteps=155648, episode_reward=-82.40 +/- 6.39\n",
      "Episode length: 83.40 +/- 6.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00063 |       0.00000 |      51.94921 |       0.00110 |       0.22796\n",
      "     -0.00018 |       0.00000 |      51.60237 |       0.00117 |       0.22491\n",
      "      0.00075 |       0.00000 |      51.17701 |       0.00128 |       0.22280\n",
      "     -0.00039 |       0.00000 |      50.86641 |       0.00123 |       0.22350\n",
      "     -0.00032 |       0.00000 |      50.81235 |       0.00139 |       0.22190\n",
      "     -0.00059 |       0.00000 |      50.64202 |       0.00156 |       0.22012\n",
      "     -0.00055 |       0.00000 |      50.60467 |       0.00156 |       0.21980\n",
      "     -0.00168 |       0.00000 |      50.63782 |       0.00159 |       0.22155\n",
      "     -0.00106 |       0.00000 |      50.45475 |       0.00161 |       0.21816\n",
      "     -0.00128 |       0.00000 |      50.22464 |       0.00170 |       0.21952\n",
      "Evaluating losses...\n",
      "     -0.00031 |       0.00000 |      50.37952 |       0.00168 |       0.21898\n",
      "------------------------------------\n",
      "| EpLenMean       | 96             |\n",
      "| EpRewMean       | -95            |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 1288           |\n",
      "| TimeElapsed     | 587            |\n",
      "| TimestepsSoFar  | 159744         |\n",
      "| ev_tdlam_before | 0.779          |\n",
      "| loss_ent        | 0.21897736     |\n",
      "| loss_kl         | 0.0016833397   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00030527776 |\n",
      "| loss_vf_loss    | 50.37952       |\n",
      "------------------------------------\n",
      "********** Iteration 39 ************\n",
      "Eval num_timesteps=159744, episode_reward=-83.80 +/- 12.16\n",
      "Episode length: 84.80 +/- 12.16\n",
      "Eval num_timesteps=159744, episode_reward=-91.20 +/- 31.97\n",
      "Episode length: 92.20 +/- 31.97\n",
      "Eval num_timesteps=159744, episode_reward=-81.50 +/- 9.11\n",
      "Episode length: 82.50 +/- 9.11\n",
      "Eval num_timesteps=159744, episode_reward=-80.50 +/- 5.95\n",
      "Episode length: 81.50 +/- 5.95\n",
      "Eval num_timesteps=159744, episode_reward=-81.60 +/- 9.61\n",
      "Episode length: 82.60 +/- 9.61\n",
      "Eval num_timesteps=159744, episode_reward=-85.90 +/- 7.97\n",
      "Episode length: 86.90 +/- 7.97\n",
      "Eval num_timesteps=159744, episode_reward=-81.90 +/- 7.73\n",
      "Episode length: 82.90 +/- 7.73\n",
      "Eval num_timesteps=159744, episode_reward=-81.80 +/- 5.55\n",
      "Episode length: 82.80 +/- 5.55\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00017 |       0.00000 |      38.45817 |       0.00112 |       0.22400\n",
      "     -0.00047 |       0.00000 |      38.12100 |       0.00139 |       0.22622\n",
      "    -8.12e-05 |       0.00000 |      37.76463 |       0.00141 |       0.22378\n",
      "     -0.00143 |       0.00000 |      37.48482 |       0.00153 |       0.22425\n",
      "     -0.00147 |       0.00000 |      37.36861 |       0.00157 |       0.22606\n",
      "     -0.00144 |       0.00000 |      37.16828 |       0.00172 |       0.22626\n",
      "     -0.00112 |       0.00000 |      36.97515 |       0.00176 |       0.22329\n",
      "     -0.00069 |       0.00000 |      36.79694 |       0.00186 |       0.22036\n",
      "     -0.00107 |       0.00000 |      36.73442 |       0.00175 |       0.22239\n",
      "     -0.00144 |       0.00000 |      36.70884 |       0.00194 |       0.21961\n",
      "Evaluating losses...\n",
      "     -0.00089 |       0.00000 |      36.55905 |       0.00180 |       0.21978\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.5          |\n",
      "| EpRewMean       | -90.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 1335          |\n",
      "| TimeElapsed     | 599           |\n",
      "| TimestepsSoFar  | 163840        |\n",
      "| ev_tdlam_before | 0.851         |\n",
      "| loss_ent        | 0.21977893    |\n",
      "| loss_kl         | 0.0017955014  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0008884607 |\n",
      "| loss_vf_loss    | 36.55905      |\n",
      "-----------------------------------\n",
      "********** Iteration 40 ************\n",
      "Eval num_timesteps=163840, episode_reward=-84.90 +/- 9.90\n",
      "Episode length: 85.90 +/- 9.90\n",
      "Eval num_timesteps=163840, episode_reward=-87.00 +/- 20.39\n",
      "Episode length: 88.00 +/- 20.39\n",
      "Eval num_timesteps=163840, episode_reward=-85.40 +/- 20.34\n",
      "Episode length: 86.40 +/- 20.34\n",
      "Eval num_timesteps=163840, episode_reward=-87.30 +/- 9.24\n",
      "Episode length: 88.30 +/- 9.24\n",
      "Eval num_timesteps=163840, episode_reward=-81.60 +/- 7.81\n",
      "Episode length: 82.60 +/- 7.81\n",
      "Eval num_timesteps=163840, episode_reward=-86.50 +/- 16.92\n",
      "Episode length: 87.50 +/- 16.92\n",
      "Eval num_timesteps=163840, episode_reward=-87.20 +/- 18.90\n",
      "Episode length: 88.20 +/- 18.90\n",
      "Eval num_timesteps=163840, episode_reward=-81.40 +/- 7.75\n",
      "Episode length: 82.40 +/- 7.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00111 |       0.00000 |      53.83609 |       0.00106 |       0.20849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00037 |       0.00000 |      52.21084 |       0.00111 |       0.20659\n",
      "      0.00052 |       0.00000 |      51.34479 |       0.00126 |       0.20565\n",
      "      0.00043 |       0.00000 |      51.04622 |       0.00135 |       0.20476\n",
      "     -0.00054 |       0.00000 |      50.20975 |       0.00135 |       0.20451\n",
      "     -0.00063 |       0.00000 |      50.28919 |       0.00148 |       0.20303\n",
      "     -0.00019 |       0.00000 |      49.61690 |       0.00144 |       0.20295\n",
      "    -9.94e-06 |       0.00000 |      49.74189 |       0.00155 |       0.20342\n",
      "      0.00035 |       0.00000 |      49.44082 |       0.00173 |       0.20289\n",
      "     7.27e-05 |       0.00000 |      49.30635 |       0.00172 |       0.20349\n",
      "Evaluating losses...\n",
      "     -0.00124 |       0.00000 |      49.03875 |       0.00161 |       0.20356\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.4          |\n",
      "| EpRewMean       | -91.4         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 1376          |\n",
      "| TimeElapsed     | 609           |\n",
      "| TimestepsSoFar  | 167936        |\n",
      "| ev_tdlam_before | 0.763         |\n",
      "| loss_ent        | 0.20355608    |\n",
      "| loss_kl         | 0.0016099792  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012405617 |\n",
      "| loss_vf_loss    | 49.038746     |\n",
      "-----------------------------------\n",
      "********** Iteration 41 ************\n",
      "Eval num_timesteps=167936, episode_reward=-81.20 +/- 4.12\n",
      "Episode length: 82.20 +/- 4.12\n",
      "Eval num_timesteps=167936, episode_reward=-86.00 +/- 18.77\n",
      "Episode length: 87.00 +/- 18.77\n",
      "Eval num_timesteps=167936, episode_reward=-76.50 +/- 6.38\n",
      "Episode length: 77.50 +/- 6.38\n",
      "Eval num_timesteps=167936, episode_reward=-90.80 +/- 23.44\n",
      "Episode length: 91.80 +/- 23.44\n",
      "Eval num_timesteps=167936, episode_reward=-87.60 +/- 21.25\n",
      "Episode length: 88.60 +/- 21.25\n",
      "Eval num_timesteps=167936, episode_reward=-82.50 +/- 5.84\n",
      "Episode length: 83.50 +/- 5.84\n",
      "Eval num_timesteps=167936, episode_reward=-84.80 +/- 7.22\n",
      "Episode length: 85.80 +/- 7.22\n",
      "Eval num_timesteps=167936, episode_reward=-80.20 +/- 3.06\n",
      "Episode length: 81.20 +/- 3.06\n",
      "Eval num_timesteps=167936, episode_reward=-85.50 +/- 11.67\n",
      "Episode length: 86.50 +/- 11.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00030 |       0.00000 |      52.51815 |       0.00099 |       0.20698\n",
      "     -0.00045 |       0.00000 |      50.91468 |       0.00121 |       0.20610\n",
      "     -0.00087 |       0.00000 |      50.02599 |       0.00129 |       0.20217\n",
      "     -0.00014 |       0.00000 |      49.37374 |       0.00134 |       0.20198\n",
      "     -0.00055 |       0.00000 |      48.57502 |       0.00155 |       0.20278\n",
      "     -0.00046 |       0.00000 |      48.35819 |       0.00165 |       0.19920\n",
      "     -0.00066 |       0.00000 |      48.26565 |       0.00160 |       0.19952\n",
      "     -0.00083 |       0.00000 |      47.86879 |       0.00189 |       0.19986\n",
      "     -0.00185 |       0.00000 |      47.62288 |       0.00161 |       0.20036\n",
      "     -0.00161 |       0.00000 |      47.69752 |       0.00176 |       0.19845\n",
      "Evaluating losses...\n",
      "     -0.00092 |       0.00000 |      47.39287 |       0.00188 |       0.19968\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.5          |\n",
      "| EpRewMean       | -94.5         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 1419          |\n",
      "| TimeElapsed     | 622           |\n",
      "| TimestepsSoFar  | 172032        |\n",
      "| ev_tdlam_before | 0.779         |\n",
      "| loss_ent        | 0.19967568    |\n",
      "| loss_kl         | 0.0018820019  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0009247321 |\n",
      "| loss_vf_loss    | 47.39287      |\n",
      "-----------------------------------\n",
      "********** Iteration 42 ************\n",
      "Eval num_timesteps=172032, episode_reward=-84.10 +/- 9.12\n",
      "Episode length: 85.10 +/- 9.12\n",
      "Eval num_timesteps=172032, episode_reward=-79.60 +/- 6.02\n",
      "Episode length: 80.60 +/- 6.02\n",
      "Eval num_timesteps=172032, episode_reward=-84.00 +/- 5.42\n",
      "Episode length: 85.00 +/- 5.42\n",
      "Eval num_timesteps=172032, episode_reward=-88.20 +/- 23.01\n",
      "Episode length: 89.20 +/- 23.01\n",
      "Eval num_timesteps=172032, episode_reward=-83.30 +/- 6.20\n",
      "Episode length: 84.30 +/- 6.20\n",
      "Eval num_timesteps=172032, episode_reward=-80.80 +/- 11.33\n",
      "Episode length: 81.80 +/- 11.33\n",
      "Eval num_timesteps=172032, episode_reward=-82.40 +/- 6.36\n",
      "Episode length: 83.40 +/- 6.36\n",
      "Eval num_timesteps=172032, episode_reward=-133.50 +/- 123.85\n",
      "Episode length: 134.40 +/- 123.56\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00147 |       0.00000 |      53.10572 |       0.00110 |       0.19429\n",
      "     -0.00029 |       0.00000 |      52.22829 |       0.00119 |       0.19921\n",
      "     -0.00035 |       0.00000 |      51.78891 |       0.00138 |       0.20159\n",
      "      0.00027 |       0.00000 |      51.39667 |       0.00147 |       0.20060\n",
      "     3.53e-05 |       0.00000 |      51.31281 |       0.00163 |       0.20335\n",
      "     2.38e-05 |       0.00000 |      51.29468 |       0.00174 |       0.20345\n",
      "     -0.00055 |       0.00000 |      50.76606 |       0.00181 |       0.20366\n",
      "     -0.00036 |       0.00000 |      51.03465 |       0.00191 |       0.20311\n",
      "     -0.00099 |       0.00000 |      50.56017 |       0.00192 |       0.20643\n",
      "     -0.00120 |       0.00000 |      50.90316 |       0.00219 |       0.20513\n",
      "Evaluating losses...\n",
      "     -0.00186 |       0.00000 |      50.71866 |       0.00208 |       0.20465\n",
      "----------------------------------\n",
      "| EpLenMean       | 96.4         |\n",
      "| EpRewMean       | -95.4        |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 1460         |\n",
      "| TimeElapsed     | 633          |\n",
      "| TimestepsSoFar  | 176128       |\n",
      "| ev_tdlam_before | 0.769        |\n",
      "| loss_ent        | 0.20464948   |\n",
      "| loss_kl         | 0.0020825695 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001859288 |\n",
      "| loss_vf_loss    | 50.71866     |\n",
      "----------------------------------\n",
      "********** Iteration 43 ************\n",
      "Eval num_timesteps=176128, episode_reward=-84.00 +/- 10.84\n",
      "Episode length: 85.00 +/- 10.84\n",
      "Eval num_timesteps=176128, episode_reward=-94.50 +/- 34.86\n",
      "Episode length: 95.50 +/- 34.86\n",
      "Eval num_timesteps=176128, episode_reward=-83.50 +/- 4.63\n",
      "Episode length: 84.50 +/- 4.63\n",
      "Eval num_timesteps=176128, episode_reward=-84.00 +/- 13.32\n",
      "Episode length: 85.00 +/- 13.32\n",
      "Eval num_timesteps=176128, episode_reward=-84.20 +/- 11.61\n",
      "Episode length: 85.20 +/- 11.61\n",
      "Eval num_timesteps=176128, episode_reward=-81.60 +/- 6.67\n",
      "Episode length: 82.60 +/- 6.67\n",
      "Eval num_timesteps=176128, episode_reward=-82.40 +/- 5.62\n",
      "Episode length: 83.40 +/- 5.62\n",
      "Eval num_timesteps=176128, episode_reward=-97.10 +/- 24.74\n",
      "Episode length: 98.10 +/- 24.74\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00135 |       0.00000 |      41.78024 |       0.00106 |       0.21418\n",
      "      0.00044 |       0.00000 |      40.89354 |       0.00140 |       0.20744\n",
      "      0.00015 |       0.00000 |      40.47149 |       0.00157 |       0.20392\n",
      "      0.00058 |       0.00000 |      39.90185 |       0.00176 |       0.20214\n",
      "     -0.00018 |       0.00000 |      40.01955 |       0.00188 |       0.20090\n",
      "     -0.00035 |       0.00000 |      39.60733 |       0.00177 |       0.20227\n",
      "     -0.00036 |       0.00000 |      39.39775 |       0.00190 |       0.20461\n",
      "      0.00025 |       0.00000 |      39.06156 |       0.00179 |       0.20613\n",
      "     -0.00099 |       0.00000 |      39.37189 |       0.00189 |       0.20355\n",
      "     -0.00077 |       0.00000 |      39.09617 |       0.00177 |       0.20380\n",
      "Evaluating losses...\n",
      "     -0.00049 |       0.00000 |      38.82379 |       0.00220 |       0.20346\n",
      "------------------------------------\n",
      "| EpLenMean       | 93.7           |\n",
      "| EpRewMean       | -92.7          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 1506           |\n",
      "| TimeElapsed     | 644            |\n",
      "| TimestepsSoFar  | 180224         |\n",
      "| ev_tdlam_before | 0.835          |\n",
      "| loss_ent        | 0.20346458     |\n",
      "| loss_kl         | 0.0021959941   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00048864505 |\n",
      "| loss_vf_loss    | 38.82379       |\n",
      "------------------------------------\n",
      "********** Iteration 44 ************\n",
      "Eval num_timesteps=180224, episode_reward=-81.40 +/- 5.08\n",
      "Episode length: 82.40 +/- 5.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=180224, episode_reward=-88.00 +/- 15.13\n",
      "Episode length: 89.00 +/- 15.13\n",
      "Eval num_timesteps=180224, episode_reward=-82.40 +/- 5.06\n",
      "Episode length: 83.40 +/- 5.06\n",
      "Eval num_timesteps=180224, episode_reward=-85.50 +/- 17.56\n",
      "Episode length: 86.50 +/- 17.56\n",
      "Eval num_timesteps=180224, episode_reward=-81.30 +/- 7.95\n",
      "Episode length: 82.30 +/- 7.95\n",
      "Eval num_timesteps=180224, episode_reward=-103.60 +/- 55.18\n",
      "Episode length: 104.60 +/- 55.18\n",
      "Eval num_timesteps=180224, episode_reward=-80.30 +/- 8.68\n",
      "Episode length: 81.30 +/- 8.68\n",
      "Eval num_timesteps=180224, episode_reward=-90.90 +/- 26.89\n",
      "Episode length: 91.90 +/- 26.89\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00092 |       0.00000 |      55.62629 |       0.00096 |       0.18481\n",
      "      0.00084 |       0.00000 |      50.56889 |       0.00112 |       0.18328\n",
      "      0.00033 |       0.00000 |      48.73831 |       0.00135 |       0.18195\n",
      "     -0.00083 |       0.00000 |      47.81469 |       0.00133 |       0.18084\n",
      "      0.00042 |       0.00000 |      47.11306 |       0.00150 |       0.18214\n",
      "     -0.00073 |       0.00000 |      46.55387 |       0.00135 |       0.17959\n",
      "     7.18e-05 |       0.00000 |      46.22269 |       0.00137 |       0.18176\n",
      "      0.00106 |       0.00000 |      46.01826 |       0.00130 |       0.18274\n",
      "     -0.00088 |       0.00000 |      46.04848 |       0.00144 |       0.18299\n",
      "     -0.00023 |       0.00000 |      45.85265 |       0.00133 |       0.18198\n",
      "Evaluating losses...\n",
      "      0.00022 |       0.00000 |      46.00550 |       0.00128 |       0.18067\n",
      "-----------------------------------\n",
      "| EpLenMean       | 97.5          |\n",
      "| EpRewMean       | -96.5         |\n",
      "| EpThisIter      | 38            |\n",
      "| EpisodesSoFar   | 1544          |\n",
      "| TimeElapsed     | 656           |\n",
      "| TimestepsSoFar  | 184320        |\n",
      "| ev_tdlam_before | 0.73          |\n",
      "| loss_ent        | 0.18067282    |\n",
      "| loss_kl         | 0.0012821427  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00022163929 |\n",
      "| loss_vf_loss    | 46.005497     |\n",
      "-----------------------------------\n",
      "********** Iteration 45 ************\n",
      "Eval num_timesteps=184320, episode_reward=-89.20 +/- 20.52\n",
      "Episode length: 90.20 +/- 20.52\n",
      "Eval num_timesteps=184320, episode_reward=-87.50 +/- 12.75\n",
      "Episode length: 88.50 +/- 12.75\n",
      "Eval num_timesteps=184320, episode_reward=-82.90 +/- 5.54\n",
      "Episode length: 83.90 +/- 5.54\n",
      "Eval num_timesteps=184320, episode_reward=-86.10 +/- 18.25\n",
      "Episode length: 87.10 +/- 18.25\n",
      "Eval num_timesteps=184320, episode_reward=-81.10 +/- 13.63\n",
      "Episode length: 82.10 +/- 13.63\n",
      "Eval num_timesteps=184320, episode_reward=-83.80 +/- 8.78\n",
      "Episode length: 84.80 +/- 8.78\n",
      "Eval num_timesteps=184320, episode_reward=-81.60 +/- 4.67\n",
      "Episode length: 82.60 +/- 4.67\n",
      "Eval num_timesteps=184320, episode_reward=-81.20 +/- 6.37\n",
      "Episode length: 82.20 +/- 6.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00030 |       0.00000 |      49.83036 |       0.00116 |       0.20434\n",
      "     6.24e-05 |       0.00000 |      48.52486 |       0.00105 |       0.20253\n",
      "    -4.12e-05 |       0.00000 |      47.93843 |       0.00118 |       0.20252\n",
      "     -0.00046 |       0.00000 |      47.18705 |       0.00136 |       0.20219\n",
      "     -0.00030 |       0.00000 |      46.85469 |       0.00152 |       0.20198\n",
      "      0.00020 |       0.00000 |      46.68898 |       0.00171 |       0.20197\n",
      "     -0.00058 |       0.00000 |      46.47671 |       0.00173 |       0.20196\n",
      "     -0.00122 |       0.00000 |      46.38581 |       0.00186 |       0.20164\n",
      "     -0.00078 |       0.00000 |      46.08776 |       0.00184 |       0.20221\n",
      "     -0.00029 |       0.00000 |      45.84245 |       0.00175 |       0.20364\n",
      "Evaluating losses...\n",
      "     -0.00048 |       0.00000 |      45.79926 |       0.00176 |       0.20196\n",
      "------------------------------------\n",
      "| EpLenMean       | 95.4           |\n",
      "| EpRewMean       | -94.4          |\n",
      "| EpThisIter      | 47             |\n",
      "| EpisodesSoFar   | 1591           |\n",
      "| TimeElapsed     | 666            |\n",
      "| TimestepsSoFar  | 188416         |\n",
      "| ev_tdlam_before | 0.804          |\n",
      "| loss_ent        | 0.2019551      |\n",
      "| loss_kl         | 0.0017633616   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00048495678 |\n",
      "| loss_vf_loss    | 45.79926       |\n",
      "------------------------------------\n",
      "********** Iteration 46 ************\n",
      "Eval num_timesteps=188416, episode_reward=-84.50 +/- 6.53\n",
      "Episode length: 85.50 +/- 6.53\n",
      "Eval num_timesteps=188416, episode_reward=-78.80 +/- 4.38\n",
      "Episode length: 79.80 +/- 4.38\n",
      "Eval num_timesteps=188416, episode_reward=-80.20 +/- 7.64\n",
      "Episode length: 81.20 +/- 7.64\n",
      "Eval num_timesteps=188416, episode_reward=-82.00 +/- 8.06\n",
      "Episode length: 83.00 +/- 8.06\n",
      "Eval num_timesteps=188416, episode_reward=-84.10 +/- 7.37\n",
      "Episode length: 85.10 +/- 7.37\n",
      "Eval num_timesteps=188416, episode_reward=-82.20 +/- 5.90\n",
      "Episode length: 83.20 +/- 5.90\n",
      "Eval num_timesteps=188416, episode_reward=-80.70 +/- 9.84\n",
      "Episode length: 81.70 +/- 9.84\n",
      "Eval num_timesteps=188416, episode_reward=-79.20 +/- 4.21\n",
      "Episode length: 80.20 +/- 4.21\n",
      "Eval num_timesteps=188416, episode_reward=-89.70 +/- 17.86\n",
      "Episode length: 90.70 +/- 17.86\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00020 |       0.00000 |      39.73329 |       0.00110 |       0.19817\n",
      "     -0.00031 |       0.00000 |      39.03648 |       0.00157 |       0.19896\n",
      "    -6.67e-05 |       0.00000 |      38.68500 |       0.00220 |       0.19761\n",
      "     -0.00094 |       0.00000 |      38.21295 |       0.00245 |       0.19798\n",
      "     -0.00029 |       0.00000 |      38.11068 |       0.00229 |       0.20043\n",
      "     -0.00078 |       0.00000 |      37.90850 |       0.00258 |       0.20184\n",
      "     -0.00053 |       0.00000 |      37.67501 |       0.00274 |       0.19952\n",
      "     -0.00137 |       0.00000 |      37.65874 |       0.00282 |       0.20032\n",
      "     -0.00036 |       0.00000 |      37.18411 |       0.00257 |       0.19982\n",
      "     -0.00133 |       0.00000 |      36.96502 |       0.00307 |       0.20023\n",
      "Evaluating losses...\n",
      "     -0.00128 |       0.00000 |      36.89704 |       0.00315 |       0.20030\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.7          |\n",
      "| EpRewMean       | -86.7         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 1638          |\n",
      "| TimeElapsed     | 677           |\n",
      "| TimestepsSoFar  | 192512        |\n",
      "| ev_tdlam_before | 0.842         |\n",
      "| loss_ent        | 0.20029585    |\n",
      "| loss_kl         | 0.0031515346  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012791518 |\n",
      "| loss_vf_loss    | 36.897038     |\n",
      "-----------------------------------\n",
      "********** Iteration 47 ************\n",
      "Eval num_timesteps=192512, episode_reward=-83.90 +/- 9.75\n",
      "Episode length: 84.90 +/- 9.75\n",
      "Eval num_timesteps=192512, episode_reward=-80.60 +/- 6.59\n",
      "Episode length: 81.60 +/- 6.59\n",
      "Eval num_timesteps=192512, episode_reward=-87.90 +/- 21.57\n",
      "Episode length: 88.90 +/- 21.57\n",
      "Eval num_timesteps=192512, episode_reward=-94.90 +/- 35.28\n",
      "Episode length: 95.90 +/- 35.28\n",
      "Eval num_timesteps=192512, episode_reward=-83.20 +/- 8.07\n",
      "Episode length: 84.20 +/- 8.07\n",
      "Eval num_timesteps=192512, episode_reward=-83.40 +/- 6.28\n",
      "Episode length: 84.40 +/- 6.28\n",
      "Eval num_timesteps=192512, episode_reward=-88.40 +/- 18.60\n",
      "Episode length: 89.40 +/- 18.60\n",
      "Eval num_timesteps=192512, episode_reward=-92.90 +/- 33.23\n",
      "Episode length: 93.90 +/- 33.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00132 |       0.00000 |      41.36999 |       0.00103 |       0.19197\n",
      "      0.00021 |       0.00000 |      40.76282 |       0.00115 |       0.19074\n",
      "      0.00065 |       0.00000 |      40.44107 |       0.00123 |       0.19087\n",
      "     -0.00016 |       0.00000 |      40.21933 |       0.00135 |       0.19036\n",
      "     -0.00104 |       0.00000 |      39.87630 |       0.00139 |       0.19143\n",
      "     -0.00066 |       0.00000 |      39.62513 |       0.00134 |       0.18947\n",
      "     -0.00061 |       0.00000 |      39.45657 |       0.00150 |       0.18991\n",
      "     -0.00051 |       0.00000 |      39.46481 |       0.00147 |       0.18964\n",
      "     -0.00077 |       0.00000 |      39.27291 |       0.00149 |       0.19168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00089 |       0.00000 |      39.25552 |       0.00166 |       0.19205\n",
      "Evaluating losses...\n",
      "     -0.00099 |       0.00000 |      39.04103 |       0.00168 |       0.19231\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.6          |\n",
      "| EpRewMean       | -87.6         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 1683          |\n",
      "| TimeElapsed     | 687           |\n",
      "| TimestepsSoFar  | 196608        |\n",
      "| ev_tdlam_before | 0.825         |\n",
      "| loss_ent        | 0.19231458    |\n",
      "| loss_kl         | 0.001684053   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0009922045 |\n",
      "| loss_vf_loss    | 39.041035     |\n",
      "-----------------------------------\n",
      "********** Iteration 48 ************\n",
      "Eval num_timesteps=196608, episode_reward=-81.10 +/- 4.61\n",
      "Episode length: 82.10 +/- 4.61\n",
      "Eval num_timesteps=196608, episode_reward=-110.30 +/- 53.78\n",
      "Episode length: 111.30 +/- 53.78\n",
      "Eval num_timesteps=196608, episode_reward=-86.10 +/- 24.28\n",
      "Episode length: 87.10 +/- 24.28\n",
      "Eval num_timesteps=196608, episode_reward=-75.30 +/- 4.31\n",
      "Episode length: 76.30 +/- 4.31\n",
      "Eval num_timesteps=196608, episode_reward=-89.00 +/- 15.69\n",
      "Episode length: 90.00 +/- 15.69\n",
      "Eval num_timesteps=196608, episode_reward=-79.40 +/- 5.31\n",
      "Episode length: 80.40 +/- 5.31\n",
      "Eval num_timesteps=196608, episode_reward=-89.40 +/- 24.06\n",
      "Episode length: 90.40 +/- 24.06\n",
      "Eval num_timesteps=196608, episode_reward=-78.70 +/- 6.28\n",
      "Episode length: 79.70 +/- 6.28\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00016 |       0.00000 |      39.52563 |       0.00105 |       0.19825\n",
      "      0.00048 |       0.00000 |      39.31409 |       0.00111 |       0.19784\n",
      "      0.00080 |       0.00000 |      38.97523 |       0.00125 |       0.19827\n",
      "      0.00055 |       0.00000 |      38.88884 |       0.00125 |       0.19787\n",
      "     -0.00092 |       0.00000 |      38.67019 |       0.00146 |       0.19848\n",
      "     -0.00051 |       0.00000 |      38.50258 |       0.00151 |       0.19823\n",
      "     -0.00020 |       0.00000 |      38.15001 |       0.00157 |       0.19869\n",
      "     -0.00064 |       0.00000 |      38.22451 |       0.00141 |       0.19933\n",
      "     -0.00126 |       0.00000 |      38.13190 |       0.00136 |       0.19768\n",
      "     -0.00045 |       0.00000 |      37.84926 |       0.00139 |       0.20035\n",
      "Evaluating losses...\n",
      "     -0.00071 |       0.00000 |      37.72671 |       0.00144 |       0.20140\n",
      "------------------------------------\n",
      "| EpLenMean       | 89             |\n",
      "| EpRewMean       | -88            |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 1729           |\n",
      "| TimeElapsed     | 698            |\n",
      "| TimestepsSoFar  | 200704         |\n",
      "| ev_tdlam_before | 0.836          |\n",
      "| loss_ent        | 0.20140167     |\n",
      "| loss_kl         | 0.0014396012   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00071251485 |\n",
      "| loss_vf_loss    | 37.726707      |\n",
      "------------------------------------\n",
      "********** Iteration 49 ************\n",
      "Eval num_timesteps=200704, episode_reward=-82.20 +/- 4.94\n",
      "Episode length: 83.20 +/- 4.94\n",
      "Eval num_timesteps=200704, episode_reward=-80.90 +/- 6.86\n",
      "Episode length: 81.90 +/- 6.86\n",
      "Eval num_timesteps=200704, episode_reward=-89.80 +/- 24.29\n",
      "Episode length: 90.80 +/- 24.29\n",
      "Eval num_timesteps=200704, episode_reward=-78.80 +/- 4.73\n",
      "Episode length: 79.80 +/- 4.73\n",
      "Eval num_timesteps=200704, episode_reward=-82.00 +/- 5.25\n",
      "Episode length: 83.00 +/- 5.25\n",
      "Eval num_timesteps=200704, episode_reward=-78.10 +/- 6.47\n",
      "Episode length: 79.10 +/- 6.47\n",
      "Eval num_timesteps=200704, episode_reward=-86.10 +/- 11.74\n",
      "Episode length: 87.10 +/- 11.74\n",
      "Eval num_timesteps=200704, episode_reward=-77.10 +/- 6.76\n",
      "Episode length: 78.10 +/- 6.76\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00082 |       0.00000 |      33.76375 |       0.00107 |       0.19924\n",
      "      0.00189 |       0.00000 |      33.66170 |       0.00112 |       0.19797\n",
      "     7.79e-06 |       0.00000 |      33.16886 |       0.00123 |       0.19541\n",
      "      0.00172 |       0.00000 |      33.26792 |       0.00121 |       0.19514\n",
      "     -0.00050 |       0.00000 |      32.99402 |       0.00124 |       0.19444\n",
      "     -0.00084 |       0.00000 |      32.87872 |       0.00127 |       0.19454\n",
      "     -0.00166 |       0.00000 |      32.89326 |       0.00146 |       0.19484\n",
      "      0.00021 |       0.00000 |      32.79941 |       0.00147 |       0.19266\n",
      "     -0.00086 |       0.00000 |      32.56787 |       0.00163 |       0.19367\n",
      "     -0.00104 |       0.00000 |      32.41217 |       0.00158 |       0.19247\n",
      "Evaluating losses...\n",
      "     -0.00124 |       0.00000 |      32.45406 |       0.00155 |       0.19243\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.5          |\n",
      "| EpRewMean       | -88.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 1776          |\n",
      "| TimeElapsed     | 708           |\n",
      "| TimestepsSoFar  | 204800        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.19242592    |\n",
      "| loss_kl         | 0.0015533394  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012421091 |\n",
      "| loss_vf_loss    | 32.454056     |\n",
      "-----------------------------------\n",
      "********** Iteration 50 ************\n",
      "Eval num_timesteps=204800, episode_reward=-84.20 +/- 11.37\n",
      "Episode length: 85.20 +/- 11.37\n",
      "Eval num_timesteps=204800, episode_reward=-80.10 +/- 7.49\n",
      "Episode length: 81.10 +/- 7.49\n",
      "Eval num_timesteps=204800, episode_reward=-83.40 +/- 16.80\n",
      "Episode length: 84.40 +/- 16.80\n",
      "Eval num_timesteps=204800, episode_reward=-83.50 +/- 7.75\n",
      "Episode length: 84.50 +/- 7.75\n",
      "Eval num_timesteps=204800, episode_reward=-75.50 +/- 4.01\n",
      "Episode length: 76.50 +/- 4.01\n",
      "Eval num_timesteps=204800, episode_reward=-80.00 +/- 3.90\n",
      "Episode length: 81.00 +/- 3.90\n",
      "Eval num_timesteps=204800, episode_reward=-80.00 +/- 8.11\n",
      "Episode length: 81.00 +/- 8.11\n",
      "Eval num_timesteps=204800, episode_reward=-87.70 +/- 22.71\n",
      "Episode length: 88.70 +/- 22.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00039 |       0.00000 |      43.88252 |       0.00110 |       0.18942\n",
      "     -0.00114 |       0.00000 |      43.65568 |       0.00122 |       0.18944\n",
      "     -0.00069 |       0.00000 |      43.58672 |       0.00178 |       0.19028\n",
      "     -0.00124 |       0.00000 |      43.29064 |       0.00238 |       0.18863\n",
      "     -0.00158 |       0.00000 |      43.36230 |       0.00241 |       0.19014\n",
      "     -0.00159 |       0.00000 |      43.12700 |       0.00270 |       0.18897\n",
      "     -0.00094 |       0.00000 |      43.33461 |       0.00261 |       0.18857\n",
      "     -0.00198 |       0.00000 |      42.91438 |       0.00247 |       0.19023\n",
      "     -0.00103 |       0.00000 |      42.82926 |       0.00279 |       0.19055\n",
      "     -0.00182 |       0.00000 |      42.78398 |       0.00260 |       0.19011\n",
      "Evaluating losses...\n",
      "     -0.00191 |       0.00000 |      42.81858 |       0.00247 |       0.18940\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.4         |\n",
      "| EpRewMean       | -88.4        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 1820         |\n",
      "| TimeElapsed     | 718          |\n",
      "| TimestepsSoFar  | 208896       |\n",
      "| ev_tdlam_before | 0.817        |\n",
      "| loss_ent        | 0.1894       |\n",
      "| loss_kl         | 0.0024728621 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001905326 |\n",
      "| loss_vf_loss    | 42.818584    |\n",
      "----------------------------------\n",
      "********** Iteration 51 ************\n",
      "Eval num_timesteps=208896, episode_reward=-85.10 +/- 6.91\n",
      "Episode length: 86.10 +/- 6.91\n",
      "Eval num_timesteps=208896, episode_reward=-81.40 +/- 9.08\n",
      "Episode length: 82.40 +/- 9.08\n",
      "Eval num_timesteps=208896, episode_reward=-77.90 +/- 5.54\n",
      "Episode length: 78.90 +/- 5.54\n",
      "Eval num_timesteps=208896, episode_reward=-79.30 +/- 8.74\n",
      "Episode length: 80.30 +/- 8.74\n",
      "Eval num_timesteps=208896, episode_reward=-88.90 +/- 25.63\n",
      "Episode length: 89.90 +/- 25.63\n",
      "Eval num_timesteps=208896, episode_reward=-78.10 +/- 4.09\n",
      "Episode length: 79.10 +/- 4.09\n",
      "Eval num_timesteps=208896, episode_reward=-88.70 +/- 31.98\n",
      "Episode length: 89.70 +/- 31.98\n",
      "Eval num_timesteps=208896, episode_reward=-83.20 +/- 9.12\n",
      "Episode length: 84.20 +/- 9.12\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00160 |       0.00000 |      53.22729 |       0.00122 |       0.18823\n",
      "      0.00026 |       0.00000 |      52.47631 |       0.00141 |       0.18656\n",
      "     -0.00023 |       0.00000 |      52.11075 |       0.00130 |       0.18529\n",
      "      0.00045 |       0.00000 |      51.90776 |       0.00144 |       0.18617\n",
      "      0.00042 |       0.00000 |      51.30360 |       0.00148 |       0.18584\n",
      "      0.00015 |       0.00000 |      51.29501 |       0.00158 |       0.18479\n",
      "    -6.43e-05 |       0.00000 |      51.35600 |       0.00151 |       0.18460\n",
      "      0.00016 |       0.00000 |      51.17054 |       0.00163 |       0.18472\n",
      "     -0.00112 |       0.00000 |      51.00142 |       0.00176 |       0.18665\n",
      "      0.00079 |       0.00000 |      51.11538 |       0.00186 |       0.18427\n",
      "Evaluating losses...\n",
      "     -0.00123 |       0.00000 |      50.82336 |       0.00178 |       0.18366\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.7          |\n",
      "| EpRewMean       | -92.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 1862          |\n",
      "| TimeElapsed     | 728           |\n",
      "| TimestepsSoFar  | 212992        |\n",
      "| ev_tdlam_before | 0.767         |\n",
      "| loss_ent        | 0.18366392    |\n",
      "| loss_kl         | 0.001784559   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012333556 |\n",
      "| loss_vf_loss    | 50.823357     |\n",
      "-----------------------------------\n",
      "********** Iteration 52 ************\n",
      "Eval num_timesteps=212992, episode_reward=-79.90 +/- 11.41\n",
      "Episode length: 80.90 +/- 11.41\n",
      "Eval num_timesteps=212992, episode_reward=-87.30 +/- 15.15\n",
      "Episode length: 88.30 +/- 15.15\n",
      "Eval num_timesteps=212992, episode_reward=-83.00 +/- 11.92\n",
      "Episode length: 84.00 +/- 11.92\n",
      "Eval num_timesteps=212992, episode_reward=-85.90 +/- 9.51\n",
      "Episode length: 86.90 +/- 9.51\n",
      "Eval num_timesteps=212992, episode_reward=-92.40 +/- 17.89\n",
      "Episode length: 93.40 +/- 17.89\n",
      "Eval num_timesteps=212992, episode_reward=-87.20 +/- 9.44\n",
      "Episode length: 88.20 +/- 9.44\n",
      "Eval num_timesteps=212992, episode_reward=-87.60 +/- 28.05\n",
      "Episode length: 88.60 +/- 28.05\n",
      "Eval num_timesteps=212992, episode_reward=-79.30 +/- 4.58\n",
      "Episode length: 80.30 +/- 4.58\n",
      "Eval num_timesteps=212992, episode_reward=-78.90 +/- 5.13\n",
      "Episode length: 79.90 +/- 5.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00091 |       0.00000 |      59.70091 |       0.00105 |       0.17784\n",
      "      0.00090 |       0.00000 |      58.12645 |       0.00126 |       0.17540\n",
      "     -0.00050 |       0.00000 |      57.27864 |       0.00131 |       0.17536\n",
      "      0.00020 |       0.00000 |      56.78684 |       0.00149 |       0.17303\n",
      "     -0.00093 |       0.00000 |      56.30759 |       0.00125 |       0.17458\n",
      "     -0.00101 |       0.00000 |      56.34381 |       0.00128 |       0.17647\n",
      "     -0.00031 |       0.00000 |      56.02226 |       0.00129 |       0.17819\n",
      "     -0.00067 |       0.00000 |      55.91186 |       0.00119 |       0.17931\n",
      "      0.00016 |       0.00000 |      55.83138 |       0.00148 |       0.18110\n",
      "     -0.00053 |       0.00000 |      55.79929 |       0.00146 |       0.18062\n",
      "Evaluating losses...\n",
      "     -0.00025 |       0.00000 |      55.45074 |       0.00141 |       0.18186\n",
      "------------------------------------\n",
      "| EpLenMean       | 100            |\n",
      "| EpRewMean       | -99            |\n",
      "| EpThisIter      | 39             |\n",
      "| EpisodesSoFar   | 1901           |\n",
      "| TimeElapsed     | 740            |\n",
      "| TimestepsSoFar  | 217088         |\n",
      "| ev_tdlam_before | 0.732          |\n",
      "| loss_ent        | 0.18186302     |\n",
      "| loss_kl         | 0.0014062277   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00025353348 |\n",
      "| loss_vf_loss    | 55.45074       |\n",
      "------------------------------------\n",
      "********** Iteration 53 ************\n",
      "Eval num_timesteps=217088, episode_reward=-78.10 +/- 6.25\n",
      "Episode length: 79.10 +/- 6.25\n",
      "Eval num_timesteps=217088, episode_reward=-77.40 +/- 7.23\n",
      "Episode length: 78.40 +/- 7.23\n",
      "Eval num_timesteps=217088, episode_reward=-75.70 +/- 5.69\n",
      "Episode length: 76.70 +/- 5.69\n",
      "Eval num_timesteps=217088, episode_reward=-79.70 +/- 2.72\n",
      "Episode length: 80.70 +/- 2.72\n",
      "Eval num_timesteps=217088, episode_reward=-83.40 +/- 12.25\n",
      "Episode length: 84.40 +/- 12.25\n",
      "Eval num_timesteps=217088, episode_reward=-82.30 +/- 10.15\n",
      "Episode length: 83.30 +/- 10.15\n",
      "Eval num_timesteps=217088, episode_reward=-81.10 +/- 4.81\n",
      "Episode length: 82.10 +/- 4.81\n",
      "Eval num_timesteps=217088, episode_reward=-86.10 +/- 7.33\n",
      "Episode length: 87.10 +/- 7.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00036 |       0.00000 |      48.60775 |       0.00105 |       0.17833\n",
      "      0.00077 |       0.00000 |      47.69130 |       0.00118 |       0.17529\n",
      "      0.00028 |       0.00000 |      47.23162 |       0.00131 |       0.17506\n",
      "     7.98e-05 |       0.00000 |      46.64324 |       0.00124 |       0.17561\n",
      "     -0.00010 |       0.00000 |      46.30806 |       0.00135 |       0.17307\n",
      "      0.00070 |       0.00000 |      45.94500 |       0.00133 |       0.17510\n",
      "      0.00086 |       0.00000 |      46.07396 |       0.00156 |       0.17486\n",
      "      0.00103 |       0.00000 |      45.73065 |       0.00162 |       0.17735\n",
      "    -7.80e-05 |       0.00000 |      45.79651 |       0.00167 |       0.17499\n",
      "     -0.00044 |       0.00000 |      45.39124 |       0.00159 |       0.17543\n",
      "Evaluating losses...\n",
      "     -0.00076 |       0.00000 |      45.44951 |       0.00182 |       0.17542\n",
      "-----------------------------------\n",
      "| EpLenMean       | 99.7          |\n",
      "| EpRewMean       | -98.7         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 1944          |\n",
      "| TimeElapsed     | 750           |\n",
      "| TimestepsSoFar  | 221184        |\n",
      "| ev_tdlam_before | 0.79          |\n",
      "| loss_ent        | 0.17541568    |\n",
      "| loss_kl         | 0.0018190545  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0007578373 |\n",
      "| loss_vf_loss    | 45.44951      |\n",
      "-----------------------------------\n",
      "********** Iteration 54 ************\n",
      "Eval num_timesteps=221184, episode_reward=-78.00 +/- 4.69\n",
      "Episode length: 79.00 +/- 4.69\n",
      "Eval num_timesteps=221184, episode_reward=-76.50 +/- 6.84\n",
      "Episode length: 77.50 +/- 6.84\n",
      "Eval num_timesteps=221184, episode_reward=-85.00 +/- 16.02\n",
      "Episode length: 86.00 +/- 16.02\n",
      "Eval num_timesteps=221184, episode_reward=-81.50 +/- 7.57\n",
      "Episode length: 82.50 +/- 7.57\n",
      "Eval num_timesteps=221184, episode_reward=-77.10 +/- 6.91\n",
      "Episode length: 78.10 +/- 6.91\n",
      "Eval num_timesteps=221184, episode_reward=-85.80 +/- 11.94\n",
      "Episode length: 86.80 +/- 11.94\n",
      "Eval num_timesteps=221184, episode_reward=-89.60 +/- 23.21\n",
      "Episode length: 90.60 +/- 23.21\n",
      "Eval num_timesteps=221184, episode_reward=-77.70 +/- 3.35\n",
      "Episode length: 78.70 +/- 3.35\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00169 |       0.00000 |      47.99210 |       0.00120 |       0.18021\n",
      "    -9.82e-05 |       0.00000 |      47.19133 |       0.00127 |       0.18051\n",
      "      0.00078 |       0.00000 |      46.70253 |       0.00147 |       0.18055\n",
      "      0.00023 |       0.00000 |      46.76466 |       0.00144 |       0.18155\n",
      "     -0.00056 |       0.00000 |      46.36153 |       0.00146 |       0.17993\n",
      "      0.00049 |       0.00000 |      46.16127 |       0.00162 |       0.17990\n",
      "      0.00045 |       0.00000 |      46.09293 |       0.00172 |       0.18047\n",
      "     -0.00049 |       0.00000 |      45.97106 |       0.00181 |       0.18132\n",
      "     -0.00048 |       0.00000 |      46.05254 |       0.00167 |       0.18091\n",
      "     -0.00081 |       0.00000 |      45.64479 |       0.00178 |       0.18140\n",
      "Evaluating losses...\n",
      "    -6.61e-05 |       0.00000 |      45.49091 |       0.00168 |       0.18040\n",
      "----------------------------------\n",
      "| EpLenMean       | 96           |\n",
      "| EpRewMean       | -95          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 1988         |\n",
      "| TimeElapsed     | 760          |\n",
      "| TimestepsSoFar  | 225280       |\n",
      "| ev_tdlam_before | 0.806        |\n",
      "| loss_ent        | 0.18040179   |\n",
      "| loss_kl         | 0.0016772252 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -6.61033e-05 |\n",
      "| loss_vf_loss    | 45.490913    |\n",
      "----------------------------------\n",
      "********** Iteration 55 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=225280, episode_reward=-80.70 +/- 9.22\n",
      "Episode length: 81.70 +/- 9.22\n",
      "Eval num_timesteps=225280, episode_reward=-83.70 +/- 6.59\n",
      "Episode length: 84.70 +/- 6.59\n",
      "Eval num_timesteps=225280, episode_reward=-84.90 +/- 17.18\n",
      "Episode length: 85.90 +/- 17.18\n",
      "Eval num_timesteps=225280, episode_reward=-83.20 +/- 15.69\n",
      "Episode length: 84.20 +/- 15.69\n",
      "Eval num_timesteps=225280, episode_reward=-89.10 +/- 14.09\n",
      "Episode length: 90.10 +/- 14.09\n",
      "Eval num_timesteps=225280, episode_reward=-78.90 +/- 8.23\n",
      "Episode length: 79.90 +/- 8.23\n",
      "Eval num_timesteps=225280, episode_reward=-86.50 +/- 8.85\n",
      "Episode length: 87.50 +/- 8.85\n",
      "Eval num_timesteps=225280, episode_reward=-81.20 +/- 9.45\n",
      "Episode length: 82.20 +/- 9.45\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00047 |       0.00000 |      37.23400 |       0.00126 |       0.18284\n",
      "     -0.00113 |       0.00000 |      36.63463 |       0.00122 |       0.18510\n",
      "      0.00094 |       0.00000 |      36.07994 |       0.00140 |       0.18674\n",
      "      0.00029 |       0.00000 |      36.04700 |       0.00138 |       0.18756\n",
      "      0.00070 |       0.00000 |      35.76991 |       0.00162 |       0.18759\n",
      "      0.00157 |       0.00000 |      35.75188 |       0.00147 |       0.18737\n",
      "     2.28e-05 |       0.00000 |      35.59390 |       0.00163 |       0.18922\n",
      "      0.00010 |       0.00000 |      35.43420 |       0.00166 |       0.18975\n",
      "      0.00073 |       0.00000 |      35.34893 |       0.00179 |       0.19054\n",
      "      0.00094 |       0.00000 |      35.34878 |       0.00225 |       0.19232\n",
      "Evaluating losses...\n",
      "     -0.00011 |       0.00000 |      35.23103 |       0.00201 |       0.19127\n",
      "-------------------------------------\n",
      "| EpLenMean       | 92              |\n",
      "| EpRewMean       | -91             |\n",
      "| EpThisIter      | 47              |\n",
      "| EpisodesSoFar   | 2035            |\n",
      "| TimeElapsed     | 771             |\n",
      "| TimestepsSoFar  | 229376          |\n",
      "| ev_tdlam_before | 0.856           |\n",
      "| loss_ent        | 0.19127108      |\n",
      "| loss_kl         | 0.0020144873    |\n",
      "| loss_pol_entpen | 0.0             |\n",
      "| loss_pol_surr   | -0.000112497306 |\n",
      "| loss_vf_loss    | 35.23103        |\n",
      "-------------------------------------\n",
      "********** Iteration 56 ************\n",
      "Eval num_timesteps=229376, episode_reward=-83.10 +/- 7.52\n",
      "Episode length: 84.10 +/- 7.52\n",
      "Eval num_timesteps=229376, episode_reward=-79.30 +/- 10.63\n",
      "Episode length: 80.30 +/- 10.63\n",
      "Eval num_timesteps=229376, episode_reward=-78.90 +/- 5.68\n",
      "Episode length: 79.90 +/- 5.68\n",
      "Eval num_timesteps=229376, episode_reward=-84.20 +/- 5.98\n",
      "Episode length: 85.20 +/- 5.98\n",
      "Eval num_timesteps=229376, episode_reward=-78.50 +/- 10.44\n",
      "Episode length: 79.50 +/- 10.44\n",
      "Eval num_timesteps=229376, episode_reward=-88.60 +/- 32.44\n",
      "Episode length: 89.60 +/- 32.44\n",
      "Eval num_timesteps=229376, episode_reward=-77.70 +/- 11.89\n",
      "Episode length: 78.70 +/- 11.89\n",
      "Eval num_timesteps=229376, episode_reward=-73.70 +/- 8.38\n",
      "Episode length: 74.70 +/- 8.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00116 |       0.00000 |      48.43842 |       0.00125 |       0.19308\n",
      "     -0.00014 |       0.00000 |      47.82028 |       0.00126 |       0.19470\n",
      "      0.00146 |       0.00000 |      47.79478 |       0.00135 |       0.19353\n",
      "     6.53e-05 |       0.00000 |      47.41071 |       0.00142 |       0.19279\n",
      "     -0.00123 |       0.00000 |      47.19709 |       0.00120 |       0.19345\n",
      "      0.00048 |       0.00000 |      47.07545 |       0.00135 |       0.19179\n",
      "     2.55e-05 |       0.00000 |      47.17160 |       0.00162 |       0.18978\n",
      "     -0.00100 |       0.00000 |      46.73792 |       0.00156 |       0.19325\n",
      "     -0.00087 |       0.00000 |      46.74231 |       0.00159 |       0.19583\n",
      "     -0.00114 |       0.00000 |      46.61188 |       0.00154 |       0.19739\n",
      "Evaluating losses...\n",
      "     -0.00118 |       0.00000 |      46.38247 |       0.00152 |       0.19647\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.4          |\n",
      "| EpRewMean       | -89.4         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 2080          |\n",
      "| TimeElapsed     | 782           |\n",
      "| TimestepsSoFar  | 233472        |\n",
      "| ev_tdlam_before | 0.803         |\n",
      "| loss_ent        | 0.19647443    |\n",
      "| loss_kl         | 0.0015213417  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0011795931 |\n",
      "| loss_vf_loss    | 46.38247      |\n",
      "-----------------------------------\n",
      "********** Iteration 57 ************\n",
      "Eval num_timesteps=233472, episode_reward=-80.50 +/- 8.16\n",
      "Episode length: 81.50 +/- 8.16\n",
      "Eval num_timesteps=233472, episode_reward=-80.00 +/- 6.32\n",
      "Episode length: 81.00 +/- 6.32\n",
      "Eval num_timesteps=233472, episode_reward=-81.10 +/- 12.71\n",
      "Episode length: 82.10 +/- 12.71\n",
      "Eval num_timesteps=233472, episode_reward=-80.60 +/- 8.97\n",
      "Episode length: 81.60 +/- 8.97\n",
      "Eval num_timesteps=233472, episode_reward=-79.20 +/- 3.94\n",
      "Episode length: 80.20 +/- 3.94\n",
      "Eval num_timesteps=233472, episode_reward=-80.40 +/- 4.59\n",
      "Episode length: 81.40 +/- 4.59\n",
      "Eval num_timesteps=233472, episode_reward=-82.70 +/- 5.60\n",
      "Episode length: 83.70 +/- 5.60\n",
      "Eval num_timesteps=233472, episode_reward=-79.60 +/- 7.30\n",
      "Episode length: 80.60 +/- 7.30\n",
      "Eval num_timesteps=233472, episode_reward=-80.30 +/- 4.78\n",
      "Episode length: 81.30 +/- 4.78\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00098 |       0.00000 |      46.93870 |       0.00130 |       0.19640\n",
      "    -8.88e-05 |       0.00000 |      46.38805 |       0.00158 |       0.19112\n",
      "     -0.00142 |       0.00000 |      46.17382 |       0.00172 |       0.18864\n",
      "     -0.00074 |       0.00000 |      46.09996 |       0.00217 |       0.18474\n",
      "     -0.00142 |       0.00000 |      45.74395 |       0.00218 |       0.18510\n",
      "     -0.00082 |       0.00000 |      45.86145 |       0.00228 |       0.18553\n",
      "     -0.00075 |       0.00000 |      45.57974 |       0.00241 |       0.18460\n",
      "     -0.00225 |       0.00000 |      45.36118 |       0.00227 |       0.18501\n",
      "     -0.00173 |       0.00000 |      45.50502 |       0.00228 |       0.18406\n",
      "     -0.00117 |       0.00000 |      45.31405 |       0.00236 |       0.18449\n",
      "Evaluating losses...\n",
      "     -0.00159 |       0.00000 |      45.47870 |       0.00228 |       0.18452\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.1          |\n",
      "| EpRewMean       | -92.1         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 2122          |\n",
      "| TimeElapsed     | 792           |\n",
      "| TimestepsSoFar  | 237568        |\n",
      "| ev_tdlam_before | 0.804         |\n",
      "| loss_ent        | 0.18452045    |\n",
      "| loss_kl         | 0.002275561   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015917785 |\n",
      "| loss_vf_loss    | 45.478703     |\n",
      "-----------------------------------\n",
      "********** Iteration 58 ************\n",
      "Eval num_timesteps=237568, episode_reward=-75.30 +/- 7.43\n",
      "Episode length: 76.30 +/- 7.43\n",
      "Eval num_timesteps=237568, episode_reward=-80.80 +/- 5.00\n",
      "Episode length: 81.80 +/- 5.00\n",
      "Eval num_timesteps=237568, episode_reward=-83.20 +/- 9.15\n",
      "Episode length: 84.20 +/- 9.15\n",
      "Eval num_timesteps=237568, episode_reward=-90.90 +/- 32.35\n",
      "Episode length: 91.90 +/- 32.35\n",
      "Eval num_timesteps=237568, episode_reward=-77.90 +/- 6.79\n",
      "Episode length: 78.90 +/- 6.79\n",
      "Eval num_timesteps=237568, episode_reward=-87.40 +/- 12.69\n",
      "Episode length: 88.40 +/- 12.69\n",
      "Eval num_timesteps=237568, episode_reward=-81.00 +/- 4.73\n",
      "Episode length: 82.00 +/- 4.73\n",
      "Eval num_timesteps=237568, episode_reward=-89.50 +/- 13.35\n",
      "Episode length: 90.50 +/- 13.35\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00013 |       0.00000 |      43.71463 |       0.00130 |       0.18891\n",
      "     -0.00065 |       0.00000 |      43.24932 |       0.00140 |       0.19035\n",
      "     -0.00024 |       0.00000 |      42.73232 |       0.00173 |       0.19049\n",
      "     -0.00021 |       0.00000 |      42.61951 |       0.00196 |       0.19215\n",
      "     -0.00143 |       0.00000 |      42.48841 |       0.00222 |       0.19283\n",
      "     -0.00129 |       0.00000 |      42.42974 |       0.00269 |       0.19165\n",
      "     -0.00078 |       0.00000 |      42.34338 |       0.00272 |       0.19360\n",
      "     -0.00152 |       0.00000 |      42.20908 |       0.00266 |       0.19183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00131 |       0.00000 |      42.03891 |       0.00284 |       0.19194\n",
      "     -0.00185 |       0.00000 |      42.06626 |       0.00282 |       0.19233\n",
      "Evaluating losses...\n",
      "     -0.00089 |       0.00000 |      42.00501 |       0.00274 |       0.19316\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.2          |\n",
      "| EpRewMean       | -93.2         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 2167          |\n",
      "| TimeElapsed     | 803           |\n",
      "| TimestepsSoFar  | 241664        |\n",
      "| ev_tdlam_before | 0.825         |\n",
      "| loss_ent        | 0.1931626     |\n",
      "| loss_kl         | 0.0027415736  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0008918565 |\n",
      "| loss_vf_loss    | 42.00501      |\n",
      "-----------------------------------\n",
      "********** Iteration 59 ************\n",
      "Eval num_timesteps=241664, episode_reward=-81.70 +/- 7.23\n",
      "Episode length: 82.70 +/- 7.23\n",
      "Eval num_timesteps=241664, episode_reward=-83.20 +/- 11.39\n",
      "Episode length: 84.20 +/- 11.39\n",
      "Eval num_timesteps=241664, episode_reward=-80.00 +/- 4.43\n",
      "Episode length: 81.00 +/- 4.43\n",
      "Eval num_timesteps=241664, episode_reward=-77.70 +/- 6.00\n",
      "Episode length: 78.70 +/- 6.00\n",
      "Eval num_timesteps=241664, episode_reward=-85.70 +/- 16.08\n",
      "Episode length: 86.70 +/- 16.08\n",
      "Eval num_timesteps=241664, episode_reward=-78.90 +/- 8.42\n",
      "Episode length: 79.90 +/- 8.42\n",
      "Eval num_timesteps=241664, episode_reward=-78.90 +/- 5.49\n",
      "Episode length: 79.90 +/- 5.49\n",
      "Eval num_timesteps=241664, episode_reward=-77.40 +/- 4.61\n",
      "Episode length: 78.40 +/- 4.61\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00129 |       0.00000 |      49.65776 |       0.00108 |       0.18812\n",
      "      0.00040 |       0.00000 |      48.87278 |       0.00128 |       0.18813\n",
      "      0.00038 |       0.00000 |      48.27266 |       0.00126 |       0.18682\n",
      "     -0.00087 |       0.00000 |      47.82305 |       0.00138 |       0.18699\n",
      "      0.00024 |       0.00000 |      47.49469 |       0.00140 |       0.18718\n",
      "     -0.00060 |       0.00000 |      47.09160 |       0.00154 |       0.18649\n",
      "     -0.00037 |       0.00000 |      47.06308 |       0.00169 |       0.18675\n",
      "     4.22e-05 |       0.00000 |      46.81097 |       0.00161 |       0.18765\n",
      "    -7.86e-05 |       0.00000 |      46.79659 |       0.00173 |       0.18712\n",
      "     -0.00041 |       0.00000 |      46.79462 |       0.00190 |       0.18573\n",
      "Evaluating losses...\n",
      "     -0.00125 |       0.00000 |      46.51878 |       0.00164 |       0.18602\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.7          |\n",
      "| EpRewMean       | -91.7         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 2211          |\n",
      "| TimeElapsed     | 813           |\n",
      "| TimestepsSoFar  | 245760        |\n",
      "| ev_tdlam_before | 0.793         |\n",
      "| loss_ent        | 0.18601714    |\n",
      "| loss_kl         | 0.0016394806  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012532637 |\n",
      "| loss_vf_loss    | 46.518784     |\n",
      "-----------------------------------\n",
      "********** Iteration 60 ************\n",
      "Eval num_timesteps=245760, episode_reward=-79.70 +/- 9.65\n",
      "Episode length: 80.70 +/- 9.65\n",
      "Eval num_timesteps=245760, episode_reward=-78.20 +/- 8.94\n",
      "Episode length: 79.20 +/- 8.94\n",
      "Eval num_timesteps=245760, episode_reward=-75.30 +/- 8.80\n",
      "Episode length: 76.30 +/- 8.80\n",
      "Eval num_timesteps=245760, episode_reward=-79.80 +/- 3.97\n",
      "Episode length: 80.80 +/- 3.97\n",
      "Eval num_timesteps=245760, episode_reward=-73.80 +/- 5.84\n",
      "Episode length: 74.80 +/- 5.84\n",
      "Eval num_timesteps=245760, episode_reward=-77.30 +/- 5.64\n",
      "Episode length: 78.30 +/- 5.64\n",
      "Eval num_timesteps=245760, episode_reward=-85.00 +/- 14.41\n",
      "Episode length: 86.00 +/- 14.41\n",
      "Eval num_timesteps=245760, episode_reward=-84.20 +/- 10.37\n",
      "Episode length: 85.20 +/- 10.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00208 |       0.00000 |      47.23801 |       0.00140 |       0.18587\n",
      "      0.00017 |       0.00000 |      47.06057 |       0.00148 |       0.18268\n",
      "      0.00093 |       0.00000 |      46.80190 |       0.00150 |       0.18136\n",
      "      0.00079 |       0.00000 |      46.46653 |       0.00162 |       0.18102\n",
      "      0.00021 |       0.00000 |      46.48094 |       0.00160 |       0.17971\n",
      "    -9.36e-05 |       0.00000 |      46.26960 |       0.00162 |       0.18021\n",
      "     -0.00109 |       0.00000 |      46.07605 |       0.00177 |       0.18138\n",
      "      0.00087 |       0.00000 |      46.13744 |       0.00198 |       0.18300\n",
      "     3.43e-05 |       0.00000 |      45.98940 |       0.00196 |       0.18204\n",
      "     2.73e-05 |       0.00000 |      45.87761 |       0.00193 |       0.18214\n",
      "Evaluating losses...\n",
      "     -0.00083 |       0.00000 |      46.02528 |       0.00188 |       0.18281\n",
      "------------------------------------\n",
      "| EpLenMean       | 93.4           |\n",
      "| EpRewMean       | -92.4          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 2254           |\n",
      "| TimeElapsed     | 823            |\n",
      "| TimestepsSoFar  | 249856         |\n",
      "| ev_tdlam_before | 0.801          |\n",
      "| loss_ent        | 0.18280703     |\n",
      "| loss_kl         | 0.0018811424   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00082724076 |\n",
      "| loss_vf_loss    | 46.02528       |\n",
      "------------------------------------\n",
      "********** Iteration 61 ************\n",
      "Eval num_timesteps=249856, episode_reward=-89.40 +/- 30.41\n",
      "Episode length: 90.40 +/- 30.41\n",
      "Eval num_timesteps=249856, episode_reward=-82.40 +/- 7.85\n",
      "Episode length: 83.40 +/- 7.85\n",
      "Eval num_timesteps=249856, episode_reward=-87.30 +/- 11.84\n",
      "Episode length: 88.30 +/- 11.84\n",
      "Eval num_timesteps=249856, episode_reward=-81.10 +/- 11.49\n",
      "Episode length: 82.10 +/- 11.49\n",
      "Eval num_timesteps=249856, episode_reward=-76.10 +/- 8.34\n",
      "Episode length: 77.10 +/- 8.34\n",
      "Eval num_timesteps=249856, episode_reward=-88.70 +/- 23.50\n",
      "Episode length: 89.70 +/- 23.50\n",
      "Eval num_timesteps=249856, episode_reward=-83.10 +/- 19.37\n",
      "Episode length: 84.10 +/- 19.37\n",
      "Eval num_timesteps=249856, episode_reward=-80.90 +/- 4.23\n",
      "Episode length: 81.90 +/- 4.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00125 |       0.00000 |      60.69452 |       0.00120 |       0.16407\n",
      "      0.00073 |       0.00000 |      59.02483 |       0.00116 |       0.16308\n",
      "      0.00087 |       0.00000 |      58.20016 |       0.00138 |       0.16130\n",
      "      0.00085 |       0.00000 |      57.83583 |       0.00141 |       0.16138\n",
      "      0.00102 |       0.00000 |      57.29870 |       0.00129 |       0.16090\n",
      "     -0.00013 |       0.00000 |      57.04090 |       0.00141 |       0.16028\n",
      "     -0.00013 |       0.00000 |      56.71583 |       0.00143 |       0.16077\n",
      "    -4.02e-05 |       0.00000 |      56.21415 |       0.00140 |       0.16087\n",
      "    -2.89e-05 |       0.00000 |      55.97373 |       0.00126 |       0.16163\n",
      "      0.00022 |       0.00000 |      55.42208 |       0.00137 |       0.16245\n",
      "Evaluating losses...\n",
      "      0.00027 |       0.00000 |      55.59364 |       0.00127 |       0.16179\n",
      "-----------------------------------\n",
      "| EpLenMean       | 97.6          |\n",
      "| EpRewMean       | -96.6         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 2296          |\n",
      "| TimeElapsed     | 833           |\n",
      "| TimestepsSoFar  | 253952        |\n",
      "| ev_tdlam_before | 0.727         |\n",
      "| loss_ent        | 0.16179366    |\n",
      "| loss_kl         | 0.0012664092  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00026968925 |\n",
      "| loss_vf_loss    | 55.593636     |\n",
      "-----------------------------------\n",
      "********** Iteration 62 ************\n",
      "Eval num_timesteps=253952, episode_reward=-80.10 +/- 10.27\n",
      "Episode length: 81.10 +/- 10.27\n",
      "Eval num_timesteps=253952, episode_reward=-82.30 +/- 5.44\n",
      "Episode length: 83.30 +/- 5.44\n",
      "Eval num_timesteps=253952, episode_reward=-85.50 +/- 30.19\n",
      "Episode length: 86.50 +/- 30.19\n",
      "Eval num_timesteps=253952, episode_reward=-88.70 +/- 24.56\n",
      "Episode length: 89.70 +/- 24.56\n",
      "Eval num_timesteps=253952, episode_reward=-89.40 +/- 24.88\n",
      "Episode length: 90.40 +/- 24.88\n",
      "Eval num_timesteps=253952, episode_reward=-87.60 +/- 27.60\n",
      "Episode length: 88.60 +/- 27.60\n",
      "Eval num_timesteps=253952, episode_reward=-82.00 +/- 11.77\n",
      "Episode length: 83.00 +/- 11.77\n",
      "Eval num_timesteps=253952, episode_reward=-82.30 +/- 8.61\n",
      "Episode length: 83.30 +/- 8.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=253952, episode_reward=-83.40 +/- 16.63\n",
      "Episode length: 84.40 +/- 16.63\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -4.84e-05 |       0.00000 |      51.17569 |       0.00109 |       0.17181\n",
      "      0.00122 |       0.00000 |      50.77089 |       0.00127 |       0.16997\n",
      "     3.21e-05 |       0.00000 |      50.31416 |       0.00122 |       0.16785\n",
      "     -0.00105 |       0.00000 |      50.38898 |       0.00136 |       0.16656\n",
      "     -0.00116 |       0.00000 |      49.97726 |       0.00139 |       0.16768\n",
      "      0.00060 |       0.00000 |      49.87004 |       0.00147 |       0.16724\n",
      "     -0.00023 |       0.00000 |      49.74947 |       0.00160 |       0.16602\n",
      "    -1.81e-05 |       0.00000 |      49.73763 |       0.00154 |       0.16455\n",
      "      0.00045 |       0.00000 |      49.65398 |       0.00166 |       0.16439\n",
      "     -0.00036 |       0.00000 |      49.49890 |       0.00156 |       0.16608\n",
      "Evaluating losses...\n",
      "     -0.00076 |       0.00000 |      49.14147 |       0.00157 |       0.16649\n",
      "------------------------------------\n",
      "| EpLenMean       | 95.8           |\n",
      "| EpRewMean       | -94.8          |\n",
      "| EpThisIter      | 42             |\n",
      "| EpisodesSoFar   | 2338           |\n",
      "| TimeElapsed     | 844            |\n",
      "| TimestepsSoFar  | 258048         |\n",
      "| ev_tdlam_before | 0.783          |\n",
      "| loss_ent        | 0.16648722     |\n",
      "| loss_kl         | 0.0015695979   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00075656734 |\n",
      "| loss_vf_loss    | 49.14147       |\n",
      "------------------------------------\n",
      "********** Iteration 63 ************\n",
      "Eval num_timesteps=258048, episode_reward=-87.10 +/- 15.20\n",
      "Episode length: 88.10 +/- 15.20\n",
      "Eval num_timesteps=258048, episode_reward=-78.80 +/- 5.15\n",
      "Episode length: 79.80 +/- 5.15\n",
      "Eval num_timesteps=258048, episode_reward=-82.70 +/- 5.80\n",
      "Episode length: 83.70 +/- 5.80\n",
      "Eval num_timesteps=258048, episode_reward=-81.80 +/- 4.14\n",
      "Episode length: 82.80 +/- 4.14\n",
      "Eval num_timesteps=258048, episode_reward=-77.80 +/- 9.52\n",
      "Episode length: 78.80 +/- 9.52\n",
      "Eval num_timesteps=258048, episode_reward=-84.00 +/- 6.21\n",
      "Episode length: 85.00 +/- 6.21\n",
      "Eval num_timesteps=258048, episode_reward=-85.10 +/- 14.70\n",
      "Episode length: 86.10 +/- 14.70\n",
      "Eval num_timesteps=258048, episode_reward=-86.60 +/- 22.04\n",
      "Episode length: 87.60 +/- 22.04\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00105 |       0.00000 |      54.09462 |       0.00105 |       0.16761\n",
      "      0.00022 |       0.00000 |      53.73104 |       0.00147 |       0.16424\n",
      "      0.00119 |       0.00000 |      53.19714 |       0.00164 |       0.16162\n",
      "      0.00040 |       0.00000 |      53.08986 |       0.00195 |       0.16285\n",
      "     -0.00032 |       0.00000 |      52.91764 |       0.00190 |       0.16130\n",
      "     -0.00070 |       0.00000 |      52.79134 |       0.00186 |       0.16263\n",
      "      0.00031 |       0.00000 |      52.91057 |       0.00192 |       0.16384\n",
      "     -0.00092 |       0.00000 |      52.50900 |       0.00196 |       0.16353\n",
      "     -0.00082 |       0.00000 |      52.52367 |       0.00194 |       0.16385\n",
      "     -0.00115 |       0.00000 |      52.47285 |       0.00201 |       0.16446\n",
      "Evaluating losses...\n",
      "     1.83e-05 |       0.00000 |      52.65780 |       0.00186 |       0.16507\n",
      "----------------------------------\n",
      "| EpLenMean       | 97.6         |\n",
      "| EpRewMean       | -96.6        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 2381         |\n",
      "| TimeElapsed     | 854          |\n",
      "| TimestepsSoFar  | 262144       |\n",
      "| ev_tdlam_before | 0.774        |\n",
      "| loss_ent        | 0.16507086   |\n",
      "| loss_kl         | 0.0018552674 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 1.829525e-05 |\n",
      "| loss_vf_loss    | 52.657803    |\n",
      "----------------------------------\n",
      "********** Iteration 64 ************\n",
      "Eval num_timesteps=262144, episode_reward=-75.40 +/- 5.70\n",
      "Episode length: 76.40 +/- 5.70\n",
      "Eval num_timesteps=262144, episode_reward=-75.60 +/- 5.39\n",
      "Episode length: 76.60 +/- 5.39\n",
      "Eval num_timesteps=262144, episode_reward=-102.90 +/- 37.10\n",
      "Episode length: 103.90 +/- 37.10\n",
      "Eval num_timesteps=262144, episode_reward=-80.40 +/- 6.77\n",
      "Episode length: 81.40 +/- 6.77\n",
      "Eval num_timesteps=262144, episode_reward=-76.20 +/- 11.86\n",
      "Episode length: 77.20 +/- 11.86\n",
      "Eval num_timesteps=262144, episode_reward=-88.00 +/- 27.23\n",
      "Episode length: 89.00 +/- 27.23\n",
      "Eval num_timesteps=262144, episode_reward=-78.20 +/- 6.51\n",
      "Episode length: 79.20 +/- 6.51\n",
      "Eval num_timesteps=262144, episode_reward=-80.80 +/- 7.85\n",
      "Episode length: 81.80 +/- 7.85\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00204 |       0.00000 |      52.21832 |       0.00113 |       0.17154\n",
      "      0.00023 |       0.00000 |      51.15509 |       0.00113 |       0.17043\n",
      "      0.00017 |       0.00000 |      50.95306 |       0.00109 |       0.17045\n",
      "      0.00022 |       0.00000 |      50.76810 |       0.00127 |       0.16915\n",
      "     -0.00092 |       0.00000 |      50.24296 |       0.00134 |       0.16756\n",
      "     -0.00122 |       0.00000 |      50.08543 |       0.00136 |       0.16943\n",
      "     -0.00085 |       0.00000 |      50.25717 |       0.00129 |       0.17010\n",
      "      0.00016 |       0.00000 |      49.94925 |       0.00142 |       0.16996\n",
      "     -0.00055 |       0.00000 |      49.78579 |       0.00130 |       0.17117\n",
      "      0.00037 |       0.00000 |      49.62545 |       0.00141 |       0.17093\n",
      "Evaluating losses...\n",
      "     -0.00085 |       0.00000 |      49.62653 |       0.00144 |       0.17216\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.4          |\n",
      "| EpRewMean       | -91.4         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2427          |\n",
      "| TimeElapsed     | 865           |\n",
      "| TimestepsSoFar  | 266240        |\n",
      "| ev_tdlam_before | 0.793         |\n",
      "| loss_ent        | 0.1721644     |\n",
      "| loss_kl         | 0.0014391987  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0008534594 |\n",
      "| loss_vf_loss    | 49.626534     |\n",
      "-----------------------------------\n",
      "********** Iteration 65 ************\n",
      "Eval num_timesteps=266240, episode_reward=-79.20 +/- 9.33\n",
      "Episode length: 80.20 +/- 9.33\n",
      "Eval num_timesteps=266240, episode_reward=-80.20 +/- 5.86\n",
      "Episode length: 81.20 +/- 5.86\n",
      "Eval num_timesteps=266240, episode_reward=-78.80 +/- 6.78\n",
      "Episode length: 79.80 +/- 6.78\n",
      "Eval num_timesteps=266240, episode_reward=-76.00 +/- 6.36\n",
      "Episode length: 77.00 +/- 6.36\n",
      "Eval num_timesteps=266240, episode_reward=-82.50 +/- 16.21\n",
      "Episode length: 83.50 +/- 16.21\n",
      "Eval num_timesteps=266240, episode_reward=-78.80 +/- 9.33\n",
      "Episode length: 79.80 +/- 9.33\n",
      "Eval num_timesteps=266240, episode_reward=-78.60 +/- 5.83\n",
      "Episode length: 79.60 +/- 5.83\n",
      "Eval num_timesteps=266240, episode_reward=-94.40 +/- 43.69\n",
      "Episode length: 95.40 +/- 43.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00117 |       0.00000 |      46.17576 |       0.00113 |       0.16682\n",
      "      0.00035 |       0.00000 |      45.59507 |       0.00161 |       0.16644\n",
      "     -0.00051 |       0.00000 |      45.54826 |       0.00144 |       0.16669\n",
      "     -0.00015 |       0.00000 |      45.03811 |       0.00181 |       0.16608\n",
      "      0.00051 |       0.00000 |      44.91167 |       0.00168 |       0.16538\n",
      "     -0.00059 |       0.00000 |      44.70417 |       0.00153 |       0.16502\n",
      "     -0.00064 |       0.00000 |      44.39574 |       0.00155 |       0.16455\n",
      "      0.00080 |       0.00000 |      44.24098 |       0.00175 |       0.16530\n",
      "     -0.00137 |       0.00000 |      44.21836 |       0.00188 |       0.16518\n",
      "     -0.00067 |       0.00000 |      43.90742 |       0.00185 |       0.16512\n",
      "Evaluating losses...\n",
      "     -0.00094 |       0.00000 |      44.06543 |       0.00206 |       0.16414\n",
      "------------------------------------\n",
      "| EpLenMean       | 91.2           |\n",
      "| EpRewMean       | -90.2          |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 2471           |\n",
      "| TimeElapsed     | 877            |\n",
      "| TimestepsSoFar  | 270336         |\n",
      "| ev_tdlam_before | 0.813          |\n",
      "| loss_ent        | 0.16413507     |\n",
      "| loss_kl         | 0.0020591815   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00094078085 |\n",
      "| loss_vf_loss    | 44.06543       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 66 ************\n",
      "Eval num_timesteps=270336, episode_reward=-80.60 +/- 7.63\n",
      "Episode length: 81.60 +/- 7.63\n",
      "Eval num_timesteps=270336, episode_reward=-85.50 +/- 24.82\n",
      "Episode length: 86.50 +/- 24.82\n",
      "Eval num_timesteps=270336, episode_reward=-77.20 +/- 9.01\n",
      "Episode length: 78.20 +/- 9.01\n",
      "Eval num_timesteps=270336, episode_reward=-80.20 +/- 10.88\n",
      "Episode length: 81.20 +/- 10.88\n",
      "Eval num_timesteps=270336, episode_reward=-78.50 +/- 5.57\n",
      "Episode length: 79.50 +/- 5.57\n",
      "Eval num_timesteps=270336, episode_reward=-80.40 +/- 7.76\n",
      "Episode length: 81.40 +/- 7.76\n",
      "Eval num_timesteps=270336, episode_reward=-77.30 +/- 10.10\n",
      "Episode length: 78.30 +/- 10.10\n",
      "Eval num_timesteps=270336, episode_reward=-83.50 +/- 9.77\n",
      "Episode length: 84.50 +/- 9.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00151 |       0.00000 |      43.73664 |       0.00132 |       0.16449\n",
      "      0.00049 |       0.00000 |      43.37626 |       0.00137 |       0.16474\n",
      "      0.00075 |       0.00000 |      43.31126 |       0.00117 |       0.16522\n",
      "      0.00083 |       0.00000 |      43.11739 |       0.00122 |       0.16607\n",
      "      0.00044 |       0.00000 |      43.08499 |       0.00125 |       0.16719\n",
      "     -0.00181 |       0.00000 |      43.05958 |       0.00147 |       0.16710\n",
      "     -0.00019 |       0.00000 |      42.85528 |       0.00139 |       0.16753\n",
      "     -0.00047 |       0.00000 |      43.04002 |       0.00148 |       0.16772\n",
      "      0.00037 |       0.00000 |      42.86009 |       0.00173 |       0.16791\n",
      "     -0.00053 |       0.00000 |      42.76443 |       0.00163 |       0.16861\n",
      "Evaluating losses...\n",
      "     -0.00107 |       0.00000 |      42.74276 |       0.00156 |       0.16794\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.1          |\n",
      "| EpRewMean       | -90.1         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2517          |\n",
      "| TimeElapsed     | 889           |\n",
      "| TimestepsSoFar  | 274432        |\n",
      "| ev_tdlam_before | 0.827         |\n",
      "| loss_ent        | 0.16793703    |\n",
      "| loss_kl         | 0.0015646835  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010676112 |\n",
      "| loss_vf_loss    | 42.742756     |\n",
      "-----------------------------------\n",
      "********** Iteration 67 ************\n",
      "Eval num_timesteps=274432, episode_reward=-83.60 +/- 9.38\n",
      "Episode length: 84.60 +/- 9.38\n",
      "Eval num_timesteps=274432, episode_reward=-83.50 +/- 11.89\n",
      "Episode length: 84.50 +/- 11.89\n",
      "Eval num_timesteps=274432, episode_reward=-78.50 +/- 5.73\n",
      "Episode length: 79.50 +/- 5.73\n",
      "Eval num_timesteps=274432, episode_reward=-79.20 +/- 6.49\n",
      "Episode length: 80.20 +/- 6.49\n",
      "Eval num_timesteps=274432, episode_reward=-77.60 +/- 7.50\n",
      "Episode length: 78.60 +/- 7.50\n",
      "Eval num_timesteps=274432, episode_reward=-80.20 +/- 5.13\n",
      "Episode length: 81.20 +/- 5.13\n",
      "Eval num_timesteps=274432, episode_reward=-89.70 +/- 18.43\n",
      "Episode length: 90.70 +/- 18.43\n",
      "Eval num_timesteps=274432, episode_reward=-77.40 +/- 7.35\n",
      "Episode length: 78.40 +/- 7.35\n",
      "Eval num_timesteps=274432, episode_reward=-80.70 +/- 5.62\n",
      "Episode length: 81.70 +/- 5.62\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00027 |       0.00000 |      50.73635 |       0.00107 |       0.15925\n",
      "     -0.00033 |       0.00000 |      49.93827 |       0.00112 |       0.15712\n",
      "     -0.00075 |       0.00000 |      49.16221 |       0.00128 |       0.15470\n",
      "    -6.69e-05 |       0.00000 |      48.74255 |       0.00138 |       0.15368\n",
      "     -0.00109 |       0.00000 |      48.55859 |       0.00138 |       0.15347\n",
      "     -0.00078 |       0.00000 |      48.13763 |       0.00145 |       0.15357\n",
      "     -0.00076 |       0.00000 |      47.85921 |       0.00149 |       0.15317\n",
      "     -0.00112 |       0.00000 |      47.83458 |       0.00156 |       0.15379\n",
      "     -0.00042 |       0.00000 |      47.62703 |       0.00154 |       0.15370\n",
      "     -0.00188 |       0.00000 |      47.52948 |       0.00166 |       0.15235\n",
      "Evaluating losses...\n",
      "     -0.00144 |       0.00000 |      47.49453 |       0.00139 |       0.15252\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.9          |\n",
      "| EpRewMean       | -90.9         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 2559          |\n",
      "| TimeElapsed     | 903           |\n",
      "| TimestepsSoFar  | 278528        |\n",
      "| ev_tdlam_before | 0.784         |\n",
      "| loss_ent        | 0.15252276    |\n",
      "| loss_kl         | 0.0013911531  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0014396617 |\n",
      "| loss_vf_loss    | 47.494534     |\n",
      "-----------------------------------\n",
      "********** Iteration 68 ************\n",
      "Eval num_timesteps=278528, episode_reward=-77.10 +/- 7.75\n",
      "Episode length: 78.10 +/- 7.75\n",
      "Eval num_timesteps=278528, episode_reward=-79.60 +/- 8.75\n",
      "Episode length: 80.60 +/- 8.75\n",
      "Eval num_timesteps=278528, episode_reward=-121.70 +/- 126.51\n",
      "Episode length: 122.60 +/- 126.21\n",
      "Eval num_timesteps=278528, episode_reward=-81.50 +/- 17.07\n",
      "Episode length: 82.50 +/- 17.07\n",
      "Eval num_timesteps=278528, episode_reward=-84.30 +/- 14.09\n",
      "Episode length: 85.30 +/- 14.09\n",
      "Eval num_timesteps=278528, episode_reward=-82.60 +/- 10.26\n",
      "Episode length: 83.60 +/- 10.26\n",
      "Eval num_timesteps=278528, episode_reward=-76.70 +/- 5.59\n",
      "Episode length: 77.70 +/- 5.59\n",
      "Eval num_timesteps=278528, episode_reward=-91.40 +/- 33.65\n",
      "Episode length: 92.40 +/- 33.65\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00123 |       0.00000 |      45.15258 |       0.00117 |       0.16397\n",
      "      0.00020 |       0.00000 |      44.69800 |       0.00120 |       0.16138\n",
      "      0.00077 |       0.00000 |      44.54388 |       0.00130 |       0.15993\n",
      "      0.00069 |       0.00000 |      44.48810 |       0.00127 |       0.15886\n",
      "    -7.79e-06 |       0.00000 |      44.17648 |       0.00130 |       0.15749\n",
      "    -1.44e-05 |       0.00000 |      43.87937 |       0.00156 |       0.15659\n",
      "      0.00018 |       0.00000 |      43.57970 |       0.00149 |       0.15765\n",
      "     -0.00013 |       0.00000 |      43.59143 |       0.00154 |       0.15669\n",
      "     -0.00028 |       0.00000 |      43.49614 |       0.00147 |       0.15659\n",
      "     4.86e-05 |       0.00000 |      43.56905 |       0.00143 |       0.15666\n",
      "Evaluating losses...\n",
      "      0.00085 |       0.00000 |      43.39960 |       0.00151 |       0.15616\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.7          |\n",
      "| EpRewMean       | -90.7         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 2604          |\n",
      "| TimeElapsed     | 917           |\n",
      "| TimestepsSoFar  | 282624        |\n",
      "| ev_tdlam_before | 0.815         |\n",
      "| loss_ent        | 0.15616067    |\n",
      "| loss_kl         | 0.0015096543  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00084803125 |\n",
      "| loss_vf_loss    | 43.3996       |\n",
      "-----------------------------------\n",
      "********** Iteration 69 ************\n",
      "Eval num_timesteps=282624, episode_reward=-78.80 +/- 9.04\n",
      "Episode length: 79.80 +/- 9.04\n",
      "Eval num_timesteps=282624, episode_reward=-81.10 +/- 3.94\n",
      "Episode length: 82.10 +/- 3.94\n",
      "Eval num_timesteps=282624, episode_reward=-82.10 +/- 16.49\n",
      "Episode length: 83.10 +/- 16.49\n",
      "Eval num_timesteps=282624, episode_reward=-80.60 +/- 6.87\n",
      "Episode length: 81.60 +/- 6.87\n",
      "Eval num_timesteps=282624, episode_reward=-81.10 +/- 8.42\n",
      "Episode length: 82.10 +/- 8.42\n",
      "Eval num_timesteps=282624, episode_reward=-83.90 +/- 9.16\n",
      "Episode length: 84.90 +/- 9.16\n",
      "Eval num_timesteps=282624, episode_reward=-82.50 +/- 16.37\n",
      "Episode length: 83.50 +/- 16.37\n",
      "Eval num_timesteps=282624, episode_reward=-77.90 +/- 6.27\n",
      "Episode length: 78.90 +/- 6.27\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00221 |       0.00000 |      37.56425 |       0.00125 |       0.15698\n",
      "      0.00021 |       0.00000 |      37.44285 |       0.00133 |       0.15671\n",
      "     -0.00024 |       0.00000 |      37.43491 |       0.00135 |       0.15574\n",
      "     -0.00021 |       0.00000 |      37.18394 |       0.00133 |       0.15508\n",
      "      0.00082 |       0.00000 |      37.24111 |       0.00139 |       0.15424\n",
      "     8.98e-05 |       0.00000 |      37.21162 |       0.00151 |       0.15368\n",
      "      0.00183 |       0.00000 |      37.00159 |       0.00160 |       0.15223\n",
      "      0.00042 |       0.00000 |      36.74927 |       0.00169 |       0.15154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00016 |       0.00000 |      36.72698 |       0.00183 |       0.15070\n",
      "    -9.40e-05 |       0.00000 |      36.59540 |       0.00165 |       0.15076\n",
      "Evaluating losses...\n",
      "      0.00025 |       0.00000 |      36.41353 |       0.00189 |       0.15133\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.8          |\n",
      "| EpRewMean       | -89.8         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2650          |\n",
      "| TimeElapsed     | 928           |\n",
      "| TimestepsSoFar  | 286720        |\n",
      "| ev_tdlam_before | 0.848         |\n",
      "| loss_ent        | 0.15132555    |\n",
      "| loss_kl         | 0.001888219   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00024678232 |\n",
      "| loss_vf_loss    | 36.413532     |\n",
      "-----------------------------------\n",
      "********** Iteration 70 ************\n",
      "Eval num_timesteps=286720, episode_reward=-75.90 +/- 5.87\n",
      "Episode length: 76.90 +/- 5.87\n",
      "Eval num_timesteps=286720, episode_reward=-77.00 +/- 6.80\n",
      "Episode length: 78.00 +/- 6.80\n",
      "Eval num_timesteps=286720, episode_reward=-83.00 +/- 10.44\n",
      "Episode length: 84.00 +/- 10.44\n",
      "Eval num_timesteps=286720, episode_reward=-83.00 +/- 6.87\n",
      "Episode length: 84.00 +/- 6.87\n",
      "Eval num_timesteps=286720, episode_reward=-83.20 +/- 13.40\n",
      "Episode length: 84.20 +/- 13.40\n",
      "Eval num_timesteps=286720, episode_reward=-84.00 +/- 14.46\n",
      "Episode length: 85.00 +/- 14.46\n",
      "Eval num_timesteps=286720, episode_reward=-88.60 +/- 29.24\n",
      "Episode length: 89.60 +/- 29.24\n",
      "Eval num_timesteps=286720, episode_reward=-78.00 +/- 8.77\n",
      "Episode length: 79.00 +/- 8.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00024 |       0.00000 |      36.03064 |       0.00118 |       0.15556\n",
      "     -0.00038 |       0.00000 |      35.70014 |       0.00132 |       0.15764\n",
      "     -0.00028 |       0.00000 |      35.53493 |       0.00149 |       0.15936\n",
      "     -0.00132 |       0.00000 |      35.50921 |       0.00162 |       0.16023\n",
      "     -0.00032 |       0.00000 |      35.29211 |       0.00174 |       0.16086\n",
      "     -0.00156 |       0.00000 |      35.48453 |       0.00169 |       0.16152\n",
      "     -0.00148 |       0.00000 |      35.17805 |       0.00186 |       0.16237\n",
      "     -0.00046 |       0.00000 |      34.80773 |       0.00196 |       0.16244\n",
      "     -0.00092 |       0.00000 |      35.03631 |       0.00178 |       0.16186\n",
      "     -0.00011 |       0.00000 |      34.94398 |       0.00207 |       0.16208\n",
      "Evaluating losses...\n",
      "     -0.00118 |       0.00000 |      34.74902 |       0.00217 |       0.16328\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.3          |\n",
      "| EpRewMean       | -86.3         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 2698          |\n",
      "| TimeElapsed     | 940           |\n",
      "| TimestepsSoFar  | 290816        |\n",
      "| ev_tdlam_before | 0.856         |\n",
      "| loss_ent        | 0.16327542    |\n",
      "| loss_kl         | 0.0021656824  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0011840328 |\n",
      "| loss_vf_loss    | 34.749023     |\n",
      "-----------------------------------\n",
      "********** Iteration 71 ************\n",
      "Eval num_timesteps=290816, episode_reward=-85.10 +/- 25.05\n",
      "Episode length: 86.10 +/- 25.05\n",
      "Eval num_timesteps=290816, episode_reward=-91.30 +/- 27.30\n",
      "Episode length: 92.30 +/- 27.30\n",
      "Eval num_timesteps=290816, episode_reward=-76.00 +/- 9.32\n",
      "Episode length: 77.00 +/- 9.32\n",
      "Eval num_timesteps=290816, episode_reward=-80.60 +/- 7.64\n",
      "Episode length: 81.60 +/- 7.64\n",
      "Eval num_timesteps=290816, episode_reward=-79.60 +/- 7.58\n",
      "Episode length: 80.60 +/- 7.58\n",
      "Eval num_timesteps=290816, episode_reward=-78.20 +/- 8.21\n",
      "Episode length: 79.20 +/- 8.21\n",
      "Eval num_timesteps=290816, episode_reward=-87.50 +/- 12.67\n",
      "Episode length: 88.50 +/- 12.67\n",
      "Eval num_timesteps=290816, episode_reward=-90.60 +/- 40.72\n",
      "Episode length: 91.60 +/- 40.72\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00014 |       0.00000 |      31.63973 |       0.00128 |       0.16028\n",
      "      0.00047 |       0.00000 |      31.60405 |       0.00135 |       0.16002\n",
      "      0.00092 |       0.00000 |      31.70785 |       0.00144 |       0.16031\n",
      "     7.49e-05 |       0.00000 |      31.55420 |       0.00134 |       0.15782\n",
      "     9.34e-07 |       0.00000 |      31.52545 |       0.00158 |       0.15797\n",
      "     -0.00091 |       0.00000 |      31.24254 |       0.00176 |       0.15889\n",
      "     -0.00072 |       0.00000 |      31.35658 |       0.00157 |       0.15727\n",
      "     -0.00092 |       0.00000 |      31.13304 |       0.00187 |       0.15827\n",
      "     -0.00037 |       0.00000 |      31.16393 |       0.00186 |       0.15824\n",
      "     -0.00038 |       0.00000 |      31.15822 |       0.00188 |       0.15829\n",
      "Evaluating losses...\n",
      "     -0.00152 |       0.00000 |      31.05768 |       0.00190 |       0.15867\n",
      "-----------------------------------\n",
      "| EpLenMean       | 86.8          |\n",
      "| EpRewMean       | -85.8         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2744          |\n",
      "| TimeElapsed     | 951           |\n",
      "| TimestepsSoFar  | 294912        |\n",
      "| ev_tdlam_before | 0.873         |\n",
      "| loss_ent        | 0.1586714     |\n",
      "| loss_kl         | 0.0019023683  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0015211538 |\n",
      "| loss_vf_loss    | 31.057678     |\n",
      "-----------------------------------\n",
      "********** Iteration 72 ************\n",
      "Eval num_timesteps=294912, episode_reward=-81.30 +/- 6.23\n",
      "Episode length: 82.30 +/- 6.23\n",
      "Eval num_timesteps=294912, episode_reward=-72.20 +/- 8.32\n",
      "Episode length: 73.20 +/- 8.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=294912, episode_reward=-78.20 +/- 10.91\n",
      "Episode length: 79.20 +/- 10.91\n",
      "Eval num_timesteps=294912, episode_reward=-76.40 +/- 4.27\n",
      "Episode length: 77.40 +/- 4.27\n",
      "Eval num_timesteps=294912, episode_reward=-94.80 +/- 48.51\n",
      "Episode length: 95.80 +/- 48.51\n",
      "Eval num_timesteps=294912, episode_reward=-88.60 +/- 23.00\n",
      "Episode length: 89.60 +/- 23.00\n",
      "Eval num_timesteps=294912, episode_reward=-76.00 +/- 8.74\n",
      "Episode length: 77.00 +/- 8.74\n",
      "Eval num_timesteps=294912, episode_reward=-78.20 +/- 5.71\n",
      "Episode length: 79.20 +/- 5.71\n",
      "Eval num_timesteps=294912, episode_reward=-82.60 +/- 17.11\n",
      "Episode length: 83.60 +/- 17.11\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00077 |       0.00000 |      47.81747 |       0.00132 |       0.15464\n",
      "      0.00077 |       0.00000 |      47.26273 |       0.00133 |       0.15438\n",
      "     -0.00028 |       0.00000 |      46.73801 |       0.00126 |       0.15477\n",
      "      0.00013 |       0.00000 |      46.25937 |       0.00148 |       0.15517\n",
      "      0.00044 |       0.00000 |      46.31360 |       0.00149 |       0.15322\n",
      "      0.00068 |       0.00000 |      46.04803 |       0.00138 |       0.15313\n",
      "     -0.00060 |       0.00000 |      45.56052 |       0.00149 |       0.15205\n",
      "      0.00025 |       0.00000 |      45.47753 |       0.00148 |       0.15305\n",
      "    -7.82e-05 |       0.00000 |      45.40890 |       0.00172 |       0.15158\n",
      "     -0.00056 |       0.00000 |      45.26204 |       0.00169 |       0.15095\n",
      "Evaluating losses...\n",
      "     -0.00023 |       0.00000 |      45.11319 |       0.00168 |       0.15207\n",
      "------------------------------------\n",
      "| EpLenMean       | 91             |\n",
      "| EpRewMean       | -90            |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 2788           |\n",
      "| TimeElapsed     | 964            |\n",
      "| TimestepsSoFar  | 299008         |\n",
      "| ev_tdlam_before | 0.795          |\n",
      "| loss_ent        | 0.15207368     |\n",
      "| loss_kl         | 0.0016842432   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00023022108 |\n",
      "| loss_vf_loss    | 45.113194      |\n",
      "------------------------------------\n",
      "********** Iteration 73 ************\n",
      "Eval num_timesteps=299008, episode_reward=-82.40 +/- 19.27\n",
      "Episode length: 83.40 +/- 19.27\n",
      "Eval num_timesteps=299008, episode_reward=-77.60 +/- 7.05\n",
      "Episode length: 78.60 +/- 7.05\n",
      "Eval num_timesteps=299008, episode_reward=-86.60 +/- 20.13\n",
      "Episode length: 87.60 +/- 20.13\n",
      "Eval num_timesteps=299008, episode_reward=-79.20 +/- 8.86\n",
      "Episode length: 80.20 +/- 8.86\n",
      "Eval num_timesteps=299008, episode_reward=-88.30 +/- 22.44\n",
      "Episode length: 89.30 +/- 22.44\n",
      "Eval num_timesteps=299008, episode_reward=-81.20 +/- 8.99\n",
      "Episode length: 82.20 +/- 8.99\n",
      "Eval num_timesteps=299008, episode_reward=-94.00 +/- 20.48\n",
      "Episode length: 95.00 +/- 20.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=299008, episode_reward=-80.90 +/- 8.77\n",
      "Episode length: 81.90 +/- 8.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00132 |       0.00000 |      37.65918 |       0.00111 |       0.15072\n",
      "      0.00085 |       0.00000 |      37.53454 |       0.00120 |       0.15143\n",
      "      0.00015 |       0.00000 |      37.33014 |       0.00118 |       0.15186\n",
      "      0.00123 |       0.00000 |      37.13539 |       0.00121 |       0.15145\n",
      "      0.00162 |       0.00000 |      37.01648 |       0.00127 |       0.15177\n",
      "      0.00028 |       0.00000 |      36.67911 |       0.00136 |       0.15104\n",
      "     -0.00026 |       0.00000 |      36.73990 |       0.00129 |       0.15243\n",
      "      0.00043 |       0.00000 |      36.49235 |       0.00149 |       0.15391\n",
      "     5.80e-05 |       0.00000 |      36.56316 |       0.00126 |       0.15478\n",
      "      0.00095 |       0.00000 |      36.59117 |       0.00140 |       0.15388\n",
      "Evaluating losses...\n",
      "    -8.60e-05 |       0.00000 |      36.47156 |       0.00132 |       0.15394\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.7          |\n",
      "| EpRewMean       | -89.7         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 2834          |\n",
      "| TimeElapsed     | 976           |\n",
      "| TimestepsSoFar  | 303104        |\n",
      "| ev_tdlam_before | 0.844         |\n",
      "| loss_ent        | 0.15393998    |\n",
      "| loss_kl         | 0.001319179   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -8.602813e-05 |\n",
      "| loss_vf_loss    | 36.471558     |\n",
      "-----------------------------------\n",
      "********** Iteration 74 ************\n",
      "Eval num_timesteps=303104, episode_reward=-86.20 +/- 17.33\n",
      "Episode length: 87.20 +/- 17.33\n",
      "Eval num_timesteps=303104, episode_reward=-95.60 +/- 43.06\n",
      "Episode length: 96.60 +/- 43.06\n",
      "Eval num_timesteps=303104, episode_reward=-92.80 +/- 37.75\n",
      "Episode length: 93.80 +/- 37.75\n",
      "Eval num_timesteps=303104, episode_reward=-76.00 +/- 8.06\n",
      "Episode length: 77.00 +/- 8.06\n",
      "Eval num_timesteps=303104, episode_reward=-95.70 +/- 26.36\n",
      "Episode length: 96.70 +/- 26.36\n",
      "Eval num_timesteps=303104, episode_reward=-80.10 +/- 6.58\n",
      "Episode length: 81.10 +/- 6.58\n",
      "Eval num_timesteps=303104, episode_reward=-77.10 +/- 6.33\n",
      "Episode length: 78.10 +/- 6.33\n",
      "Eval num_timesteps=303104, episode_reward=-82.90 +/- 9.58\n",
      "Episode length: 83.90 +/- 9.58\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00060 |       0.00000 |      41.89404 |       0.00125 |       0.15252\n",
      "     -0.00020 |       0.00000 |      41.47565 |       0.00141 |       0.15346\n",
      "    -7.97e-05 |       0.00000 |      41.42706 |       0.00137 |       0.15407\n",
      "     -0.00051 |       0.00000 |      40.98777 |       0.00152 |       0.15420\n",
      "      0.00175 |       0.00000 |      40.95511 |       0.00157 |       0.15344\n",
      "     5.46e-05 |       0.00000 |      40.73491 |       0.00177 |       0.15401\n",
      "     -0.00123 |       0.00000 |      40.72432 |       0.00168 |       0.15301\n",
      "     -0.00057 |       0.00000 |      40.53496 |       0.00196 |       0.15389\n",
      "    -6.26e-05 |       0.00000 |      40.43663 |       0.00213 |       0.15451\n",
      "     -0.00142 |       0.00000 |      40.38073 |       0.00178 |       0.15375\n",
      "Evaluating losses...\n",
      "     -0.00076 |       0.00000 |      40.35200 |       0.00177 |       0.15330\n",
      "------------------------------------\n",
      "| EpLenMean       | 89.2           |\n",
      "| EpRewMean       | -88.2          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 2880           |\n",
      "| TimeElapsed     | 988            |\n",
      "| TimestepsSoFar  | 307200         |\n",
      "| ev_tdlam_before | 0.825          |\n",
      "| loss_ent        | 0.15329854     |\n",
      "| loss_kl         | 0.0017703861   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00076407613 |\n",
      "| loss_vf_loss    | 40.351997      |\n",
      "------------------------------------\n",
      "********** Iteration 75 ************\n",
      "Eval num_timesteps=307200, episode_reward=-85.10 +/- 13.55\n",
      "Episode length: 86.10 +/- 13.55\n",
      "Eval num_timesteps=307200, episode_reward=-79.20 +/- 7.77\n",
      "Episode length: 80.20 +/- 7.77\n",
      "Eval num_timesteps=307200, episode_reward=-86.60 +/- 31.32\n",
      "Episode length: 87.60 +/- 31.32\n",
      "Eval num_timesteps=307200, episode_reward=-84.70 +/- 23.04\n",
      "Episode length: 85.70 +/- 23.04\n",
      "Eval num_timesteps=307200, episode_reward=-77.70 +/- 9.60\n",
      "Episode length: 78.70 +/- 9.60\n",
      "Eval num_timesteps=307200, episode_reward=-78.40 +/- 6.89\n",
      "Episode length: 79.40 +/- 6.89\n",
      "Eval num_timesteps=307200, episode_reward=-75.60 +/- 6.67\n",
      "Episode length: 76.60 +/- 6.67\n",
      "Eval num_timesteps=307200, episode_reward=-81.70 +/- 5.75\n",
      "Episode length: 82.70 +/- 5.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00162 |       0.00000 |      42.77082 |       0.00119 |       0.14965\n",
      "      0.00046 |       0.00000 |      42.71955 |       0.00127 |       0.14744\n",
      "      0.00149 |       0.00000 |      42.44382 |       0.00133 |       0.14702\n",
      "      0.00052 |       0.00000 |      42.34570 |       0.00140 |       0.14463\n",
      "     -0.00058 |       0.00000 |      42.41854 |       0.00150 |       0.14293\n",
      "      0.00057 |       0.00000 |      42.27503 |       0.00174 |       0.14261\n",
      "      0.00050 |       0.00000 |      41.98216 |       0.00153 |       0.14179\n",
      "      0.00024 |       0.00000 |      42.25373 |       0.00165 |       0.14244\n",
      "     -0.00092 |       0.00000 |      41.93359 |       0.00150 |       0.14382\n",
      "     1.96e-06 |       0.00000 |      41.80726 |       0.00164 |       0.14313\n",
      "Evaluating losses...\n",
      "     9.40e-05 |       0.00000 |      41.72873 |       0.00179 |       0.14313\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.9         |\n",
      "| EpRewMean       | -89.9        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 2925         |\n",
      "| TimeElapsed     | 999          |\n",
      "| TimestepsSoFar  | 311296       |\n",
      "| ev_tdlam_before | 0.819        |\n",
      "| loss_ent        | 0.1431297    |\n",
      "| loss_kl         | 0.0017883261 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 9.397406e-05 |\n",
      "| loss_vf_loss    | 41.728733    |\n",
      "----------------------------------\n",
      "********** Iteration 76 ************\n",
      "Eval num_timesteps=311296, episode_reward=-80.30 +/- 10.73\n",
      "Episode length: 81.30 +/- 10.73\n",
      "Eval num_timesteps=311296, episode_reward=-71.90 +/- 4.23\n",
      "Episode length: 72.90 +/- 4.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=311296, episode_reward=-72.40 +/- 8.25\n",
      "Episode length: 73.40 +/- 8.25\n",
      "Eval num_timesteps=311296, episode_reward=-78.50 +/- 6.80\n",
      "Episode length: 79.50 +/- 6.80\n",
      "Eval num_timesteps=311296, episode_reward=-77.60 +/- 9.57\n",
      "Episode length: 78.60 +/- 9.57\n",
      "Eval num_timesteps=311296, episode_reward=-86.70 +/- 16.74\n",
      "Episode length: 87.70 +/- 16.74\n",
      "Eval num_timesteps=311296, episode_reward=-82.80 +/- 8.00\n",
      "Episode length: 83.80 +/- 8.00\n",
      "Eval num_timesteps=311296, episode_reward=-99.70 +/- 65.29\n",
      "Episode length: 100.70 +/- 65.29\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00078 |       0.00000 |      48.82939 |       0.00122 |       0.14581\n",
      "      0.00041 |       0.00000 |      48.56749 |       0.00132 |       0.14682\n",
      "     -0.00022 |       0.00000 |      48.38678 |       0.00135 |       0.14745\n",
      "     -0.00056 |       0.00000 |      48.17625 |       0.00167 |       0.14866\n",
      "      0.00041 |       0.00000 |      47.92690 |       0.00172 |       0.14897\n",
      "      0.00010 |       0.00000 |      47.82630 |       0.00180 |       0.14943\n",
      "     -0.00074 |       0.00000 |      47.85650 |       0.00201 |       0.15001\n",
      "     -0.00151 |       0.00000 |      47.52023 |       0.00196 |       0.15023\n",
      "     -0.00086 |       0.00000 |      47.19974 |       0.00199 |       0.15074\n",
      "     -0.00163 |       0.00000 |      47.42872 |       0.00205 |       0.15094\n",
      "Evaluating losses...\n",
      "     -0.00134 |       0.00000 |      47.11313 |       0.00215 |       0.15140\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.7          |\n",
      "| EpRewMean       | -90.7         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 2968          |\n",
      "| TimeElapsed     | 1.01e+03      |\n",
      "| TimestepsSoFar  | 315392        |\n",
      "| ev_tdlam_before | 0.79          |\n",
      "| loss_ent        | 0.1514006     |\n",
      "| loss_kl         | 0.0021479924  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0013362141 |\n",
      "| loss_vf_loss    | 47.113132     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 77 ************\n",
      "Eval num_timesteps=315392, episode_reward=-81.60 +/- 16.04\n",
      "Episode length: 82.60 +/- 16.04\n",
      "Eval num_timesteps=315392, episode_reward=-85.20 +/- 40.39\n",
      "Episode length: 86.20 +/- 40.39\n",
      "Eval num_timesteps=315392, episode_reward=-76.30 +/- 9.72\n",
      "Episode length: 77.30 +/- 9.72\n",
      "Eval num_timesteps=315392, episode_reward=-77.80 +/- 9.43\n",
      "Episode length: 78.80 +/- 9.43\n",
      "Eval num_timesteps=315392, episode_reward=-90.00 +/- 27.20\n",
      "Episode length: 91.00 +/- 27.20\n",
      "Eval num_timesteps=315392, episode_reward=-79.80 +/- 17.22\n",
      "Episode length: 80.80 +/- 17.22\n",
      "Eval num_timesteps=315392, episode_reward=-78.80 +/- 6.24\n",
      "Episode length: 79.80 +/- 6.24\n",
      "Eval num_timesteps=315392, episode_reward=-75.60 +/- 6.70\n",
      "Episode length: 76.60 +/- 6.70\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00050 |       0.00000 |      38.26860 |       0.00130 |       0.15115\n",
      "      0.00161 |       0.00000 |      37.98298 |       0.00140 |       0.15067\n",
      "      0.00024 |       0.00000 |      37.95520 |       0.00137 |       0.15020\n",
      "      0.00054 |       0.00000 |      37.65498 |       0.00127 |       0.15010\n",
      "      0.00019 |       0.00000 |      37.43237 |       0.00136 |       0.14866\n",
      "     -0.00019 |       0.00000 |      37.45188 |       0.00149 |       0.14859\n",
      "     -0.00059 |       0.00000 |      37.32445 |       0.00151 |       0.14828\n",
      "     -0.00083 |       0.00000 |      37.18010 |       0.00172 |       0.14787\n",
      "    -7.56e-05 |       0.00000 |      37.09980 |       0.00166 |       0.14832\n",
      "     -0.00087 |       0.00000 |      37.02556 |       0.00169 |       0.14853\n",
      "Evaluating losses...\n",
      "     -0.00109 |       0.00000 |      37.10279 |       0.00171 |       0.14853\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.4          |\n",
      "| EpRewMean       | -91.4         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3013          |\n",
      "| TimeElapsed     | 1.03e+03      |\n",
      "| TimestepsSoFar  | 319488        |\n",
      "| ev_tdlam_before | 0.842         |\n",
      "| loss_ent        | 0.14852673    |\n",
      "| loss_kl         | 0.0017127484  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0010862366 |\n",
      "| loss_vf_loss    | 37.102795     |\n",
      "-----------------------------------\n",
      "********** Iteration 78 ************\n",
      "Eval num_timesteps=319488, episode_reward=-72.10 +/- 9.73\n",
      "Episode length: 73.10 +/- 9.73\n",
      "Eval num_timesteps=319488, episode_reward=-77.40 +/- 15.41\n",
      "Episode length: 78.40 +/- 15.41\n",
      "Eval num_timesteps=319488, episode_reward=-81.00 +/- 29.69\n",
      "Episode length: 82.00 +/- 29.69\n",
      "Eval num_timesteps=319488, episode_reward=-87.80 +/- 42.99\n",
      "Episode length: 88.80 +/- 42.99\n",
      "Eval num_timesteps=319488, episode_reward=-92.90 +/- 30.60\n",
      "Episode length: 93.90 +/- 30.60\n",
      "Eval num_timesteps=319488, episode_reward=-76.10 +/- 9.59\n",
      "Episode length: 77.10 +/- 9.59\n",
      "Eval num_timesteps=319488, episode_reward=-77.60 +/- 6.97\n",
      "Episode length: 78.60 +/- 6.97\n",
      "Eval num_timesteps=319488, episode_reward=-79.20 +/- 11.06\n",
      "Episode length: 80.20 +/- 11.06\n",
      "Eval num_timesteps=319488, episode_reward=-79.70 +/- 7.76\n",
      "Episode length: 80.70 +/- 7.76\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00093 |       0.00000 |      35.81286 |       0.00130 |       0.14573\n",
      "      0.00099 |       0.00000 |      35.00641 |       0.00142 |       0.14556\n",
      "     -0.00015 |       0.00000 |      34.57874 |       0.00137 |       0.14526\n",
      "      0.00079 |       0.00000 |      34.24803 |       0.00139 |       0.14574\n",
      "     -0.00031 |       0.00000 |      34.18071 |       0.00153 |       0.14617\n",
      "      0.00051 |       0.00000 |      34.05874 |       0.00142 |       0.14653\n",
      "     3.75e-05 |       0.00000 |      33.66204 |       0.00166 |       0.14609\n",
      "     6.83e-05 |       0.00000 |      33.45917 |       0.00171 |       0.14501\n",
      "      0.00035 |       0.00000 |      33.37603 |       0.00171 |       0.14558\n",
      "     -0.00013 |       0.00000 |      33.22689 |       0.00186 |       0.14491\n",
      "Evaluating losses...\n",
      "     -0.00017 |       0.00000 |      33.17418 |       0.00174 |       0.14568\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.8           |\n",
      "| EpRewMean       | -89.8          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 3058           |\n",
      "| TimeElapsed     | 1.04e+03       |\n",
      "| TimestepsSoFar  | 323584         |\n",
      "| ev_tdlam_before | 0.854          |\n",
      "| loss_ent        | 0.14568228     |\n",
      "| loss_kl         | 0.0017435466   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00017438713 |\n",
      "| loss_vf_loss    | 33.17418       |\n",
      "------------------------------------\n",
      "********** Iteration 79 ************\n",
      "Eval num_timesteps=323584, episode_reward=-75.10 +/- 9.00\n",
      "Episode length: 76.10 +/- 9.00\n",
      "Eval num_timesteps=323584, episode_reward=-75.40 +/- 4.98\n",
      "Episode length: 76.40 +/- 4.98\n",
      "Eval num_timesteps=323584, episode_reward=-76.80 +/- 7.59\n",
      "Episode length: 77.80 +/- 7.59\n",
      "Eval num_timesteps=323584, episode_reward=-81.20 +/- 12.46\n",
      "Episode length: 82.20 +/- 12.46\n",
      "Eval num_timesteps=323584, episode_reward=-79.00 +/- 9.88\n",
      "Episode length: 80.00 +/- 9.88\n",
      "Eval num_timesteps=323584, episode_reward=-76.90 +/- 9.90\n",
      "Episode length: 77.90 +/- 9.90\n",
      "Eval num_timesteps=323584, episode_reward=-98.60 +/- 49.04\n",
      "Episode length: 99.60 +/- 49.04\n",
      "Eval num_timesteps=323584, episode_reward=-88.50 +/- 19.95\n",
      "Episode length: 89.50 +/- 19.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00029 |       0.00000 |      43.13702 |       0.00123 |       0.14564\n",
      "      0.00030 |       0.00000 |      42.70741 |       0.00144 |       0.14508\n",
      "      0.00079 |       0.00000 |      42.42074 |       0.00152 |       0.14413\n",
      "      0.00082 |       0.00000 |      41.96626 |       0.00159 |       0.14216\n",
      "      0.00181 |       0.00000 |      41.84497 |       0.00159 |       0.14210\n",
      "      0.00021 |       0.00000 |      41.42805 |       0.00192 |       0.14227\n",
      "     -0.00025 |       0.00000 |      41.60591 |       0.00226 |       0.14188\n",
      "     -0.00037 |       0.00000 |      41.42346 |       0.00239 |       0.14132\n",
      "      0.00046 |       0.00000 |      41.29787 |       0.00238 |       0.14066\n",
      "      0.00112 |       0.00000 |      41.17450 |       0.00224 |       0.14085\n",
      "Evaluating losses...\n",
      "     -0.00038 |       0.00000 |      41.17121 |       0.00212 |       0.13978\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.7           |\n",
      "| EpRewMean       | -89.7          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 3103           |\n",
      "| TimeElapsed     | 1.05e+03       |\n",
      "| TimestepsSoFar  | 327680         |\n",
      "| ev_tdlam_before | 0.82           |\n",
      "| loss_ent        | 0.13977575     |\n",
      "| loss_kl         | 0.002121869    |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00038404443 |\n",
      "| loss_vf_loss    | 41.171207      |\n",
      "------------------------------------\n",
      "********** Iteration 80 ************\n",
      "Eval num_timesteps=327680, episode_reward=-116.60 +/- 66.29\n",
      "Episode length: 117.60 +/- 66.29\n",
      "Eval num_timesteps=327680, episode_reward=-75.70 +/- 6.75\n",
      "Episode length: 76.70 +/- 6.75\n",
      "Eval num_timesteps=327680, episode_reward=-79.20 +/- 6.29\n",
      "Episode length: 80.20 +/- 6.29\n",
      "Eval num_timesteps=327680, episode_reward=-81.10 +/- 14.52\n",
      "Episode length: 82.10 +/- 14.52\n",
      "Eval num_timesteps=327680, episode_reward=-82.00 +/- 9.94\n",
      "Episode length: 83.00 +/- 9.94\n",
      "Eval num_timesteps=327680, episode_reward=-110.70 +/- 74.34\n",
      "Episode length: 111.70 +/- 74.34\n",
      "Eval num_timesteps=327680, episode_reward=-75.00 +/- 8.22\n",
      "Episode length: 76.00 +/- 8.22\n",
      "Eval num_timesteps=327680, episode_reward=-88.50 +/- 23.95\n",
      "Episode length: 89.50 +/- 23.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00155 |       0.00000 |      39.78896 |       0.00122 |       0.14349\n",
      "      0.00122 |       0.00000 |      39.08836 |       0.00127 |       0.14266\n",
      "      0.00037 |       0.00000 |      38.56261 |       0.00131 |       0.14183\n",
      "      0.00058 |       0.00000 |      38.25218 |       0.00137 |       0.14100\n",
      "      0.00107 |       0.00000 |      38.06747 |       0.00144 |       0.14191\n",
      "      0.00069 |       0.00000 |      37.58346 |       0.00137 |       0.14126\n",
      "      0.00043 |       0.00000 |      37.40203 |       0.00153 |       0.14101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00023 |       0.00000 |      37.23994 |       0.00143 |       0.14161\n",
      "      0.00068 |       0.00000 |      37.15146 |       0.00154 |       0.14199\n",
      "      0.00076 |       0.00000 |      36.89784 |       0.00152 |       0.14259\n",
      "Evaluating losses...\n",
      "    -5.66e-05 |       0.00000 |      36.66660 |       0.00170 |       0.14281\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.1          |\n",
      "| EpRewMean       | -89.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 3150          |\n",
      "| TimeElapsed     | 1.06e+03      |\n",
      "| TimestepsSoFar  | 331776        |\n",
      "| ev_tdlam_before | 0.841         |\n",
      "| loss_ent        | 0.14281195    |\n",
      "| loss_kl         | 0.0017000939  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -5.661708e-05 |\n",
      "| loss_vf_loss    | 36.666595     |\n",
      "-----------------------------------\n",
      "********** Iteration 81 ************\n",
      "Eval num_timesteps=331776, episode_reward=-83.20 +/- 13.44\n",
      "Episode length: 84.20 +/- 13.44\n",
      "Eval num_timesteps=331776, episode_reward=-82.40 +/- 7.10\n",
      "Episode length: 83.40 +/- 7.10\n",
      "Eval num_timesteps=331776, episode_reward=-79.10 +/- 11.49\n",
      "Episode length: 80.10 +/- 11.49\n",
      "Eval num_timesteps=331776, episode_reward=-75.30 +/- 8.91\n",
      "Episode length: 76.30 +/- 8.91\n",
      "Eval num_timesteps=331776, episode_reward=-81.50 +/- 7.63\n",
      "Episode length: 82.50 +/- 7.63\n",
      "Eval num_timesteps=331776, episode_reward=-91.50 +/- 43.97\n",
      "Episode length: 92.50 +/- 43.97\n",
      "Eval num_timesteps=331776, episode_reward=-79.70 +/- 11.67\n",
      "Episode length: 80.70 +/- 11.67\n",
      "Eval num_timesteps=331776, episode_reward=-87.60 +/- 30.21\n",
      "Episode length: 88.60 +/- 30.21\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00067 |       0.00000 |      39.57614 |       0.00145 |       0.13954\n",
      "     -0.00068 |       0.00000 |      39.43440 |       0.00151 |       0.14100\n",
      "     -0.00043 |       0.00000 |      39.27605 |       0.00155 |       0.14116\n",
      "     -0.00063 |       0.00000 |      38.80722 |       0.00183 |       0.14409\n",
      "     7.53e-05 |       0.00000 |      38.73191 |       0.00218 |       0.14311\n",
      "     -0.00128 |       0.00000 |      38.88574 |       0.00214 |       0.14361\n",
      "     -0.00089 |       0.00000 |      38.67600 |       0.00218 |       0.14494\n",
      "     -0.00125 |       0.00000 |      38.75870 |       0.00243 |       0.14587\n",
      "     -0.00101 |       0.00000 |      38.56741 |       0.00265 |       0.14498\n",
      "     -0.00141 |       0.00000 |      38.33107 |       0.00278 |       0.14520\n",
      "Evaluating losses...\n",
      "     -0.00128 |       0.00000 |      38.38250 |       0.00283 |       0.14511\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.3          |\n",
      "| EpRewMean       | -88.3         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3195          |\n",
      "| TimeElapsed     | 1.08e+03      |\n",
      "| TimestepsSoFar  | 335872        |\n",
      "| ev_tdlam_before | 0.837         |\n",
      "| loss_ent        | 0.14511241    |\n",
      "| loss_kl         | 0.002827868   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0012771264 |\n",
      "| loss_vf_loss    | 38.382496     |\n",
      "-----------------------------------\n",
      "********** Iteration 82 ************\n",
      "Eval num_timesteps=335872, episode_reward=-84.70 +/- 28.51\n",
      "Episode length: 85.70 +/- 28.51\n",
      "Eval num_timesteps=335872, episode_reward=-78.70 +/- 16.47\n",
      "Episode length: 79.70 +/- 16.47\n",
      "Eval num_timesteps=335872, episode_reward=-70.80 +/- 6.06\n",
      "Episode length: 71.80 +/- 6.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=335872, episode_reward=-76.30 +/- 9.17\n",
      "Episode length: 77.30 +/- 9.17\n",
      "Eval num_timesteps=335872, episode_reward=-85.40 +/- 28.60\n",
      "Episode length: 86.40 +/- 28.60\n",
      "Eval num_timesteps=335872, episode_reward=-82.70 +/- 9.84\n",
      "Episode length: 83.70 +/- 9.84\n",
      "Eval num_timesteps=335872, episode_reward=-76.00 +/- 8.07\n",
      "Episode length: 77.00 +/- 8.07\n",
      "Eval num_timesteps=335872, episode_reward=-78.00 +/- 13.86\n",
      "Episode length: 79.00 +/- 13.86\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00150 |       0.00000 |      47.63964 |       0.00148 |       0.14338\n",
      "      0.00109 |       0.00000 |      46.96727 |       0.00139 |       0.14186\n",
      "      0.00017 |       0.00000 |      46.64521 |       0.00146 |       0.14183\n",
      "    -7.97e-05 |       0.00000 |      46.08864 |       0.00151 |       0.14099\n",
      "      0.00070 |       0.00000 |      46.10147 |       0.00172 |       0.13983\n",
      "     -0.00022 |       0.00000 |      45.73550 |       0.00192 |       0.13937\n",
      "     -0.00045 |       0.00000 |      45.54996 |       0.00188 |       0.13899\n",
      "      0.00131 |       0.00000 |      45.43309 |       0.00186 |       0.13800\n",
      "     -0.00045 |       0.00000 |      45.18271 |       0.00174 |       0.13741\n",
      "     -0.00112 |       0.00000 |      45.27523 |       0.00177 |       0.13927\n",
      "Evaluating losses...\n",
      "     -0.00041 |       0.00000 |      45.08605 |       0.00173 |       0.13986\n",
      "------------------------------------\n",
      "| EpLenMean       | 92.6           |\n",
      "| EpRewMean       | -91.6          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 3238           |\n",
      "| TimeElapsed     | 1.09e+03       |\n",
      "| TimestepsSoFar  | 339968         |\n",
      "| ev_tdlam_before | 0.798          |\n",
      "| loss_ent        | 0.13985878     |\n",
      "| loss_kl         | 0.0017281089   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00041493808 |\n",
      "| loss_vf_loss    | 45.08605       |\n",
      "------------------------------------\n",
      "********** Iteration 83 ************\n",
      "Eval num_timesteps=339968, episode_reward=-80.60 +/- 6.02\n",
      "Episode length: 81.60 +/- 6.02\n",
      "Eval num_timesteps=339968, episode_reward=-101.30 +/- 62.14\n",
      "Episode length: 102.30 +/- 62.14\n",
      "Eval num_timesteps=339968, episode_reward=-77.70 +/- 6.87\n",
      "Episode length: 78.70 +/- 6.87\n",
      "Eval num_timesteps=339968, episode_reward=-83.60 +/- 20.13\n",
      "Episode length: 84.60 +/- 20.13\n",
      "Eval num_timesteps=339968, episode_reward=-84.60 +/- 24.55\n",
      "Episode length: 85.60 +/- 24.55\n",
      "Eval num_timesteps=339968, episode_reward=-93.90 +/- 30.07\n",
      "Episode length: 94.90 +/- 30.07\n",
      "Eval num_timesteps=339968, episode_reward=-93.60 +/- 46.39\n",
      "Episode length: 94.60 +/- 46.39\n",
      "Eval num_timesteps=339968, episode_reward=-74.90 +/- 8.62\n",
      "Episode length: 75.90 +/- 8.62\n",
      "Eval num_timesteps=339968, episode_reward=-80.60 +/- 9.47\n",
      "Episode length: 81.60 +/- 9.47\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00021 |       0.00000 |      38.09523 |       0.00143 |       0.14093\n",
      "      0.00046 |       0.00000 |      37.72474 |       0.00139 |       0.14269\n",
      "      0.00059 |       0.00000 |      37.70127 |       0.00150 |       0.14255\n",
      "      0.00033 |       0.00000 |      37.52755 |       0.00125 |       0.14204\n",
      "     -0.00041 |       0.00000 |      37.44659 |       0.00131 |       0.14269\n",
      "      0.00117 |       0.00000 |      37.52832 |       0.00139 |       0.14209\n",
      "      0.00140 |       0.00000 |      37.29340 |       0.00144 |       0.14281\n",
      "      0.00023 |       0.00000 |      37.23641 |       0.00137 |       0.14261\n",
      "     -0.00067 |       0.00000 |      37.15903 |       0.00152 |       0.14303\n",
      "      0.00054 |       0.00000 |      37.14230 |       0.00162 |       0.14444\n",
      "Evaluating losses...\n",
      "     -0.00060 |       0.00000 |      36.90606 |       0.00153 |       0.14376\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.7           |\n",
      "| EpRewMean       | -89.7          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 3284           |\n",
      "| TimeElapsed     | 1.1e+03        |\n",
      "| TimestepsSoFar  | 344064         |\n",
      "| ev_tdlam_before | 0.843          |\n",
      "| loss_ent        | 0.14375646     |\n",
      "| loss_kl         | 0.0015279272   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00060396804 |\n",
      "| loss_vf_loss    | 36.906063      |\n",
      "------------------------------------\n",
      "********** Iteration 84 ************\n",
      "Eval num_timesteps=344064, episode_reward=-81.30 +/- 15.50\n",
      "Episode length: 82.30 +/- 15.50\n",
      "Eval num_timesteps=344064, episode_reward=-82.40 +/- 28.28\n",
      "Episode length: 83.40 +/- 28.28\n",
      "Eval num_timesteps=344064, episode_reward=-78.50 +/- 13.60\n",
      "Episode length: 79.50 +/- 13.60\n",
      "Eval num_timesteps=344064, episode_reward=-76.70 +/- 13.09\n",
      "Episode length: 77.70 +/- 13.09\n",
      "Eval num_timesteps=344064, episode_reward=-76.70 +/- 4.08\n",
      "Episode length: 77.70 +/- 4.08\n",
      "Eval num_timesteps=344064, episode_reward=-94.40 +/- 25.17\n",
      "Episode length: 95.40 +/- 25.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=344064, episode_reward=-81.70 +/- 23.53\n",
      "Episode length: 82.70 +/- 23.53\n",
      "Eval num_timesteps=344064, episode_reward=-77.80 +/- 5.02\n",
      "Episode length: 78.80 +/- 5.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00219 |       0.00000 |      44.12963 |       0.00133 |       0.14282\n",
      "      0.00101 |       0.00000 |      43.49828 |       0.00125 |       0.14351\n",
      "     -0.00012 |       0.00000 |      43.16460 |       0.00132 |       0.14334\n",
      "      0.00050 |       0.00000 |      42.65399 |       0.00133 |       0.14399\n",
      "      0.00094 |       0.00000 |      42.35358 |       0.00141 |       0.14527\n",
      "      0.00082 |       0.00000 |      42.08713 |       0.00133 |       0.14544\n",
      "      0.00034 |       0.00000 |      41.76685 |       0.00130 |       0.14489\n",
      "      0.00097 |       0.00000 |      41.67020 |       0.00145 |       0.14410\n",
      "      0.00018 |       0.00000 |      41.60620 |       0.00132 |       0.14517\n",
      "     -0.00041 |       0.00000 |      41.45276 |       0.00150 |       0.14587\n",
      "Evaluating losses...\n",
      "      0.00019 |       0.00000 |      41.48996 |       0.00145 |       0.14565\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.5         |\n",
      "| EpRewMean       | -91.5        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 3329         |\n",
      "| TimeElapsed     | 1.12e+03     |\n",
      "| TimestepsSoFar  | 348160       |\n",
      "| ev_tdlam_before | 0.817        |\n",
      "| loss_ent        | 0.14564785   |\n",
      "| loss_kl         | 0.0014542317 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0001866708 |\n",
      "| loss_vf_loss    | 41.489956    |\n",
      "----------------------------------\n",
      "********** Iteration 85 ************\n",
      "Eval num_timesteps=348160, episode_reward=-76.50 +/- 11.87\n",
      "Episode length: 77.50 +/- 11.87\n",
      "Eval num_timesteps=348160, episode_reward=-104.00 +/- 56.69\n",
      "Episode length: 105.00 +/- 56.69\n",
      "Eval num_timesteps=348160, episode_reward=-75.00 +/- 6.51\n",
      "Episode length: 76.00 +/- 6.51\n",
      "Eval num_timesteps=348160, episode_reward=-75.10 +/- 11.60\n",
      "Episode length: 76.10 +/- 11.60\n",
      "Eval num_timesteps=348160, episode_reward=-75.30 +/- 7.94\n",
      "Episode length: 76.30 +/- 7.94\n",
      "Eval num_timesteps=348160, episode_reward=-76.40 +/- 7.16\n",
      "Episode length: 77.40 +/- 7.16\n",
      "Eval num_timesteps=348160, episode_reward=-78.00 +/- 9.40\n",
      "Episode length: 79.00 +/- 9.40\n",
      "Eval num_timesteps=348160, episode_reward=-81.00 +/- 10.09\n",
      "Episode length: 82.00 +/- 10.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00098 |       0.00000 |      46.03989 |       0.00121 |       0.14722\n",
      "      0.00070 |       0.00000 |      45.61017 |       0.00148 |       0.14608\n",
      "     -0.00024 |       0.00000 |      45.61154 |       0.00137 |       0.14649\n",
      "      0.00075 |       0.00000 |      45.02868 |       0.00143 |       0.14522\n",
      "      0.00048 |       0.00000 |      45.10735 |       0.00168 |       0.14489\n",
      "      0.00110 |       0.00000 |      44.94854 |       0.00163 |       0.14382\n",
      "      0.00102 |       0.00000 |      44.90159 |       0.00160 |       0.14493\n",
      "     -0.00038 |       0.00000 |      44.70159 |       0.00178 |       0.14597\n",
      "    -1.70e-07 |       0.00000 |      44.72871 |       0.00180 |       0.14542\n",
      "     -0.00049 |       0.00000 |      44.59839 |       0.00174 |       0.14518\n",
      "Evaluating losses...\n",
      "     -0.00145 |       0.00000 |      44.43370 |       0.00176 |       0.14510\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.7          |\n",
      "| EpRewMean       | -89.7         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 3373          |\n",
      "| TimeElapsed     | 1.13e+03      |\n",
      "| TimestepsSoFar  | 352256        |\n",
      "| ev_tdlam_before | 0.811         |\n",
      "| loss_ent        | 0.14509724    |\n",
      "| loss_kl         | 0.0017611835  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0014503201 |\n",
      "| loss_vf_loss    | 44.433704     |\n",
      "-----------------------------------\n",
      "********** Iteration 86 ************\n",
      "Eval num_timesteps=352256, episode_reward=-78.50 +/- 8.59\n",
      "Episode length: 79.50 +/- 8.59\n",
      "Eval num_timesteps=352256, episode_reward=-75.00 +/- 8.66\n",
      "Episode length: 76.00 +/- 8.66\n",
      "Eval num_timesteps=352256, episode_reward=-72.50 +/- 8.33\n",
      "Episode length: 73.50 +/- 8.33\n",
      "Eval num_timesteps=352256, episode_reward=-75.20 +/- 7.30\n",
      "Episode length: 76.20 +/- 7.30\n",
      "Eval num_timesteps=352256, episode_reward=-84.70 +/- 27.62\n",
      "Episode length: 85.70 +/- 27.62\n",
      "Eval num_timesteps=352256, episode_reward=-92.70 +/- 26.39\n",
      "Episode length: 93.70 +/- 26.39\n",
      "Eval num_timesteps=352256, episode_reward=-74.70 +/- 5.76\n",
      "Episode length: 75.70 +/- 5.76\n",
      "Eval num_timesteps=352256, episode_reward=-120.30 +/- 48.18\n",
      "Episode length: 121.30 +/- 48.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00041 |       0.00000 |      37.77449 |       0.00116 |       0.14206\n",
      "      0.00039 |       0.00000 |      36.69189 |       0.00136 |       0.14353\n",
      "      0.00096 |       0.00000 |      35.72943 |       0.00140 |       0.14269\n",
      "      0.00052 |       0.00000 |      35.23400 |       0.00146 |       0.14327\n",
      "      0.00049 |       0.00000 |      34.61491 |       0.00132 |       0.14406\n",
      "      0.00120 |       0.00000 |      34.34775 |       0.00135 |       0.14352\n",
      "      0.00085 |       0.00000 |      33.79587 |       0.00132 |       0.14435\n",
      "     -0.00030 |       0.00000 |      33.64253 |       0.00168 |       0.14387\n",
      "     3.40e-05 |       0.00000 |      33.47287 |       0.00162 |       0.14499\n",
      "      0.00024 |       0.00000 |      33.21126 |       0.00161 |       0.14478\n",
      "Evaluating losses...\n",
      "     8.28e-05 |       0.00000 |      33.25900 |       0.00158 |       0.14595\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.6         |\n",
      "| EpRewMean       | -87.6        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 3418         |\n",
      "| TimeElapsed     | 1.14e+03     |\n",
      "| TimestepsSoFar  | 356352       |\n",
      "| ev_tdlam_before | 0.844        |\n",
      "| loss_ent        | 0.14595188   |\n",
      "| loss_kl         | 0.0015830472 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 8.282799e-05 |\n",
      "| loss_vf_loss    | 33.259003    |\n",
      "----------------------------------\n",
      "********** Iteration 87 ************\n",
      "Eval num_timesteps=356352, episode_reward=-87.50 +/- 19.29\n",
      "Episode length: 88.50 +/- 19.29\n",
      "Eval num_timesteps=356352, episode_reward=-74.00 +/- 8.68\n",
      "Episode length: 75.00 +/- 8.68\n",
      "Eval num_timesteps=356352, episode_reward=-78.70 +/- 9.75\n",
      "Episode length: 79.70 +/- 9.75\n",
      "Eval num_timesteps=356352, episode_reward=-79.20 +/- 8.00\n",
      "Episode length: 80.20 +/- 8.00\n",
      "Eval num_timesteps=356352, episode_reward=-78.10 +/- 5.82\n",
      "Episode length: 79.10 +/- 5.82\n",
      "Eval num_timesteps=356352, episode_reward=-85.40 +/- 24.14\n",
      "Episode length: 86.40 +/- 24.14\n",
      "Eval num_timesteps=356352, episode_reward=-83.10 +/- 7.54\n",
      "Episode length: 84.10 +/- 7.54\n",
      "Eval num_timesteps=356352, episode_reward=-74.80 +/- 4.98\n",
      "Episode length: 75.80 +/- 4.98\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00102 |       0.00000 |      40.61751 |       0.00127 |       0.14630\n",
      "      0.00074 |       0.00000 |      40.39922 |       0.00135 |       0.14673\n",
      "      0.00128 |       0.00000 |      39.93249 |       0.00145 |       0.14718\n",
      "      0.00062 |       0.00000 |      40.06499 |       0.00146 |       0.14809\n",
      "     1.25e-05 |       0.00000 |      39.74902 |       0.00143 |       0.14882\n",
      "      0.00116 |       0.00000 |      39.59689 |       0.00141 |       0.14804\n",
      "      0.00045 |       0.00000 |      39.57185 |       0.00157 |       0.14709\n",
      "      0.00047 |       0.00000 |      39.43082 |       0.00150 |       0.14800\n",
      "      0.00016 |       0.00000 |      39.24675 |       0.00156 |       0.14816\n",
      "     -0.00039 |       0.00000 |      39.21902 |       0.00155 |       0.14840\n",
      "Evaluating losses...\n",
      "     -0.00064 |       0.00000 |      39.27310 |       0.00142 |       0.14863\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.4          |\n",
      "| EpRewMean       | -89.4         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 3464          |\n",
      "| TimeElapsed     | 1.15e+03      |\n",
      "| TimestepsSoFar  | 360448        |\n",
      "| ev_tdlam_before | 0.839         |\n",
      "| loss_ent        | 0.14863078    |\n",
      "| loss_kl         | 0.0014156128  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0006436525 |\n",
      "| loss_vf_loss    | 39.2731       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 88 ************\n",
      "Eval num_timesteps=360448, episode_reward=-85.70 +/- 18.53\n",
      "Episode length: 86.70 +/- 18.53\n",
      "Eval num_timesteps=360448, episode_reward=-74.60 +/- 8.95\n",
      "Episode length: 75.60 +/- 8.95\n",
      "Eval num_timesteps=360448, episode_reward=-89.80 +/- 26.10\n",
      "Episode length: 90.80 +/- 26.10\n",
      "Eval num_timesteps=360448, episode_reward=-75.70 +/- 6.69\n",
      "Episode length: 76.70 +/- 6.69\n",
      "Eval num_timesteps=360448, episode_reward=-88.30 +/- 33.51\n",
      "Episode length: 89.30 +/- 33.51\n",
      "Eval num_timesteps=360448, episode_reward=-75.00 +/- 7.40\n",
      "Episode length: 76.00 +/- 7.40\n",
      "Eval num_timesteps=360448, episode_reward=-83.90 +/- 25.04\n",
      "Episode length: 84.90 +/- 25.04\n",
      "Eval num_timesteps=360448, episode_reward=-85.40 +/- 22.65\n",
      "Episode length: 86.40 +/- 22.65\n",
      "Eval num_timesteps=360448, episode_reward=-78.50 +/- 7.31\n",
      "Episode length: 79.50 +/- 7.31\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00100 |       0.00000 |      27.95963 |       0.00127 |       0.14849\n",
      "      0.00031 |       0.00000 |      27.54618 |       0.00153 |       0.14715\n",
      "     -0.00016 |       0.00000 |      27.09314 |       0.00156 |       0.14759\n",
      "      0.00035 |       0.00000 |      26.77221 |       0.00164 |       0.14662\n",
      "      0.00139 |       0.00000 |      26.50806 |       0.00200 |       0.14584\n",
      "      0.00043 |       0.00000 |      26.41455 |       0.00149 |       0.14676\n",
      "      0.00115 |       0.00000 |      26.15469 |       0.00174 |       0.14651\n",
      "      0.00119 |       0.00000 |      25.91865 |       0.00165 |       0.14638\n",
      "      0.00035 |       0.00000 |      25.78212 |       0.00166 |       0.14611\n",
      "      0.00046 |       0.00000 |      25.63779 |       0.00178 |       0.14673\n",
      "Evaluating losses...\n",
      "      0.00107 |       0.00000 |      25.60010 |       0.00179 |       0.14540\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.6         |\n",
      "| EpRewMean       | -88.6        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 3511         |\n",
      "| TimeElapsed     | 1.16e+03     |\n",
      "| TimestepsSoFar  | 364544       |\n",
      "| ev_tdlam_before | 0.89         |\n",
      "| loss_ent        | 0.14539683   |\n",
      "| loss_kl         | 0.0017904987 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0010681632 |\n",
      "| loss_vf_loss    | 25.600103    |\n",
      "----------------------------------\n",
      "********** Iteration 89 ************\n",
      "Eval num_timesteps=364544, episode_reward=-71.70 +/- 6.21\n",
      "Episode length: 72.70 +/- 6.21\n",
      "Eval num_timesteps=364544, episode_reward=-73.70 +/- 6.75\n",
      "Episode length: 74.70 +/- 6.75\n",
      "Eval num_timesteps=364544, episode_reward=-86.10 +/- 22.29\n",
      "Episode length: 87.10 +/- 22.29\n",
      "Eval num_timesteps=364544, episode_reward=-78.80 +/- 11.48\n",
      "Episode length: 79.80 +/- 11.48\n",
      "Eval num_timesteps=364544, episode_reward=-94.20 +/- 54.42\n",
      "Episode length: 95.20 +/- 54.42\n",
      "Eval num_timesteps=364544, episode_reward=-77.90 +/- 9.32\n",
      "Episode length: 78.90 +/- 9.32\n",
      "Eval num_timesteps=364544, episode_reward=-77.30 +/- 5.88\n",
      "Episode length: 78.30 +/- 5.88\n",
      "Eval num_timesteps=364544, episode_reward=-80.00 +/- 10.77\n",
      "Episode length: 81.00 +/- 10.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -7.19e-05 |       0.00000 |      41.97454 |       0.00141 |       0.15488\n",
      "     9.46e-05 |       0.00000 |      41.18510 |       0.00150 |       0.15480\n",
      "      0.00085 |       0.00000 |      40.97927 |       0.00148 |       0.15449\n",
      "     -0.00037 |       0.00000 |      40.64037 |       0.00164 |       0.15358\n",
      "     -0.00037 |       0.00000 |      40.38848 |       0.00167 |       0.15319\n",
      "      0.00039 |       0.00000 |      40.07710 |       0.00171 |       0.15347\n",
      "      0.00144 |       0.00000 |      40.26692 |       0.00173 |       0.15371\n",
      "      0.00050 |       0.00000 |      39.92057 |       0.00187 |       0.15322\n",
      "     -0.00052 |       0.00000 |      39.82088 |       0.00194 |       0.15288\n",
      "     -0.00096 |       0.00000 |      39.82040 |       0.00182 |       0.15356\n",
      "Evaluating losses...\n",
      "     -0.00110 |       0.00000 |      39.71328 |       0.00171 |       0.15372\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.7         |\n",
      "| EpRewMean       | -86.7        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 3558         |\n",
      "| TimeElapsed     | 1.18e+03     |\n",
      "| TimestepsSoFar  | 368640       |\n",
      "| ev_tdlam_before | 0.831        |\n",
      "| loss_ent        | 0.1537187    |\n",
      "| loss_kl         | 0.0017117705 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.001098894 |\n",
      "| loss_vf_loss    | 39.713276    |\n",
      "----------------------------------\n",
      "********** Iteration 90 ************\n",
      "Eval num_timesteps=368640, episode_reward=-75.00 +/- 5.85\n",
      "Episode length: 76.00 +/- 5.85\n",
      "Eval num_timesteps=368640, episode_reward=-81.00 +/- 10.62\n",
      "Episode length: 82.00 +/- 10.62\n",
      "Eval num_timesteps=368640, episode_reward=-80.90 +/- 11.53\n",
      "Episode length: 81.90 +/- 11.53\n",
      "Eval num_timesteps=368640, episode_reward=-91.60 +/- 30.94\n",
      "Episode length: 92.60 +/- 30.94\n",
      "Eval num_timesteps=368640, episode_reward=-82.20 +/- 14.93\n",
      "Episode length: 83.20 +/- 14.93\n",
      "Eval num_timesteps=368640, episode_reward=-79.80 +/- 14.75\n",
      "Episode length: 80.80 +/- 14.75\n",
      "Eval num_timesteps=368640, episode_reward=-80.90 +/- 6.95\n",
      "Episode length: 81.90 +/- 6.95\n",
      "Eval num_timesteps=368640, episode_reward=-85.90 +/- 31.28\n",
      "Episode length: 86.90 +/- 31.28\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00153 |       0.00000 |      45.17227 |       0.00127 |       0.14278\n",
      "      0.00015 |       0.00000 |      44.56666 |       0.00135 |       0.14413\n",
      "      0.00157 |       0.00000 |      43.94106 |       0.00146 |       0.14515\n",
      "      0.00174 |       0.00000 |      43.66959 |       0.00146 |       0.14461\n",
      "      0.00081 |       0.00000 |      43.38814 |       0.00161 |       0.14508\n",
      "      0.00059 |       0.00000 |      43.03913 |       0.00164 |       0.14587\n",
      "    -8.54e-06 |       0.00000 |      42.85998 |       0.00155 |       0.14600\n",
      "     -0.00024 |       0.00000 |      42.67209 |       0.00171 |       0.14690\n",
      "      0.00181 |       0.00000 |      42.48178 |       0.00169 |       0.14708\n",
      "      0.00082 |       0.00000 |      42.34372 |       0.00172 |       0.14749\n",
      "Evaluating losses...\n",
      "     -0.00013 |       0.00000 |      42.19778 |       0.00177 |       0.14670\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.2           |\n",
      "| EpRewMean       | -89.2          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 3601           |\n",
      "| TimeElapsed     | 1.19e+03       |\n",
      "| TimestepsSoFar  | 372736         |\n",
      "| ev_tdlam_before | 0.807          |\n",
      "| loss_ent        | 0.14670075     |\n",
      "| loss_kl         | 0.0017682487   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00013010786 |\n",
      "| loss_vf_loss    | 42.19778       |\n",
      "------------------------------------\n",
      "********** Iteration 91 ************\n",
      "Eval num_timesteps=372736, episode_reward=-81.70 +/- 9.78\n",
      "Episode length: 82.70 +/- 9.78\n",
      "Eval num_timesteps=372736, episode_reward=-76.50 +/- 8.63\n",
      "Episode length: 77.50 +/- 8.63\n",
      "Eval num_timesteps=372736, episode_reward=-81.10 +/- 13.44\n",
      "Episode length: 82.10 +/- 13.44\n",
      "Eval num_timesteps=372736, episode_reward=-82.80 +/- 26.26\n",
      "Episode length: 83.80 +/- 26.26\n",
      "Eval num_timesteps=372736, episode_reward=-89.30 +/- 33.50\n",
      "Episode length: 90.30 +/- 33.50\n",
      "Eval num_timesteps=372736, episode_reward=-76.20 +/- 5.21\n",
      "Episode length: 77.20 +/- 5.21\n",
      "Eval num_timesteps=372736, episode_reward=-77.80 +/- 6.35\n",
      "Episode length: 78.80 +/- 6.35\n",
      "Eval num_timesteps=372736, episode_reward=-96.30 +/- 52.14\n",
      "Episode length: 97.30 +/- 52.14\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00042 |       0.00000 |      56.58852 |       0.00139 |       0.14639\n",
      "      0.00121 |       0.00000 |      55.87499 |       0.00132 |       0.14521\n",
      "      0.00040 |       0.00000 |      55.45129 |       0.00146 |       0.14256\n",
      "      0.00019 |       0.00000 |      55.12463 |       0.00154 |       0.14238\n",
      "      0.00053 |       0.00000 |      54.95797 |       0.00167 |       0.14193\n",
      "      0.00041 |       0.00000 |      54.73867 |       0.00176 |       0.14330\n",
      "      0.00084 |       0.00000 |      54.56774 |       0.00175 |       0.14429\n",
      "      0.00105 |       0.00000 |      54.30566 |       0.00172 |       0.14322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     -0.00052 |       0.00000 |      54.28536 |       0.00182 |       0.14359\n",
      "     8.57e-05 |       0.00000 |      54.03745 |       0.00189 |       0.14238\n",
      "Evaluating losses...\n",
      "     -0.00096 |       0.00000 |      53.88383 |       0.00173 |       0.14338\n",
      "------------------------------------\n",
      "| EpLenMean       | 94.3           |\n",
      "| EpRewMean       | -93.3          |\n",
      "| EpThisIter      | 43             |\n",
      "| EpisodesSoFar   | 3644           |\n",
      "| TimeElapsed     | 1.2e+03        |\n",
      "| TimestepsSoFar  | 376832         |\n",
      "| ev_tdlam_before | 0.748          |\n",
      "| loss_ent        | 0.14337818     |\n",
      "| loss_kl         | 0.0017319014   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00096492236 |\n",
      "| loss_vf_loss    | 53.883835      |\n",
      "------------------------------------\n",
      "********** Iteration 92 ************\n",
      "Eval num_timesteps=376832, episode_reward=-74.30 +/- 9.23\n",
      "Episode length: 75.30 +/- 9.23\n",
      "Eval num_timesteps=376832, episode_reward=-78.80 +/- 9.66\n",
      "Episode length: 79.80 +/- 9.66\n",
      "Eval num_timesteps=376832, episode_reward=-74.90 +/- 5.52\n",
      "Episode length: 75.90 +/- 5.52\n",
      "Eval num_timesteps=376832, episode_reward=-76.20 +/- 10.15\n",
      "Episode length: 77.20 +/- 10.15\n",
      "Eval num_timesteps=376832, episode_reward=-74.40 +/- 5.99\n",
      "Episode length: 75.40 +/- 5.99\n",
      "Eval num_timesteps=376832, episode_reward=-77.10 +/- 11.08\n",
      "Episode length: 78.10 +/- 11.08\n",
      "Eval num_timesteps=376832, episode_reward=-72.50 +/- 5.26\n",
      "Episode length: 73.50 +/- 5.26\n",
      "Eval num_timesteps=376832, episode_reward=-75.80 +/- 8.08\n",
      "Episode length: 76.80 +/- 8.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00155 |       0.00000 |      48.37988 |       0.00131 |       0.14473\n",
      "      0.00345 |       0.00000 |      47.87965 |       0.00136 |       0.14126\n",
      "      0.00067 |       0.00000 |      47.54881 |       0.00132 |       0.14169\n",
      "      0.00076 |       0.00000 |      47.04085 |       0.00151 |       0.14239\n",
      "      0.00155 |       0.00000 |      46.91797 |       0.00156 |       0.14182\n",
      "      0.00048 |       0.00000 |      46.77829 |       0.00153 |       0.14122\n",
      "      0.00105 |       0.00000 |      46.59998 |       0.00156 |       0.14052\n",
      "      0.00125 |       0.00000 |      46.25098 |       0.00160 |       0.14148\n",
      "      0.00148 |       0.00000 |      46.12963 |       0.00164 |       0.14103\n",
      "     -0.00107 |       0.00000 |      46.09077 |       0.00167 |       0.13980\n",
      "Evaluating losses...\n",
      "      0.00092 |       0.00000 |      45.77468 |       0.00173 |       0.14000\n",
      "----------------------------------\n",
      "| EpLenMean       | 95           |\n",
      "| EpRewMean       | -94          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 3688         |\n",
      "| TimeElapsed     | 1.21e+03     |\n",
      "| TimestepsSoFar  | 380928       |\n",
      "| ev_tdlam_before | 0.79         |\n",
      "| loss_ent        | 0.14000167   |\n",
      "| loss_kl         | 0.0017269102 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0009170616 |\n",
      "| loss_vf_loss    | 45.774677    |\n",
      "----------------------------------\n",
      "********** Iteration 93 ************\n",
      "Eval num_timesteps=380928, episode_reward=-74.30 +/- 7.67\n",
      "Episode length: 75.30 +/- 7.67\n",
      "Eval num_timesteps=380928, episode_reward=-81.70 +/- 7.25\n",
      "Episode length: 82.70 +/- 7.25\n",
      "Eval num_timesteps=380928, episode_reward=-83.30 +/- 10.55\n",
      "Episode length: 84.30 +/- 10.55\n",
      "Eval num_timesteps=380928, episode_reward=-89.80 +/- 44.71\n",
      "Episode length: 90.80 +/- 44.71\n",
      "Eval num_timesteps=380928, episode_reward=-79.40 +/- 8.79\n",
      "Episode length: 80.40 +/- 8.79\n",
      "Eval num_timesteps=380928, episode_reward=-77.40 +/- 10.59\n",
      "Episode length: 78.40 +/- 10.59\n",
      "Eval num_timesteps=380928, episode_reward=-78.70 +/- 11.97\n",
      "Episode length: 79.70 +/- 11.97\n",
      "Eval num_timesteps=380928, episode_reward=-76.90 +/- 10.78\n",
      "Episode length: 77.90 +/- 10.78\n",
      "Eval num_timesteps=380928, episode_reward=-73.30 +/- 10.86\n",
      "Episode length: 74.30 +/- 10.86\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00029 |       0.00000 |      48.74686 |       0.00126 |       0.12729\n",
      "     -0.00024 |       0.00000 |      47.65471 |       0.00131 |       0.12672\n",
      "      0.00011 |       0.00000 |      47.18534 |       0.00132 |       0.12696\n",
      "      0.00166 |       0.00000 |      46.93088 |       0.00123 |       0.12623\n",
      "     7.68e-05 |       0.00000 |      46.76738 |       0.00135 |       0.12636\n",
      "      0.00034 |       0.00000 |      46.61281 |       0.00139 |       0.12545\n",
      "      0.00065 |       0.00000 |      46.36868 |       0.00127 |       0.12576\n",
      "      0.00026 |       0.00000 |      46.60580 |       0.00141 |       0.12686\n",
      "     3.39e-05 |       0.00000 |      46.25171 |       0.00148 |       0.12669\n",
      "      0.00066 |       0.00000 |      46.33169 |       0.00167 |       0.12662\n",
      "Evaluating losses...\n",
      "     -0.00034 |       0.00000 |      46.31486 |       0.00148 |       0.12629\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.7          |\n",
      "| EpRewMean       | -94.7         |\n",
      "| EpThisIter      | 42            |\n",
      "| EpisodesSoFar   | 3730          |\n",
      "| TimeElapsed     | 1.22e+03      |\n",
      "| TimestepsSoFar  | 385024        |\n",
      "| ev_tdlam_before | 0.791         |\n",
      "| loss_ent        | 0.12629147    |\n",
      "| loss_kl         | 0.0014816123  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0003431911 |\n",
      "| loss_vf_loss    | 46.31486      |\n",
      "-----------------------------------\n",
      "********** Iteration 94 ************\n",
      "Eval num_timesteps=385024, episode_reward=-76.70 +/- 8.96\n",
      "Episode length: 77.70 +/- 8.96\n",
      "Eval num_timesteps=385024, episode_reward=-77.20 +/- 6.90\n",
      "Episode length: 78.20 +/- 6.90\n",
      "Eval num_timesteps=385024, episode_reward=-90.60 +/- 25.85\n",
      "Episode length: 91.60 +/- 25.85\n",
      "Eval num_timesteps=385024, episode_reward=-72.80 +/- 6.35\n",
      "Episode length: 73.80 +/- 6.35\n",
      "Eval num_timesteps=385024, episode_reward=-78.10 +/- 11.59\n",
      "Episode length: 79.10 +/- 11.59\n",
      "Eval num_timesteps=385024, episode_reward=-67.90 +/- 5.47\n",
      "Episode length: 68.90 +/- 5.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=385024, episode_reward=-73.00 +/- 4.75\n",
      "Episode length: 74.00 +/- 4.75\n",
      "Eval num_timesteps=385024, episode_reward=-77.90 +/- 7.56\n",
      "Episode length: 78.90 +/- 7.56\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00060 |       0.00000 |      38.94843 |       0.00139 |       0.13910\n",
      "      0.00146 |       0.00000 |      38.08438 |       0.00148 |       0.13935\n",
      "      0.00034 |       0.00000 |      37.82027 |       0.00168 |       0.13960\n",
      "      0.00010 |       0.00000 |      37.56438 |       0.00203 |       0.14036\n",
      "     -0.00068 |       0.00000 |      37.28343 |       0.00183 |       0.14145\n",
      "      0.00056 |       0.00000 |      36.78772 |       0.00203 |       0.14185\n",
      "      0.00024 |       0.00000 |      36.83270 |       0.00202 |       0.14213\n",
      "      0.00056 |       0.00000 |      36.85598 |       0.00196 |       0.14289\n",
      "      0.00077 |       0.00000 |      36.66903 |       0.00202 |       0.14241\n",
      "     5.45e-05 |       0.00000 |      36.60786 |       0.00211 |       0.14259\n",
      "Evaluating losses...\n",
      "     -0.00011 |       0.00000 |      36.55883 |       0.00233 |       0.14289\n",
      "-------------------------------------\n",
      "| EpLenMean       | 92.8            |\n",
      "| EpRewMean       | -91.8           |\n",
      "| EpThisIter      | 47              |\n",
      "| EpisodesSoFar   | 3777            |\n",
      "| TimeElapsed     | 1.23e+03        |\n",
      "| TimestepsSoFar  | 389120          |\n",
      "| ev_tdlam_before | 0.848           |\n",
      "| loss_ent        | 0.14288975      |\n",
      "| loss_kl         | 0.0023317968    |\n",
      "| loss_pol_entpen | 0.0             |\n",
      "| loss_pol_surr   | -0.000107899774 |\n",
      "| loss_vf_loss    | 36.55883        |\n",
      "-------------------------------------\n",
      "********** Iteration 95 ************\n",
      "Eval num_timesteps=389120, episode_reward=-102.00 +/- 95.43\n",
      "Episode length: 103.00 +/- 95.43\n",
      "Eval num_timesteps=389120, episode_reward=-77.60 +/- 11.68\n",
      "Episode length: 78.60 +/- 11.68\n",
      "Eval num_timesteps=389120, episode_reward=-71.90 +/- 5.26\n",
      "Episode length: 72.90 +/- 5.26\n",
      "Eval num_timesteps=389120, episode_reward=-76.20 +/- 7.36\n",
      "Episode length: 77.20 +/- 7.36\n",
      "Eval num_timesteps=389120, episode_reward=-80.70 +/- 20.78\n",
      "Episode length: 81.70 +/- 20.78\n",
      "Eval num_timesteps=389120, episode_reward=-84.80 +/- 30.55\n",
      "Episode length: 85.80 +/- 30.55\n",
      "Eval num_timesteps=389120, episode_reward=-74.70 +/- 7.47\n",
      "Episode length: 75.70 +/- 7.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=389120, episode_reward=-106.50 +/- 56.50\n",
      "Episode length: 107.50 +/- 56.50\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00109 |       0.00000 |      30.30678 |       0.00140 |       0.14806\n",
      "      0.00168 |       0.00000 |      29.86744 |       0.00149 |       0.14888\n",
      "      0.00072 |       0.00000 |      29.83146 |       0.00139 |       0.14949\n",
      "     -0.00024 |       0.00000 |      29.35267 |       0.00143 |       0.14802\n",
      "     4.21e-05 |       0.00000 |      29.42326 |       0.00160 |       0.14878\n",
      "      0.00025 |       0.00000 |      29.33521 |       0.00167 |       0.14896\n",
      "      0.00042 |       0.00000 |      28.98901 |       0.00166 |       0.14906\n",
      "     -0.00035 |       0.00000 |      29.05752 |       0.00157 |       0.14891\n",
      "     -0.00019 |       0.00000 |      28.90594 |       0.00154 |       0.14975\n",
      "     -0.00118 |       0.00000 |      28.80017 |       0.00160 |       0.15148\n",
      "Evaluating losses...\n",
      "     -0.00035 |       0.00000 |      28.81189 |       0.00167 |       0.15085\n",
      "------------------------------------\n",
      "| EpLenMean       | 89.4           |\n",
      "| EpRewMean       | -88.4          |\n",
      "| EpThisIter      | 48             |\n",
      "| EpisodesSoFar   | 3825           |\n",
      "| TimeElapsed     | 1.24e+03       |\n",
      "| TimestepsSoFar  | 393216         |\n",
      "| ev_tdlam_before | 0.885          |\n",
      "| loss_ent        | 0.15084909     |\n",
      "| loss_kl         | 0.0016717875   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00035467139 |\n",
      "| loss_vf_loss    | 28.811893      |\n",
      "------------------------------------\n",
      "********** Iteration 96 ************\n",
      "Eval num_timesteps=393216, episode_reward=-80.10 +/- 8.80\n",
      "Episode length: 81.10 +/- 8.80\n",
      "Eval num_timesteps=393216, episode_reward=-74.30 +/- 5.14\n",
      "Episode length: 75.30 +/- 5.14\n",
      "Eval num_timesteps=393216, episode_reward=-82.50 +/- 17.42\n",
      "Episode length: 83.50 +/- 17.42\n",
      "Eval num_timesteps=393216, episode_reward=-77.10 +/- 7.96\n",
      "Episode length: 78.10 +/- 7.96\n",
      "Eval num_timesteps=393216, episode_reward=-80.10 +/- 17.61\n",
      "Episode length: 81.10 +/- 17.61\n",
      "Eval num_timesteps=393216, episode_reward=-85.60 +/- 24.23\n",
      "Episode length: 86.60 +/- 24.23\n",
      "Eval num_timesteps=393216, episode_reward=-91.50 +/- 38.41\n",
      "Episode length: 92.50 +/- 38.41\n",
      "Eval num_timesteps=393216, episode_reward=-76.70 +/- 7.87\n",
      "Episode length: 77.70 +/- 7.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00072 |       0.00000 |      46.56327 |       0.00131 |       0.14633\n",
      "      0.00305 |       0.00000 |      44.50289 |       0.00142 |       0.14427\n",
      "      0.00144 |       0.00000 |      43.40047 |       0.00173 |       0.14304\n",
      "      0.00064 |       0.00000 |      42.47039 |       0.00153 |       0.14327\n",
      "     4.84e-05 |       0.00000 |      41.78122 |       0.00168 |       0.14153\n",
      "      0.00012 |       0.00000 |      41.28041 |       0.00161 |       0.14232\n",
      "      0.00118 |       0.00000 |      40.87276 |       0.00176 |       0.14208\n",
      "      0.00014 |       0.00000 |      40.47602 |       0.00167 |       0.14058\n",
      "      0.00083 |       0.00000 |      40.02765 |       0.00173 |       0.14124\n",
      "     -0.00069 |       0.00000 |      40.03118 |       0.00182 |       0.14067\n",
      "Evaluating losses...\n",
      "      0.00013 |       0.00000 |      39.73861 |       0.00200 |       0.14093\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.4          |\n",
      "| EpRewMean       | -87.4         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 3870          |\n",
      "| TimeElapsed     | 1.26e+03      |\n",
      "| TimestepsSoFar  | 397312        |\n",
      "| ev_tdlam_before | 0.801         |\n",
      "| loss_ent        | 0.14093399    |\n",
      "| loss_kl         | 0.002003229   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00013201387 |\n",
      "| loss_vf_loss    | 39.73861      |\n",
      "-----------------------------------\n",
      "********** Iteration 97 ************\n",
      "Eval num_timesteps=397312, episode_reward=-95.60 +/- 55.72\n",
      "Episode length: 96.60 +/- 55.72\n",
      "Eval num_timesteps=397312, episode_reward=-81.50 +/- 21.19\n",
      "Episode length: 82.50 +/- 21.19\n",
      "Eval num_timesteps=397312, episode_reward=-78.10 +/- 16.56\n",
      "Episode length: 79.10 +/- 16.56\n",
      "Eval num_timesteps=397312, episode_reward=-76.40 +/- 6.58\n",
      "Episode length: 77.40 +/- 6.58\n",
      "Eval num_timesteps=397312, episode_reward=-79.10 +/- 10.57\n",
      "Episode length: 80.10 +/- 10.57\n",
      "Eval num_timesteps=397312, episode_reward=-92.30 +/- 39.63\n",
      "Episode length: 93.30 +/- 39.63\n",
      "Eval num_timesteps=397312, episode_reward=-76.50 +/- 7.47\n",
      "Episode length: 77.50 +/- 7.47\n",
      "Eval num_timesteps=397312, episode_reward=-85.00 +/- 25.43\n",
      "Episode length: 86.00 +/- 25.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00121 |       0.00000 |      31.63756 |       0.00143 |       0.14626\n",
      "      0.00110 |       0.00000 |      31.05293 |       0.00136 |       0.14731\n",
      "      0.00069 |       0.00000 |      31.11653 |       0.00148 |       0.14700\n",
      "      0.00223 |       0.00000 |      31.07199 |       0.00143 |       0.14560\n",
      "      0.00085 |       0.00000 |      30.82557 |       0.00151 |       0.14526\n",
      "      0.00061 |       0.00000 |      30.74363 |       0.00163 |       0.14490\n",
      "      0.00102 |       0.00000 |      30.71719 |       0.00179 |       0.14449\n",
      "      0.00010 |       0.00000 |      30.56821 |       0.00174 |       0.14417\n",
      "      0.00020 |       0.00000 |      30.39825 |       0.00155 |       0.14562\n",
      "      0.00039 |       0.00000 |      30.49566 |       0.00154 |       0.14413\n",
      "Evaluating losses...\n",
      "      0.00105 |       0.00000 |      30.47996 |       0.00171 |       0.14423\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.6         |\n",
      "| EpRewMean       | -86.6        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 3918         |\n",
      "| TimeElapsed     | 1.46e+03     |\n",
      "| TimestepsSoFar  | 401408       |\n",
      "| ev_tdlam_before | 0.878        |\n",
      "| loss_ent        | 0.1442284    |\n",
      "| loss_kl         | 0.0017092841 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0010498463 |\n",
      "| loss_vf_loss    | 30.479958    |\n",
      "----------------------------------\n",
      "********** Iteration 98 ************\n",
      "Eval num_timesteps=401408, episode_reward=-77.90 +/- 11.92\n",
      "Episode length: 78.90 +/- 11.92\n",
      "Eval num_timesteps=401408, episode_reward=-76.80 +/- 7.59\n",
      "Episode length: 77.80 +/- 7.59\n",
      "Eval num_timesteps=401408, episode_reward=-86.50 +/- 31.16\n",
      "Episode length: 87.50 +/- 31.16\n",
      "Eval num_timesteps=401408, episode_reward=-71.40 +/- 4.57\n",
      "Episode length: 72.40 +/- 4.57\n",
      "Eval num_timesteps=401408, episode_reward=-77.60 +/- 7.35\n",
      "Episode length: 78.60 +/- 7.35\n",
      "Eval num_timesteps=401408, episode_reward=-76.00 +/- 6.78\n",
      "Episode length: 77.00 +/- 6.78\n",
      "Eval num_timesteps=401408, episode_reward=-82.20 +/- 26.91\n",
      "Episode length: 83.20 +/- 26.91\n",
      "Eval num_timesteps=401408, episode_reward=-85.60 +/- 16.98\n",
      "Episode length: 86.60 +/- 16.98\n",
      "Eval num_timesteps=401408, episode_reward=-74.40 +/- 10.51\n",
      "Episode length: 75.40 +/- 10.51\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00090 |       0.00000 |      29.61275 |       0.00161 |       0.14648\n",
      "      0.00167 |       0.00000 |      29.33554 |       0.00153 |       0.14663\n",
      "      0.00205 |       0.00000 |      29.44761 |       0.00142 |       0.14649\n",
      "      0.00191 |       0.00000 |      29.30977 |       0.00158 |       0.14715\n",
      "      0.00102 |       0.00000 |      29.12866 |       0.00150 |       0.14782\n",
      "      0.00187 |       0.00000 |      29.16627 |       0.00149 |       0.14509\n",
      "      0.00079 |       0.00000 |      29.10079 |       0.00164 |       0.14463\n",
      "      0.00107 |       0.00000 |      29.05885 |       0.00155 |       0.14538\n",
      "      0.00048 |       0.00000 |      28.94461 |       0.00168 |       0.14517\n",
      "      0.00114 |       0.00000 |      28.95197 |       0.00160 |       0.14471\n",
      "Evaluating losses...\n",
      "      0.00153 |       0.00000 |      28.92723 |       0.00175 |       0.14503\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.2         |\n",
      "| EpRewMean       | -83.2        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 3967         |\n",
      "| TimeElapsed     | 1.48e+03     |\n",
      "| TimestepsSoFar  | 405504       |\n",
      "| ev_tdlam_before | 0.884        |\n",
      "| loss_ent        | 0.14502822   |\n",
      "| loss_kl         | 0.001747231  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0015267225 |\n",
      "| loss_vf_loss    | 28.92723     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 99 ************\n",
      "Eval num_timesteps=405504, episode_reward=-92.50 +/- 48.99\n",
      "Episode length: 93.50 +/- 48.99\n",
      "Eval num_timesteps=405504, episode_reward=-73.00 +/- 5.76\n",
      "Episode length: 74.00 +/- 5.76\n",
      "Eval num_timesteps=405504, episode_reward=-85.50 +/- 24.90\n",
      "Episode length: 86.50 +/- 24.90\n",
      "Eval num_timesteps=405504, episode_reward=-78.80 +/- 18.10\n",
      "Episode length: 79.80 +/- 18.10\n",
      "Eval num_timesteps=405504, episode_reward=-116.00 +/- 128.49\n",
      "Episode length: 116.90 +/- 128.19\n",
      "Eval num_timesteps=405504, episode_reward=-93.80 +/- 49.93\n",
      "Episode length: 94.80 +/- 49.93\n",
      "Eval num_timesteps=405504, episode_reward=-81.70 +/- 33.05\n",
      "Episode length: 82.70 +/- 33.05\n",
      "Eval num_timesteps=405504, episode_reward=-72.00 +/- 6.87\n",
      "Episode length: 73.00 +/- 6.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00017 |       0.00000 |      41.05684 |       0.00140 |       0.13802\n",
      "      0.00061 |       0.00000 |      40.77546 |       0.00140 |       0.13767\n",
      "      0.00205 |       0.00000 |      40.53019 |       0.00159 |       0.13763\n",
      "      0.00020 |       0.00000 |      40.53736 |       0.00181 |       0.13900\n",
      "     7.30e-05 |       0.00000 |      40.12603 |       0.00171 |       0.13931\n",
      "      0.00118 |       0.00000 |      39.93859 |       0.00170 |       0.13935\n",
      "      0.00072 |       0.00000 |      39.90199 |       0.00178 |       0.13991\n",
      "     -0.00021 |       0.00000 |      39.88252 |       0.00166 |       0.14095\n",
      "    -8.15e-05 |       0.00000 |      39.94322 |       0.00196 |       0.14066\n",
      "      0.00154 |       0.00000 |      39.72387 |       0.00195 |       0.14002\n",
      "Evaluating losses...\n",
      "     -0.00094 |       0.00000 |      39.74556 |       0.00196 |       0.13963\n",
      "------------------------------------\n",
      "| EpLenMean       | 88.3           |\n",
      "| EpRewMean       | -87.3          |\n",
      "| EpThisIter      | 44             |\n",
      "| EpisodesSoFar   | 4011           |\n",
      "| TimeElapsed     | 1.5e+03        |\n",
      "| TimestepsSoFar  | 409600         |\n",
      "| ev_tdlam_before | 0.827          |\n",
      "| loss_ent        | 0.13963312     |\n",
      "| loss_kl         | 0.0019635684   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00093872426 |\n",
      "| loss_vf_loss    | 39.745556      |\n",
      "------------------------------------\n",
      "********** Iteration 100 ************\n",
      "Eval num_timesteps=409600, episode_reward=-75.60 +/- 5.44\n",
      "Episode length: 76.60 +/- 5.44\n",
      "Eval num_timesteps=409600, episode_reward=-71.30 +/- 7.63\n",
      "Episode length: 72.30 +/- 7.63\n",
      "Eval num_timesteps=409600, episode_reward=-76.20 +/- 13.08\n",
      "Episode length: 77.20 +/- 13.08\n",
      "Eval num_timesteps=409600, episode_reward=-86.30 +/- 11.01\n",
      "Episode length: 87.30 +/- 11.01\n",
      "Eval num_timesteps=409600, episode_reward=-77.60 +/- 10.72\n",
      "Episode length: 78.60 +/- 10.72\n",
      "Eval num_timesteps=409600, episode_reward=-82.40 +/- 21.39\n",
      "Episode length: 83.40 +/- 21.39\n",
      "Eval num_timesteps=409600, episode_reward=-72.30 +/- 5.29\n",
      "Episode length: 73.30 +/- 5.29\n",
      "Eval num_timesteps=409600, episode_reward=-77.90 +/- 15.20\n",
      "Episode length: 78.90 +/- 15.20\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00120 |       0.00000 |      37.72123 |       0.00135 |       0.13770\n",
      "      0.00036 |       0.00000 |      37.31980 |       0.00122 |       0.13775\n",
      "      0.00153 |       0.00000 |      37.21140 |       0.00136 |       0.13735\n",
      "      0.00026 |       0.00000 |      37.31029 |       0.00127 |       0.13915\n",
      "      0.00050 |       0.00000 |      37.21076 |       0.00130 |       0.13857\n",
      "      0.00036 |       0.00000 |      37.05795 |       0.00130 |       0.13956\n",
      "      0.00134 |       0.00000 |      36.91853 |       0.00148 |       0.13707\n",
      "      0.00079 |       0.00000 |      37.03333 |       0.00140 |       0.13799\n",
      "      0.00112 |       0.00000 |      36.80718 |       0.00155 |       0.13698\n",
      "      0.00031 |       0.00000 |      36.72054 |       0.00158 |       0.13740\n",
      "Evaluating losses...\n",
      "      0.00093 |       0.00000 |      36.74230 |       0.00153 |       0.13746\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92            |\n",
      "| EpRewMean       | -91           |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4055          |\n",
      "| TimeElapsed     | 1.51e+03      |\n",
      "| TimestepsSoFar  | 413696        |\n",
      "| ev_tdlam_before | 0.841         |\n",
      "| loss_ent        | 0.13746157    |\n",
      "| loss_kl         | 0.0015264363  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00093157636 |\n",
      "| loss_vf_loss    | 36.742302     |\n",
      "-----------------------------------\n",
      "********** Iteration 101 ************\n",
      "Eval num_timesteps=413696, episode_reward=-70.50 +/- 5.75\n",
      "Episode length: 71.50 +/- 5.75\n",
      "Eval num_timesteps=413696, episode_reward=-84.10 +/- 30.90\n",
      "Episode length: 85.10 +/- 30.90\n",
      "Eval num_timesteps=413696, episode_reward=-77.90 +/- 10.34\n",
      "Episode length: 78.90 +/- 10.34\n",
      "Eval num_timesteps=413696, episode_reward=-72.30 +/- 8.74\n",
      "Episode length: 73.30 +/- 8.74\n",
      "Eval num_timesteps=413696, episode_reward=-73.70 +/- 7.81\n",
      "Episode length: 74.70 +/- 7.81\n",
      "Eval num_timesteps=413696, episode_reward=-74.00 +/- 11.40\n",
      "Episode length: 75.00 +/- 11.40\n",
      "Eval num_timesteps=413696, episode_reward=-76.10 +/- 10.09\n",
      "Episode length: 77.10 +/- 10.09\n",
      "Eval num_timesteps=413696, episode_reward=-80.80 +/- 21.06\n",
      "Episode length: 81.80 +/- 21.06\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00237 |       0.00000 |      37.30737 |       0.00138 |       0.13310\n",
      "      0.00160 |       0.00000 |      36.90303 |       0.00171 |       0.13073\n",
      "      0.00099 |       0.00000 |      36.53657 |       0.00169 |       0.13034\n",
      "      0.00116 |       0.00000 |      36.50333 |       0.00174 |       0.12903\n",
      "      0.00079 |       0.00000 |      36.33418 |       0.00161 |       0.12787\n",
      "      0.00048 |       0.00000 |      36.37224 |       0.00179 |       0.12826\n",
      "      0.00119 |       0.00000 |      36.40005 |       0.00187 |       0.12811\n",
      "     -0.00044 |       0.00000 |      36.14880 |       0.00170 |       0.12811\n",
      "      0.00130 |       0.00000 |      36.14448 |       0.00191 |       0.12745\n",
      "     -0.00049 |       0.00000 |      36.05825 |       0.00193 |       0.12688\n",
      "Evaluating losses...\n",
      "      0.00044 |       0.00000 |      35.82581 |       0.00173 |       0.12731\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.3          |\n",
      "| EpRewMean       | -92.3         |\n",
      "| EpThisIter      | 43            |\n",
      "| EpisodesSoFar   | 4098          |\n",
      "| TimeElapsed     | 1.52e+03      |\n",
      "| TimestepsSoFar  | 417792        |\n",
      "| ev_tdlam_before | 0.842         |\n",
      "| loss_ent        | 0.12731102    |\n",
      "| loss_kl         | 0.0017251126  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00044284627 |\n",
      "| loss_vf_loss    | 35.82581      |\n",
      "-----------------------------------\n",
      "********** Iteration 102 ************\n",
      "Eval num_timesteps=417792, episode_reward=-87.30 +/- 41.48\n",
      "Episode length: 88.30 +/- 41.48\n",
      "Eval num_timesteps=417792, episode_reward=-78.40 +/- 7.66\n",
      "Episode length: 79.40 +/- 7.66\n",
      "Eval num_timesteps=417792, episode_reward=-71.60 +/- 3.93\n",
      "Episode length: 72.60 +/- 3.93\n",
      "Eval num_timesteps=417792, episode_reward=-73.50 +/- 8.30\n",
      "Episode length: 74.50 +/- 8.30\n",
      "Eval num_timesteps=417792, episode_reward=-81.60 +/- 27.40\n",
      "Episode length: 82.60 +/- 27.40\n",
      "Eval num_timesteps=417792, episode_reward=-75.00 +/- 6.66\n",
      "Episode length: 76.00 +/- 6.66\n",
      "Eval num_timesteps=417792, episode_reward=-84.60 +/- 18.14\n",
      "Episode length: 85.60 +/- 18.14\n",
      "Eval num_timesteps=417792, episode_reward=-76.40 +/- 9.34\n",
      "Episode length: 77.40 +/- 9.34\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00198 |       0.00000 |      40.64424 |       0.00134 |       0.13038\n",
      "      0.00083 |       0.00000 |      40.01408 |       0.00135 |       0.12995\n",
      "      0.00012 |       0.00000 |      39.62300 |       0.00124 |       0.12985\n",
      "      0.00042 |       0.00000 |      38.95384 |       0.00128 |       0.12953\n",
      "      0.00049 |       0.00000 |      38.85046 |       0.00140 |       0.13049\n",
      "      0.00039 |       0.00000 |      38.69045 |       0.00154 |       0.12944\n",
      "      0.00043 |       0.00000 |      38.39046 |       0.00126 |       0.12924\n",
      "     1.13e-05 |       0.00000 |      38.20806 |       0.00124 |       0.12921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00061 |       0.00000 |      37.95794 |       0.00151 |       0.12856\n",
      "      0.00049 |       0.00000 |      38.12561 |       0.00146 |       0.12945\n",
      "Evaluating losses...\n",
      "      0.00012 |       0.00000 |      37.89732 |       0.00149 |       0.12887\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.6          |\n",
      "| EpRewMean       | -90.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 4144          |\n",
      "| TimeElapsed     | 1.54e+03      |\n",
      "| TimestepsSoFar  | 421888        |\n",
      "| ev_tdlam_before | 0.836         |\n",
      "| loss_ent        | 0.12887008    |\n",
      "| loss_kl         | 0.0014882935  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00011795154 |\n",
      "| loss_vf_loss    | 37.897324     |\n",
      "-----------------------------------\n",
      "********** Iteration 103 ************\n",
      "Eval num_timesteps=421888, episode_reward=-76.30 +/- 9.73\n",
      "Episode length: 77.30 +/- 9.73\n",
      "Eval num_timesteps=421888, episode_reward=-88.20 +/- 27.57\n",
      "Episode length: 89.20 +/- 27.57\n",
      "Eval num_timesteps=421888, episode_reward=-76.90 +/- 6.39\n",
      "Episode length: 77.90 +/- 6.39\n",
      "Eval num_timesteps=421888, episode_reward=-86.10 +/- 19.99\n",
      "Episode length: 87.10 +/- 19.99\n",
      "Eval num_timesteps=421888, episode_reward=-78.00 +/- 9.26\n",
      "Episode length: 79.00 +/- 9.26\n",
      "Eval num_timesteps=421888, episode_reward=-98.60 +/- 60.87\n",
      "Episode length: 99.60 +/- 60.87\n",
      "Eval num_timesteps=421888, episode_reward=-77.70 +/- 6.03\n",
      "Episode length: 78.70 +/- 6.03\n",
      "Eval num_timesteps=421888, episode_reward=-89.90 +/- 42.04\n",
      "Episode length: 90.90 +/- 42.04\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00159 |       0.00000 |      44.98029 |       0.00131 |       0.12384\n",
      "      0.00165 |       0.00000 |      44.75138 |       0.00161 |       0.12286\n",
      "      0.00081 |       0.00000 |      44.36558 |       0.00138 |       0.12206\n",
      "      0.00197 |       0.00000 |      43.88769 |       0.00169 |       0.12202\n",
      "      0.00204 |       0.00000 |      43.87553 |       0.00142 |       0.12203\n",
      "      0.00046 |       0.00000 |      43.62663 |       0.00157 |       0.12179\n",
      "      0.00108 |       0.00000 |      43.76700 |       0.00149 |       0.12178\n",
      "      0.00018 |       0.00000 |      43.38444 |       0.00144 |       0.12277\n",
      "      0.00027 |       0.00000 |      43.30129 |       0.00161 |       0.12289\n",
      "     -0.00026 |       0.00000 |      43.15531 |       0.00139 |       0.12208\n",
      "Evaluating losses...\n",
      "      0.00187 |       0.00000 |      43.18837 |       0.00149 |       0.12258\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.5         |\n",
      "| EpRewMean       | -90.5        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 4187         |\n",
      "| TimeElapsed     | 1.55e+03     |\n",
      "| TimestepsSoFar  | 425984       |\n",
      "| ev_tdlam_before | 0.811        |\n",
      "| loss_ent        | 0.12257858   |\n",
      "| loss_kl         | 0.0014885608 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0018705636 |\n",
      "| loss_vf_loss    | 43.188366    |\n",
      "----------------------------------\n",
      "********** Iteration 104 ************\n",
      "Eval num_timesteps=425984, episode_reward=-117.80 +/- 71.28\n",
      "Episode length: 118.80 +/- 71.28\n",
      "Eval num_timesteps=425984, episode_reward=-71.30 +/- 5.22\n",
      "Episode length: 72.30 +/- 5.22\n",
      "Eval num_timesteps=425984, episode_reward=-70.30 +/- 7.81\n",
      "Episode length: 71.30 +/- 7.81\n",
      "Eval num_timesteps=425984, episode_reward=-85.90 +/- 20.37\n",
      "Episode length: 86.90 +/- 20.37\n",
      "Eval num_timesteps=425984, episode_reward=-82.00 +/- 19.98\n",
      "Episode length: 83.00 +/- 19.98\n",
      "Eval num_timesteps=425984, episode_reward=-88.60 +/- 44.03\n",
      "Episode length: 89.60 +/- 44.03\n",
      "Eval num_timesteps=425984, episode_reward=-81.40 +/- 23.68\n",
      "Episode length: 82.40 +/- 23.68\n",
      "Eval num_timesteps=425984, episode_reward=-74.60 +/- 8.39\n",
      "Episode length: 75.60 +/- 8.39\n",
      "Eval num_timesteps=425984, episode_reward=-77.50 +/- 10.59\n",
      "Episode length: 78.50 +/- 10.59\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "    -6.22e-05 |       0.00000 |      35.08471 |       0.00130 |       0.12849\n",
      "      0.00037 |       0.00000 |      34.74673 |       0.00127 |       0.12730\n",
      "      0.00077 |       0.00000 |      34.45606 |       0.00136 |       0.12683\n",
      "      0.00065 |       0.00000 |      34.14540 |       0.00125 |       0.12735\n",
      "      0.00085 |       0.00000 |      34.12204 |       0.00134 |       0.12761\n",
      "     -0.00018 |       0.00000 |      33.93618 |       0.00137 |       0.12705\n",
      "     7.29e-05 |       0.00000 |      33.74793 |       0.00138 |       0.12655\n",
      "      0.00132 |       0.00000 |      33.68012 |       0.00142 |       0.12665\n",
      "      0.00019 |       0.00000 |      33.74593 |       0.00143 |       0.12700\n",
      "      0.00027 |       0.00000 |      33.63387 |       0.00128 |       0.12629\n",
      "Evaluating losses...\n",
      "      0.00021 |       0.00000 |      33.26410 |       0.00138 |       0.12692\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.9          |\n",
      "| EpRewMean       | -92.9         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 4233          |\n",
      "| TimeElapsed     | 1.56e+03      |\n",
      "| TimestepsSoFar  | 430080        |\n",
      "| ev_tdlam_before | 0.856         |\n",
      "| loss_ent        | 0.12692246    |\n",
      "| loss_kl         | 0.0013823875  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00020797004 |\n",
      "| loss_vf_loss    | 33.2641       |\n",
      "-----------------------------------\n",
      "********** Iteration 105 ************\n",
      "Eval num_timesteps=430080, episode_reward=-79.10 +/- 8.62\n",
      "Episode length: 80.10 +/- 8.62\n",
      "Eval num_timesteps=430080, episode_reward=-76.00 +/- 11.23\n",
      "Episode length: 77.00 +/- 11.23\n",
      "Eval num_timesteps=430080, episode_reward=-79.70 +/- 10.51\n",
      "Episode length: 80.70 +/- 10.51\n",
      "Eval num_timesteps=430080, episode_reward=-79.60 +/- 11.95\n",
      "Episode length: 80.60 +/- 11.95\n",
      "Eval num_timesteps=430080, episode_reward=-80.50 +/- 17.07\n",
      "Episode length: 81.50 +/- 17.07\n",
      "Eval num_timesteps=430080, episode_reward=-76.40 +/- 6.55\n",
      "Episode length: 77.40 +/- 6.55\n",
      "Eval num_timesteps=430080, episode_reward=-74.60 +/- 11.51\n",
      "Episode length: 75.60 +/- 11.51\n",
      "Eval num_timesteps=430080, episode_reward=-73.10 +/- 9.64\n",
      "Episode length: 74.10 +/- 9.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00149 |       0.00000 |      37.78864 |       0.00125 |       0.13035\n",
      "      0.00148 |       0.00000 |      37.36876 |       0.00137 |       0.12965\n",
      "      0.00114 |       0.00000 |      36.93802 |       0.00134 |       0.12944\n",
      "      0.00089 |       0.00000 |      36.83488 |       0.00138 |       0.12894\n",
      "      0.00066 |       0.00000 |      36.69882 |       0.00151 |       0.12877\n",
      "      0.00059 |       0.00000 |      36.42405 |       0.00160 |       0.12948\n",
      "      0.00069 |       0.00000 |      36.55146 |       0.00162 |       0.12971\n",
      "      0.00018 |       0.00000 |      36.35000 |       0.00165 |       0.13041\n",
      "      0.00071 |       0.00000 |      36.22247 |       0.00177 |       0.13140\n",
      "     -0.00034 |       0.00000 |      36.11370 |       0.00163 |       0.13152\n",
      "Evaluating losses...\n",
      "      0.00032 |       0.00000 |      36.09491 |       0.00195 |       0.13093\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.3         |\n",
      "| EpRewMean       | -88.3        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 4279         |\n",
      "| TimeElapsed     | 1.57e+03     |\n",
      "| TimestepsSoFar  | 434176       |\n",
      "| ev_tdlam_before | 0.848        |\n",
      "| loss_ent        | 0.1309256    |\n",
      "| loss_kl         | 0.001954175  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0003193256 |\n",
      "| loss_vf_loss    | 36.09491     |\n",
      "----------------------------------\n",
      "********** Iteration 106 ************\n",
      "Eval num_timesteps=434176, episode_reward=-73.20 +/- 9.69\n",
      "Episode length: 74.20 +/- 9.69\n",
      "Eval num_timesteps=434176, episode_reward=-90.90 +/- 38.23\n",
      "Episode length: 91.90 +/- 38.23\n",
      "Eval num_timesteps=434176, episode_reward=-87.40 +/- 24.34\n",
      "Episode length: 88.40 +/- 24.34\n",
      "Eval num_timesteps=434176, episode_reward=-75.70 +/- 6.80\n",
      "Episode length: 76.70 +/- 6.80\n",
      "Eval num_timesteps=434176, episode_reward=-74.50 +/- 10.35\n",
      "Episode length: 75.50 +/- 10.35\n",
      "Eval num_timesteps=434176, episode_reward=-89.00 +/- 38.48\n",
      "Episode length: 90.00 +/- 38.48\n",
      "Eval num_timesteps=434176, episode_reward=-73.80 +/- 11.01\n",
      "Episode length: 74.80 +/- 11.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=434176, episode_reward=-75.10 +/- 6.44\n",
      "Episode length: 76.10 +/- 6.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00043 |       0.00000 |      36.69480 |       0.00135 |       0.12623\n",
      "      0.00073 |       0.00000 |      36.48742 |       0.00144 |       0.12640\n",
      "    -1.99e-05 |       0.00000 |      36.29884 |       0.00150 |       0.12608\n",
      "      0.00204 |       0.00000 |      36.10067 |       0.00153 |       0.12612\n",
      "      0.00033 |       0.00000 |      36.25743 |       0.00148 |       0.12307\n",
      "     5.33e-05 |       0.00000 |      36.23444 |       0.00147 |       0.12366\n",
      "      0.00078 |       0.00000 |      35.98703 |       0.00153 |       0.12359\n",
      "      0.00059 |       0.00000 |      36.08612 |       0.00169 |       0.12312\n",
      "      0.00119 |       0.00000 |      35.77155 |       0.00178 |       0.12272\n",
      "      0.00049 |       0.00000 |      36.15335 |       0.00161 |       0.12097\n",
      "Evaluating losses...\n",
      "      0.00031 |       0.00000 |      35.66680 |       0.00173 |       0.12086\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.3         |\n",
      "| EpRewMean       | -86.3        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 4326         |\n",
      "| TimeElapsed     | 1.58e+03     |\n",
      "| TimestepsSoFar  | 438272       |\n",
      "| ev_tdlam_before | 0.858        |\n",
      "| loss_ent        | 0.12085716   |\n",
      "| loss_kl         | 0.0017277163 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0003107778 |\n",
      "| loss_vf_loss    | 35.666798    |\n",
      "----------------------------------\n",
      "********** Iteration 107 ************\n",
      "Eval num_timesteps=438272, episode_reward=-92.10 +/- 55.78\n",
      "Episode length: 93.10 +/- 55.78\n",
      "Eval num_timesteps=438272, episode_reward=-73.70 +/- 9.17\n",
      "Episode length: 74.70 +/- 9.17\n",
      "Eval num_timesteps=438272, episode_reward=-76.90 +/- 12.17\n",
      "Episode length: 77.90 +/- 12.17\n",
      "Eval num_timesteps=438272, episode_reward=-86.10 +/- 22.02\n",
      "Episode length: 87.10 +/- 22.02\n",
      "Eval num_timesteps=438272, episode_reward=-79.40 +/- 10.46\n",
      "Episode length: 80.40 +/- 10.46\n",
      "Eval num_timesteps=438272, episode_reward=-81.50 +/- 18.46\n",
      "Episode length: 82.50 +/- 18.46\n",
      "Eval num_timesteps=438272, episode_reward=-82.60 +/- 21.30\n",
      "Episode length: 83.60 +/- 21.30\n",
      "Eval num_timesteps=438272, episode_reward=-76.30 +/- 5.25\n",
      "Episode length: 77.30 +/- 5.25\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00058 |       0.00000 |      39.04198 |       0.00121 |       0.11914\n",
      "     9.94e-05 |       0.00000 |      37.68475 |       0.00125 |       0.11819\n",
      "      0.00141 |       0.00000 |      37.35090 |       0.00135 |       0.11875\n",
      "      0.00134 |       0.00000 |      36.52100 |       0.00130 |       0.11897\n",
      "     -0.00047 |       0.00000 |      36.19636 |       0.00148 |       0.11915\n",
      "      0.00043 |       0.00000 |      35.64134 |       0.00137 |       0.11974\n",
      "     4.72e-05 |       0.00000 |      35.57980 |       0.00134 |       0.11915\n",
      "      0.00012 |       0.00000 |      35.21481 |       0.00131 |       0.11848\n",
      "      0.00078 |       0.00000 |      35.12853 |       0.00146 |       0.11893\n",
      "      0.00062 |       0.00000 |      35.11077 |       0.00153 |       0.11866\n",
      "Evaluating losses...\n",
      "      0.00051 |       0.00000 |      34.85864 |       0.00157 |       0.11827\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.8         |\n",
      "| EpRewMean       | -88.8        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 4369         |\n",
      "| TimeElapsed     | 1.59e+03     |\n",
      "| TimestepsSoFar  | 442368       |\n",
      "| ev_tdlam_before | 0.837        |\n",
      "| loss_ent        | 0.118273154  |\n",
      "| loss_kl         | 0.0015688459 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0005086819 |\n",
      "| loss_vf_loss    | 34.858643    |\n",
      "----------------------------------\n",
      "********** Iteration 108 ************\n",
      "Eval num_timesteps=442368, episode_reward=-75.00 +/- 8.81\n",
      "Episode length: 76.00 +/- 8.81\n",
      "Eval num_timesteps=442368, episode_reward=-89.50 +/- 48.70\n",
      "Episode length: 90.50 +/- 48.70\n",
      "Eval num_timesteps=442368, episode_reward=-78.20 +/- 15.14\n",
      "Episode length: 79.20 +/- 15.14\n",
      "Eval num_timesteps=442368, episode_reward=-78.60 +/- 12.62\n",
      "Episode length: 79.60 +/- 12.62\n",
      "Eval num_timesteps=442368, episode_reward=-78.30 +/- 18.52\n",
      "Episode length: 79.30 +/- 18.52\n",
      "Eval num_timesteps=442368, episode_reward=-92.50 +/- 43.66\n",
      "Episode length: 93.50 +/- 43.66\n",
      "Eval num_timesteps=442368, episode_reward=-80.70 +/- 14.44\n",
      "Episode length: 81.70 +/- 14.44\n",
      "Eval num_timesteps=442368, episode_reward=-75.60 +/- 6.44\n",
      "Episode length: 76.60 +/- 6.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00161 |       0.00000 |      33.02198 |       0.00139 |       0.12208\n",
      "      0.00040 |       0.00000 |      32.37867 |       0.00140 |       0.12034\n",
      "      0.00025 |       0.00000 |      32.14566 |       0.00151 |       0.11992\n",
      "      0.00065 |       0.00000 |      31.99231 |       0.00147 |       0.11971\n",
      "      0.00100 |       0.00000 |      31.79446 |       0.00155 |       0.11979\n",
      "     -0.00017 |       0.00000 |      31.65437 |       0.00170 |       0.11906\n",
      "     -0.00016 |       0.00000 |      31.40858 |       0.00150 |       0.11909\n",
      "      0.00090 |       0.00000 |      31.38340 |       0.00155 |       0.11795\n",
      "      0.00062 |       0.00000 |      31.23474 |       0.00192 |       0.11717\n",
      "      0.00048 |       0.00000 |      31.09659 |       0.00199 |       0.11756\n",
      "Evaluating losses...\n",
      "     -0.00058 |       0.00000 |      30.93988 |       0.00195 |       0.11658\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.1          |\n",
      "| EpRewMean       | -88.1         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 4417          |\n",
      "| TimeElapsed     | 1.6e+03       |\n",
      "| TimestepsSoFar  | 446464        |\n",
      "| ev_tdlam_before | 0.872         |\n",
      "| loss_ent        | 0.11658214    |\n",
      "| loss_kl         | 0.0019529673  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0005762945 |\n",
      "| loss_vf_loss    | 30.939882     |\n",
      "-----------------------------------\n",
      "********** Iteration 109 ************\n",
      "Eval num_timesteps=446464, episode_reward=-79.10 +/- 7.50\n",
      "Episode length: 80.10 +/- 7.50\n",
      "Eval num_timesteps=446464, episode_reward=-75.20 +/- 8.61\n",
      "Episode length: 76.20 +/- 8.61\n",
      "Eval num_timesteps=446464, episode_reward=-77.00 +/- 8.94\n",
      "Episode length: 78.00 +/- 8.94\n",
      "Eval num_timesteps=446464, episode_reward=-73.20 +/- 6.40\n",
      "Episode length: 74.20 +/- 6.40\n",
      "Eval num_timesteps=446464, episode_reward=-74.30 +/- 8.21\n",
      "Episode length: 75.30 +/- 8.21\n",
      "Eval num_timesteps=446464, episode_reward=-79.00 +/- 11.20\n",
      "Episode length: 80.00 +/- 11.20\n",
      "Eval num_timesteps=446464, episode_reward=-92.90 +/- 38.53\n",
      "Episode length: 93.90 +/- 38.53\n",
      "Eval num_timesteps=446464, episode_reward=-73.40 +/- 9.48\n",
      "Episode length: 74.40 +/- 9.48\n",
      "Eval num_timesteps=446464, episode_reward=-75.70 +/- 10.93\n",
      "Episode length: 76.70 +/- 10.93\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00086 |       0.00000 |      35.76560 |       0.00120 |       0.11869\n",
      "      0.00319 |       0.00000 |      35.84105 |       0.00144 |       0.11652\n",
      "      0.00117 |       0.00000 |      35.67661 |       0.00170 |       0.11565\n",
      "      0.00138 |       0.00000 |      35.69719 |       0.00170 |       0.11524\n",
      "      0.00130 |       0.00000 |      35.28913 |       0.00183 |       0.11404\n",
      "     9.75e-05 |       0.00000 |      35.33538 |       0.00188 |       0.11408\n",
      "      0.00105 |       0.00000 |      35.13984 |       0.00185 |       0.11344\n",
      "      0.00047 |       0.00000 |      35.29322 |       0.00204 |       0.11345\n",
      "      0.00030 |       0.00000 |      35.24594 |       0.00172 |       0.11352\n",
      "     -0.00051 |       0.00000 |      34.94640 |       0.00187 |       0.11398\n",
      "Evaluating losses...\n",
      "      0.00101 |       0.00000 |      34.99227 |       0.00185 |       0.11386\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.6         |\n",
      "| EpRewMean       | -85.6        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 4464         |\n",
      "| TimeElapsed     | 1.61e+03     |\n",
      "| TimestepsSoFar  | 450560       |\n",
      "| ev_tdlam_before | 0.858        |\n",
      "| loss_ent        | 0.11386145   |\n",
      "| loss_kl         | 0.0018486889 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0010070346 |\n",
      "| loss_vf_loss    | 34.99227     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 110 ************\n",
      "Eval num_timesteps=450560, episode_reward=-77.40 +/- 13.02\n",
      "Episode length: 78.40 +/- 13.02\n",
      "Eval num_timesteps=450560, episode_reward=-77.50 +/- 4.88\n",
      "Episode length: 78.50 +/- 4.88\n",
      "Eval num_timesteps=450560, episode_reward=-81.80 +/- 13.68\n",
      "Episode length: 82.80 +/- 13.68\n",
      "Eval num_timesteps=450560, episode_reward=-94.30 +/- 42.01\n",
      "Episode length: 95.30 +/- 42.01\n",
      "Eval num_timesteps=450560, episode_reward=-74.80 +/- 7.70\n",
      "Episode length: 75.80 +/- 7.70\n",
      "Eval num_timesteps=450560, episode_reward=-78.40 +/- 17.56\n",
      "Episode length: 79.40 +/- 17.56\n",
      "Eval num_timesteps=450560, episode_reward=-82.80 +/- 22.17\n",
      "Episode length: 83.80 +/- 22.17\n",
      "Eval num_timesteps=450560, episode_reward=-103.50 +/- 54.96\n",
      "Episode length: 104.50 +/- 54.96\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00183 |       0.00000 |      49.60439 |       0.00115 |       0.10497\n",
      "      0.00052 |       0.00000 |      47.18995 |       0.00125 |       0.10595\n",
      "      0.00066 |       0.00000 |      46.34423 |       0.00131 |       0.10593\n",
      "      0.00100 |       0.00000 |      45.68562 |       0.00136 |       0.10680\n",
      "      0.00113 |       0.00000 |      45.41311 |       0.00127 |       0.10650\n",
      "      0.00103 |       0.00000 |      44.92988 |       0.00130 |       0.10576\n",
      "      0.00071 |       0.00000 |      44.50880 |       0.00134 |       0.10650\n",
      "    -8.92e-05 |       0.00000 |      44.25111 |       0.00153 |       0.10686\n",
      "      0.00171 |       0.00000 |      44.36654 |       0.00135 |       0.10687\n",
      "     -0.00022 |       0.00000 |      44.10674 |       0.00152 |       0.10617\n",
      "Evaluating losses...\n",
      "      0.00082 |       0.00000 |      43.91065 |       0.00134 |       0.10626\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.6          |\n",
      "| EpRewMean       | -90.6         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 4505          |\n",
      "| TimeElapsed     | 1.62e+03      |\n",
      "| TimestepsSoFar  | 454656        |\n",
      "| ev_tdlam_before | 0.779         |\n",
      "| loss_ent        | 0.10625832    |\n",
      "| loss_kl         | 0.001340641   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00081690925 |\n",
      "| loss_vf_loss    | 43.910652     |\n",
      "-----------------------------------\n",
      "********** Iteration 111 ************\n",
      "Eval num_timesteps=454656, episode_reward=-85.80 +/- 15.61\n",
      "Episode length: 86.80 +/- 15.61\n",
      "Eval num_timesteps=454656, episode_reward=-84.90 +/- 29.95\n",
      "Episode length: 85.90 +/- 29.95\n",
      "Eval num_timesteps=454656, episode_reward=-74.70 +/- 9.52\n",
      "Episode length: 75.70 +/- 9.52\n",
      "Eval num_timesteps=454656, episode_reward=-76.20 +/- 10.33\n",
      "Episode length: 77.20 +/- 10.33\n",
      "Eval num_timesteps=454656, episode_reward=-78.80 +/- 12.29\n",
      "Episode length: 79.80 +/- 12.29\n",
      "Eval num_timesteps=454656, episode_reward=-79.70 +/- 13.65\n",
      "Episode length: 80.70 +/- 13.65\n",
      "Eval num_timesteps=454656, episode_reward=-70.50 +/- 7.13\n",
      "Episode length: 71.50 +/- 7.13\n",
      "Eval num_timesteps=454656, episode_reward=-85.30 +/- 26.87\n",
      "Episode length: 86.30 +/- 26.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00093 |       0.00000 |      35.89203 |       0.00126 |       0.11055\n",
      "      0.00096 |       0.00000 |      35.33551 |       0.00130 |       0.11119\n",
      "      0.00095 |       0.00000 |      35.03453 |       0.00131 |       0.11107\n",
      "      0.00050 |       0.00000 |      34.93087 |       0.00131 |       0.11129\n",
      "      0.00149 |       0.00000 |      34.73818 |       0.00140 |       0.11216\n",
      "      0.00133 |       0.00000 |      34.74699 |       0.00135 |       0.11210\n",
      "      0.00054 |       0.00000 |      34.56774 |       0.00162 |       0.11160\n",
      "     7.15e-05 |       0.00000 |      34.47718 |       0.00165 |       0.11182\n",
      "      0.00029 |       0.00000 |      34.47747 |       0.00161 |       0.11204\n",
      "     -0.00026 |       0.00000 |      34.32757 |       0.00143 |       0.11249\n",
      "Evaluating losses...\n",
      "      0.00025 |       0.00000 |      34.32643 |       0.00162 |       0.11204\n",
      "-----------------------------------\n",
      "| EpLenMean       | 92.9          |\n",
      "| EpRewMean       | -91.9         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 4553          |\n",
      "| TimeElapsed     | 1.63e+03      |\n",
      "| TimestepsSoFar  | 458752        |\n",
      "| ev_tdlam_before | 0.859         |\n",
      "| loss_ent        | 0.11203698    |\n",
      "| loss_kl         | 0.0016238262  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00025474175 |\n",
      "| loss_vf_loss    | 34.32643      |\n",
      "-----------------------------------\n",
      "********** Iteration 112 ************\n",
      "Eval num_timesteps=458752, episode_reward=-74.20 +/- 4.49\n",
      "Episode length: 75.20 +/- 4.49\n",
      "Eval num_timesteps=458752, episode_reward=-79.00 +/- 11.04\n",
      "Episode length: 80.00 +/- 11.04\n",
      "Eval num_timesteps=458752, episode_reward=-77.90 +/- 19.82\n",
      "Episode length: 78.90 +/- 19.82\n",
      "Eval num_timesteps=458752, episode_reward=-75.70 +/- 7.07\n",
      "Episode length: 76.70 +/- 7.07\n",
      "Eval num_timesteps=458752, episode_reward=-73.50 +/- 5.99\n",
      "Episode length: 74.50 +/- 5.99\n",
      "Eval num_timesteps=458752, episode_reward=-77.40 +/- 7.36\n",
      "Episode length: 78.40 +/- 7.36\n",
      "Eval num_timesteps=458752, episode_reward=-80.50 +/- 14.24\n",
      "Episode length: 81.50 +/- 14.24\n",
      "Eval num_timesteps=458752, episode_reward=-86.20 +/- 30.11\n",
      "Episode length: 87.20 +/- 30.11\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00023 |       0.00000 |      45.76159 |       0.00114 |       0.11136\n",
      "      0.00101 |       0.00000 |      45.03622 |       0.00128 |       0.11162\n",
      "      0.00073 |       0.00000 |      44.66946 |       0.00136 |       0.11074\n",
      "     -0.00024 |       0.00000 |      44.26601 |       0.00134 |       0.11046\n",
      "      0.00021 |       0.00000 |      44.21058 |       0.00138 |       0.11049\n",
      "     -0.00013 |       0.00000 |      43.77327 |       0.00138 |       0.11016\n",
      "     -0.00021 |       0.00000 |      43.60385 |       0.00189 |       0.11021\n",
      "     -0.00042 |       0.00000 |      43.48434 |       0.00177 |       0.11038\n",
      "     -0.00014 |       0.00000 |      43.20955 |       0.00185 |       0.10928\n",
      "     -0.00015 |       0.00000 |      42.99495 |       0.00168 |       0.10863\n",
      "Evaluating losses...\n",
      "    -8.05e-05 |       0.00000 |      43.09886 |       0.00182 |       0.10854\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.2          |\n",
      "| EpRewMean       | -89.2         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4597          |\n",
      "| TimeElapsed     | 1.64e+03      |\n",
      "| TimestepsSoFar  | 462848        |\n",
      "| ev_tdlam_before | 0.808         |\n",
      "| loss_ent        | 0.10854171    |\n",
      "| loss_kl         | 0.0018154574  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -8.045696e-05 |\n",
      "| loss_vf_loss    | 43.098858     |\n",
      "-----------------------------------\n",
      "********** Iteration 113 ************\n",
      "Eval num_timesteps=462848, episode_reward=-81.40 +/- 7.90\n",
      "Episode length: 82.40 +/- 7.90\n",
      "Eval num_timesteps=462848, episode_reward=-79.60 +/- 9.62\n",
      "Episode length: 80.60 +/- 9.62\n",
      "Eval num_timesteps=462848, episode_reward=-83.50 +/- 11.78\n",
      "Episode length: 84.50 +/- 11.78\n",
      "Eval num_timesteps=462848, episode_reward=-71.80 +/- 7.24\n",
      "Episode length: 72.80 +/- 7.24\n",
      "Eval num_timesteps=462848, episode_reward=-92.50 +/- 30.87\n",
      "Episode length: 93.50 +/- 30.87\n",
      "Eval num_timesteps=462848, episode_reward=-80.50 +/- 20.96\n",
      "Episode length: 81.50 +/- 20.96\n",
      "Eval num_timesteps=462848, episode_reward=-92.10 +/- 25.01\n",
      "Episode length: 93.10 +/- 25.01\n",
      "Eval num_timesteps=462848, episode_reward=-73.00 +/- 5.97\n",
      "Episode length: 74.00 +/- 5.97\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00233 |       0.00000 |      41.52266 |       0.00131 |       0.10519\n",
      "     2.77e-06 |       0.00000 |      40.83029 |       0.00127 |       0.10438\n",
      "      0.00034 |       0.00000 |      40.68451 |       0.00155 |       0.10414\n",
      "      0.00021 |       0.00000 |      40.13788 |       0.00154 |       0.10319\n",
      "      0.00078 |       0.00000 |      40.10327 |       0.00152 |       0.10299\n",
      "      0.00094 |       0.00000 |      40.14955 |       0.00142 |       0.10285\n",
      "      0.00095 |       0.00000 |      39.91558 |       0.00147 |       0.10319\n",
      "      0.00114 |       0.00000 |      39.71302 |       0.00186 |       0.10287\n",
      "      0.00071 |       0.00000 |      39.79646 |       0.00170 |       0.10275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00068 |       0.00000 |      39.56589 |       0.00169 |       0.10236\n",
      "Evaluating losses...\n",
      "     -0.00057 |       0.00000 |      39.45074 |       0.00166 |       0.10207\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.5          |\n",
      "| EpRewMean       | -90.5         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 4641          |\n",
      "| TimeElapsed     | 1.65e+03      |\n",
      "| TimestepsSoFar  | 466944        |\n",
      "| ev_tdlam_before | 0.825         |\n",
      "| loss_ent        | 0.10207019    |\n",
      "| loss_kl         | 0.0016578261  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0005670709 |\n",
      "| loss_vf_loss    | 39.450745     |\n",
      "-----------------------------------\n",
      "********** Iteration 114 ************\n",
      "Eval num_timesteps=466944, episode_reward=-75.00 +/- 9.58\n",
      "Episode length: 76.00 +/- 9.58\n",
      "Eval num_timesteps=466944, episode_reward=-76.00 +/- 9.34\n",
      "Episode length: 77.00 +/- 9.34\n",
      "Eval num_timesteps=466944, episode_reward=-84.30 +/- 19.76\n",
      "Episode length: 85.30 +/- 19.76\n",
      "Eval num_timesteps=466944, episode_reward=-90.60 +/- 29.43\n",
      "Episode length: 91.60 +/- 29.43\n",
      "Eval num_timesteps=466944, episode_reward=-73.90 +/- 6.59\n",
      "Episode length: 74.90 +/- 6.59\n",
      "Eval num_timesteps=466944, episode_reward=-80.30 +/- 15.24\n",
      "Episode length: 81.30 +/- 15.24\n",
      "Eval num_timesteps=466944, episode_reward=-72.60 +/- 7.05\n",
      "Episode length: 73.60 +/- 7.05\n",
      "Eval num_timesteps=466944, episode_reward=-81.60 +/- 7.16\n",
      "Episode length: 82.60 +/- 7.16\n",
      "Eval num_timesteps=466944, episode_reward=-75.20 +/- 9.13\n",
      "Episode length: 76.20 +/- 9.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00162 |       0.00000 |      44.22375 |       0.00112 |       0.10009\n",
      "      0.00113 |       0.00000 |      41.27829 |       0.00121 |       0.09987\n",
      "      0.00050 |       0.00000 |      39.99389 |       0.00123 |       0.09842\n",
      "      0.00012 |       0.00000 |      38.94910 |       0.00136 |       0.09770\n",
      "      0.00024 |       0.00000 |      38.38736 |       0.00165 |       0.09836\n",
      "     -0.00049 |       0.00000 |      37.59153 |       0.00160 |       0.09826\n",
      "     -0.00028 |       0.00000 |      37.27002 |       0.00150 |       0.09797\n",
      "     -0.00042 |       0.00000 |      36.86338 |       0.00163 |       0.09812\n",
      "      0.00083 |       0.00000 |      36.64092 |       0.00156 |       0.09746\n",
      "     -0.00027 |       0.00000 |      36.29727 |       0.00147 |       0.09724\n",
      "Evaluating losses...\n",
      "      0.00035 |       0.00000 |      36.28696 |       0.00151 |       0.09811\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.5         |\n",
      "| EpRewMean       | -92.5        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 4684         |\n",
      "| TimeElapsed     | 1.66e+03     |\n",
      "| TimestepsSoFar  | 471040       |\n",
      "| ev_tdlam_before | 0.807        |\n",
      "| loss_ent        | 0.09810856   |\n",
      "| loss_kl         | 0.001510674  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0003478726 |\n",
      "| loss_vf_loss    | 36.286964    |\n",
      "----------------------------------\n",
      "********** Iteration 115 ************\n",
      "Eval num_timesteps=471040, episode_reward=-77.50 +/- 9.20\n",
      "Episode length: 78.50 +/- 9.20\n",
      "Eval num_timesteps=471040, episode_reward=-74.20 +/- 9.45\n",
      "Episode length: 75.20 +/- 9.45\n",
      "Eval num_timesteps=471040, episode_reward=-82.20 +/- 12.50\n",
      "Episode length: 83.20 +/- 12.50\n",
      "Eval num_timesteps=471040, episode_reward=-76.40 +/- 13.30\n",
      "Episode length: 77.40 +/- 13.30\n",
      "Eval num_timesteps=471040, episode_reward=-79.20 +/- 7.15\n",
      "Episode length: 80.20 +/- 7.15\n",
      "Eval num_timesteps=471040, episode_reward=-97.60 +/- 42.31\n",
      "Episode length: 98.60 +/- 42.31\n",
      "Eval num_timesteps=471040, episode_reward=-76.10 +/- 6.74\n",
      "Episode length: 77.10 +/- 6.74\n",
      "Eval num_timesteps=471040, episode_reward=-116.90 +/- 73.20\n",
      "Episode length: 117.90 +/- 73.20\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00121 |       0.00000 |      38.00048 |       0.00106 |       0.10245\n",
      "      0.00063 |       0.00000 |      37.15806 |       0.00113 |       0.10308\n",
      "      0.00046 |       0.00000 |      36.87936 |       0.00118 |       0.10315\n",
      "      0.00034 |       0.00000 |      36.66544 |       0.00121 |       0.10380\n",
      "      0.00043 |       0.00000 |      36.34533 |       0.00112 |       0.10422\n",
      "      0.00075 |       0.00000 |      36.14106 |       0.00134 |       0.10397\n",
      "      0.00068 |       0.00000 |      36.03466 |       0.00138 |       0.10417\n",
      "      0.00062 |       0.00000 |      36.05372 |       0.00131 |       0.10420\n",
      "      0.00090 |       0.00000 |      35.99416 |       0.00146 |       0.10385\n",
      "      0.00049 |       0.00000 |      35.84649 |       0.00128 |       0.10472\n",
      "Evaluating losses...\n",
      "      0.00032 |       0.00000 |      35.84004 |       0.00135 |       0.10392\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90            |\n",
      "| EpRewMean       | -89           |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 4731          |\n",
      "| TimeElapsed     | 1.67e+03      |\n",
      "| TimestepsSoFar  | 475136        |\n",
      "| ev_tdlam_before | 0.85          |\n",
      "| loss_ent        | 0.10392347    |\n",
      "| loss_kl         | 0.0013468075  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00031598564 |\n",
      "| loss_vf_loss    | 35.840042     |\n",
      "-----------------------------------\n",
      "********** Iteration 116 ************\n",
      "Eval num_timesteps=475136, episode_reward=-86.40 +/- 28.36\n",
      "Episode length: 87.40 +/- 28.36\n",
      "Eval num_timesteps=475136, episode_reward=-81.50 +/- 15.09\n",
      "Episode length: 82.50 +/- 15.09\n",
      "Eval num_timesteps=475136, episode_reward=-75.70 +/- 8.19\n",
      "Episode length: 76.70 +/- 8.19\n",
      "Eval num_timesteps=475136, episode_reward=-75.10 +/- 9.37\n",
      "Episode length: 76.10 +/- 9.37\n",
      "Eval num_timesteps=475136, episode_reward=-72.60 +/- 6.65\n",
      "Episode length: 73.60 +/- 6.65\n",
      "Eval num_timesteps=475136, episode_reward=-80.10 +/- 12.54\n",
      "Episode length: 81.10 +/- 12.54\n",
      "Eval num_timesteps=475136, episode_reward=-79.80 +/- 7.41\n",
      "Episode length: 80.80 +/- 7.41\n",
      "Eval num_timesteps=475136, episode_reward=-78.40 +/- 10.18\n",
      "Episode length: 79.40 +/- 10.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00096 |       0.00000 |      47.71064 |       0.00104 |       0.09748\n",
      "      0.00171 |       0.00000 |      45.69275 |       0.00110 |       0.09792\n",
      "      0.00055 |       0.00000 |      44.55305 |       0.00118 |       0.09686\n",
      "      0.00153 |       0.00000 |      43.70854 |       0.00120 |       0.09698\n",
      "      0.00045 |       0.00000 |      43.11424 |       0.00130 |       0.09761\n",
      "      0.00048 |       0.00000 |      42.53674 |       0.00121 |       0.09715\n",
      "      0.00146 |       0.00000 |      42.06498 |       0.00131 |       0.09601\n",
      "      0.00138 |       0.00000 |      41.82972 |       0.00130 |       0.09567\n",
      "      0.00120 |       0.00000 |      41.39298 |       0.00145 |       0.09603\n",
      "      0.00146 |       0.00000 |      41.21242 |       0.00153 |       0.09598\n",
      "Evaluating losses...\n",
      "      0.00055 |       0.00000 |      41.08018 |       0.00134 |       0.09609\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 4774         |\n",
      "| TimeElapsed     | 1.68e+03     |\n",
      "| TimestepsSoFar  | 479232       |\n",
      "| ev_tdlam_before | 0.793        |\n",
      "| loss_ent        | 0.096090175  |\n",
      "| loss_kl         | 0.0013362938 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0005518354 |\n",
      "| loss_vf_loss    | 41.080177    |\n",
      "----------------------------------\n",
      "********** Iteration 117 ************\n",
      "Eval num_timesteps=479232, episode_reward=-76.80 +/- 4.98\n",
      "Episode length: 77.80 +/- 4.98\n",
      "Eval num_timesteps=479232, episode_reward=-80.40 +/- 19.93\n",
      "Episode length: 81.40 +/- 19.93\n",
      "Eval num_timesteps=479232, episode_reward=-89.40 +/- 22.55\n",
      "Episode length: 90.40 +/- 22.55\n",
      "Eval num_timesteps=479232, episode_reward=-78.90 +/- 6.27\n",
      "Episode length: 79.90 +/- 6.27\n",
      "Eval num_timesteps=479232, episode_reward=-76.10 +/- 6.85\n",
      "Episode length: 77.10 +/- 6.85\n",
      "Eval num_timesteps=479232, episode_reward=-82.00 +/- 10.43\n",
      "Episode length: 83.00 +/- 10.43\n",
      "Eval num_timesteps=479232, episode_reward=-78.90 +/- 15.74\n",
      "Episode length: 79.90 +/- 15.74\n",
      "Eval num_timesteps=479232, episode_reward=-80.60 +/- 7.00\n",
      "Episode length: 81.60 +/- 7.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00145 |       0.00000 |      41.48295 |       0.00140 |       0.10638\n",
      "      0.00121 |       0.00000 |      41.47909 |       0.00127 |       0.10678\n",
      "      0.00144 |       0.00000 |      41.26909 |       0.00144 |       0.10535\n",
      "      0.00071 |       0.00000 |      41.15204 |       0.00148 |       0.10645\n",
      "      0.00211 |       0.00000 |      41.02608 |       0.00141 |       0.10595\n",
      "      0.00133 |       0.00000 |      40.77937 |       0.00156 |       0.10692\n",
      "      0.00292 |       0.00000 |      40.83502 |       0.00170 |       0.10771\n",
      "      0.00172 |       0.00000 |      40.70482 |       0.00169 |       0.10818\n",
      "      0.00098 |       0.00000 |      40.67109 |       0.00171 |       0.10836\n",
      "     9.12e-05 |       0.00000 |      40.55569 |       0.00150 |       0.10826\n",
      "Evaluating losses...\n",
      "      0.00173 |       0.00000 |      40.46705 |       0.00149 |       0.10712\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.4         |\n",
      "| EpRewMean       | -91.5        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 4820         |\n",
      "| TimeElapsed     | 1.69e+03     |\n",
      "| TimestepsSoFar  | 483328       |\n",
      "| ev_tdlam_before | 0.831        |\n",
      "| loss_ent        | 0.10712476   |\n",
      "| loss_kl         | 0.0014871381 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017288292 |\n",
      "| loss_vf_loss    | 40.46705     |\n",
      "----------------------------------\n",
      "********** Iteration 118 ************\n",
      "Eval num_timesteps=483328, episode_reward=-105.30 +/- 58.36\n",
      "Episode length: 106.30 +/- 58.36\n",
      "Eval num_timesteps=483328, episode_reward=-80.90 +/- 11.22\n",
      "Episode length: 81.90 +/- 11.22\n",
      "Eval num_timesteps=483328, episode_reward=-89.70 +/- 24.90\n",
      "Episode length: 90.70 +/- 24.90\n",
      "Eval num_timesteps=483328, episode_reward=-74.70 +/- 6.03\n",
      "Episode length: 75.70 +/- 6.03\n",
      "Eval num_timesteps=483328, episode_reward=-75.10 +/- 5.79\n",
      "Episode length: 76.10 +/- 5.79\n",
      "Eval num_timesteps=483328, episode_reward=-76.10 +/- 6.85\n",
      "Episode length: 77.10 +/- 6.85\n",
      "Eval num_timesteps=483328, episode_reward=-76.70 +/- 8.19\n",
      "Episode length: 77.70 +/- 8.19\n",
      "Eval num_timesteps=483328, episode_reward=-71.20 +/- 6.48\n",
      "Episode length: 72.20 +/- 6.48\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00069 |       0.00000 |      44.06065 |       0.00112 |       0.09980\n",
      "      0.00102 |       0.00000 |      43.75081 |       0.00113 |       0.10064\n",
      "     -0.00013 |       0.00000 |      43.35491 |       0.00125 |       0.10140\n",
      "      0.00032 |       0.00000 |      43.17043 |       0.00125 |       0.10164\n",
      "      0.00144 |       0.00000 |      43.28490 |       0.00135 |       0.10134\n",
      "      0.00044 |       0.00000 |      43.00812 |       0.00129 |       0.10181\n",
      "      0.00100 |       0.00000 |      42.87917 |       0.00144 |       0.10139\n",
      "      0.00043 |       0.00000 |      42.65174 |       0.00134 |       0.10107\n",
      "      0.00063 |       0.00000 |      42.46230 |       0.00121 |       0.10052\n",
      "      0.00053 |       0.00000 |      42.56899 |       0.00137 |       0.10103\n",
      "Evaluating losses...\n",
      "      0.00037 |       0.00000 |      42.42173 |       0.00139 |       0.10057\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.2         |\n",
      "| EpRewMean       | -94.2        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 4864         |\n",
      "| TimeElapsed     | 1.7e+03      |\n",
      "| TimestepsSoFar  | 487424       |\n",
      "| ev_tdlam_before | 0.811        |\n",
      "| loss_ent        | 0.10056878   |\n",
      "| loss_kl         | 0.001393638  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0003698453 |\n",
      "| loss_vf_loss    | 42.421726    |\n",
      "----------------------------------\n",
      "********** Iteration 119 ************\n",
      "Eval num_timesteps=487424, episode_reward=-90.30 +/- 29.60\n",
      "Episode length: 91.30 +/- 29.60\n",
      "Eval num_timesteps=487424, episode_reward=-76.10 +/- 13.62\n",
      "Episode length: 77.10 +/- 13.62\n",
      "Eval num_timesteps=487424, episode_reward=-76.90 +/- 6.96\n",
      "Episode length: 77.90 +/- 6.96\n",
      "Eval num_timesteps=487424, episode_reward=-82.50 +/- 15.53\n",
      "Episode length: 83.50 +/- 15.53\n",
      "Eval num_timesteps=487424, episode_reward=-79.50 +/- 10.76\n",
      "Episode length: 80.50 +/- 10.76\n",
      "Eval num_timesteps=487424, episode_reward=-81.00 +/- 7.01\n",
      "Episode length: 82.00 +/- 7.01\n",
      "Eval num_timesteps=487424, episode_reward=-76.40 +/- 10.47\n",
      "Episode length: 77.40 +/- 10.47\n",
      "Eval num_timesteps=487424, episode_reward=-81.80 +/- 24.96\n",
      "Episode length: 82.80 +/- 24.96\n",
      "Eval num_timesteps=487424, episode_reward=-88.80 +/- 19.99\n",
      "Episode length: 89.80 +/- 19.99\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00074 |       0.00000 |      33.63593 |       0.00112 |       0.10833\n",
      "      0.00176 |       0.00000 |      32.92345 |       0.00136 |       0.10756\n",
      "      0.00062 |       0.00000 |      32.78934 |       0.00134 |       0.10665\n",
      "      0.00081 |       0.00000 |      32.31486 |       0.00126 |       0.10672\n",
      "     -0.00039 |       0.00000 |      32.34124 |       0.00163 |       0.10726\n",
      "      0.00027 |       0.00000 |      32.22509 |       0.00141 |       0.10690\n",
      "      0.00058 |       0.00000 |      31.94910 |       0.00144 |       0.10739\n",
      "      0.00038 |       0.00000 |      32.06515 |       0.00168 |       0.10698\n",
      "      0.00078 |       0.00000 |      31.62997 |       0.00189 |       0.10779\n",
      "      0.00045 |       0.00000 |      31.66848 |       0.00174 |       0.10739\n",
      "Evaluating losses...\n",
      "     -0.00017 |       0.00000 |      31.73374 |       0.00177 |       0.10718\n",
      "------------------------------------\n",
      "| EpLenMean       | 88             |\n",
      "| EpRewMean       | -87            |\n",
      "| EpThisIter      | 48             |\n",
      "| EpisodesSoFar   | 4912           |\n",
      "| TimeElapsed     | 1.71e+03       |\n",
      "| TimestepsSoFar  | 491520         |\n",
      "| ev_tdlam_before | 0.87           |\n",
      "| loss_ent        | 0.10718151     |\n",
      "| loss_kl         | 0.0017743553   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00016719996 |\n",
      "| loss_vf_loss    | 31.733742      |\n",
      "------------------------------------\n",
      "********** Iteration 120 ************\n",
      "Eval num_timesteps=491520, episode_reward=-72.50 +/- 5.87\n",
      "Episode length: 73.50 +/- 5.87\n",
      "Eval num_timesteps=491520, episode_reward=-81.50 +/- 22.11\n",
      "Episode length: 82.50 +/- 22.11\n",
      "Eval num_timesteps=491520, episode_reward=-76.40 +/- 7.86\n",
      "Episode length: 77.40 +/- 7.86\n",
      "Eval num_timesteps=491520, episode_reward=-75.60 +/- 5.80\n",
      "Episode length: 76.60 +/- 5.80\n",
      "Eval num_timesteps=491520, episode_reward=-77.30 +/- 7.95\n",
      "Episode length: 78.30 +/- 7.95\n",
      "Eval num_timesteps=491520, episode_reward=-82.50 +/- 17.38\n",
      "Episode length: 83.50 +/- 17.38\n",
      "Eval num_timesteps=491520, episode_reward=-84.20 +/- 20.53\n",
      "Episode length: 85.20 +/- 20.53\n",
      "Eval num_timesteps=491520, episode_reward=-74.70 +/- 6.33\n",
      "Episode length: 75.70 +/- 6.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00082 |       0.00000 |      36.95786 |       0.00114 |       0.09964\n",
      "      0.00034 |       0.00000 |      36.88145 |       0.00118 |       0.10042\n",
      "      0.00105 |       0.00000 |      36.63413 |       0.00125 |       0.10014\n",
      "      0.00070 |       0.00000 |      36.53169 |       0.00121 |       0.10100\n",
      "      0.00069 |       0.00000 |      36.33314 |       0.00129 |       0.10125\n",
      "      0.00144 |       0.00000 |      36.24915 |       0.00125 |       0.10084\n",
      "     -0.00023 |       0.00000 |      36.17596 |       0.00133 |       0.10196\n",
      "      0.00111 |       0.00000 |      36.06406 |       0.00124 |       0.10215\n",
      "      0.00031 |       0.00000 |      35.96799 |       0.00125 |       0.10248\n",
      "      0.00134 |       0.00000 |      35.92266 |       0.00138 |       0.10243\n",
      "Evaluating losses...\n",
      "     -0.00040 |       0.00000 |      35.64090 |       0.00131 |       0.10251\n",
      "------------------------------------\n",
      "| EpLenMean       | 87.6           |\n",
      "| EpRewMean       | -86.6          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 4958           |\n",
      "| TimeElapsed     | 1.72e+03       |\n",
      "| TimestepsSoFar  | 495616         |\n",
      "| ev_tdlam_before | 0.848          |\n",
      "| loss_ent        | 0.10250939     |\n",
      "| loss_kl         | 0.0013105258   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00039564382 |\n",
      "| loss_vf_loss    | 35.6409        |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 121 ************\n",
      "Eval num_timesteps=495616, episode_reward=-92.50 +/- 21.53\n",
      "Episode length: 93.50 +/- 21.53\n",
      "Eval num_timesteps=495616, episode_reward=-84.10 +/- 17.94\n",
      "Episode length: 85.10 +/- 17.94\n",
      "Eval num_timesteps=495616, episode_reward=-75.20 +/- 10.73\n",
      "Episode length: 76.20 +/- 10.73\n",
      "Eval num_timesteps=495616, episode_reward=-78.20 +/- 17.16\n",
      "Episode length: 79.20 +/- 17.16\n",
      "Eval num_timesteps=495616, episode_reward=-81.80 +/- 17.61\n",
      "Episode length: 82.80 +/- 17.61\n",
      "Eval num_timesteps=495616, episode_reward=-78.10 +/- 6.55\n",
      "Episode length: 79.10 +/- 6.55\n",
      "Eval num_timesteps=495616, episode_reward=-88.00 +/- 21.52\n",
      "Episode length: 89.00 +/- 21.52\n",
      "Eval num_timesteps=495616, episode_reward=-71.00 +/- 6.43\n",
      "Episode length: 72.00 +/- 6.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00145 |       0.00000 |      37.79253 |       0.00118 |       0.10807\n",
      "      0.00053 |       0.00000 |      37.74561 |       0.00120 |       0.10923\n",
      "      0.00227 |       0.00000 |      37.68662 |       0.00133 |       0.10902\n",
      "      0.00141 |       0.00000 |      37.47605 |       0.00137 |       0.10927\n",
      "      0.00156 |       0.00000 |      37.53218 |       0.00162 |       0.10970\n",
      "      0.00111 |       0.00000 |      37.43449 |       0.00160 |       0.10974\n",
      "      0.00039 |       0.00000 |      37.52749 |       0.00168 |       0.11039\n",
      "      0.00031 |       0.00000 |      37.40691 |       0.00162 |       0.10986\n",
      "      0.00011 |       0.00000 |      37.44290 |       0.00148 |       0.11027\n",
      "      0.00058 |       0.00000 |      37.31978 |       0.00171 |       0.10997\n",
      "Evaluating losses...\n",
      "      0.00058 |       0.00000 |      37.30157 |       0.00183 |       0.10971\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.6         |\n",
      "| EpRewMean       | -87.6        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 5004         |\n",
      "| TimeElapsed     | 1.73e+03     |\n",
      "| TimestepsSoFar  | 499712       |\n",
      "| ev_tdlam_before | 0.845        |\n",
      "| loss_ent        | 0.10970518   |\n",
      "| loss_kl         | 0.0018304589 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0005812652 |\n",
      "| loss_vf_loss    | 37.30157     |\n",
      "----------------------------------\n",
      "********** Iteration 122 ************\n",
      "Eval num_timesteps=499712, episode_reward=-74.80 +/- 12.05\n",
      "Episode length: 75.80 +/- 12.05\n",
      "Eval num_timesteps=499712, episode_reward=-75.60 +/- 7.55\n",
      "Episode length: 76.60 +/- 7.55\n",
      "Eval num_timesteps=499712, episode_reward=-77.20 +/- 6.40\n",
      "Episode length: 78.20 +/- 6.40\n",
      "Eval num_timesteps=499712, episode_reward=-95.80 +/- 52.35\n",
      "Episode length: 96.80 +/- 52.35\n",
      "Eval num_timesteps=499712, episode_reward=-84.90 +/- 31.56\n",
      "Episode length: 85.90 +/- 31.56\n",
      "Eval num_timesteps=499712, episode_reward=-78.60 +/- 8.81\n",
      "Episode length: 79.60 +/- 8.81\n",
      "Eval num_timesteps=499712, episode_reward=-77.50 +/- 10.09\n",
      "Episode length: 78.50 +/- 10.09\n",
      "Eval num_timesteps=499712, episode_reward=-80.80 +/- 25.08\n",
      "Episode length: 81.80 +/- 25.08\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00276 |       0.00000 |      35.56939 |       0.00129 |       0.11083\n",
      "      0.00068 |       0.00000 |      35.59932 |       0.00127 |       0.10933\n",
      "      0.00038 |       0.00000 |      35.32574 |       0.00123 |       0.10996\n",
      "      0.00089 |       0.00000 |      35.39756 |       0.00149 |       0.10963\n",
      "      0.00100 |       0.00000 |      35.34193 |       0.00151 |       0.10888\n",
      "      0.00104 |       0.00000 |      35.08199 |       0.00177 |       0.10780\n",
      "      0.00085 |       0.00000 |      35.08721 |       0.00159 |       0.10851\n",
      "      0.00160 |       0.00000 |      35.08138 |       0.00159 |       0.10865\n",
      "      0.00071 |       0.00000 |      35.10645 |       0.00152 |       0.10918\n",
      "    -1.56e-06 |       0.00000 |      35.00567 |       0.00175 |       0.10861\n",
      "Evaluating losses...\n",
      "      0.00084 |       0.00000 |      34.89240 |       0.00154 |       0.10968\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.8          |\n",
      "| EpRewMean       | -88.8         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5050          |\n",
      "| TimeElapsed     | 1.74e+03      |\n",
      "| TimestepsSoFar  | 503808        |\n",
      "| ev_tdlam_before | 0.857         |\n",
      "| loss_ent        | 0.10968261    |\n",
      "| loss_kl         | 0.0015378256  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00084144145 |\n",
      "| loss_vf_loss    | 34.892395     |\n",
      "-----------------------------------\n",
      "********** Iteration 123 ************\n",
      "Eval num_timesteps=503808, episode_reward=-91.80 +/- 43.82\n",
      "Episode length: 92.80 +/- 43.82\n",
      "Eval num_timesteps=503808, episode_reward=-92.50 +/- 41.02\n",
      "Episode length: 93.50 +/- 41.02\n",
      "Eval num_timesteps=503808, episode_reward=-81.90 +/- 10.89\n",
      "Episode length: 82.90 +/- 10.89\n",
      "Eval num_timesteps=503808, episode_reward=-82.10 +/- 12.49\n",
      "Episode length: 83.10 +/- 12.49\n",
      "Eval num_timesteps=503808, episode_reward=-90.40 +/- 57.79\n",
      "Episode length: 91.40 +/- 57.79\n",
      "Eval num_timesteps=503808, episode_reward=-81.20 +/- 14.30\n",
      "Episode length: 82.20 +/- 14.30\n",
      "Eval num_timesteps=503808, episode_reward=-79.60 +/- 5.92\n",
      "Episode length: 80.60 +/- 5.92\n",
      "Eval num_timesteps=503808, episode_reward=-77.90 +/- 10.16\n",
      "Episode length: 78.90 +/- 10.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00170 |       0.00000 |      45.77926 |       0.00128 |       0.10811\n",
      "      0.00047 |       0.00000 |      45.64036 |       0.00127 |       0.10779\n",
      "      0.00205 |       0.00000 |      45.35470 |       0.00135 |       0.10874\n",
      "      0.00072 |       0.00000 |      45.36578 |       0.00140 |       0.10911\n",
      "      0.00044 |       0.00000 |      45.25910 |       0.00150 |       0.10904\n",
      "      0.00067 |       0.00000 |      45.11908 |       0.00144 |       0.10974\n",
      "      0.00121 |       0.00000 |      44.78592 |       0.00140 |       0.11014\n",
      "      0.00134 |       0.00000 |      45.11019 |       0.00140 |       0.10977\n",
      "     -0.00014 |       0.00000 |      44.91269 |       0.00158 |       0.11016\n",
      "      0.00045 |       0.00000 |      44.71388 |       0.00135 |       0.11056\n",
      "Evaluating losses...\n",
      "      0.00138 |       0.00000 |      44.55280 |       0.00154 |       0.11030\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.2         |\n",
      "| EpRewMean       | -90.2        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 5095         |\n",
      "| TimeElapsed     | 1.76e+03     |\n",
      "| TimestepsSoFar  | 507904       |\n",
      "| ev_tdlam_before | 0.81         |\n",
      "| loss_ent        | 0.11029577   |\n",
      "| loss_kl         | 0.0015380195 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013797558 |\n",
      "| loss_vf_loss    | 44.5528      |\n",
      "----------------------------------\n",
      "********** Iteration 124 ************\n",
      "Eval num_timesteps=507904, episode_reward=-74.90 +/- 5.49\n",
      "Episode length: 75.90 +/- 5.49\n",
      "Eval num_timesteps=507904, episode_reward=-74.40 +/- 4.39\n",
      "Episode length: 75.40 +/- 4.39\n",
      "Eval num_timesteps=507904, episode_reward=-80.10 +/- 12.42\n",
      "Episode length: 81.10 +/- 12.42\n",
      "Eval num_timesteps=507904, episode_reward=-74.20 +/- 7.92\n",
      "Episode length: 75.20 +/- 7.92\n",
      "Eval num_timesteps=507904, episode_reward=-85.30 +/- 30.06\n",
      "Episode length: 86.30 +/- 30.06\n",
      "Eval num_timesteps=507904, episode_reward=-99.60 +/- 58.80\n",
      "Episode length: 100.60 +/- 58.80\n",
      "Eval num_timesteps=507904, episode_reward=-76.00 +/- 6.00\n",
      "Episode length: 77.00 +/- 6.00\n",
      "Eval num_timesteps=507904, episode_reward=-78.90 +/- 12.52\n",
      "Episode length: 79.90 +/- 12.52\n",
      "Eval num_timesteps=507904, episode_reward=-90.80 +/- 55.79\n",
      "Episode length: 91.80 +/- 55.79\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00109 |       0.00000 |      48.72449 |       0.00122 |       0.11029\n",
      "      0.00106 |       0.00000 |      47.90334 |       0.00136 |       0.11126\n",
      "      0.00033 |       0.00000 |      47.52309 |       0.00125 |       0.11277\n",
      "      0.00115 |       0.00000 |      46.97294 |       0.00140 |       0.11318\n",
      "      0.00111 |       0.00000 |      46.42435 |       0.00146 |       0.11364\n",
      "     -0.00059 |       0.00000 |      46.33377 |       0.00148 |       0.11428\n",
      "      0.00084 |       0.00000 |      46.01585 |       0.00156 |       0.11428\n",
      "    -8.44e-05 |       0.00000 |      45.83139 |       0.00160 |       0.11516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00047 |       0.00000 |      45.66673 |       0.00164 |       0.11455\n",
      "      0.00045 |       0.00000 |      45.22111 |       0.00142 |       0.11470\n",
      "Evaluating losses...\n",
      "     -0.00042 |       0.00000 |      45.09519 |       0.00179 |       0.11557\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.4           |\n",
      "| EpRewMean       | -89.4          |\n",
      "| EpThisIter      | 45             |\n",
      "| EpisodesSoFar   | 5140           |\n",
      "| TimeElapsed     | 1.77e+03       |\n",
      "| TimestepsSoFar  | 512000         |\n",
      "| ev_tdlam_before | 0.794          |\n",
      "| loss_ent        | 0.115569234    |\n",
      "| loss_kl         | 0.0017896983   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00042038976 |\n",
      "| loss_vf_loss    | 45.095192      |\n",
      "------------------------------------\n",
      "********** Iteration 125 ************\n",
      "Eval num_timesteps=512000, episode_reward=-78.90 +/- 14.21\n",
      "Episode length: 79.90 +/- 14.21\n",
      "Eval num_timesteps=512000, episode_reward=-71.70 +/- 7.60\n",
      "Episode length: 72.70 +/- 7.60\n",
      "Eval num_timesteps=512000, episode_reward=-72.90 +/- 11.21\n",
      "Episode length: 73.90 +/- 11.21\n",
      "Eval num_timesteps=512000, episode_reward=-82.80 +/- 30.41\n",
      "Episode length: 83.80 +/- 30.41\n",
      "Eval num_timesteps=512000, episode_reward=-76.20 +/- 10.20\n",
      "Episode length: 77.20 +/- 10.20\n",
      "Eval num_timesteps=512000, episode_reward=-80.80 +/- 21.89\n",
      "Episode length: 81.80 +/- 21.89\n",
      "Eval num_timesteps=512000, episode_reward=-79.10 +/- 17.40\n",
      "Episode length: 80.10 +/- 17.40\n",
      "Eval num_timesteps=512000, episode_reward=-78.40 +/- 8.85\n",
      "Episode length: 79.40 +/- 8.85\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00111 |       0.00000 |      39.14344 |       0.00121 |       0.11149\n",
      "      0.00036 |       0.00000 |      38.76522 |       0.00116 |       0.11161\n",
      "      0.00062 |       0.00000 |      38.45435 |       0.00128 |       0.11213\n",
      "      0.00031 |       0.00000 |      38.18033 |       0.00133 |       0.11285\n",
      "      0.00178 |       0.00000 |      37.93191 |       0.00143 |       0.11308\n",
      "      0.00052 |       0.00000 |      37.85441 |       0.00152 |       0.11204\n",
      "      0.00097 |       0.00000 |      37.53563 |       0.00143 |       0.11142\n",
      "      0.00050 |       0.00000 |      37.32473 |       0.00149 |       0.11327\n",
      "      0.00151 |       0.00000 |      37.26116 |       0.00151 |       0.11326\n",
      "      0.00089 |       0.00000 |      37.18224 |       0.00145 |       0.11323\n",
      "Evaluating losses...\n",
      "      0.00053 |       0.00000 |      36.97062 |       0.00142 |       0.11321\n",
      "-----------------------------------\n",
      "| EpLenMean       | 89.2          |\n",
      "| EpRewMean       | -88.2         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 5186          |\n",
      "| TimeElapsed     | 1.78e+03      |\n",
      "| TimestepsSoFar  | 516096        |\n",
      "| ev_tdlam_before | 0.839         |\n",
      "| loss_ent        | 0.11321264    |\n",
      "| loss_kl         | 0.0014193059  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00053078204 |\n",
      "| loss_vf_loss    | 36.97062      |\n",
      "-----------------------------------\n",
      "********** Iteration 126 ************\n",
      "Eval num_timesteps=516096, episode_reward=-75.40 +/- 5.28\n",
      "Episode length: 76.40 +/- 5.28\n",
      "Eval num_timesteps=516096, episode_reward=-73.90 +/- 8.76\n",
      "Episode length: 74.90 +/- 8.76\n",
      "Eval num_timesteps=516096, episode_reward=-79.60 +/- 14.55\n",
      "Episode length: 80.60 +/- 14.55\n",
      "Eval num_timesteps=516096, episode_reward=-85.20 +/- 13.50\n",
      "Episode length: 86.20 +/- 13.50\n",
      "Eval num_timesteps=516096, episode_reward=-80.40 +/- 10.88\n",
      "Episode length: 81.40 +/- 10.88\n",
      "Eval num_timesteps=516096, episode_reward=-73.80 +/- 7.69\n",
      "Episode length: 74.80 +/- 7.69\n",
      "Eval num_timesteps=516096, episode_reward=-80.50 +/- 20.87\n",
      "Episode length: 81.50 +/- 20.87\n",
      "Eval num_timesteps=516096, episode_reward=-75.60 +/- 8.52\n",
      "Episode length: 76.60 +/- 8.52\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00164 |       0.00000 |      26.56214 |       0.00130 |       0.12026\n",
      "      0.00171 |       0.00000 |      26.19246 |       0.00139 |       0.11996\n",
      "      0.00179 |       0.00000 |      25.67891 |       0.00161 |       0.12163\n",
      "      0.00058 |       0.00000 |      25.45569 |       0.00164 |       0.12113\n",
      "      0.00116 |       0.00000 |      25.24814 |       0.00178 |       0.12230\n",
      "      0.00021 |       0.00000 |      25.04150 |       0.00174 |       0.12227\n",
      "      0.00212 |       0.00000 |      24.82857 |       0.00157 |       0.12204\n",
      "      0.00113 |       0.00000 |      24.73508 |       0.00176 |       0.12286\n",
      "      0.00094 |       0.00000 |      24.65413 |       0.00166 |       0.12284\n",
      "     1.13e-05 |       0.00000 |      24.50652 |       0.00170 |       0.12329\n",
      "Evaluating losses...\n",
      "      0.00058 |       0.00000 |      24.44164 |       0.00163 |       0.12188\n",
      "-----------------------------------\n",
      "| EpLenMean       | 85.9          |\n",
      "| EpRewMean       | -84.9         |\n",
      "| EpThisIter      | 49            |\n",
      "| EpisodesSoFar   | 5235          |\n",
      "| TimeElapsed     | 1.79e+03      |\n",
      "| TimestepsSoFar  | 520192        |\n",
      "| ev_tdlam_before | 0.899         |\n",
      "| loss_ent        | 0.121876776   |\n",
      "| loss_kl         | 0.0016331426  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00057748123 |\n",
      "| loss_vf_loss    | 24.441637     |\n",
      "-----------------------------------\n",
      "********** Iteration 127 ************\n",
      "Eval num_timesteps=520192, episode_reward=-86.50 +/- 34.42\n",
      "Episode length: 87.50 +/- 34.42\n",
      "Eval num_timesteps=520192, episode_reward=-75.50 +/- 12.44\n",
      "Episode length: 76.50 +/- 12.44\n",
      "Eval num_timesteps=520192, episode_reward=-80.00 +/- 21.30\n",
      "Episode length: 81.00 +/- 21.30\n",
      "Eval num_timesteps=520192, episode_reward=-73.60 +/- 7.95\n",
      "Episode length: 74.60 +/- 7.95\n",
      "Eval num_timesteps=520192, episode_reward=-95.50 +/- 42.05\n",
      "Episode length: 96.50 +/- 42.05\n",
      "Eval num_timesteps=520192, episode_reward=-86.90 +/- 41.49\n",
      "Episode length: 87.90 +/- 41.49\n",
      "Eval num_timesteps=520192, episode_reward=-80.30 +/- 29.35\n",
      "Episode length: 81.30 +/- 29.35\n",
      "Eval num_timesteps=520192, episode_reward=-84.70 +/- 12.03\n",
      "Episode length: 85.70 +/- 12.03\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00029 |       0.00000 |      43.41668 |       0.00117 |       0.11879\n",
      "      0.00224 |       0.00000 |      43.14246 |       0.00153 |       0.11774\n",
      "      0.00079 |       0.00000 |      42.93191 |       0.00138 |       0.11816\n",
      "      0.00160 |       0.00000 |      42.81757 |       0.00149 |       0.11809\n",
      "      0.00121 |       0.00000 |      42.56116 |       0.00156 |       0.11760\n",
      "      0.00165 |       0.00000 |      42.42214 |       0.00156 |       0.11780\n",
      "      0.00072 |       0.00000 |      42.25045 |       0.00136 |       0.11803\n",
      "      0.00089 |       0.00000 |      42.23357 |       0.00140 |       0.11952\n",
      "     -0.00031 |       0.00000 |      42.14618 |       0.00151 |       0.11862\n",
      "      0.00029 |       0.00000 |      42.09227 |       0.00155 |       0.11927\n",
      "Evaluating losses...\n",
      "      0.00142 |       0.00000 |      42.01503 |       0.00147 |       0.11865\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.2         |\n",
      "| EpRewMean       | -86.2        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 5280         |\n",
      "| TimeElapsed     | 1.8e+03      |\n",
      "| TimestepsSoFar  | 524288       |\n",
      "| ev_tdlam_before | 0.822        |\n",
      "| loss_ent        | 0.11865236   |\n",
      "| loss_kl         | 0.0014687335 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001416368  |\n",
      "| loss_vf_loss    | 42.015026    |\n",
      "----------------------------------\n",
      "********** Iteration 128 ************\n",
      "Eval num_timesteps=524288, episode_reward=-77.10 +/- 18.51\n",
      "Episode length: 78.10 +/- 18.51\n",
      "Eval num_timesteps=524288, episode_reward=-100.60 +/- 44.35\n",
      "Episode length: 101.60 +/- 44.35\n",
      "Eval num_timesteps=524288, episode_reward=-75.70 +/- 11.86\n",
      "Episode length: 76.70 +/- 11.86\n",
      "Eval num_timesteps=524288, episode_reward=-72.10 +/- 6.96\n",
      "Episode length: 73.10 +/- 6.96\n",
      "Eval num_timesteps=524288, episode_reward=-90.20 +/- 24.62\n",
      "Episode length: 91.20 +/- 24.62\n",
      "Eval num_timesteps=524288, episode_reward=-84.90 +/- 17.00\n",
      "Episode length: 85.90 +/- 17.00\n",
      "Eval num_timesteps=524288, episode_reward=-85.20 +/- 15.78\n",
      "Episode length: 86.20 +/- 15.78\n",
      "Eval num_timesteps=524288, episode_reward=-88.30 +/- 33.61\n",
      "Episode length: 89.30 +/- 33.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00094 |       0.00000 |      45.85931 |       0.00118 |       0.11437\n",
      "      0.00081 |       0.00000 |      44.54581 |       0.00123 |       0.11396\n",
      "      0.00049 |       0.00000 |      43.88601 |       0.00133 |       0.11387\n",
      "      0.00196 |       0.00000 |      43.61364 |       0.00146 |       0.11418\n",
      "     -0.00037 |       0.00000 |      43.24280 |       0.00144 |       0.11404\n",
      "      0.00130 |       0.00000 |      42.71390 |       0.00143 |       0.11431\n",
      "     1.87e-05 |       0.00000 |      42.61050 |       0.00129 |       0.11440\n",
      "      0.00033 |       0.00000 |      42.74343 |       0.00152 |       0.11500\n",
      "      0.00077 |       0.00000 |      42.46221 |       0.00140 |       0.11583\n",
      "      0.00114 |       0.00000 |      42.34351 |       0.00167 |       0.11535\n",
      "Evaluating losses...\n",
      "     -0.00017 |       0.00000 |      42.21457 |       0.00149 |       0.11461\n",
      "------------------------------------\n",
      "| EpLenMean       | 91.8           |\n",
      "| EpRewMean       | -90.8          |\n",
      "| EpThisIter      | 42             |\n",
      "| EpisodesSoFar   | 5322           |\n",
      "| TimeElapsed     | 1.81e+03       |\n",
      "| TimestepsSoFar  | 528384         |\n",
      "| ev_tdlam_before | 0.799          |\n",
      "| loss_ent        | 0.11461065     |\n",
      "| loss_kl         | 0.0014889857   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00017026253 |\n",
      "| loss_vf_loss    | 42.21457       |\n",
      "------------------------------------\n",
      "********** Iteration 129 ************\n",
      "Eval num_timesteps=528384, episode_reward=-78.40 +/- 8.13\n",
      "Episode length: 79.40 +/- 8.13\n",
      "Eval num_timesteps=528384, episode_reward=-76.20 +/- 7.59\n",
      "Episode length: 77.20 +/- 7.59\n",
      "Eval num_timesteps=528384, episode_reward=-78.00 +/- 12.87\n",
      "Episode length: 79.00 +/- 12.87\n",
      "Eval num_timesteps=528384, episode_reward=-76.20 +/- 8.32\n",
      "Episode length: 77.20 +/- 8.32\n",
      "Eval num_timesteps=528384, episode_reward=-73.70 +/- 10.91\n",
      "Episode length: 74.70 +/- 10.91\n",
      "Eval num_timesteps=528384, episode_reward=-77.00 +/- 9.17\n",
      "Episode length: 78.00 +/- 9.17\n",
      "Eval num_timesteps=528384, episode_reward=-95.50 +/- 62.54\n",
      "Episode length: 96.50 +/- 62.54\n",
      "Eval num_timesteps=528384, episode_reward=-76.50 +/- 10.18\n",
      "Episode length: 77.50 +/- 10.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00260 |       0.00000 |      34.48593 |       0.00132 |       0.12082\n",
      "      0.00143 |       0.00000 |      34.18419 |       0.00128 |       0.12102\n",
      "      0.00137 |       0.00000 |      34.05112 |       0.00136 |       0.12014\n",
      "      0.00294 |       0.00000 |      33.86013 |       0.00118 |       0.12027\n",
      "      0.00088 |       0.00000 |      33.85730 |       0.00130 |       0.12034\n",
      "      0.00146 |       0.00000 |      33.84692 |       0.00134 |       0.11979\n",
      "      0.00038 |       0.00000 |      33.76410 |       0.00135 |       0.11893\n",
      "      0.00171 |       0.00000 |      33.56764 |       0.00129 |       0.12008\n",
      "      0.00180 |       0.00000 |      33.57415 |       0.00137 |       0.12001\n",
      "      0.00169 |       0.00000 |      33.57263 |       0.00157 |       0.11947\n",
      "Evaluating losses...\n",
      "     -0.00016 |       0.00000 |      33.48903 |       0.00155 |       0.11984\n",
      "------------------------------------\n",
      "| EpLenMean       | 90.7           |\n",
      "| EpRewMean       | -89.7          |\n",
      "| EpThisIter      | 47             |\n",
      "| EpisodesSoFar   | 5369           |\n",
      "| TimeElapsed     | 1.82e+03       |\n",
      "| TimestepsSoFar  | 532480         |\n",
      "| ev_tdlam_before | 0.863          |\n",
      "| loss_ent        | 0.11983996     |\n",
      "| loss_kl         | 0.0015513774   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00015829247 |\n",
      "| loss_vf_loss    | 33.489033      |\n",
      "------------------------------------\n",
      "********** Iteration 130 ************\n",
      "Eval num_timesteps=532480, episode_reward=-73.60 +/- 9.33\n",
      "Episode length: 74.60 +/- 9.33\n",
      "Eval num_timesteps=532480, episode_reward=-78.10 +/- 8.08\n",
      "Episode length: 79.10 +/- 8.08\n",
      "Eval num_timesteps=532480, episode_reward=-76.40 +/- 6.17\n",
      "Episode length: 77.40 +/- 6.17\n",
      "Eval num_timesteps=532480, episode_reward=-74.40 +/- 6.55\n",
      "Episode length: 75.40 +/- 6.55\n",
      "Eval num_timesteps=532480, episode_reward=-81.20 +/- 17.98\n",
      "Episode length: 82.20 +/- 17.98\n",
      "Eval num_timesteps=532480, episode_reward=-76.40 +/- 10.26\n",
      "Episode length: 77.40 +/- 10.26\n",
      "Eval num_timesteps=532480, episode_reward=-78.70 +/- 11.23\n",
      "Episode length: 79.70 +/- 11.23\n",
      "Eval num_timesteps=532480, episode_reward=-79.10 +/- 8.90\n",
      "Episode length: 80.10 +/- 8.90\n",
      "Eval num_timesteps=532480, episode_reward=-76.70 +/- 5.24\n",
      "Episode length: 77.70 +/- 5.24\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00032 |       0.00000 |      36.99433 |       0.00114 |       0.11474\n",
      "      0.00093 |       0.00000 |      36.83504 |       0.00119 |       0.11503\n",
      "      0.00130 |       0.00000 |      36.65269 |       0.00134 |       0.11390\n",
      "      0.00229 |       0.00000 |      36.38868 |       0.00140 |       0.11321\n",
      "      0.00048 |       0.00000 |      36.19118 |       0.00127 |       0.11375\n",
      "      0.00092 |       0.00000 |      36.16969 |       0.00143 |       0.11338\n",
      "      0.00059 |       0.00000 |      36.05299 |       0.00139 |       0.11318\n",
      "      0.00045 |       0.00000 |      35.98171 |       0.00141 |       0.11327\n",
      "      0.00278 |       0.00000 |      35.87096 |       0.00150 |       0.11263\n",
      "      0.00048 |       0.00000 |      35.71985 |       0.00145 |       0.11233\n",
      "Evaluating losses...\n",
      "    -9.46e-05 |       0.00000 |      35.70433 |       0.00142 |       0.11259\n",
      "-----------------------------------\n",
      "| EpLenMean       | 88.5          |\n",
      "| EpRewMean       | -87.5         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 5416          |\n",
      "| TimeElapsed     | 1.83e+03      |\n",
      "| TimestepsSoFar  | 536576        |\n",
      "| ev_tdlam_before | 0.85          |\n",
      "| loss_ent        | 0.112588674   |\n",
      "| loss_kl         | 0.0014227087  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -9.456265e-05 |\n",
      "| loss_vf_loss    | 35.70433      |\n",
      "-----------------------------------\n",
      "********** Iteration 131 ************\n",
      "Eval num_timesteps=536576, episode_reward=-84.20 +/- 20.90\n",
      "Episode length: 85.20 +/- 20.90\n",
      "Eval num_timesteps=536576, episode_reward=-88.10 +/- 18.00\n",
      "Episode length: 89.10 +/- 18.00\n",
      "Eval num_timesteps=536576, episode_reward=-75.00 +/- 7.91\n",
      "Episode length: 76.00 +/- 7.91\n",
      "Eval num_timesteps=536576, episode_reward=-78.00 +/- 11.47\n",
      "Episode length: 79.00 +/- 11.47\n",
      "Eval num_timesteps=536576, episode_reward=-75.20 +/- 5.55\n",
      "Episode length: 76.20 +/- 5.55\n",
      "Eval num_timesteps=536576, episode_reward=-83.50 +/- 21.23\n",
      "Episode length: 84.50 +/- 21.23\n",
      "Eval num_timesteps=536576, episode_reward=-84.00 +/- 13.50\n",
      "Episode length: 85.00 +/- 13.50\n",
      "Eval num_timesteps=536576, episode_reward=-77.10 +/- 13.64\n",
      "Episode length: 78.10 +/- 13.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00152 |       0.00000 |      40.96011 |       0.00122 |       0.11051\n",
      "      0.00079 |       0.00000 |      40.42641 |       0.00132 |       0.11062\n",
      "      0.00088 |       0.00000 |      39.87446 |       0.00137 |       0.11086\n",
      "      0.00046 |       0.00000 |      39.68062 |       0.00144 |       0.11088\n",
      "      0.00133 |       0.00000 |      39.50177 |       0.00139 |       0.11163\n",
      "      0.00190 |       0.00000 |      39.23624 |       0.00149 |       0.11218\n",
      "      0.00093 |       0.00000 |      39.07378 |       0.00143 |       0.11249\n",
      "      0.00114 |       0.00000 |      38.91994 |       0.00134 |       0.11224\n",
      "      0.00094 |       0.00000 |      38.53913 |       0.00137 |       0.11191\n",
      "      0.00049 |       0.00000 |      38.64208 |       0.00146 |       0.11190\n",
      "Evaluating losses...\n",
      "      0.00192 |       0.00000 |      38.66898 |       0.00142 |       0.11215\n",
      "---------------------------------\n",
      "| EpLenMean       | 89.4        |\n",
      "| EpRewMean       | -88.4       |\n",
      "| EpThisIter      | 43          |\n",
      "| EpisodesSoFar   | 5459        |\n",
      "| TimeElapsed     | 1.84e+03    |\n",
      "| TimestepsSoFar  | 540672      |\n",
      "| ev_tdlam_before | 0.828       |\n",
      "| loss_ent        | 0.11214753  |\n",
      "| loss_kl         | 0.001418081 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | 0.001917646 |\n",
      "| loss_vf_loss    | 38.66898    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 132 ************\n",
      "Eval num_timesteps=540672, episode_reward=-78.00 +/- 8.52\n",
      "Episode length: 79.00 +/- 8.52\n",
      "Eval num_timesteps=540672, episode_reward=-80.20 +/- 18.87\n",
      "Episode length: 81.20 +/- 18.87\n",
      "Eval num_timesteps=540672, episode_reward=-76.80 +/- 11.97\n",
      "Episode length: 77.80 +/- 11.97\n",
      "Eval num_timesteps=540672, episode_reward=-72.10 +/- 5.96\n",
      "Episode length: 73.10 +/- 5.96\n",
      "Eval num_timesteps=540672, episode_reward=-85.20 +/- 24.89\n",
      "Episode length: 86.20 +/- 24.89\n",
      "Eval num_timesteps=540672, episode_reward=-107.40 +/- 44.42\n",
      "Episode length: 108.40 +/- 44.42\n",
      "Eval num_timesteps=540672, episode_reward=-82.20 +/- 19.64\n",
      "Episode length: 83.20 +/- 19.64\n",
      "Eval num_timesteps=540672, episode_reward=-70.40 +/- 4.18\n",
      "Episode length: 71.40 +/- 4.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00181 |       0.00000 |      40.35133 |       0.00119 |       0.10516\n",
      "      0.00120 |       0.00000 |      39.89603 |       0.00124 |       0.10558\n",
      "      0.00117 |       0.00000 |      39.36964 |       0.00120 |       0.10604\n",
      "      0.00139 |       0.00000 |      39.17085 |       0.00107 |       0.10554\n",
      "      0.00092 |       0.00000 |      39.09359 |       0.00116 |       0.10605\n",
      "      0.00060 |       0.00000 |      38.65442 |       0.00122 |       0.10620\n",
      "      0.00055 |       0.00000 |      38.61661 |       0.00110 |       0.10652\n",
      "      0.00093 |       0.00000 |      38.53685 |       0.00110 |       0.10698\n",
      "      0.00052 |       0.00000 |      38.52570 |       0.00127 |       0.10765\n",
      "      0.00060 |       0.00000 |      38.33962 |       0.00114 |       0.10698\n",
      "Evaluating losses...\n",
      "      0.00041 |       0.00000 |      38.16801 |       0.00114 |       0.10703\n",
      "-----------------------------------\n",
      "| EpLenMean       | 93.4          |\n",
      "| EpRewMean       | -92.4         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5504          |\n",
      "| TimeElapsed     | 1.85e+03      |\n",
      "| TimestepsSoFar  | 544768        |\n",
      "| ev_tdlam_before | 0.832         |\n",
      "| loss_ent        | 0.107030116   |\n",
      "| loss_kl         | 0.0011351508  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00040657585 |\n",
      "| loss_vf_loss    | 38.168007     |\n",
      "-----------------------------------\n",
      "********** Iteration 133 ************\n",
      "Eval num_timesteps=544768, episode_reward=-85.50 +/- 30.03\n",
      "Episode length: 86.50 +/- 30.03\n",
      "Eval num_timesteps=544768, episode_reward=-82.40 +/- 21.05\n",
      "Episode length: 83.40 +/- 21.05\n",
      "Eval num_timesteps=544768, episode_reward=-75.30 +/- 7.80\n",
      "Episode length: 76.30 +/- 7.80\n",
      "Eval num_timesteps=544768, episode_reward=-80.50 +/- 14.65\n",
      "Episode length: 81.50 +/- 14.65\n",
      "Eval num_timesteps=544768, episode_reward=-76.00 +/- 6.51\n",
      "Episode length: 77.00 +/- 6.51\n",
      "Eval num_timesteps=544768, episode_reward=-74.00 +/- 7.94\n",
      "Episode length: 75.00 +/- 7.94\n",
      "Eval num_timesteps=544768, episode_reward=-76.20 +/- 5.17\n",
      "Episode length: 77.20 +/- 5.17\n",
      "Eval num_timesteps=544768, episode_reward=-77.30 +/- 13.94\n",
      "Episode length: 78.30 +/- 13.94\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00047 |       0.00000 |      45.89544 |       0.00146 |       0.11435\n",
      "      0.00151 |       0.00000 |      45.33617 |       0.00123 |       0.11346\n",
      "      0.00163 |       0.00000 |      45.10028 |       0.00134 |       0.11382\n",
      "      0.00082 |       0.00000 |      44.87669 |       0.00126 |       0.11309\n",
      "      0.00097 |       0.00000 |      44.82871 |       0.00136 |       0.11255\n",
      "      0.00124 |       0.00000 |      44.40952 |       0.00140 |       0.11228\n",
      "      0.00191 |       0.00000 |      44.40840 |       0.00142 |       0.11223\n",
      "      0.00118 |       0.00000 |      44.04914 |       0.00143 |       0.11139\n",
      "      0.00057 |       0.00000 |      43.95948 |       0.00151 |       0.11217\n",
      "      0.00203 |       0.00000 |      43.94792 |       0.00143 |       0.11239\n",
      "Evaluating losses...\n",
      "      0.00140 |       0.00000 |      43.79165 |       0.00162 |       0.11186\n",
      "----------------------------------\n",
      "| EpLenMean       | 95.4         |\n",
      "| EpRewMean       | -94.4        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 5548         |\n",
      "| TimeElapsed     | 1.86e+03     |\n",
      "| TimestepsSoFar  | 548864       |\n",
      "| ev_tdlam_before | 0.805        |\n",
      "| loss_ent        | 0.11186036   |\n",
      "| loss_kl         | 0.0016152636 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013954975 |\n",
      "| loss_vf_loss    | 43.791653    |\n",
      "----------------------------------\n",
      "********** Iteration 134 ************\n",
      "Eval num_timesteps=548864, episode_reward=-82.10 +/- 35.58\n",
      "Episode length: 83.10 +/- 35.58\n",
      "Eval num_timesteps=548864, episode_reward=-72.90 +/- 10.71\n",
      "Episode length: 73.90 +/- 10.71\n",
      "Eval num_timesteps=548864, episode_reward=-76.90 +/- 13.24\n",
      "Episode length: 77.90 +/- 13.24\n",
      "Eval num_timesteps=548864, episode_reward=-77.60 +/- 12.65\n",
      "Episode length: 78.60 +/- 12.65\n",
      "Eval num_timesteps=548864, episode_reward=-71.70 +/- 7.86\n",
      "Episode length: 72.70 +/- 7.86\n",
      "Eval num_timesteps=548864, episode_reward=-79.30 +/- 11.17\n",
      "Episode length: 80.30 +/- 11.17\n",
      "Eval num_timesteps=548864, episode_reward=-81.80 +/- 22.68\n",
      "Episode length: 82.80 +/- 22.68\n",
      "Eval num_timesteps=548864, episode_reward=-79.40 +/- 15.68\n",
      "Episode length: 80.40 +/- 15.68\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00100 |       0.00000 |      37.88327 |       0.00128 |       0.11849\n",
      "      0.00162 |       0.00000 |      37.65014 |       0.00137 |       0.11827\n",
      "      0.00166 |       0.00000 |      37.30395 |       0.00156 |       0.11756\n",
      "      0.00144 |       0.00000 |      36.98675 |       0.00150 |       0.11699\n",
      "      0.00089 |       0.00000 |      37.18516 |       0.00143 |       0.11624\n",
      "      0.00130 |       0.00000 |      36.86192 |       0.00179 |       0.11584\n",
      "      0.00142 |       0.00000 |      36.44484 |       0.00173 |       0.11533\n",
      "      0.00089 |       0.00000 |      36.49446 |       0.00149 |       0.11552\n",
      "      0.00108 |       0.00000 |      36.52197 |       0.00152 |       0.11531\n",
      "      0.00240 |       0.00000 |      36.17377 |       0.00175 |       0.11559\n",
      "Evaluating losses...\n",
      "      0.00193 |       0.00000 |      36.04308 |       0.00161 |       0.11574\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.4         |\n",
      "| EpRewMean       | -90.4        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 5593         |\n",
      "| TimeElapsed     | 1.87e+03     |\n",
      "| TimestepsSoFar  | 552960       |\n",
      "| ev_tdlam_before | 0.846        |\n",
      "| loss_ent        | 0.11573623   |\n",
      "| loss_kl         | 0.0016097841 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019313599 |\n",
      "| loss_vf_loss    | 36.04308     |\n",
      "----------------------------------\n",
      "********** Iteration 135 ************\n",
      "Eval num_timesteps=552960, episode_reward=-73.40 +/- 3.85\n",
      "Episode length: 74.40 +/- 3.85\n",
      "Eval num_timesteps=552960, episode_reward=-81.30 +/- 10.95\n",
      "Episode length: 82.30 +/- 10.95\n",
      "Eval num_timesteps=552960, episode_reward=-75.60 +/- 8.59\n",
      "Episode length: 76.60 +/- 8.59\n",
      "Eval num_timesteps=552960, episode_reward=-75.50 +/- 7.45\n",
      "Episode length: 76.50 +/- 7.45\n",
      "Eval num_timesteps=552960, episode_reward=-72.10 +/- 8.28\n",
      "Episode length: 73.10 +/- 8.28\n",
      "Eval num_timesteps=552960, episode_reward=-87.90 +/- 39.86\n",
      "Episode length: 88.90 +/- 39.86\n",
      "Eval num_timesteps=552960, episode_reward=-79.70 +/- 9.27\n",
      "Episode length: 80.70 +/- 9.27\n",
      "Eval num_timesteps=552960, episode_reward=-74.10 +/- 7.61\n",
      "Episode length: 75.10 +/- 7.61\n",
      "Eval num_timesteps=552960, episode_reward=-77.20 +/- 10.16\n",
      "Episode length: 78.20 +/- 10.16\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00181 |       0.00000 |      54.92869 |       0.00130 |       0.11091\n",
      "      0.00104 |       0.00000 |      54.48809 |       0.00131 |       0.11107\n",
      "      0.00156 |       0.00000 |      54.11619 |       0.00134 |       0.11088\n",
      "      0.00104 |       0.00000 |      53.68976 |       0.00136 |       0.11077\n",
      "      0.00037 |       0.00000 |      53.60880 |       0.00157 |       0.11090\n",
      "      0.00034 |       0.00000 |      53.32324 |       0.00132 |       0.11012\n",
      "      0.00103 |       0.00000 |      52.93892 |       0.00142 |       0.11032\n",
      "      0.00169 |       0.00000 |      53.14650 |       0.00155 |       0.10998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00054 |       0.00000 |      52.82657 |       0.00136 |       0.10989\n",
      "      0.00144 |       0.00000 |      52.72614 |       0.00133 |       0.11098\n",
      "Evaluating losses...\n",
      "      0.00047 |       0.00000 |      52.40551 |       0.00168 |       0.11029\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.9         |\n",
      "| EpRewMean       | -89.9        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 5638         |\n",
      "| TimeElapsed     | 1.88e+03     |\n",
      "| TimestepsSoFar  | 557056       |\n",
      "| ev_tdlam_before | 0.765        |\n",
      "| loss_ent        | 0.11029166   |\n",
      "| loss_kl         | 0.0016754641 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0004732837 |\n",
      "| loss_vf_loss    | 52.405514    |\n",
      "----------------------------------\n",
      "********** Iteration 136 ************\n",
      "Eval num_timesteps=557056, episode_reward=-73.90 +/- 11.10\n",
      "Episode length: 74.90 +/- 11.10\n",
      "Eval num_timesteps=557056, episode_reward=-81.80 +/- 24.39\n",
      "Episode length: 82.80 +/- 24.39\n",
      "Eval num_timesteps=557056, episode_reward=-79.90 +/- 18.81\n",
      "Episode length: 80.90 +/- 18.81\n",
      "Eval num_timesteps=557056, episode_reward=-78.40 +/- 17.08\n",
      "Episode length: 79.40 +/- 17.08\n",
      "Eval num_timesteps=557056, episode_reward=-84.70 +/- 22.28\n",
      "Episode length: 85.70 +/- 22.28\n",
      "Eval num_timesteps=557056, episode_reward=-74.80 +/- 6.60\n",
      "Episode length: 75.80 +/- 6.60\n",
      "Eval num_timesteps=557056, episode_reward=-78.00 +/- 8.32\n",
      "Episode length: 79.00 +/- 8.32\n",
      "Eval num_timesteps=557056, episode_reward=-76.00 +/- 10.97\n",
      "Episode length: 77.00 +/- 10.97\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00153 |       0.00000 |      41.55890 |       0.00147 |       0.11622\n",
      "      0.00086 |       0.00000 |      41.30670 |       0.00140 |       0.11726\n",
      "      0.00157 |       0.00000 |      41.17291 |       0.00147 |       0.11524\n",
      "      0.00226 |       0.00000 |      40.96808 |       0.00145 |       0.11526\n",
      "      0.00171 |       0.00000 |      40.88418 |       0.00146 |       0.11565\n",
      "      0.00207 |       0.00000 |      40.65881 |       0.00162 |       0.11478\n",
      "      0.00191 |       0.00000 |      40.64824 |       0.00169 |       0.11506\n",
      "      0.00198 |       0.00000 |      40.59643 |       0.00193 |       0.11397\n",
      "     6.44e-05 |       0.00000 |      40.31153 |       0.00169 |       0.11512\n",
      "      0.00158 |       0.00000 |      40.26551 |       0.00155 |       0.11447\n",
      "Evaluating losses...\n",
      "      0.00034 |       0.00000 |      40.18332 |       0.00163 |       0.11559\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90            |\n",
      "| EpRewMean       | -89           |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5683          |\n",
      "| TimeElapsed     | 1.89e+03      |\n",
      "| TimestepsSoFar  | 561152        |\n",
      "| ev_tdlam_before | 0.828         |\n",
      "| loss_ent        | 0.1155934     |\n",
      "| loss_kl         | 0.0016266821  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00034026604 |\n",
      "| loss_vf_loss    | 40.18332      |\n",
      "-----------------------------------\n",
      "********** Iteration 137 ************\n",
      "Eval num_timesteps=561152, episode_reward=-88.60 +/- 41.27\n",
      "Episode length: 89.60 +/- 41.27\n",
      "Eval num_timesteps=561152, episode_reward=-71.90 +/- 6.33\n",
      "Episode length: 72.90 +/- 6.33\n",
      "Eval num_timesteps=561152, episode_reward=-75.90 +/- 9.98\n",
      "Episode length: 76.90 +/- 9.98\n",
      "Eval num_timesteps=561152, episode_reward=-76.40 +/- 7.63\n",
      "Episode length: 77.40 +/- 7.63\n",
      "Eval num_timesteps=561152, episode_reward=-79.10 +/- 8.01\n",
      "Episode length: 80.10 +/- 8.01\n",
      "Eval num_timesteps=561152, episode_reward=-78.30 +/- 8.30\n",
      "Episode length: 79.30 +/- 8.30\n",
      "Eval num_timesteps=561152, episode_reward=-74.50 +/- 9.12\n",
      "Episode length: 75.50 +/- 9.12\n",
      "Eval num_timesteps=561152, episode_reward=-72.90 +/- 6.56\n",
      "Episode length: 73.90 +/- 6.56\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00252 |       0.00000 |      31.74194 |       0.00129 |       0.11389\n",
      "      0.00019 |       0.00000 |      31.53965 |       0.00119 |       0.11545\n",
      "      0.00119 |       0.00000 |      31.21721 |       0.00153 |       0.11571\n",
      "      0.00148 |       0.00000 |      31.11163 |       0.00143 |       0.11603\n",
      "      0.00189 |       0.00000 |      30.82497 |       0.00142 |       0.11573\n",
      "      0.00084 |       0.00000 |      30.63369 |       0.00140 |       0.11623\n",
      "      0.00025 |       0.00000 |      30.62672 |       0.00153 |       0.11604\n",
      "      0.00114 |       0.00000 |      30.32641 |       0.00165 |       0.11614\n",
      "      0.00137 |       0.00000 |      30.31015 |       0.00165 |       0.11561\n",
      "      0.00120 |       0.00000 |      30.15734 |       0.00144 |       0.11533\n",
      "Evaluating losses...\n",
      "      0.00080 |       0.00000 |      30.06780 |       0.00137 |       0.11549\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.1         |\n",
      "| EpRewMean       | -87.1        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 5732         |\n",
      "| TimeElapsed     | 1.9e+03      |\n",
      "| TimestepsSoFar  | 565248       |\n",
      "| ev_tdlam_before | 0.878        |\n",
      "| loss_ent        | 0.11548697   |\n",
      "| loss_kl         | 0.0013706645 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0008037427 |\n",
      "| loss_vf_loss    | 30.067797    |\n",
      "----------------------------------\n",
      "********** Iteration 138 ************\n",
      "Eval num_timesteps=565248, episode_reward=-97.40 +/- 50.71\n",
      "Episode length: 98.40 +/- 50.71\n",
      "Eval num_timesteps=565248, episode_reward=-76.80 +/- 9.34\n",
      "Episode length: 77.80 +/- 9.34\n",
      "Eval num_timesteps=565248, episode_reward=-75.80 +/- 7.39\n",
      "Episode length: 76.80 +/- 7.39\n",
      "Eval num_timesteps=565248, episode_reward=-85.50 +/- 28.14\n",
      "Episode length: 86.50 +/- 28.14\n",
      "Eval num_timesteps=565248, episode_reward=-72.50 +/- 4.98\n",
      "Episode length: 73.50 +/- 4.98\n",
      "Eval num_timesteps=565248, episode_reward=-83.70 +/- 10.89\n",
      "Episode length: 84.70 +/- 10.89\n",
      "Eval num_timesteps=565248, episode_reward=-83.00 +/- 26.85\n",
      "Episode length: 84.00 +/- 26.85\n",
      "Eval num_timesteps=565248, episode_reward=-94.30 +/- 57.97\n",
      "Episode length: 95.30 +/- 57.97\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00175 |       0.00000 |      30.50597 |       0.00124 |       0.11391\n",
      "      0.00156 |       0.00000 |      30.36259 |       0.00128 |       0.11247\n",
      "      0.00122 |       0.00000 |      30.03743 |       0.00142 |       0.11200\n",
      "      0.00163 |       0.00000 |      29.97122 |       0.00145 |       0.11202\n",
      "      0.00178 |       0.00000 |      29.90345 |       0.00138 |       0.11178\n",
      "      0.00083 |       0.00000 |      29.84676 |       0.00140 |       0.11116\n",
      "      0.00051 |       0.00000 |      29.41083 |       0.00151 |       0.11109\n",
      "      0.00133 |       0.00000 |      29.55737 |       0.00132 |       0.11097\n",
      "      0.00110 |       0.00000 |      29.43581 |       0.00144 |       0.11079\n",
      "      0.00157 |       0.00000 |      29.22601 |       0.00165 |       0.11006\n",
      "Evaluating losses...\n",
      "      0.00257 |       0.00000 |      29.45895 |       0.00147 |       0.11026\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.4         |\n",
      "| EpRewMean       | -85.4        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 5779         |\n",
      "| TimeElapsed     | 1.91e+03     |\n",
      "| TimestepsSoFar  | 569344       |\n",
      "| ev_tdlam_before | 0.879        |\n",
      "| loss_ent        | 0.11026466   |\n",
      "| loss_kl         | 0.0014718369 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002570888  |\n",
      "| loss_vf_loss    | 29.458946    |\n",
      "----------------------------------\n",
      "********** Iteration 139 ************\n",
      "Eval num_timesteps=569344, episode_reward=-83.30 +/- 31.36\n",
      "Episode length: 84.30 +/- 31.36\n",
      "Eval num_timesteps=569344, episode_reward=-79.60 +/- 6.86\n",
      "Episode length: 80.60 +/- 6.86\n",
      "Eval num_timesteps=569344, episode_reward=-78.20 +/- 12.11\n",
      "Episode length: 79.20 +/- 12.11\n",
      "Eval num_timesteps=569344, episode_reward=-78.10 +/- 16.15\n",
      "Episode length: 79.10 +/- 16.15\n",
      "Eval num_timesteps=569344, episode_reward=-82.30 +/- 21.10\n",
      "Episode length: 83.30 +/- 21.10\n",
      "Eval num_timesteps=569344, episode_reward=-82.00 +/- 18.12\n",
      "Episode length: 83.00 +/- 18.12\n",
      "Eval num_timesteps=569344, episode_reward=-74.80 +/- 11.58\n",
      "Episode length: 75.80 +/- 11.58\n",
      "Eval num_timesteps=569344, episode_reward=-73.10 +/- 10.89\n",
      "Episode length: 74.10 +/- 10.89\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00147 |       0.00000 |      30.60696 |       0.00124 |       0.10922\n",
      "      0.00219 |       0.00000 |      30.61824 |       0.00117 |       0.11012\n",
      "      0.00066 |       0.00000 |      30.53042 |       0.00139 |       0.11082\n",
      "      0.00190 |       0.00000 |      30.43767 |       0.00137 |       0.11003\n",
      "      0.00134 |       0.00000 |      30.36450 |       0.00143 |       0.10976\n",
      "      0.00142 |       0.00000 |      30.17836 |       0.00138 |       0.11083\n",
      "      0.00136 |       0.00000 |      30.22877 |       0.00162 |       0.11021\n",
      "     7.32e-05 |       0.00000 |      30.07542 |       0.00150 |       0.10976\n",
      "      0.00089 |       0.00000 |      30.10594 |       0.00145 |       0.10935\n",
      "      0.00067 |       0.00000 |      29.89830 |       0.00149 |       0.10961\n",
      "Evaluating losses...\n",
      "      0.00189 |       0.00000 |      29.72525 |       0.00156 |       0.10925\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.6         |\n",
      "| EpRewMean       | -85.6        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 5826         |\n",
      "| TimeElapsed     | 1.92e+03     |\n",
      "| TimestepsSoFar  | 573440       |\n",
      "| ev_tdlam_before | 0.877        |\n",
      "| loss_ent        | 0.109252065  |\n",
      "| loss_kl         | 0.0015649952 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0018878209 |\n",
      "| loss_vf_loss    | 29.725252    |\n",
      "----------------------------------\n",
      "********** Iteration 140 ************\n",
      "Eval num_timesteps=573440, episode_reward=-82.40 +/- 18.29\n",
      "Episode length: 83.40 +/- 18.29\n",
      "Eval num_timesteps=573440, episode_reward=-74.90 +/- 8.63\n",
      "Episode length: 75.90 +/- 8.63\n",
      "Eval num_timesteps=573440, episode_reward=-77.90 +/- 13.40\n",
      "Episode length: 78.90 +/- 13.40\n",
      "Eval num_timesteps=573440, episode_reward=-75.70 +/- 6.05\n",
      "Episode length: 76.70 +/- 6.05\n",
      "Eval num_timesteps=573440, episode_reward=-73.20 +/- 5.29\n",
      "Episode length: 74.20 +/- 5.29\n",
      "Eval num_timesteps=573440, episode_reward=-74.20 +/- 13.22\n",
      "Episode length: 75.20 +/- 13.22\n",
      "Eval num_timesteps=573440, episode_reward=-81.60 +/- 13.81\n",
      "Episode length: 82.60 +/- 13.81\n",
      "Eval num_timesteps=573440, episode_reward=-71.70 +/- 9.82\n",
      "Episode length: 72.70 +/- 9.82\n",
      "Eval num_timesteps=573440, episode_reward=-76.00 +/- 7.81\n",
      "Episode length: 77.00 +/- 7.81\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00106 |       0.00000 |      42.77981 |       0.00126 |       0.10972\n",
      "      0.00177 |       0.00000 |      42.45210 |       0.00131 |       0.10895\n",
      "      0.00112 |       0.00000 |      42.07586 |       0.00125 |       0.10813\n",
      "      0.00171 |       0.00000 |      41.89000 |       0.00127 |       0.10881\n",
      "      0.00329 |       0.00000 |      41.69214 |       0.00136 |       0.10935\n",
      "      0.00112 |       0.00000 |      41.66125 |       0.00122 |       0.10916\n",
      "      0.00179 |       0.00000 |      41.54185 |       0.00141 |       0.10934\n",
      "      0.00118 |       0.00000 |      41.53351 |       0.00140 |       0.11038\n",
      "      0.00107 |       0.00000 |      41.34860 |       0.00138 |       0.10936\n",
      "      0.00115 |       0.00000 |      41.35329 |       0.00149 |       0.10934\n",
      "Evaluating losses...\n",
      "      0.00129 |       0.00000 |      41.21358 |       0.00128 |       0.10962\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.8         |\n",
      "| EpRewMean       | -86.8        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 5871         |\n",
      "| TimeElapsed     | 1.94e+03     |\n",
      "| TimestepsSoFar  | 577536       |\n",
      "| ev_tdlam_before | 0.822        |\n",
      "| loss_ent        | 0.109616704  |\n",
      "| loss_kl         | 0.0012827795 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0012900559 |\n",
      "| loss_vf_loss    | 41.213585    |\n",
      "----------------------------------\n",
      "********** Iteration 141 ************\n",
      "Eval num_timesteps=577536, episode_reward=-71.00 +/- 6.08\n",
      "Episode length: 72.00 +/- 6.08\n",
      "Eval num_timesteps=577536, episode_reward=-79.20 +/- 7.78\n",
      "Episode length: 80.20 +/- 7.78\n",
      "Eval num_timesteps=577536, episode_reward=-73.30 +/- 7.14\n",
      "Episode length: 74.30 +/- 7.14\n",
      "Eval num_timesteps=577536, episode_reward=-87.80 +/- 47.53\n",
      "Episode length: 88.80 +/- 47.53\n",
      "Eval num_timesteps=577536, episode_reward=-86.30 +/- 19.68\n",
      "Episode length: 87.30 +/- 19.68\n",
      "Eval num_timesteps=577536, episode_reward=-78.40 +/- 14.95\n",
      "Episode length: 79.40 +/- 14.95\n",
      "Eval num_timesteps=577536, episode_reward=-77.50 +/- 8.63\n",
      "Episode length: 78.50 +/- 8.63\n",
      "Eval num_timesteps=577536, episode_reward=-72.40 +/- 6.80\n",
      "Episode length: 73.40 +/- 6.80\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00090 |       0.00000 |      32.96629 |       0.00105 |       0.10315\n",
      "      0.00064 |       0.00000 |      32.66945 |       0.00127 |       0.10450\n",
      "      0.00114 |       0.00000 |      32.59994 |       0.00134 |       0.10409\n",
      "      0.00046 |       0.00000 |      32.43684 |       0.00144 |       0.10517\n",
      "      0.00079 |       0.00000 |      32.33603 |       0.00150 |       0.10549\n",
      "    -3.09e-05 |       0.00000 |      32.20649 |       0.00149 |       0.10630\n",
      "      0.00043 |       0.00000 |      32.14422 |       0.00130 |       0.10575\n",
      "      0.00099 |       0.00000 |      32.13801 |       0.00169 |       0.10608\n",
      "      0.00031 |       0.00000 |      32.00783 |       0.00163 |       0.10595\n",
      "      0.00032 |       0.00000 |      32.04220 |       0.00171 |       0.10627\n",
      "Evaluating losses...\n",
      "      0.00062 |       0.00000 |      31.82321 |       0.00164 |       0.10638\n",
      "-----------------------------------\n",
      "| EpLenMean       | 91.9          |\n",
      "| EpRewMean       | -90.9         |\n",
      "| EpThisIter      | 45            |\n",
      "| EpisodesSoFar   | 5916          |\n",
      "| TimeElapsed     | 1.94e+03      |\n",
      "| TimestepsSoFar  | 581632        |\n",
      "| ev_tdlam_before | 0.863         |\n",
      "| loss_ent        | 0.10637769    |\n",
      "| loss_kl         | 0.0016360445  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00062045234 |\n",
      "| loss_vf_loss    | 31.823206     |\n",
      "-----------------------------------\n",
      "********** Iteration 142 ************\n",
      "Eval num_timesteps=581632, episode_reward=-74.50 +/- 10.28\n",
      "Episode length: 75.50 +/- 10.28\n",
      "Eval num_timesteps=581632, episode_reward=-83.50 +/- 30.20\n",
      "Episode length: 84.50 +/- 30.20\n",
      "Eval num_timesteps=581632, episode_reward=-76.50 +/- 15.98\n",
      "Episode length: 77.50 +/- 15.98\n",
      "Eval num_timesteps=581632, episode_reward=-77.80 +/- 20.06\n",
      "Episode length: 78.80 +/- 20.06\n",
      "Eval num_timesteps=581632, episode_reward=-71.70 +/- 4.54\n",
      "Episode length: 72.70 +/- 4.54\n",
      "Eval num_timesteps=581632, episode_reward=-74.30 +/- 13.22\n",
      "Episode length: 75.30 +/- 13.22\n",
      "Eval num_timesteps=581632, episode_reward=-86.90 +/- 31.48\n",
      "Episode length: 87.90 +/- 31.48\n",
      "Eval num_timesteps=581632, episode_reward=-108.40 +/- 70.85\n",
      "Episode length: 109.40 +/- 70.85\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00090 |       0.00000 |      49.29211 |       0.00119 |       0.10774\n",
      "      0.00255 |       0.00000 |      48.42646 |       0.00144 |       0.10864\n",
      "      0.00132 |       0.00000 |      47.83883 |       0.00146 |       0.10838\n",
      "      0.00127 |       0.00000 |      47.53910 |       0.00148 |       0.10885\n",
      "      0.00046 |       0.00000 |      47.09498 |       0.00152 |       0.10905\n",
      "      0.00107 |       0.00000 |      46.91411 |       0.00156 |       0.10914\n",
      "    -4.57e-05 |       0.00000 |      46.74874 |       0.00177 |       0.10965\n",
      "      0.00017 |       0.00000 |      46.63465 |       0.00163 |       0.10956\n",
      "      0.00099 |       0.00000 |      46.18814 |       0.00139 |       0.10875\n",
      "      0.00039 |       0.00000 |      46.15108 |       0.00174 |       0.10892\n",
      "Evaluating losses...\n",
      "      0.00113 |       0.00000 |      46.00879 |       0.00166 |       0.10922\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.9         |\n",
      "| EpRewMean       | -91.9        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 5958         |\n",
      "| TimeElapsed     | 1.95e+03     |\n",
      "| TimestepsSoFar  | 585728       |\n",
      "| ev_tdlam_before | 0.789        |\n",
      "| loss_ent        | 0.10921638   |\n",
      "| loss_kl         | 0.0016621887 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001131482  |\n",
      "| loss_vf_loss    | 46.008785    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 143 ************\n",
      "Eval num_timesteps=585728, episode_reward=-73.30 +/- 5.10\n",
      "Episode length: 74.30 +/- 5.10\n",
      "Eval num_timesteps=585728, episode_reward=-99.10 +/- 71.51\n",
      "Episode length: 100.10 +/- 71.51\n",
      "Eval num_timesteps=585728, episode_reward=-74.90 +/- 13.83\n",
      "Episode length: 75.90 +/- 13.83\n",
      "Eval num_timesteps=585728, episode_reward=-85.40 +/- 27.72\n",
      "Episode length: 86.40 +/- 27.72\n",
      "Eval num_timesteps=585728, episode_reward=-79.20 +/- 16.31\n",
      "Episode length: 80.20 +/- 16.31\n",
      "Eval num_timesteps=585728, episode_reward=-102.40 +/- 79.35\n",
      "Episode length: 103.40 +/- 79.35\n",
      "Eval num_timesteps=585728, episode_reward=-70.40 +/- 5.64\n",
      "Episode length: 71.40 +/- 5.64\n",
      "Eval num_timesteps=585728, episode_reward=-77.30 +/- 11.25\n",
      "Episode length: 78.30 +/- 11.25\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00140 |       0.00000 |      48.36295 |       0.00125 |       0.09974\n",
      "      0.00096 |       0.00000 |      44.64742 |       0.00123 |       0.10030\n",
      "      0.00075 |       0.00000 |      42.88334 |       0.00126 |       0.10125\n",
      "      0.00104 |       0.00000 |      41.67240 |       0.00128 |       0.10109\n",
      "      0.00089 |       0.00000 |      40.82843 |       0.00133 |       0.10094\n",
      "    -1.78e-05 |       0.00000 |      40.04813 |       0.00143 |       0.10060\n",
      "      0.00211 |       0.00000 |      39.43153 |       0.00141 |       0.09951\n",
      "      0.00065 |       0.00000 |      38.83683 |       0.00148 |       0.10049\n",
      "      0.00034 |       0.00000 |      38.61839 |       0.00150 |       0.10104\n",
      "      0.00061 |       0.00000 |      38.24531 |       0.00159 |       0.10031\n",
      "Evaluating losses...\n",
      "      0.00024 |       0.00000 |      38.05294 |       0.00133 |       0.10005\n",
      "-----------------------------------\n",
      "| EpLenMean       | 98.4          |\n",
      "| EpRewMean       | -97.4         |\n",
      "| EpThisIter      | 41            |\n",
      "| EpisodesSoFar   | 5999          |\n",
      "| TimeElapsed     | 1.97e+03      |\n",
      "| TimestepsSoFar  | 589824        |\n",
      "| ev_tdlam_before | 0.777         |\n",
      "| loss_ent        | 0.10005048    |\n",
      "| loss_kl         | 0.0013323643  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00024112419 |\n",
      "| loss_vf_loss    | 38.05294      |\n",
      "-----------------------------------\n",
      "********** Iteration 144 ************\n",
      "Eval num_timesteps=589824, episode_reward=-69.00 +/- 7.25\n",
      "Episode length: 70.00 +/- 7.25\n",
      "Eval num_timesteps=589824, episode_reward=-78.10 +/- 8.68\n",
      "Episode length: 79.10 +/- 8.68\n",
      "Eval num_timesteps=589824, episode_reward=-83.50 +/- 19.98\n",
      "Episode length: 84.50 +/- 19.98\n",
      "Eval num_timesteps=589824, episode_reward=-74.50 +/- 9.35\n",
      "Episode length: 75.50 +/- 9.35\n",
      "Eval num_timesteps=589824, episode_reward=-71.50 +/- 10.54\n",
      "Episode length: 72.50 +/- 10.54\n",
      "Eval num_timesteps=589824, episode_reward=-74.00 +/- 7.04\n",
      "Episode length: 75.00 +/- 7.04\n",
      "Eval num_timesteps=589824, episode_reward=-72.60 +/- 9.31\n",
      "Episode length: 73.60 +/- 9.31\n",
      "Eval num_timesteps=589824, episode_reward=-83.90 +/- 25.70\n",
      "Episode length: 84.90 +/- 25.70\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00120 |       0.00000 |      42.35577 |       0.00142 |       0.11743\n",
      "      0.00121 |       0.00000 |      41.53098 |       0.00133 |       0.11752\n",
      "      0.00078 |       0.00000 |      40.91235 |       0.00137 |       0.11777\n",
      "      0.00141 |       0.00000 |      40.80429 |       0.00134 |       0.11788\n",
      "      0.00185 |       0.00000 |      40.54969 |       0.00138 |       0.11904\n",
      "      0.00133 |       0.00000 |      40.40520 |       0.00133 |       0.11823\n",
      "      0.00132 |       0.00000 |      40.41161 |       0.00137 |       0.11900\n",
      "      0.00178 |       0.00000 |      40.29356 |       0.00146 |       0.11988\n",
      "      0.00061 |       0.00000 |      40.30260 |       0.00134 |       0.11980\n",
      "      0.00077 |       0.00000 |      40.09324 |       0.00130 |       0.11993\n",
      "Evaluating losses...\n",
      "      0.00076 |       0.00000 |      40.23376 |       0.00151 |       0.11985\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.3          |\n",
      "| EpRewMean       | -93.3         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6046          |\n",
      "| TimeElapsed     | 1.97e+03      |\n",
      "| TimestepsSoFar  | 593920        |\n",
      "| ev_tdlam_before | 0.834         |\n",
      "| loss_ent        | 0.11984793    |\n",
      "| loss_kl         | 0.001506177   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00076059066 |\n",
      "| loss_vf_loss    | 40.233757     |\n",
      "-----------------------------------\n",
      "********** Iteration 145 ************\n",
      "Eval num_timesteps=593920, episode_reward=-73.20 +/- 9.23\n",
      "Episode length: 74.20 +/- 9.23\n",
      "Eval num_timesteps=593920, episode_reward=-71.70 +/- 7.39\n",
      "Episode length: 72.70 +/- 7.39\n",
      "Eval num_timesteps=593920, episode_reward=-73.90 +/- 11.61\n",
      "Episode length: 74.90 +/- 11.61\n",
      "Eval num_timesteps=593920, episode_reward=-76.50 +/- 14.02\n",
      "Episode length: 77.50 +/- 14.02\n",
      "Eval num_timesteps=593920, episode_reward=-78.90 +/- 12.35\n",
      "Episode length: 79.90 +/- 12.35\n",
      "Eval num_timesteps=593920, episode_reward=-75.70 +/- 9.47\n",
      "Episode length: 76.70 +/- 9.47\n",
      "Eval num_timesteps=593920, episode_reward=-80.60 +/- 11.75\n",
      "Episode length: 81.60 +/- 11.75\n",
      "Eval num_timesteps=593920, episode_reward=-73.10 +/- 11.56\n",
      "Episode length: 74.10 +/- 11.56\n",
      "Eval num_timesteps=593920, episode_reward=-71.40 +/- 5.75\n",
      "Episode length: 72.40 +/- 5.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00133 |       0.00000 |      51.80920 |       0.00133 |       0.11479\n",
      "      0.00147 |       0.00000 |      51.21603 |       0.00128 |       0.11497\n",
      "      0.00040 |       0.00000 |      50.83663 |       0.00137 |       0.11527\n",
      "      0.00159 |       0.00000 |      50.37919 |       0.00136 |       0.11430\n",
      "      0.00184 |       0.00000 |      50.32740 |       0.00138 |       0.11422\n",
      "      0.00069 |       0.00000 |      50.01967 |       0.00146 |       0.11360\n",
      "      0.00309 |       0.00000 |      49.97622 |       0.00161 |       0.11384\n",
      "      0.00168 |       0.00000 |      49.70343 |       0.00158 |       0.11345\n",
      "      0.00041 |       0.00000 |      49.47774 |       0.00150 |       0.11372\n",
      "      0.00068 |       0.00000 |      49.59626 |       0.00134 |       0.11390\n",
      "Evaluating losses...\n",
      "      0.00108 |       0.00000 |      49.33368 |       0.00146 |       0.11330\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.9         |\n",
      "| EpRewMean       | -87.9        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 6090         |\n",
      "| TimeElapsed     | 1.98e+03     |\n",
      "| TimestepsSoFar  | 598016       |\n",
      "| ev_tdlam_before | 0.784        |\n",
      "| loss_ent        | 0.11329524   |\n",
      "| loss_kl         | 0.0014568483 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0010793488 |\n",
      "| loss_vf_loss    | 49.33368     |\n",
      "----------------------------------\n",
      "********** Iteration 146 ************\n",
      "Eval num_timesteps=598016, episode_reward=-80.10 +/- 7.85\n",
      "Episode length: 81.10 +/- 7.85\n",
      "Eval num_timesteps=598016, episode_reward=-75.30 +/- 11.13\n",
      "Episode length: 76.30 +/- 11.13\n",
      "Eval num_timesteps=598016, episode_reward=-74.80 +/- 11.25\n",
      "Episode length: 75.80 +/- 11.25\n",
      "Eval num_timesteps=598016, episode_reward=-72.80 +/- 5.71\n",
      "Episode length: 73.80 +/- 5.71\n",
      "Eval num_timesteps=598016, episode_reward=-70.50 +/- 4.18\n",
      "Episode length: 71.50 +/- 4.18\n",
      "Eval num_timesteps=598016, episode_reward=-87.30 +/- 35.63\n",
      "Episode length: 88.30 +/- 35.63\n",
      "Eval num_timesteps=598016, episode_reward=-81.00 +/- 25.39\n",
      "Episode length: 82.00 +/- 25.39\n",
      "Eval num_timesteps=598016, episode_reward=-77.40 +/- 15.47\n",
      "Episode length: 78.40 +/- 15.47\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00316 |       0.00000 |      58.48212 |       0.00133 |       0.11595\n",
      "      0.00173 |       0.00000 |      57.77799 |       0.00156 |       0.11541\n",
      "      0.00260 |       0.00000 |      57.40474 |       0.00150 |       0.11537\n",
      "      0.00246 |       0.00000 |      57.10487 |       0.00142 |       0.11443\n",
      "      0.00265 |       0.00000 |      56.84376 |       0.00147 |       0.11413\n",
      "      0.00101 |       0.00000 |      56.75071 |       0.00160 |       0.11403\n",
      "      0.00082 |       0.00000 |      56.38523 |       0.00151 |       0.11422\n",
      "      0.00165 |       0.00000 |      56.31742 |       0.00173 |       0.11374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00192 |       0.00000 |      56.42304 |       0.00168 |       0.11438\n",
      "      0.00242 |       0.00000 |      56.09668 |       0.00157 |       0.11393\n",
      "Evaluating losses...\n",
      "      0.00212 |       0.00000 |      55.95340 |       0.00181 |       0.11359\n",
      "----------------------------------\n",
      "| EpLenMean       | 94.4         |\n",
      "| EpRewMean       | -93.4        |\n",
      "| EpThisIter      | 41           |\n",
      "| EpisodesSoFar   | 6131         |\n",
      "| TimeElapsed     | 1.99e+03     |\n",
      "| TimestepsSoFar  | 602112       |\n",
      "| ev_tdlam_before | 0.747        |\n",
      "| loss_ent        | 0.1135858    |\n",
      "| loss_kl         | 0.0018132178 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0021153619 |\n",
      "| loss_vf_loss    | 55.953396    |\n",
      "----------------------------------\n",
      "********** Iteration 147 ************\n",
      "Eval num_timesteps=602112, episode_reward=-82.00 +/- 12.66\n",
      "Episode length: 83.00 +/- 12.66\n",
      "Eval num_timesteps=602112, episode_reward=-72.90 +/- 8.62\n",
      "Episode length: 73.90 +/- 8.62\n",
      "Eval num_timesteps=602112, episode_reward=-78.30 +/- 13.54\n",
      "Episode length: 79.30 +/- 13.54\n",
      "Eval num_timesteps=602112, episode_reward=-73.50 +/- 10.74\n",
      "Episode length: 74.50 +/- 10.74\n",
      "Eval num_timesteps=602112, episode_reward=-73.60 +/- 10.55\n",
      "Episode length: 74.60 +/- 10.55\n",
      "Eval num_timesteps=602112, episode_reward=-72.10 +/- 8.44\n",
      "Episode length: 73.10 +/- 8.44\n",
      "Eval num_timesteps=602112, episode_reward=-80.40 +/- 18.48\n",
      "Episode length: 81.40 +/- 18.48\n",
      "Eval num_timesteps=602112, episode_reward=-88.60 +/- 31.37\n",
      "Episode length: 89.60 +/- 31.37\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00236 |       0.00000 |      50.40160 |       0.00143 |       0.10492\n",
      "      0.00101 |       0.00000 |      49.68497 |       0.00136 |       0.10407\n",
      "      0.00111 |       0.00000 |      49.14123 |       0.00136 |       0.10442\n",
      "      0.00209 |       0.00000 |      48.82682 |       0.00150 |       0.10355\n",
      "      0.00025 |       0.00000 |      48.57353 |       0.00154 |       0.10410\n",
      "      0.00076 |       0.00000 |      48.48697 |       0.00150 |       0.10460\n",
      "      0.00070 |       0.00000 |      48.08543 |       0.00162 |       0.10407\n",
      "      0.00051 |       0.00000 |      48.05597 |       0.00170 |       0.10438\n",
      "      0.00085 |       0.00000 |      47.99926 |       0.00178 |       0.10447\n",
      "      0.00020 |       0.00000 |      47.52482 |       0.00177 |       0.10407\n",
      "Evaluating losses...\n",
      "      0.00052 |       0.00000 |      47.63034 |       0.00164 |       0.10421\n",
      "----------------------------------\n",
      "| EpLenMean       | 96           |\n",
      "| EpRewMean       | -95          |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 6173         |\n",
      "| TimeElapsed     | 2e+03        |\n",
      "| TimestepsSoFar  | 606208       |\n",
      "| ev_tdlam_before | 0.785        |\n",
      "| loss_ent        | 0.10421362   |\n",
      "| loss_kl         | 0.0016383587 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0005152285 |\n",
      "| loss_vf_loss    | 47.630344    |\n",
      "----------------------------------\n",
      "********** Iteration 148 ************\n",
      "Eval num_timesteps=606208, episode_reward=-75.80 +/- 5.53\n",
      "Episode length: 76.80 +/- 5.53\n",
      "Eval num_timesteps=606208, episode_reward=-76.60 +/- 10.24\n",
      "Episode length: 77.60 +/- 10.24\n",
      "Eval num_timesteps=606208, episode_reward=-75.40 +/- 9.81\n",
      "Episode length: 76.40 +/- 9.81\n",
      "Eval num_timesteps=606208, episode_reward=-100.50 +/- 71.97\n",
      "Episode length: 101.50 +/- 71.97\n",
      "Eval num_timesteps=606208, episode_reward=-84.70 +/- 26.44\n",
      "Episode length: 85.70 +/- 26.44\n",
      "Eval num_timesteps=606208, episode_reward=-74.90 +/- 6.14\n",
      "Episode length: 75.90 +/- 6.14\n",
      "Eval num_timesteps=606208, episode_reward=-70.60 +/- 6.10\n",
      "Episode length: 71.60 +/- 6.10\n",
      "Eval num_timesteps=606208, episode_reward=-84.40 +/- 25.33\n",
      "Episode length: 85.40 +/- 25.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00104 |       0.00000 |      44.99791 |       0.00120 |       0.10492\n",
      "      0.00097 |       0.00000 |      44.82096 |       0.00122 |       0.10469\n",
      "      0.00147 |       0.00000 |      44.56676 |       0.00120 |       0.10493\n",
      "      0.00173 |       0.00000 |      44.51884 |       0.00130 |       0.10521\n",
      "      0.00118 |       0.00000 |      44.36304 |       0.00146 |       0.10503\n",
      "      0.00114 |       0.00000 |      44.11368 |       0.00126 |       0.10554\n",
      "      0.00062 |       0.00000 |      44.02591 |       0.00122 |       0.10545\n",
      "      0.00137 |       0.00000 |      44.04717 |       0.00136 |       0.10442\n",
      "      0.00162 |       0.00000 |      43.92513 |       0.00131 |       0.10560\n",
      "      0.00135 |       0.00000 |      43.86351 |       0.00156 |       0.10498\n",
      "Evaluating losses...\n",
      "      0.00022 |       0.00000 |      43.77793 |       0.00159 |       0.10496\n",
      "-----------------------------------\n",
      "| EpLenMean       | 95.6          |\n",
      "| EpRewMean       | -94.6         |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 6217          |\n",
      "| TimeElapsed     | 2.01e+03      |\n",
      "| TimestepsSoFar  | 610304        |\n",
      "| ev_tdlam_before | 0.815         |\n",
      "| loss_ent        | 0.10495682    |\n",
      "| loss_kl         | 0.0015903482  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00022494904 |\n",
      "| loss_vf_loss    | 43.777927     |\n",
      "-----------------------------------\n",
      "********** Iteration 149 ************\n",
      "Eval num_timesteps=610304, episode_reward=-73.10 +/- 8.51\n",
      "Episode length: 74.10 +/- 8.51\n",
      "Eval num_timesteps=610304, episode_reward=-92.50 +/- 38.06\n",
      "Episode length: 93.50 +/- 38.06\n",
      "Eval num_timesteps=610304, episode_reward=-73.40 +/- 8.30\n",
      "Episode length: 74.40 +/- 8.30\n",
      "Eval num_timesteps=610304, episode_reward=-78.00 +/- 8.32\n",
      "Episode length: 79.00 +/- 8.32\n",
      "Eval num_timesteps=610304, episode_reward=-78.80 +/- 12.47\n",
      "Episode length: 79.80 +/- 12.47\n",
      "Eval num_timesteps=610304, episode_reward=-80.40 +/- 10.89\n",
      "Episode length: 81.40 +/- 10.89\n",
      "Eval num_timesteps=610304, episode_reward=-83.20 +/- 10.68\n",
      "Episode length: 84.20 +/- 10.68\n",
      "Eval num_timesteps=610304, episode_reward=-75.40 +/- 11.94\n",
      "Episode length: 76.40 +/- 11.94\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00222 |       0.00000 |      46.28273 |       0.00125 |       0.10718\n",
      "      0.00055 |       0.00000 |      45.35625 |       0.00122 |       0.10639\n",
      "      0.00160 |       0.00000 |      44.94959 |       0.00125 |       0.10621\n",
      "      0.00224 |       0.00000 |      44.52938 |       0.00136 |       0.10561\n",
      "      0.00157 |       0.00000 |      44.02365 |       0.00127 |       0.10623\n",
      "      0.00208 |       0.00000 |      44.01516 |       0.00153 |       0.10651\n",
      "      0.00071 |       0.00000 |      43.83210 |       0.00158 |       0.10645\n",
      "      0.00052 |       0.00000 |      43.68935 |       0.00136 |       0.10625\n",
      "      0.00071 |       0.00000 |      43.65299 |       0.00133 |       0.10668\n",
      "      0.00089 |       0.00000 |      43.43954 |       0.00163 |       0.10613\n",
      "Evaluating losses...\n",
      "      0.00087 |       0.00000 |      43.33480 |       0.00141 |       0.10657\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.8          |\n",
      "| EpRewMean       | -89.8         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6264          |\n",
      "| TimeElapsed     | 2.02e+03      |\n",
      "| TimestepsSoFar  | 614400        |\n",
      "| ev_tdlam_before | 0.812         |\n",
      "| loss_ent        | 0.10656826    |\n",
      "| loss_kl         | 0.0014107897  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00086904323 |\n",
      "| loss_vf_loss    | 43.334797     |\n",
      "-----------------------------------\n",
      "********** Iteration 150 ************\n",
      "Eval num_timesteps=614400, episode_reward=-71.00 +/- 8.57\n",
      "Episode length: 72.00 +/- 8.57\n",
      "Eval num_timesteps=614400, episode_reward=-76.50 +/- 10.52\n",
      "Episode length: 77.50 +/- 10.52\n",
      "Eval num_timesteps=614400, episode_reward=-77.10 +/- 9.47\n",
      "Episode length: 78.10 +/- 9.47\n",
      "Eval num_timesteps=614400, episode_reward=-81.70 +/- 12.51\n",
      "Episode length: 82.70 +/- 12.51\n",
      "Eval num_timesteps=614400, episode_reward=-72.80 +/- 12.54\n",
      "Episode length: 73.80 +/- 12.54\n",
      "Eval num_timesteps=614400, episode_reward=-69.30 +/- 5.44\n",
      "Episode length: 70.30 +/- 5.44\n",
      "Eval num_timesteps=614400, episode_reward=-72.60 +/- 5.92\n",
      "Episode length: 73.60 +/- 5.92\n",
      "Eval num_timesteps=614400, episode_reward=-75.90 +/- 9.76\n",
      "Episode length: 76.90 +/- 9.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00093 |       0.00000 |      48.96506 |       0.00148 |       0.10952\n",
      "      0.00135 |       0.00000 |      48.83728 |       0.00130 |       0.10931\n",
      "      0.00064 |       0.00000 |      48.43934 |       0.00133 |       0.10880\n",
      "      0.00163 |       0.00000 |      48.35121 |       0.00121 |       0.10888\n",
      "      0.00186 |       0.00000 |      48.08371 |       0.00130 |       0.10839\n",
      "      0.00212 |       0.00000 |      48.02922 |       0.00141 |       0.10808\n",
      "      0.00010 |       0.00000 |      47.79181 |       0.00127 |       0.10861\n",
      "      0.00132 |       0.00000 |      47.74238 |       0.00135 |       0.10858\n",
      "      0.00129 |       0.00000 |      47.48948 |       0.00139 |       0.10906\n",
      "      0.00058 |       0.00000 |      47.37222 |       0.00153 |       0.10815\n",
      "Evaluating losses...\n",
      "     2.10e-05 |       0.00000 |      47.51202 |       0.00111 |       0.10857\n",
      "-----------------------------------\n",
      "| EpLenMean       | 87.5          |\n",
      "| EpRewMean       | -86.5         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 6310          |\n",
      "| TimeElapsed     | 2.03e+03      |\n",
      "| TimestepsSoFar  | 618496        |\n",
      "| ev_tdlam_before | 0.807         |\n",
      "| loss_ent        | 0.108570136   |\n",
      "| loss_kl         | 0.0011060536  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 2.0956417e-05 |\n",
      "| loss_vf_loss    | 47.512024     |\n",
      "-----------------------------------\n",
      "********** Iteration 151 ************\n",
      "Eval num_timesteps=618496, episode_reward=-69.70 +/- 8.19\n",
      "Episode length: 70.70 +/- 8.19\n",
      "Eval num_timesteps=618496, episode_reward=-76.70 +/- 8.65\n",
      "Episode length: 77.70 +/- 8.65\n",
      "Eval num_timesteps=618496, episode_reward=-76.20 +/- 11.11\n",
      "Episode length: 77.20 +/- 11.11\n",
      "Eval num_timesteps=618496, episode_reward=-79.80 +/- 17.31\n",
      "Episode length: 80.80 +/- 17.31\n",
      "Eval num_timesteps=618496, episode_reward=-76.60 +/- 12.28\n",
      "Episode length: 77.60 +/- 12.28\n",
      "Eval num_timesteps=618496, episode_reward=-83.10 +/- 30.38\n",
      "Episode length: 84.10 +/- 30.38\n",
      "Eval num_timesteps=618496, episode_reward=-74.90 +/- 12.99\n",
      "Episode length: 75.90 +/- 12.99\n",
      "Eval num_timesteps=618496, episode_reward=-73.90 +/- 7.84\n",
      "Episode length: 74.90 +/- 7.84\n",
      "Eval num_timesteps=618496, episode_reward=-70.10 +/- 7.69\n",
      "Episode length: 71.10 +/- 7.69\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00178 |       0.00000 |      43.62220 |       0.00133 |       0.10765\n",
      "      0.00190 |       0.00000 |      43.35519 |       0.00142 |       0.10692\n",
      "      0.00146 |       0.00000 |      43.37252 |       0.00134 |       0.10757\n",
      "      0.00174 |       0.00000 |      43.29275 |       0.00138 |       0.10683\n",
      "      0.00167 |       0.00000 |      43.26291 |       0.00158 |       0.10687\n",
      "      0.00100 |       0.00000 |      43.05031 |       0.00177 |       0.10706\n",
      "      0.00020 |       0.00000 |      42.93327 |       0.00153 |       0.10688\n",
      "      0.00116 |       0.00000 |      42.90601 |       0.00154 |       0.10744\n",
      "      0.00129 |       0.00000 |      42.92842 |       0.00167 |       0.10709\n",
      "      0.00025 |       0.00000 |      42.82457 |       0.00159 |       0.10714\n",
      "Evaluating losses...\n",
      "     -0.00015 |       0.00000 |      42.78394 |       0.00164 |       0.10741\n",
      "------------------------------------\n",
      "| EpLenMean       | 89.4           |\n",
      "| EpRewMean       | -88.4          |\n",
      "| EpThisIter      | 46             |\n",
      "| EpisodesSoFar   | 6356           |\n",
      "| TimeElapsed     | 2.04e+03       |\n",
      "| TimestepsSoFar  | 622592         |\n",
      "| ev_tdlam_before | 0.822          |\n",
      "| loss_ent        | 0.10740811     |\n",
      "| loss_kl         | 0.0016419529   |\n",
      "| loss_pol_entpen | 0.0            |\n",
      "| loss_pol_surr   | -0.00015339162 |\n",
      "| loss_vf_loss    | 42.783943      |\n",
      "------------------------------------\n",
      "********** Iteration 152 ************\n",
      "Eval num_timesteps=622592, episode_reward=-84.70 +/- 21.28\n",
      "Episode length: 85.70 +/- 21.28\n",
      "Eval num_timesteps=622592, episode_reward=-76.70 +/- 13.84\n",
      "Episode length: 77.70 +/- 13.84\n",
      "Eval num_timesteps=622592, episode_reward=-75.60 +/- 10.64\n",
      "Episode length: 76.60 +/- 10.64\n",
      "Eval num_timesteps=622592, episode_reward=-75.70 +/- 6.91\n",
      "Episode length: 76.70 +/- 6.91\n",
      "Eval num_timesteps=622592, episode_reward=-85.10 +/- 36.59\n",
      "Episode length: 86.10 +/- 36.59\n",
      "Eval num_timesteps=622592, episode_reward=-77.00 +/- 8.37\n",
      "Episode length: 78.00 +/- 8.37\n",
      "Eval num_timesteps=622592, episode_reward=-84.30 +/- 27.50\n",
      "Episode length: 85.30 +/- 27.50\n",
      "Eval num_timesteps=622592, episode_reward=-72.40 +/- 11.54\n",
      "Episode length: 73.40 +/- 11.54\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00119 |       0.00000 |      45.74565 |       0.00108 |       0.10270\n",
      "      0.00106 |       0.00000 |      45.37827 |       0.00115 |       0.10244\n",
      "      0.00115 |       0.00000 |      44.83760 |       0.00139 |       0.10310\n",
      "      0.00111 |       0.00000 |      44.69955 |       0.00133 |       0.10290\n",
      "      0.00150 |       0.00000 |      44.54934 |       0.00121 |       0.10215\n",
      "      0.00087 |       0.00000 |      44.42644 |       0.00138 |       0.10247\n",
      "      0.00144 |       0.00000 |      44.20116 |       0.00140 |       0.10224\n",
      "      0.00142 |       0.00000 |      43.97296 |       0.00151 |       0.10134\n",
      "      0.00133 |       0.00000 |      44.01543 |       0.00135 |       0.10181\n",
      "      0.00210 |       0.00000 |      43.97264 |       0.00171 |       0.10161\n",
      "Evaluating losses...\n",
      "      0.00083 |       0.00000 |      43.68420 |       0.00147 |       0.10185\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.6         |\n",
      "| EpRewMean       | -88.6        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 6402         |\n",
      "| TimeElapsed     | 2.05e+03     |\n",
      "| TimestepsSoFar  | 626688       |\n",
      "| ev_tdlam_before | 0.81         |\n",
      "| loss_ent        | 0.10184668   |\n",
      "| loss_kl         | 0.0014699895 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0008324246 |\n",
      "| loss_vf_loss    | 43.684196    |\n",
      "----------------------------------\n",
      "********** Iteration 153 ************\n",
      "Eval num_timesteps=626688, episode_reward=-80.90 +/- 15.37\n",
      "Episode length: 81.90 +/- 15.37\n",
      "Eval num_timesteps=626688, episode_reward=-78.70 +/- 9.28\n",
      "Episode length: 79.70 +/- 9.28\n",
      "Eval num_timesteps=626688, episode_reward=-80.60 +/- 12.01\n",
      "Episode length: 81.60 +/- 12.01\n",
      "Eval num_timesteps=626688, episode_reward=-82.50 +/- 34.89\n",
      "Episode length: 83.50 +/- 34.89\n",
      "Eval num_timesteps=626688, episode_reward=-78.50 +/- 9.77\n",
      "Episode length: 79.50 +/- 9.77\n",
      "Eval num_timesteps=626688, episode_reward=-84.50 +/- 16.29\n",
      "Episode length: 85.50 +/- 16.29\n",
      "Eval num_timesteps=626688, episode_reward=-68.40 +/- 6.26\n",
      "Episode length: 69.40 +/- 6.26\n",
      "Eval num_timesteps=626688, episode_reward=-74.50 +/- 13.09\n",
      "Episode length: 75.50 +/- 13.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00122 |       0.00000 |      56.10875 |       0.00119 |       0.10232\n",
      "      0.00130 |       0.00000 |      54.94468 |       0.00130 |       0.10287\n",
      "      0.00215 |       0.00000 |      54.50748 |       0.00141 |       0.10307\n",
      "      0.00163 |       0.00000 |      54.28016 |       0.00150 |       0.10322\n",
      "      0.00058 |       0.00000 |      53.97615 |       0.00159 |       0.10358\n",
      "      0.00067 |       0.00000 |      53.55583 |       0.00144 |       0.10280\n",
      "      0.00012 |       0.00000 |      53.30841 |       0.00144 |       0.10306\n",
      "      0.00040 |       0.00000 |      53.35096 |       0.00165 |       0.10389\n",
      "     4.04e-06 |       0.00000 |      53.10808 |       0.00172 |       0.10436\n",
      "      0.00063 |       0.00000 |      53.15627 |       0.00153 |       0.10301\n",
      "Evaluating losses...\n",
      "      0.00056 |       0.00000 |      52.94168 |       0.00140 |       0.10378\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.3         |\n",
      "| EpRewMean       | -89.3        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 6446         |\n",
      "| TimeElapsed     | 2.06e+03     |\n",
      "| TimestepsSoFar  | 630784       |\n",
      "| ev_tdlam_before | 0.755        |\n",
      "| loss_ent        | 0.103775874  |\n",
      "| loss_kl         | 0.001401426  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0005595184 |\n",
      "| loss_vf_loss    | 52.941685    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 154 ************\n",
      "Eval num_timesteps=630784, episode_reward=-75.70 +/- 7.39\n",
      "Episode length: 76.70 +/- 7.39\n",
      "Eval num_timesteps=630784, episode_reward=-78.50 +/- 7.80\n",
      "Episode length: 79.50 +/- 7.80\n",
      "Eval num_timesteps=630784, episode_reward=-77.30 +/- 11.87\n",
      "Episode length: 78.30 +/- 11.87\n",
      "Eval num_timesteps=630784, episode_reward=-93.40 +/- 61.28\n",
      "Episode length: 94.40 +/- 61.28\n",
      "Eval num_timesteps=630784, episode_reward=-74.10 +/- 9.31\n",
      "Episode length: 75.10 +/- 9.31\n",
      "Eval num_timesteps=630784, episode_reward=-77.30 +/- 10.82\n",
      "Episode length: 78.30 +/- 10.82\n",
      "Eval num_timesteps=630784, episode_reward=-81.50 +/- 19.49\n",
      "Episode length: 82.50 +/- 19.49\n",
      "Eval num_timesteps=630784, episode_reward=-71.20 +/- 7.70\n",
      "Episode length: 72.20 +/- 7.70\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00107 |       0.00000 |      29.13545 |       0.00122 |       0.11301\n",
      "      0.00213 |       0.00000 |      28.45419 |       0.00125 |       0.11284\n",
      "      0.00125 |       0.00000 |      28.29339 |       0.00134 |       0.11228\n",
      "      0.00262 |       0.00000 |      28.01449 |       0.00144 |       0.11218\n",
      "      0.00146 |       0.00000 |      27.62932 |       0.00153 |       0.11202\n",
      "      0.00037 |       0.00000 |      27.57535 |       0.00123 |       0.11194\n",
      "      0.00106 |       0.00000 |      27.49010 |       0.00137 |       0.11186\n",
      "      0.00129 |       0.00000 |      27.29688 |       0.00135 |       0.11205\n",
      "      0.00180 |       0.00000 |      27.18708 |       0.00156 |       0.11231\n",
      "      0.00168 |       0.00000 |      27.21801 |       0.00138 |       0.11185\n",
      "Evaluating losses...\n",
      "      0.00130 |       0.00000 |      26.99725 |       0.00133 |       0.11236\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.4         |\n",
      "| EpRewMean       | -86.4        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 6495         |\n",
      "| TimeElapsed     | 2.07e+03     |\n",
      "| TimestepsSoFar  | 634880       |\n",
      "| ev_tdlam_before | 0.889        |\n",
      "| loss_ent        | 0.11236004   |\n",
      "| loss_kl         | 0.0013263815 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013008709 |\n",
      "| loss_vf_loss    | 26.997252    |\n",
      "----------------------------------\n",
      "********** Iteration 155 ************\n",
      "Eval num_timesteps=634880, episode_reward=-75.60 +/- 8.36\n",
      "Episode length: 76.60 +/- 8.36\n",
      "Eval num_timesteps=634880, episode_reward=-71.40 +/- 5.66\n",
      "Episode length: 72.40 +/- 5.66\n",
      "Eval num_timesteps=634880, episode_reward=-72.50 +/- 8.13\n",
      "Episode length: 73.50 +/- 8.13\n",
      "Eval num_timesteps=634880, episode_reward=-91.40 +/- 42.68\n",
      "Episode length: 92.40 +/- 42.68\n",
      "Eval num_timesteps=634880, episode_reward=-75.80 +/- 9.16\n",
      "Episode length: 76.80 +/- 9.16\n",
      "Eval num_timesteps=634880, episode_reward=-76.50 +/- 10.05\n",
      "Episode length: 77.50 +/- 10.05\n",
      "Eval num_timesteps=634880, episode_reward=-76.90 +/- 8.71\n",
      "Episode length: 77.90 +/- 8.71\n",
      "Eval num_timesteps=634880, episode_reward=-77.10 +/- 16.46\n",
      "Episode length: 78.10 +/- 16.46\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00142 |       0.00000 |      31.33625 |       0.00132 |       0.11230\n",
      "      0.00051 |       0.00000 |      31.16213 |       0.00129 |       0.11140\n",
      "      0.00190 |       0.00000 |      31.01470 |       0.00125 |       0.11205\n",
      "      0.00194 |       0.00000 |      31.08467 |       0.00156 |       0.11203\n",
      "      0.00083 |       0.00000 |      30.86668 |       0.00150 |       0.11238\n",
      "      0.00100 |       0.00000 |      30.87394 |       0.00156 |       0.11205\n",
      "      0.00135 |       0.00000 |      30.63808 |       0.00150 |       0.11267\n",
      "      0.00163 |       0.00000 |      30.64936 |       0.00152 |       0.11184\n",
      "      0.00117 |       0.00000 |      30.42218 |       0.00142 |       0.11224\n",
      "      0.00147 |       0.00000 |      30.40172 |       0.00145 |       0.11168\n",
      "Evaluating losses...\n",
      "      0.00084 |       0.00000 |      30.36735 |       0.00149 |       0.11144\n",
      "-----------------------------------\n",
      "| EpLenMean       | 85.1          |\n",
      "| EpRewMean       | -84.1         |\n",
      "| EpThisIter      | 47            |\n",
      "| EpisodesSoFar   | 6542          |\n",
      "| TimeElapsed     | 2.08e+03      |\n",
      "| TimestepsSoFar  | 638976        |\n",
      "| ev_tdlam_before | 0.872         |\n",
      "| loss_ent        | 0.11144171    |\n",
      "| loss_kl         | 0.001492678   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00083527376 |\n",
      "| loss_vf_loss    | 30.367346     |\n",
      "-----------------------------------\n",
      "********** Iteration 156 ************\n",
      "Eval num_timesteps=638976, episode_reward=-73.20 +/- 5.00\n",
      "Episode length: 74.20 +/- 5.00\n",
      "Eval num_timesteps=638976, episode_reward=-78.50 +/- 11.75\n",
      "Episode length: 79.50 +/- 11.75\n",
      "Eval num_timesteps=638976, episode_reward=-73.70 +/- 9.74\n",
      "Episode length: 74.70 +/- 9.74\n",
      "Eval num_timesteps=638976, episode_reward=-73.40 +/- 9.40\n",
      "Episode length: 74.40 +/- 9.40\n",
      "Eval num_timesteps=638976, episode_reward=-72.80 +/- 3.76\n",
      "Episode length: 73.80 +/- 3.76\n",
      "Eval num_timesteps=638976, episode_reward=-78.30 +/- 11.49\n",
      "Episode length: 79.30 +/- 11.49\n",
      "Eval num_timesteps=638976, episode_reward=-91.10 +/- 34.92\n",
      "Episode length: 92.10 +/- 34.92\n",
      "Eval num_timesteps=638976, episode_reward=-73.00 +/- 2.57\n",
      "Episode length: 74.00 +/- 2.57\n",
      "Eval num_timesteps=638976, episode_reward=-71.20 +/- 4.38\n",
      "Episode length: 72.20 +/- 4.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00088 |       0.00000 |      35.05246 |       0.00147 |       0.11131\n",
      "      0.00076 |       0.00000 |      34.75828 |       0.00136 |       0.11133\n",
      "      0.00207 |       0.00000 |      34.66893 |       0.00147 |       0.11171\n",
      "      0.00209 |       0.00000 |      34.60207 |       0.00139 |       0.11137\n",
      "      0.00116 |       0.00000 |      34.57936 |       0.00138 |       0.11204\n",
      "      0.00157 |       0.00000 |      34.62697 |       0.00144 |       0.11123\n",
      "      0.00111 |       0.00000 |      34.59958 |       0.00151 |       0.11163\n",
      "      0.00083 |       0.00000 |      34.57917 |       0.00134 |       0.11160\n",
      "      0.00145 |       0.00000 |      34.39407 |       0.00130 |       0.11190\n",
      "      0.00083 |       0.00000 |      34.47259 |       0.00146 |       0.11200\n",
      "Evaluating losses...\n",
      "      0.00084 |       0.00000 |      34.45339 |       0.00143 |       0.11174\n",
      "-----------------------------------\n",
      "| EpLenMean       | 85.5          |\n",
      "| EpRewMean       | -84.5         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 6590          |\n",
      "| TimeElapsed     | 2.09e+03      |\n",
      "| TimestepsSoFar  | 643072        |\n",
      "| ev_tdlam_before | 0.86          |\n",
      "| loss_ent        | 0.11173558    |\n",
      "| loss_kl         | 0.0014272827  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00083545106 |\n",
      "| loss_vf_loss    | 34.45339      |\n",
      "-----------------------------------\n",
      "********** Iteration 157 ************\n",
      "Eval num_timesteps=643072, episode_reward=-77.20 +/- 8.58\n",
      "Episode length: 78.20 +/- 8.58\n",
      "Eval num_timesteps=643072, episode_reward=-75.80 +/- 7.48\n",
      "Episode length: 76.80 +/- 7.48\n",
      "Eval num_timesteps=643072, episode_reward=-75.10 +/- 9.89\n",
      "Episode length: 76.10 +/- 9.89\n",
      "Eval num_timesteps=643072, episode_reward=-83.10 +/- 17.68\n",
      "Episode length: 84.10 +/- 17.68\n",
      "Eval num_timesteps=643072, episode_reward=-79.90 +/- 11.84\n",
      "Episode length: 80.90 +/- 11.84\n",
      "Eval num_timesteps=643072, episode_reward=-69.90 +/- 6.67\n",
      "Episode length: 70.90 +/- 6.67\n",
      "Eval num_timesteps=643072, episode_reward=-74.00 +/- 3.77\n",
      "Episode length: 75.00 +/- 3.77\n",
      "Eval num_timesteps=643072, episode_reward=-98.00 +/- 40.59\n",
      "Episode length: 99.00 +/- 40.59\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00140 |       0.00000 |      37.80758 |       0.00129 |       0.10832\n",
      "      0.00118 |       0.00000 |      37.62243 |       0.00137 |       0.10877\n",
      "      0.00243 |       0.00000 |      37.51760 |       0.00143 |       0.10776\n",
      "      0.00138 |       0.00000 |      37.15895 |       0.00133 |       0.10714\n",
      "      0.00094 |       0.00000 |      37.03168 |       0.00144 |       0.10684\n",
      "      0.00125 |       0.00000 |      36.94564 |       0.00167 |       0.10676\n",
      "      0.00053 |       0.00000 |      36.82349 |       0.00168 |       0.10715\n",
      "      0.00111 |       0.00000 |      36.75694 |       0.00133 |       0.10626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7.57e-05 |       0.00000 |      36.66079 |       0.00149 |       0.10680\n",
      "      0.00069 |       0.00000 |      36.55332 |       0.00166 |       0.10645\n",
      "Evaluating losses...\n",
      "      0.00040 |       0.00000 |      36.38726 |       0.00156 |       0.10613\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.7         |\n",
      "| EpRewMean       | -86.7        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 6636         |\n",
      "| TimeElapsed     | 2.1e+03      |\n",
      "| TimestepsSoFar  | 647168       |\n",
      "| ev_tdlam_before | 0.844        |\n",
      "| loss_ent        | 0.10613044   |\n",
      "| loss_kl         | 0.0015556476 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.000395325  |\n",
      "| loss_vf_loss    | 36.387264    |\n",
      "----------------------------------\n",
      "********** Iteration 158 ************\n",
      "Eval num_timesteps=647168, episode_reward=-74.70 +/- 10.39\n",
      "Episode length: 75.70 +/- 10.39\n",
      "Eval num_timesteps=647168, episode_reward=-71.00 +/- 8.44\n",
      "Episode length: 72.00 +/- 8.44\n",
      "Eval num_timesteps=647168, episode_reward=-85.30 +/- 29.30\n",
      "Episode length: 86.30 +/- 29.30\n",
      "Eval num_timesteps=647168, episode_reward=-82.00 +/- 29.92\n",
      "Episode length: 83.00 +/- 29.92\n",
      "Eval num_timesteps=647168, episode_reward=-78.10 +/- 19.89\n",
      "Episode length: 79.10 +/- 19.89\n",
      "Eval num_timesteps=647168, episode_reward=-84.20 +/- 13.11\n",
      "Episode length: 85.20 +/- 13.11\n",
      "Eval num_timesteps=647168, episode_reward=-74.40 +/- 9.31\n",
      "Episode length: 75.40 +/- 9.31\n",
      "Eval num_timesteps=647168, episode_reward=-83.60 +/- 41.53\n",
      "Episode length: 84.60 +/- 41.53\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00386 |       0.00000 |      44.59839 |       0.00145 |       0.10265\n",
      "      0.00178 |       0.00000 |      44.16453 |       0.00154 |       0.10124\n",
      "      0.00106 |       0.00000 |      43.83685 |       0.00135 |       0.10129\n",
      "      0.00159 |       0.00000 |      43.83166 |       0.00141 |       0.10157\n",
      "      0.00157 |       0.00000 |      43.58312 |       0.00144 |       0.10055\n",
      "      0.00178 |       0.00000 |      43.43480 |       0.00139 |       0.10037\n",
      "      0.00175 |       0.00000 |      43.60276 |       0.00151 |       0.10035\n",
      "      0.00311 |       0.00000 |      43.15504 |       0.00142 |       0.10019\n",
      "      0.00140 |       0.00000 |      43.15668 |       0.00144 |       0.10026\n",
      "      0.00307 |       0.00000 |      43.02172 |       0.00170 |       0.09951\n",
      "Evaluating losses...\n",
      "      0.00221 |       0.00000 |      42.87657 |       0.00144 |       0.10019\n",
      "----------------------------------\n",
      "| EpLenMean       | 91           |\n",
      "| EpRewMean       | -90          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 6680         |\n",
      "| TimeElapsed     | 2.11e+03     |\n",
      "| TimestepsSoFar  | 651264       |\n",
      "| ev_tdlam_before | 0.808        |\n",
      "| loss_ent        | 0.10018783   |\n",
      "| loss_kl         | 0.0014370275 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002208276  |\n",
      "| loss_vf_loss    | 42.87657     |\n",
      "----------------------------------\n",
      "********** Iteration 159 ************\n",
      "Eval num_timesteps=651264, episode_reward=-78.50 +/- 17.49\n",
      "Episode length: 79.50 +/- 17.49\n",
      "Eval num_timesteps=651264, episode_reward=-91.50 +/- 53.81\n",
      "Episode length: 92.50 +/- 53.81\n",
      "Eval num_timesteps=651264, episode_reward=-75.90 +/- 8.76\n",
      "Episode length: 76.90 +/- 8.76\n",
      "Eval num_timesteps=651264, episode_reward=-77.10 +/- 22.16\n",
      "Episode length: 78.10 +/- 22.16\n",
      "Eval num_timesteps=651264, episode_reward=-70.50 +/- 8.04\n",
      "Episode length: 71.50 +/- 8.04\n",
      "Eval num_timesteps=651264, episode_reward=-81.20 +/- 13.01\n",
      "Episode length: 82.20 +/- 13.01\n",
      "Eval num_timesteps=651264, episode_reward=-78.20 +/- 16.23\n",
      "Episode length: 79.20 +/- 16.23\n",
      "Eval num_timesteps=651264, episode_reward=-77.50 +/- 14.05\n",
      "Episode length: 78.50 +/- 14.05\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00156 |       0.00000 |      44.75327 |       0.00123 |       0.10000\n",
      "      0.00193 |       0.00000 |      44.33109 |       0.00132 |       0.10000\n",
      "      0.00086 |       0.00000 |      43.73604 |       0.00131 |       0.09922\n",
      "      0.00140 |       0.00000 |      43.53709 |       0.00143 |       0.09927\n",
      "      0.00103 |       0.00000 |      43.54433 |       0.00138 |       0.09946\n",
      "      0.00109 |       0.00000 |      43.31485 |       0.00152 |       0.09921\n",
      "      0.00111 |       0.00000 |      43.08604 |       0.00141 |       0.09964\n",
      "      0.00107 |       0.00000 |      43.01577 |       0.00131 |       0.09944\n",
      "      0.00048 |       0.00000 |      42.83969 |       0.00145 |       0.09981\n",
      "      0.00158 |       0.00000 |      42.80320 |       0.00153 |       0.09974\n",
      "Evaluating losses...\n",
      "      0.00067 |       0.00000 |      42.56270 |       0.00173 |       0.09954\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.2         |\n",
      "| EpRewMean       | -92.2        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 6722         |\n",
      "| TimeElapsed     | 2.12e+03     |\n",
      "| TimestepsSoFar  | 655360       |\n",
      "| ev_tdlam_before | 0.81         |\n",
      "| loss_ent        | 0.099536195  |\n",
      "| loss_kl         | 0.0017298702 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0006685222 |\n",
      "| loss_vf_loss    | 42.5627      |\n",
      "----------------------------------\n",
      "********** Iteration 160 ************\n",
      "Eval num_timesteps=655360, episode_reward=-73.40 +/- 4.10\n",
      "Episode length: 74.40 +/- 4.10\n",
      "Eval num_timesteps=655360, episode_reward=-84.20 +/- 23.54\n",
      "Episode length: 85.20 +/- 23.54\n",
      "Eval num_timesteps=655360, episode_reward=-80.00 +/- 15.91\n",
      "Episode length: 81.00 +/- 15.91\n",
      "Eval num_timesteps=655360, episode_reward=-71.80 +/- 5.34\n",
      "Episode length: 72.80 +/- 5.34\n",
      "Eval num_timesteps=655360, episode_reward=-76.70 +/- 8.61\n",
      "Episode length: 77.70 +/- 8.61\n",
      "Eval num_timesteps=655360, episode_reward=-84.10 +/- 19.62\n",
      "Episode length: 85.10 +/- 19.62\n",
      "Eval num_timesteps=655360, episode_reward=-92.90 +/- 36.11\n",
      "Episode length: 93.90 +/- 36.11\n",
      "Eval num_timesteps=655360, episode_reward=-73.00 +/- 10.59\n",
      "Episode length: 74.00 +/- 10.59\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00132 |       0.00000 |      47.04433 |       0.00123 |       0.10048\n",
      "      0.00109 |       0.00000 |      46.64556 |       0.00134 |       0.09969\n",
      "      0.00236 |       0.00000 |      46.44184 |       0.00145 |       0.09941\n",
      "      0.00128 |       0.00000 |      46.39478 |       0.00139 |       0.09912\n",
      "      0.00173 |       0.00000 |      46.29339 |       0.00149 |       0.09785\n",
      "      0.00218 |       0.00000 |      46.11431 |       0.00149 |       0.09809\n",
      "      0.00066 |       0.00000 |      45.98364 |       0.00144 |       0.09801\n",
      "      0.00129 |       0.00000 |      46.01984 |       0.00123 |       0.09744\n",
      "      0.00128 |       0.00000 |      45.88855 |       0.00151 |       0.09786\n",
      "      0.00167 |       0.00000 |      45.79097 |       0.00152 |       0.09810\n",
      "Evaluating losses...\n",
      "      0.00134 |       0.00000 |      45.67898 |       0.00161 |       0.09817\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.6         |\n",
      "| EpRewMean       | -92.6        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 6768         |\n",
      "| TimeElapsed     | 2.13e+03     |\n",
      "| TimestepsSoFar  | 659456       |\n",
      "| ev_tdlam_before | 0.806        |\n",
      "| loss_ent        | 0.098165035  |\n",
      "| loss_kl         | 0.0016106818 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013356442 |\n",
      "| loss_vf_loss    | 45.67898     |\n",
      "----------------------------------\n",
      "********** Iteration 161 ************\n",
      "Eval num_timesteps=659456, episode_reward=-71.40 +/- 6.68\n",
      "Episode length: 72.40 +/- 6.68\n",
      "Eval num_timesteps=659456, episode_reward=-89.70 +/- 42.31\n",
      "Episode length: 90.70 +/- 42.31\n",
      "Eval num_timesteps=659456, episode_reward=-77.70 +/- 8.01\n",
      "Episode length: 78.70 +/- 8.01\n",
      "Eval num_timesteps=659456, episode_reward=-76.80 +/- 11.02\n",
      "Episode length: 77.80 +/- 11.02\n",
      "Eval num_timesteps=659456, episode_reward=-75.70 +/- 8.61\n",
      "Episode length: 76.70 +/- 8.61\n",
      "Eval num_timesteps=659456, episode_reward=-94.80 +/- 60.98\n",
      "Episode length: 95.80 +/- 60.98\n",
      "Eval num_timesteps=659456, episode_reward=-68.90 +/- 5.43\n",
      "Episode length: 69.90 +/- 5.43\n",
      "Eval num_timesteps=659456, episode_reward=-86.10 +/- 36.84\n",
      "Episode length: 87.10 +/- 36.84\n",
      "Eval num_timesteps=659456, episode_reward=-84.20 +/- 36.07\n",
      "Episode length: 85.20 +/- 36.07\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00321 |       0.00000 |      44.91740 |       0.00122 |       0.09786\n",
      "      0.00129 |       0.00000 |      44.62807 |       0.00130 |       0.09745\n",
      "      0.00248 |       0.00000 |      44.41772 |       0.00111 |       0.09759\n",
      "      0.00253 |       0.00000 |      44.13223 |       0.00132 |       0.09761\n",
      "      0.00081 |       0.00000 |      44.12296 |       0.00122 |       0.09806\n",
      "      0.00162 |       0.00000 |      43.88985 |       0.00128 |       0.09784\n",
      "      0.00146 |       0.00000 |      43.85950 |       0.00121 |       0.09739\n",
      "      0.00226 |       0.00000 |      43.85934 |       0.00119 |       0.09762\n",
      "      0.00309 |       0.00000 |      43.74337 |       0.00128 |       0.09700\n",
      "      0.00072 |       0.00000 |      43.64346 |       0.00132 |       0.09690\n",
      "Evaluating losses...\n",
      "      0.00181 |       0.00000 |      43.69867 |       0.00120 |       0.09701\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.9         |\n",
      "| EpRewMean       | -87.9        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 6814         |\n",
      "| TimeElapsed     | 2.14e+03     |\n",
      "| TimestepsSoFar  | 663552       |\n",
      "| ev_tdlam_before | 0.814        |\n",
      "| loss_ent        | 0.09700982   |\n",
      "| loss_kl         | 0.0011999778 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001813473  |\n",
      "| loss_vf_loss    | 43.698673    |\n",
      "----------------------------------\n",
      "********** Iteration 162 ************\n",
      "Eval num_timesteps=663552, episode_reward=-82.60 +/- 21.55\n",
      "Episode length: 83.60 +/- 21.55\n",
      "Eval num_timesteps=663552, episode_reward=-73.60 +/- 10.88\n",
      "Episode length: 74.60 +/- 10.88\n",
      "Eval num_timesteps=663552, episode_reward=-73.40 +/- 7.77\n",
      "Episode length: 74.40 +/- 7.77\n",
      "Eval num_timesteps=663552, episode_reward=-73.70 +/- 3.85\n",
      "Episode length: 74.70 +/- 3.85\n",
      "Eval num_timesteps=663552, episode_reward=-72.80 +/- 8.90\n",
      "Episode length: 73.80 +/- 8.90\n",
      "Eval num_timesteps=663552, episode_reward=-75.30 +/- 13.91\n",
      "Episode length: 76.30 +/- 13.91\n",
      "Eval num_timesteps=663552, episode_reward=-77.80 +/- 10.34\n",
      "Episode length: 78.80 +/- 10.34\n",
      "Eval num_timesteps=663552, episode_reward=-74.10 +/- 13.25\n",
      "Episode length: 75.10 +/- 13.25\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00268 |       0.00000 |      50.38477 |       0.00119 |       0.09535\n",
      "      0.00285 |       0.00000 |      50.17571 |       0.00126 |       0.09559\n",
      "      0.00154 |       0.00000 |      50.05626 |       0.00123 |       0.09451\n",
      "      0.00259 |       0.00000 |      49.76542 |       0.00138 |       0.09446\n",
      "      0.00210 |       0.00000 |      49.80622 |       0.00144 |       0.09403\n",
      "      0.00184 |       0.00000 |      49.51815 |       0.00138 |       0.09407\n",
      "      0.00327 |       0.00000 |      49.38059 |       0.00133 |       0.09437\n",
      "      0.00134 |       0.00000 |      49.24458 |       0.00126 |       0.09405\n",
      "      0.00083 |       0.00000 |      49.26017 |       0.00140 |       0.09425\n",
      "      0.00194 |       0.00000 |      49.05199 |       0.00157 |       0.09339\n",
      "Evaluating losses...\n",
      "      0.00180 |       0.00000 |      49.06520 |       0.00164 |       0.09404\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.5         |\n",
      "| EpRewMean       | -88.5        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 6858         |\n",
      "| TimeElapsed     | 2.15e+03     |\n",
      "| TimestepsSoFar  | 667648       |\n",
      "| ev_tdlam_before | 0.785        |\n",
      "| loss_ent        | 0.09404432   |\n",
      "| loss_kl         | 0.001644147  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0018040386 |\n",
      "| loss_vf_loss    | 49.065197    |\n",
      "----------------------------------\n",
      "********** Iteration 163 ************\n",
      "Eval num_timesteps=667648, episode_reward=-80.90 +/- 15.71\n",
      "Episode length: 81.90 +/- 15.71\n",
      "Eval num_timesteps=667648, episode_reward=-76.00 +/- 10.76\n",
      "Episode length: 77.00 +/- 10.76\n",
      "Eval num_timesteps=667648, episode_reward=-84.80 +/- 21.88\n",
      "Episode length: 85.80 +/- 21.88\n",
      "Eval num_timesteps=667648, episode_reward=-75.50 +/- 6.04\n",
      "Episode length: 76.50 +/- 6.04\n",
      "Eval num_timesteps=667648, episode_reward=-78.00 +/- 15.56\n",
      "Episode length: 79.00 +/- 15.56\n",
      "Eval num_timesteps=667648, episode_reward=-76.20 +/- 12.32\n",
      "Episode length: 77.20 +/- 12.32\n",
      "Eval num_timesteps=667648, episode_reward=-70.50 +/- 5.78\n",
      "Episode length: 71.50 +/- 5.78\n",
      "Eval num_timesteps=667648, episode_reward=-77.30 +/- 11.32\n",
      "Episode length: 78.30 +/- 11.32\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00236 |       0.00000 |      44.22252 |       0.00128 |       0.09857\n",
      "      0.00220 |       0.00000 |      44.07210 |       0.00128 |       0.09877\n",
      "      0.00204 |       0.00000 |      44.07362 |       0.00149 |       0.09857\n",
      "      0.00069 |       0.00000 |      43.79281 |       0.00124 |       0.09951\n",
      "      0.00169 |       0.00000 |      43.89745 |       0.00129 |       0.09891\n",
      "      0.00060 |       0.00000 |      43.79967 |       0.00129 |       0.09957\n",
      "      0.00156 |       0.00000 |      43.66290 |       0.00157 |       0.09951\n",
      "      0.00055 |       0.00000 |      43.62344 |       0.00134 |       0.09956\n",
      "      0.00181 |       0.00000 |      43.38204 |       0.00133 |       0.09930\n",
      "      0.00246 |       0.00000 |      43.36564 |       0.00142 |       0.09987\n",
      "Evaluating losses...\n",
      "      0.00112 |       0.00000 |      43.34269 |       0.00139 |       0.09964\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.2         |\n",
      "| EpRewMean       | -90.2        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 6904         |\n",
      "| TimeElapsed     | 2.16e+03     |\n",
      "| TimestepsSoFar  | 671744       |\n",
      "| ev_tdlam_before | 0.819        |\n",
      "| loss_ent        | 0.09964322   |\n",
      "| loss_kl         | 0.0013902327 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001121254  |\n",
      "| loss_vf_loss    | 43.342686    |\n",
      "----------------------------------\n",
      "********** Iteration 164 ************\n",
      "Eval num_timesteps=671744, episode_reward=-71.10 +/- 5.56\n",
      "Episode length: 72.10 +/- 5.56\n",
      "Eval num_timesteps=671744, episode_reward=-76.40 +/- 13.21\n",
      "Episode length: 77.40 +/- 13.21\n",
      "Eval num_timesteps=671744, episode_reward=-71.90 +/- 5.13\n",
      "Episode length: 72.90 +/- 5.13\n",
      "Eval num_timesteps=671744, episode_reward=-73.10 +/- 7.79\n",
      "Episode length: 74.10 +/- 7.79\n",
      "Eval num_timesteps=671744, episode_reward=-74.10 +/- 6.99\n",
      "Episode length: 75.10 +/- 6.99\n",
      "Eval num_timesteps=671744, episode_reward=-72.70 +/- 4.61\n",
      "Episode length: 73.70 +/- 4.61\n",
      "Eval num_timesteps=671744, episode_reward=-73.10 +/- 7.58\n",
      "Episode length: 74.10 +/- 7.58\n",
      "Eval num_timesteps=671744, episode_reward=-76.30 +/- 14.92\n",
      "Episode length: 77.30 +/- 14.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00182 |       0.00000 |      39.89865 |       0.00130 |       0.10133\n",
      "      0.00251 |       0.00000 |      39.65329 |       0.00137 |       0.10140\n",
      "      0.00199 |       0.00000 |      39.28293 |       0.00128 |       0.10146\n",
      "      0.00280 |       0.00000 |      39.21169 |       0.00144 |       0.10018\n",
      "      0.00083 |       0.00000 |      39.11986 |       0.00137 |       0.10002\n",
      "      0.00113 |       0.00000 |      38.98840 |       0.00132 |       0.10042\n",
      "      0.00160 |       0.00000 |      38.93832 |       0.00126 |       0.09980\n",
      "      0.00095 |       0.00000 |      38.79847 |       0.00134 |       0.10028\n",
      "      0.00131 |       0.00000 |      38.71870 |       0.00148 |       0.10034\n",
      "      0.00180 |       0.00000 |      38.71624 |       0.00144 |       0.10055\n",
      "Evaluating losses...\n",
      "      0.00172 |       0.00000 |      38.64705 |       0.00147 |       0.10018\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.5         |\n",
      "| EpRewMean       | -89.5        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 6949         |\n",
      "| TimeElapsed     | 2.17e+03     |\n",
      "| TimestepsSoFar  | 675840       |\n",
      "| ev_tdlam_before | 0.839        |\n",
      "| loss_ent        | 0.10017836   |\n",
      "| loss_kl         | 0.0014731534 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001721     |\n",
      "| loss_vf_loss    | 38.647053    |\n",
      "----------------------------------\n",
      "********** Iteration 165 ************\n",
      "Eval num_timesteps=675840, episode_reward=-120.00 +/- 127.03\n",
      "Episode length: 120.90 +/- 126.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=675840, episode_reward=-78.10 +/- 8.29\n",
      "Episode length: 79.10 +/- 8.29\n",
      "Eval num_timesteps=675840, episode_reward=-74.10 +/- 7.62\n",
      "Episode length: 75.10 +/- 7.62\n",
      "Eval num_timesteps=675840, episode_reward=-77.50 +/- 8.49\n",
      "Episode length: 78.50 +/- 8.49\n",
      "Eval num_timesteps=675840, episode_reward=-74.40 +/- 8.67\n",
      "Episode length: 75.40 +/- 8.67\n",
      "Eval num_timesteps=675840, episode_reward=-86.20 +/- 36.15\n",
      "Episode length: 87.20 +/- 36.15\n",
      "Eval num_timesteps=675840, episode_reward=-80.30 +/- 12.73\n",
      "Episode length: 81.30 +/- 12.73\n",
      "Eval num_timesteps=675840, episode_reward=-84.70 +/- 34.02\n",
      "Episode length: 85.70 +/- 34.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00290 |       0.00000 |      41.61715 |       0.00110 |       0.09946\n",
      "      0.00126 |       0.00000 |      41.54815 |       0.00137 |       0.09855\n",
      "      0.00276 |       0.00000 |      41.54779 |       0.00141 |       0.09887\n",
      "      0.00178 |       0.00000 |      41.23915 |       0.00141 |       0.09880\n",
      "      0.00142 |       0.00000 |      41.19453 |       0.00123 |       0.09873\n",
      "      0.00150 |       0.00000 |      41.06976 |       0.00120 |       0.09836\n",
      "      0.00222 |       0.00000 |      41.00436 |       0.00132 |       0.09931\n",
      "      0.00115 |       0.00000 |      41.04682 |       0.00132 |       0.09885\n",
      "      0.00174 |       0.00000 |      40.97621 |       0.00128 |       0.09900\n",
      "      0.00132 |       0.00000 |      40.99094 |       0.00138 |       0.09923\n",
      "Evaluating losses...\n",
      "      0.00113 |       0.00000 |      40.71514 |       0.00143 |       0.09850\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.8         |\n",
      "| EpRewMean       | -89.8        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 6994         |\n",
      "| TimeElapsed     | 2.18e+03     |\n",
      "| TimestepsSoFar  | 679936       |\n",
      "| ev_tdlam_before | 0.83         |\n",
      "| loss_ent        | 0.09850246   |\n",
      "| loss_kl         | 0.001425652  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0011289724 |\n",
      "| loss_vf_loss    | 40.71514     |\n",
      "----------------------------------\n",
      "********** Iteration 166 ************\n",
      "Eval num_timesteps=679936, episode_reward=-74.80 +/- 7.65\n",
      "Episode length: 75.80 +/- 7.65\n",
      "Eval num_timesteps=679936, episode_reward=-82.00 +/- 17.12\n",
      "Episode length: 83.00 +/- 17.12\n",
      "Eval num_timesteps=679936, episode_reward=-69.70 +/- 9.41\n",
      "Episode length: 70.70 +/- 9.41\n",
      "Eval num_timesteps=679936, episode_reward=-77.50 +/- 11.01\n",
      "Episode length: 78.50 +/- 11.01\n",
      "Eval num_timesteps=679936, episode_reward=-72.00 +/- 5.27\n",
      "Episode length: 73.00 +/- 5.27\n",
      "Eval num_timesteps=679936, episode_reward=-75.00 +/- 9.87\n",
      "Episode length: 76.00 +/- 9.87\n",
      "Eval num_timesteps=679936, episode_reward=-77.20 +/- 10.31\n",
      "Episode length: 78.20 +/- 10.31\n",
      "Eval num_timesteps=679936, episode_reward=-81.00 +/- 28.40\n",
      "Episode length: 82.00 +/- 28.40\n",
      "Eval num_timesteps=679936, episode_reward=-81.00 +/- 11.26\n",
      "Episode length: 82.00 +/- 11.26\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00229 |       0.00000 |      41.25949 |       0.00125 |       0.09882\n",
      "      0.00175 |       0.00000 |      41.00132 |       0.00111 |       0.09856\n",
      "      0.00109 |       0.00000 |      41.00769 |       0.00121 |       0.09851\n",
      "      0.00050 |       0.00000 |      40.78123 |       0.00136 |       0.09869\n",
      "      0.00134 |       0.00000 |      40.52499 |       0.00117 |       0.09819\n",
      "      0.00093 |       0.00000 |      40.47380 |       0.00118 |       0.09826\n",
      "      0.00152 |       0.00000 |      40.23737 |       0.00137 |       0.09822\n",
      "      0.00126 |       0.00000 |      40.27435 |       0.00130 |       0.09867\n",
      "      0.00108 |       0.00000 |      40.11391 |       0.00141 |       0.09740\n",
      "      0.00153 |       0.00000 |      40.05886 |       0.00130 |       0.09827\n",
      "Evaluating losses...\n",
      "      0.00066 |       0.00000 |      39.84293 |       0.00126 |       0.09823\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.5         |\n",
      "| EpRewMean       | -89.5        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7040         |\n",
      "| TimeElapsed     | 2.19e+03     |\n",
      "| TimestepsSoFar  | 684032       |\n",
      "| ev_tdlam_before | 0.836        |\n",
      "| loss_ent        | 0.09823481   |\n",
      "| loss_kl         | 0.0012556172 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0006555796 |\n",
      "| loss_vf_loss    | 39.842934    |\n",
      "----------------------------------\n",
      "********** Iteration 167 ************\n",
      "Eval num_timesteps=684032, episode_reward=-74.10 +/- 9.00\n",
      "Episode length: 75.10 +/- 9.00\n",
      "Eval num_timesteps=684032, episode_reward=-72.80 +/- 7.82\n",
      "Episode length: 73.80 +/- 7.82\n",
      "Eval num_timesteps=684032, episode_reward=-71.30 +/- 10.03\n",
      "Episode length: 72.30 +/- 10.03\n",
      "Eval num_timesteps=684032, episode_reward=-74.20 +/- 10.51\n",
      "Episode length: 75.20 +/- 10.51\n",
      "Eval num_timesteps=684032, episode_reward=-76.70 +/- 16.25\n",
      "Episode length: 77.70 +/- 16.25\n",
      "Eval num_timesteps=684032, episode_reward=-75.90 +/- 7.26\n",
      "Episode length: 76.90 +/- 7.26\n",
      "Eval num_timesteps=684032, episode_reward=-79.70 +/- 15.98\n",
      "Episode length: 80.70 +/- 15.98\n",
      "Eval num_timesteps=684032, episode_reward=-71.90 +/- 6.88\n",
      "Episode length: 72.90 +/- 6.88\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00300 |       0.00000 |      42.03340 |       0.00133 |       0.09603\n",
      "      0.00130 |       0.00000 |      41.57359 |       0.00134 |       0.09563\n",
      "      0.00196 |       0.00000 |      41.15101 |       0.00119 |       0.09600\n",
      "      0.00071 |       0.00000 |      40.88128 |       0.00129 |       0.09533\n",
      "      0.00138 |       0.00000 |      40.57677 |       0.00120 |       0.09591\n",
      "      0.00264 |       0.00000 |      40.57967 |       0.00141 |       0.09557\n",
      "      0.00229 |       0.00000 |      40.43212 |       0.00133 |       0.09454\n",
      "      0.00067 |       0.00000 |      40.17760 |       0.00126 |       0.09491\n",
      "      0.00230 |       0.00000 |      40.23101 |       0.00137 |       0.09430\n",
      "      0.00144 |       0.00000 |      40.02653 |       0.00131 |       0.09446\n",
      "Evaluating losses...\n",
      "      0.00183 |       0.00000 |      39.92246 |       0.00137 |       0.09441\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.9         |\n",
      "| EpRewMean       | -88.9        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 7087         |\n",
      "| TimeElapsed     | 2.2e+03      |\n",
      "| TimestepsSoFar  | 688128       |\n",
      "| ev_tdlam_before | 0.832        |\n",
      "| loss_ent        | 0.09440746   |\n",
      "| loss_kl         | 0.0013719294 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0018333863 |\n",
      "| loss_vf_loss    | 39.922462    |\n",
      "----------------------------------\n",
      "********** Iteration 168 ************\n",
      "Eval num_timesteps=688128, episode_reward=-97.40 +/- 39.28\n",
      "Episode length: 98.40 +/- 39.28\n",
      "Eval num_timesteps=688128, episode_reward=-73.70 +/- 7.84\n",
      "Episode length: 74.70 +/- 7.84\n",
      "Eval num_timesteps=688128, episode_reward=-75.40 +/- 7.61\n",
      "Episode length: 76.40 +/- 7.61\n",
      "Eval num_timesteps=688128, episode_reward=-72.50 +/- 9.85\n",
      "Episode length: 73.50 +/- 9.85\n",
      "Eval num_timesteps=688128, episode_reward=-71.60 +/- 7.20\n",
      "Episode length: 72.60 +/- 7.20\n",
      "Eval num_timesteps=688128, episode_reward=-71.90 +/- 8.60\n",
      "Episode length: 72.90 +/- 8.60\n",
      "Eval num_timesteps=688128, episode_reward=-94.40 +/- 64.16\n",
      "Episode length: 95.40 +/- 64.16\n",
      "Eval num_timesteps=688128, episode_reward=-72.90 +/- 8.22\n",
      "Episode length: 73.90 +/- 8.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00140 |       0.00000 |      56.67027 |       0.00122 |       0.09413\n",
      "      0.00128 |       0.00000 |      56.60934 |       0.00114 |       0.09363\n",
      "      0.00114 |       0.00000 |      56.41472 |       0.00137 |       0.09353\n",
      "      0.00294 |       0.00000 |      56.33988 |       0.00126 |       0.09299\n",
      "      0.00167 |       0.00000 |      56.21238 |       0.00113 |       0.09311\n",
      "      0.00214 |       0.00000 |      55.98313 |       0.00138 |       0.09274\n",
      "      0.00120 |       0.00000 |      55.78278 |       0.00121 |       0.09299\n",
      "      0.00138 |       0.00000 |      56.03997 |       0.00148 |       0.09237\n",
      "      0.00094 |       0.00000 |      55.79469 |       0.00129 |       0.09296\n",
      "      0.00087 |       0.00000 |      55.64148 |       0.00137 |       0.09219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "      0.00064 |       0.00000 |      55.60442 |       0.00140 |       0.09266\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.5         |\n",
      "| EpRewMean       | -88.5        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 7132         |\n",
      "| TimeElapsed     | 2.21e+03     |\n",
      "| TimestepsSoFar  | 692224       |\n",
      "| ev_tdlam_before | 0.76         |\n",
      "| loss_ent        | 0.0926594    |\n",
      "| loss_kl         | 0.0014005351 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0006401809 |\n",
      "| loss_vf_loss    | 55.604424    |\n",
      "----------------------------------\n",
      "********** Iteration 169 ************\n",
      "Eval num_timesteps=692224, episode_reward=-73.70 +/- 6.94\n",
      "Episode length: 74.70 +/- 6.94\n",
      "Eval num_timesteps=692224, episode_reward=-72.50 +/- 6.02\n",
      "Episode length: 73.50 +/- 6.02\n",
      "Eval num_timesteps=692224, episode_reward=-100.30 +/- 74.38\n",
      "Episode length: 101.30 +/- 74.38\n",
      "Eval num_timesteps=692224, episode_reward=-89.40 +/- 17.59\n",
      "Episode length: 90.40 +/- 17.59\n",
      "Eval num_timesteps=692224, episode_reward=-77.40 +/- 12.99\n",
      "Episode length: 78.40 +/- 12.99\n",
      "Eval num_timesteps=692224, episode_reward=-82.50 +/- 18.49\n",
      "Episode length: 83.50 +/- 18.49\n",
      "Eval num_timesteps=692224, episode_reward=-74.10 +/- 11.12\n",
      "Episode length: 75.10 +/- 11.12\n",
      "Eval num_timesteps=692224, episode_reward=-76.90 +/- 13.19\n",
      "Episode length: 77.90 +/- 13.19\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00196 |       0.00000 |      44.83196 |       0.00113 |       0.08932\n",
      "      0.00228 |       0.00000 |      44.53666 |       0.00117 |       0.08947\n",
      "      0.00123 |       0.00000 |      44.42505 |       0.00119 |       0.08998\n",
      "      0.00185 |       0.00000 |      44.34790 |       0.00118 |       0.08959\n",
      "      0.00127 |       0.00000 |      44.21558 |       0.00127 |       0.08976\n",
      "      0.00192 |       0.00000 |      43.87087 |       0.00114 |       0.08930\n",
      "      0.00155 |       0.00000 |      44.08819 |       0.00117 |       0.08876\n",
      "      0.00134 |       0.00000 |      43.78284 |       0.00124 |       0.08921\n",
      "      0.00197 |       0.00000 |      43.63369 |       0.00114 |       0.08910\n",
      "      0.00204 |       0.00000 |      43.82264 |       0.00127 |       0.08911\n",
      "Evaluating losses...\n",
      "      0.00085 |       0.00000 |      43.50025 |       0.00132 |       0.08896\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.2         |\n",
      "| EpRewMean       | -90.2        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 7176         |\n",
      "| TimeElapsed     | 2.22e+03     |\n",
      "| TimestepsSoFar  | 696320       |\n",
      "| ev_tdlam_before | 0.809        |\n",
      "| loss_ent        | 0.08895892   |\n",
      "| loss_kl         | 0.0013171034 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0008523492 |\n",
      "| loss_vf_loss    | 43.500248    |\n",
      "----------------------------------\n",
      "********** Iteration 170 ************\n",
      "Eval num_timesteps=696320, episode_reward=-71.40 +/- 6.70\n",
      "Episode length: 72.40 +/- 6.70\n",
      "Eval num_timesteps=696320, episode_reward=-70.40 +/- 6.68\n",
      "Episode length: 71.40 +/- 6.68\n",
      "Eval num_timesteps=696320, episode_reward=-81.10 +/- 11.41\n",
      "Episode length: 82.10 +/- 11.41\n",
      "Eval num_timesteps=696320, episode_reward=-74.70 +/- 6.69\n",
      "Episode length: 75.70 +/- 6.69\n",
      "Eval num_timesteps=696320, episode_reward=-69.00 +/- 6.12\n",
      "Episode length: 70.00 +/- 6.12\n",
      "Eval num_timesteps=696320, episode_reward=-82.90 +/- 13.28\n",
      "Episode length: 83.90 +/- 13.28\n",
      "Eval num_timesteps=696320, episode_reward=-72.40 +/- 6.93\n",
      "Episode length: 73.40 +/- 6.93\n",
      "Eval num_timesteps=696320, episode_reward=-71.20 +/- 8.00\n",
      "Episode length: 72.20 +/- 8.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00167 |       0.00000 |      40.12815 |       0.00112 |       0.09353\n",
      "      0.00123 |       0.00000 |      39.38983 |       0.00118 |       0.09445\n",
      "      0.00177 |       0.00000 |      38.92405 |       0.00113 |       0.09435\n",
      "      0.00177 |       0.00000 |      38.90735 |       0.00122 |       0.09473\n",
      "      0.00085 |       0.00000 |      38.60416 |       0.00125 |       0.09503\n",
      "      0.00072 |       0.00000 |      38.57606 |       0.00117 |       0.09508\n",
      "      0.00194 |       0.00000 |      38.24650 |       0.00116 |       0.09522\n",
      "      0.00144 |       0.00000 |      38.22076 |       0.00127 |       0.09530\n",
      "      0.00124 |       0.00000 |      38.03745 |       0.00136 |       0.09505\n",
      "      0.00129 |       0.00000 |      37.97927 |       0.00132 |       0.09537\n",
      "Evaluating losses...\n",
      "      0.00162 |       0.00000 |      37.84044 |       0.00135 |       0.09469\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.7         |\n",
      "| EpRewMean       | -85.7        |\n",
      "| EpThisIter      | 50           |\n",
      "| EpisodesSoFar   | 7226         |\n",
      "| TimeElapsed     | 2.23e+03     |\n",
      "| TimestepsSoFar  | 700416       |\n",
      "| ev_tdlam_before | 0.847        |\n",
      "| loss_ent        | 0.094688855  |\n",
      "| loss_kl         | 0.0013523443 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001618535  |\n",
      "| loss_vf_loss    | 37.840443    |\n",
      "----------------------------------\n",
      "********** Iteration 171 ************\n",
      "Eval num_timesteps=700416, episode_reward=-71.40 +/- 10.82\n",
      "Episode length: 72.40 +/- 10.82\n",
      "Eval num_timesteps=700416, episode_reward=-78.70 +/- 12.23\n",
      "Episode length: 79.70 +/- 12.23\n",
      "Eval num_timesteps=700416, episode_reward=-72.50 +/- 6.12\n",
      "Episode length: 73.50 +/- 6.12\n",
      "Eval num_timesteps=700416, episode_reward=-78.40 +/- 11.58\n",
      "Episode length: 79.40 +/- 11.58\n",
      "Eval num_timesteps=700416, episode_reward=-78.80 +/- 26.73\n",
      "Episode length: 79.80 +/- 26.73\n",
      "Eval num_timesteps=700416, episode_reward=-78.60 +/- 23.65\n",
      "Episode length: 79.60 +/- 23.65\n",
      "Eval num_timesteps=700416, episode_reward=-72.80 +/- 9.06\n",
      "Episode length: 73.80 +/- 9.06\n",
      "Eval num_timesteps=700416, episode_reward=-78.10 +/- 11.71\n",
      "Episode length: 79.10 +/- 11.71\n",
      "Eval num_timesteps=700416, episode_reward=-73.60 +/- 6.92\n",
      "Episode length: 74.60 +/- 6.92\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00210 |       0.00000 |      37.14344 |       0.00114 |       0.09704\n",
      "      0.00072 |       0.00000 |      36.85360 |       0.00130 |       0.09680\n",
      "      0.00226 |       0.00000 |      36.63489 |       0.00122 |       0.09741\n",
      "      0.00262 |       0.00000 |      36.48675 |       0.00134 |       0.09758\n",
      "      0.00262 |       0.00000 |      36.57292 |       0.00136 |       0.09770\n",
      "      0.00246 |       0.00000 |      36.48703 |       0.00140 |       0.09780\n",
      "      0.00108 |       0.00000 |      36.51324 |       0.00140 |       0.09790\n",
      "      0.00242 |       0.00000 |      36.30252 |       0.00159 |       0.09786\n",
      "      0.00293 |       0.00000 |      36.21114 |       0.00150 |       0.09768\n",
      "      0.00102 |       0.00000 |      36.18291 |       0.00145 |       0.09836\n",
      "Evaluating losses...\n",
      "      0.00223 |       0.00000 |      36.13985 |       0.00139 |       0.09907\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.3         |\n",
      "| EpRewMean       | -85.3        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7272         |\n",
      "| TimeElapsed     | 2.24e+03     |\n",
      "| TimestepsSoFar  | 704512       |\n",
      "| ev_tdlam_before | 0.847        |\n",
      "| loss_ent        | 0.099067256  |\n",
      "| loss_kl         | 0.0013913448 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002234791  |\n",
      "| loss_vf_loss    | 36.139847    |\n",
      "----------------------------------\n",
      "********** Iteration 172 ************\n",
      "Eval num_timesteps=704512, episode_reward=-76.20 +/- 10.42\n",
      "Episode length: 77.20 +/- 10.42\n",
      "Eval num_timesteps=704512, episode_reward=-102.80 +/- 87.23\n",
      "Episode length: 103.80 +/- 87.23\n",
      "Eval num_timesteps=704512, episode_reward=-83.40 +/- 22.72\n",
      "Episode length: 84.40 +/- 22.72\n",
      "Eval num_timesteps=704512, episode_reward=-80.70 +/- 16.84\n",
      "Episode length: 81.70 +/- 16.84\n",
      "Eval num_timesteps=704512, episode_reward=-96.60 +/- 61.25\n",
      "Episode length: 97.60 +/- 61.25\n",
      "Eval num_timesteps=704512, episode_reward=-75.40 +/- 7.46\n",
      "Episode length: 76.40 +/- 7.46\n",
      "Eval num_timesteps=704512, episode_reward=-76.70 +/- 7.47\n",
      "Episode length: 77.70 +/- 7.47\n",
      "Eval num_timesteps=704512, episode_reward=-73.00 +/- 9.39\n",
      "Episode length: 74.00 +/- 9.39\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00209 |       0.00000 |      38.04231 |       0.00132 |       0.10096\n",
      "      0.00140 |       0.00000 |      37.44825 |       0.00121 |       0.10077\n",
      "      0.00132 |       0.00000 |      37.23920 |       0.00112 |       0.10081\n",
      "      0.00185 |       0.00000 |      36.98726 |       0.00128 |       0.10105\n",
      "      0.00147 |       0.00000 |      36.76360 |       0.00139 |       0.10124\n",
      "      0.00105 |       0.00000 |      36.65178 |       0.00153 |       0.10103\n",
      "      0.00267 |       0.00000 |      36.44790 |       0.00146 |       0.10140\n",
      "      0.00229 |       0.00000 |      36.37229 |       0.00159 |       0.10108\n",
      "      0.00210 |       0.00000 |      36.15451 |       0.00143 |       0.10104\n",
      "      0.00085 |       0.00000 |      36.21808 |       0.00137 |       0.10085\n",
      "Evaluating losses...\n",
      "      0.00093 |       0.00000 |      36.08965 |       0.00142 |       0.10078\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90            |\n",
      "| EpRewMean       | -89           |\n",
      "| EpThisIter      | 44            |\n",
      "| EpisodesSoFar   | 7316          |\n",
      "| TimeElapsed     | 2.25e+03      |\n",
      "| TimestepsSoFar  | 708608        |\n",
      "| ev_tdlam_before | 0.84          |\n",
      "| loss_ent        | 0.10077704    |\n",
      "| loss_kl         | 0.0014232553  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00093494635 |\n",
      "| loss_vf_loss    | 36.089653     |\n",
      "-----------------------------------\n",
      "********** Iteration 173 ************\n",
      "Eval num_timesteps=708608, episode_reward=-72.10 +/- 7.84\n",
      "Episode length: 73.10 +/- 7.84\n",
      "Eval num_timesteps=708608, episode_reward=-73.50 +/- 14.99\n",
      "Episode length: 74.50 +/- 14.99\n",
      "Eval num_timesteps=708608, episode_reward=-75.00 +/- 8.37\n",
      "Episode length: 76.00 +/- 8.37\n",
      "Eval num_timesteps=708608, episode_reward=-78.60 +/- 17.40\n",
      "Episode length: 79.60 +/- 17.40\n",
      "Eval num_timesteps=708608, episode_reward=-69.20 +/- 7.18\n",
      "Episode length: 70.20 +/- 7.18\n",
      "Eval num_timesteps=708608, episode_reward=-81.70 +/- 16.32\n",
      "Episode length: 82.70 +/- 16.32\n",
      "Eval num_timesteps=708608, episode_reward=-95.80 +/- 44.40\n",
      "Episode length: 96.80 +/- 44.40\n",
      "Eval num_timesteps=708608, episode_reward=-88.50 +/- 32.58\n",
      "Episode length: 89.50 +/- 32.58\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00091 |       0.00000 |      37.57537 |       0.00095 |       0.09149\n",
      "      0.00202 |       0.00000 |      37.33977 |       0.00110 |       0.09122\n",
      "      0.00114 |       0.00000 |      37.29214 |       0.00130 |       0.09083\n",
      "      0.00140 |       0.00000 |      37.01011 |       0.00148 |       0.09165\n",
      "      0.00113 |       0.00000 |      36.92015 |       0.00119 |       0.09087\n",
      "      0.00054 |       0.00000 |      37.01014 |       0.00143 |       0.09126\n",
      "      0.00137 |       0.00000 |      36.67106 |       0.00130 |       0.09117\n",
      "      0.00103 |       0.00000 |      36.69732 |       0.00137 |       0.09150\n",
      "      0.00091 |       0.00000 |      36.62854 |       0.00124 |       0.09142\n",
      "      0.00113 |       0.00000 |      36.61282 |       0.00124 |       0.09127\n",
      "Evaluating losses...\n",
      "      0.00076 |       0.00000 |      36.53804 |       0.00144 |       0.09150\n",
      "----------------------------------\n",
      "| EpLenMean       | 91           |\n",
      "| EpRewMean       | -90          |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7362         |\n",
      "| TimeElapsed     | 2.26e+03     |\n",
      "| TimestepsSoFar  | 712704       |\n",
      "| ev_tdlam_before | 0.842        |\n",
      "| loss_ent        | 0.09150121   |\n",
      "| loss_kl         | 0.0014351702 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0007555426 |\n",
      "| loss_vf_loss    | 36.538036    |\n",
      "----------------------------------\n",
      "********** Iteration 174 ************\n",
      "Eval num_timesteps=712704, episode_reward=-72.30 +/- 6.36\n",
      "Episode length: 73.30 +/- 6.36\n",
      "Eval num_timesteps=712704, episode_reward=-87.70 +/- 39.16\n",
      "Episode length: 88.70 +/- 39.16\n",
      "Eval num_timesteps=712704, episode_reward=-78.70 +/- 15.69\n",
      "Episode length: 79.70 +/- 15.69\n",
      "Eval num_timesteps=712704, episode_reward=-75.50 +/- 8.14\n",
      "Episode length: 76.50 +/- 8.14\n",
      "Eval num_timesteps=712704, episode_reward=-69.70 +/- 6.60\n",
      "Episode length: 70.70 +/- 6.60\n",
      "Eval num_timesteps=712704, episode_reward=-73.50 +/- 6.05\n",
      "Episode length: 74.50 +/- 6.05\n",
      "Eval num_timesteps=712704, episode_reward=-93.50 +/- 39.52\n",
      "Episode length: 94.50 +/- 39.52\n",
      "Eval num_timesteps=712704, episode_reward=-78.40 +/- 11.51\n",
      "Episode length: 79.40 +/- 11.51\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00158 |       0.00000 |      37.57487 |       0.00116 |       0.09090\n",
      "      0.00079 |       0.00000 |      37.10038 |       0.00121 |       0.09069\n",
      "      0.00130 |       0.00000 |      36.83137 |       0.00102 |       0.09139\n",
      "      0.00151 |       0.00000 |      36.69925 |       0.00121 |       0.09020\n",
      "      0.00218 |       0.00000 |      36.33122 |       0.00121 |       0.09003\n",
      "      0.00172 |       0.00000 |      36.17202 |       0.00106 |       0.08965\n",
      "      0.00164 |       0.00000 |      36.05986 |       0.00108 |       0.09001\n",
      "      0.00121 |       0.00000 |      35.86489 |       0.00117 |       0.08932\n",
      "      0.00204 |       0.00000 |      35.82440 |       0.00121 |       0.08967\n",
      "      0.00050 |       0.00000 |      35.62613 |       0.00106 |       0.08959\n",
      "Evaluating losses...\n",
      "      0.00186 |       0.00000 |      35.68661 |       0.00111 |       0.08878\n",
      "---------------------------------\n",
      "| EpLenMean       | 89.4        |\n",
      "| EpRewMean       | -88.4       |\n",
      "| EpThisIter      | 46          |\n",
      "| EpisodesSoFar   | 7408        |\n",
      "| TimeElapsed     | 2.27e+03    |\n",
      "| TimestepsSoFar  | 716800      |\n",
      "| ev_tdlam_before | 0.847       |\n",
      "| loss_ent        | 0.08878081  |\n",
      "| loss_kl         | 0.001114625 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | 0.001863827 |\n",
      "| loss_vf_loss    | 35.68661    |\n",
      "---------------------------------\n",
      "********** Iteration 175 ************\n",
      "Eval num_timesteps=716800, episode_reward=-70.20 +/- 6.71\n",
      "Episode length: 71.20 +/- 6.71\n",
      "Eval num_timesteps=716800, episode_reward=-76.90 +/- 9.02\n",
      "Episode length: 77.90 +/- 9.02\n",
      "Eval num_timesteps=716800, episode_reward=-78.00 +/- 8.04\n",
      "Episode length: 79.00 +/- 8.04\n",
      "Eval num_timesteps=716800, episode_reward=-82.20 +/- 35.16\n",
      "Episode length: 83.20 +/- 35.16\n",
      "Eval num_timesteps=716800, episode_reward=-70.30 +/- 7.11\n",
      "Episode length: 71.30 +/- 7.11\n",
      "Eval num_timesteps=716800, episode_reward=-78.40 +/- 14.83\n",
      "Episode length: 79.40 +/- 14.83\n",
      "Eval num_timesteps=716800, episode_reward=-76.20 +/- 5.13\n",
      "Episode length: 77.20 +/- 5.13\n",
      "Eval num_timesteps=716800, episode_reward=-87.60 +/- 48.10\n",
      "Episode length: 88.60 +/- 48.10\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00142 |       0.00000 |      29.47671 |       0.00112 |       0.09294\n",
      "      0.00193 |       0.00000 |      29.11575 |       0.00121 |       0.09275\n",
      "      0.00105 |       0.00000 |      29.03598 |       0.00125 |       0.09300\n",
      "      0.00253 |       0.00000 |      28.87453 |       0.00109 |       0.09301\n",
      "      0.00222 |       0.00000 |      28.83625 |       0.00129 |       0.09212\n",
      "      0.00161 |       0.00000 |      28.67632 |       0.00115 |       0.09281\n",
      "      0.00142 |       0.00000 |      28.74016 |       0.00132 |       0.09300\n",
      "      0.00136 |       0.00000 |      28.66652 |       0.00116 |       0.09284\n",
      "      0.00089 |       0.00000 |      28.58745 |       0.00113 |       0.09301\n",
      "      0.00096 |       0.00000 |      28.53274 |       0.00118 |       0.09310\n",
      "Evaluating losses...\n",
      "      0.00176 |       0.00000 |      28.51436 |       0.00120 |       0.09306\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.6         |\n",
      "| EpRewMean       | -85.6        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 7456         |\n",
      "| TimeElapsed     | 2.28e+03     |\n",
      "| TimestepsSoFar  | 720896       |\n",
      "| ev_tdlam_before | 0.885        |\n",
      "| loss_ent        | 0.0930552    |\n",
      "| loss_kl         | 0.0011951869 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017609994 |\n",
      "| loss_vf_loss    | 28.514362    |\n",
      "----------------------------------\n",
      "********** Iteration 176 ************\n",
      "Eval num_timesteps=720896, episode_reward=-83.10 +/- 16.02\n",
      "Episode length: 84.10 +/- 16.02\n",
      "Eval num_timesteps=720896, episode_reward=-74.10 +/- 7.85\n",
      "Episode length: 75.10 +/- 7.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=720896, episode_reward=-78.20 +/- 15.76\n",
      "Episode length: 79.20 +/- 15.76\n",
      "Eval num_timesteps=720896, episode_reward=-89.80 +/- 55.35\n",
      "Episode length: 90.80 +/- 55.35\n",
      "Eval num_timesteps=720896, episode_reward=-77.30 +/- 15.98\n",
      "Episode length: 78.30 +/- 15.98\n",
      "Eval num_timesteps=720896, episode_reward=-83.30 +/- 33.06\n",
      "Episode length: 84.30 +/- 33.06\n",
      "Eval num_timesteps=720896, episode_reward=-85.30 +/- 16.61\n",
      "Episode length: 86.30 +/- 16.61\n",
      "Eval num_timesteps=720896, episode_reward=-79.90 +/- 18.61\n",
      "Episode length: 80.90 +/- 18.61\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00210 |       0.00000 |      34.98352 |       0.00129 |       0.08716\n",
      "      0.00135 |       0.00000 |      34.61509 |       0.00104 |       0.08762\n",
      "      0.00184 |       0.00000 |      34.05390 |       0.00112 |       0.08757\n",
      "      0.00086 |       0.00000 |      33.84057 |       0.00123 |       0.08806\n",
      "      0.00132 |       0.00000 |      33.48187 |       0.00128 |       0.08756\n",
      "      0.00197 |       0.00000 |      33.26947 |       0.00115 |       0.08813\n",
      "      0.00120 |       0.00000 |      33.32946 |       0.00133 |       0.08774\n",
      "      0.00161 |       0.00000 |      33.04840 |       0.00125 |       0.08881\n",
      "      0.00097 |       0.00000 |      32.96906 |       0.00145 |       0.08838\n",
      "      0.00124 |       0.00000 |      32.88917 |       0.00130 |       0.08830\n",
      "Evaluating losses...\n",
      "      0.00105 |       0.00000 |      32.66442 |       0.00150 |       0.08843\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.3         |\n",
      "| EpRewMean       | -86.3        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7502         |\n",
      "| TimeElapsed     | 2.29e+03     |\n",
      "| TimestepsSoFar  | 724992       |\n",
      "| ev_tdlam_before | 0.852        |\n",
      "| loss_ent        | 0.08843112   |\n",
      "| loss_kl         | 0.0014952288 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0010511051 |\n",
      "| loss_vf_loss    | 32.66442     |\n",
      "----------------------------------\n",
      "********** Iteration 177 ************\n",
      "Eval num_timesteps=724992, episode_reward=-99.40 +/- 50.36\n",
      "Episode length: 100.40 +/- 50.36\n",
      "Eval num_timesteps=724992, episode_reward=-70.80 +/- 8.72\n",
      "Episode length: 71.80 +/- 8.72\n",
      "Eval num_timesteps=724992, episode_reward=-78.80 +/- 16.42\n",
      "Episode length: 79.80 +/- 16.42\n",
      "Eval num_timesteps=724992, episode_reward=-75.20 +/- 10.93\n",
      "Episode length: 76.20 +/- 10.93\n",
      "Eval num_timesteps=724992, episode_reward=-102.00 +/- 62.27\n",
      "Episode length: 103.00 +/- 62.27\n",
      "Eval num_timesteps=724992, episode_reward=-75.90 +/- 10.23\n",
      "Episode length: 76.90 +/- 10.23\n",
      "Eval num_timesteps=724992, episode_reward=-78.00 +/- 8.12\n",
      "Episode length: 79.00 +/- 8.12\n",
      "Eval num_timesteps=724992, episode_reward=-75.00 +/- 13.69\n",
      "Episode length: 76.00 +/- 13.69\n",
      "Eval num_timesteps=724992, episode_reward=-80.60 +/- 31.09\n",
      "Episode length: 81.60 +/- 31.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00176 |       0.00000 |      43.98336 |       0.00117 |       0.09626\n",
      "      0.00157 |       0.00000 |      43.79304 |       0.00118 |       0.09624\n",
      "      0.00154 |       0.00000 |      43.61736 |       0.00123 |       0.09538\n",
      "      0.00188 |       0.00000 |      43.82013 |       0.00121 |       0.09531\n",
      "      0.00174 |       0.00000 |      43.60895 |       0.00126 |       0.09471\n",
      "      0.00181 |       0.00000 |      43.48611 |       0.00128 |       0.09462\n",
      "      0.00185 |       0.00000 |      43.43605 |       0.00144 |       0.09455\n",
      "      0.00201 |       0.00000 |      43.46375 |       0.00132 |       0.09521\n",
      "      0.00147 |       0.00000 |      43.27834 |       0.00143 |       0.09428\n",
      "      0.00063 |       0.00000 |      43.28876 |       0.00118 |       0.09467\n",
      "Evaluating losses...\n",
      "      0.00098 |       0.00000 |      43.18732 |       0.00119 |       0.09493\n",
      "----------------------------------\n",
      "| EpLenMean       | 89           |\n",
      "| EpRewMean       | -88          |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7548         |\n",
      "| TimeElapsed     | 2.3e+03      |\n",
      "| TimestepsSoFar  | 729088       |\n",
      "| ev_tdlam_before | 0.819        |\n",
      "| loss_ent        | 0.09493172   |\n",
      "| loss_kl         | 0.0011870862 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0009792061 |\n",
      "| loss_vf_loss    | 43.187325    |\n",
      "----------------------------------\n",
      "********** Iteration 178 ************\n",
      "Eval num_timesteps=729088, episode_reward=-90.50 +/- 37.39\n",
      "Episode length: 91.50 +/- 37.39\n",
      "Eval num_timesteps=729088, episode_reward=-76.80 +/- 9.60\n",
      "Episode length: 77.80 +/- 9.60\n",
      "Eval num_timesteps=729088, episode_reward=-76.50 +/- 9.05\n",
      "Episode length: 77.50 +/- 9.05\n",
      "Eval num_timesteps=729088, episode_reward=-80.00 +/- 8.31\n",
      "Episode length: 81.00 +/- 8.31\n",
      "Eval num_timesteps=729088, episode_reward=-78.50 +/- 16.37\n",
      "Episode length: 79.50 +/- 16.37\n",
      "Eval num_timesteps=729088, episode_reward=-72.60 +/- 5.44\n",
      "Episode length: 73.60 +/- 5.44\n",
      "Eval num_timesteps=729088, episode_reward=-69.90 +/- 6.33\n",
      "Episode length: 70.90 +/- 6.33\n",
      "Eval num_timesteps=729088, episode_reward=-74.10 +/- 9.67\n",
      "Episode length: 75.10 +/- 9.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00167 |       0.00000 |      29.94854 |       0.00113 |       0.09710\n",
      "      0.00151 |       0.00000 |      29.31453 |       0.00127 |       0.09735\n",
      "      0.00232 |       0.00000 |      28.95156 |       0.00127 |       0.09724\n",
      "      0.00242 |       0.00000 |      28.76032 |       0.00133 |       0.09697\n",
      "      0.00194 |       0.00000 |      28.64518 |       0.00119 |       0.09693\n",
      "      0.00177 |       0.00000 |      28.35220 |       0.00115 |       0.09652\n",
      "      0.00183 |       0.00000 |      28.46699 |       0.00132 |       0.09682\n",
      "      0.00166 |       0.00000 |      28.25787 |       0.00121 |       0.09681\n",
      "      0.00204 |       0.00000 |      28.26772 |       0.00133 |       0.09622\n",
      "      0.00112 |       0.00000 |      28.13090 |       0.00142 |       0.09609\n",
      "Evaluating losses...\n",
      "      0.00174 |       0.00000 |      28.05805 |       0.00141 |       0.09663\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.6         |\n",
      "| EpRewMean       | -83.6        |\n",
      "| EpThisIter      | 51           |\n",
      "| EpisodesSoFar   | 7599         |\n",
      "| TimeElapsed     | 2.31e+03     |\n",
      "| TimestepsSoFar  | 733184       |\n",
      "| ev_tdlam_before | 0.888        |\n",
      "| loss_ent        | 0.09663216   |\n",
      "| loss_kl         | 0.0014086163 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017409234 |\n",
      "| loss_vf_loss    | 28.058052    |\n",
      "----------------------------------\n",
      "********** Iteration 179 ************\n",
      "Eval num_timesteps=733184, episode_reward=-74.50 +/- 7.90\n",
      "Episode length: 75.50 +/- 7.90\n",
      "Eval num_timesteps=733184, episode_reward=-78.40 +/- 13.67\n",
      "Episode length: 79.40 +/- 13.67\n",
      "Eval num_timesteps=733184, episode_reward=-76.00 +/- 9.97\n",
      "Episode length: 77.00 +/- 9.97\n",
      "Eval num_timesteps=733184, episode_reward=-75.30 +/- 9.59\n",
      "Episode length: 76.30 +/- 9.59\n",
      "Eval num_timesteps=733184, episode_reward=-71.70 +/- 5.44\n",
      "Episode length: 72.70 +/- 5.44\n",
      "Eval num_timesteps=733184, episode_reward=-81.90 +/- 17.85\n",
      "Episode length: 82.90 +/- 17.85\n",
      "Eval num_timesteps=733184, episode_reward=-72.80 +/- 4.75\n",
      "Episode length: 73.80 +/- 4.75\n",
      "Eval num_timesteps=733184, episode_reward=-81.00 +/- 15.00\n",
      "Episode length: 82.00 +/- 15.00\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00118 |       0.00000 |      27.64987 |       0.00117 |       0.09391\n",
      "      0.00183 |       0.00000 |      27.48655 |       0.00138 |       0.09383\n",
      "      0.00203 |       0.00000 |      27.56415 |       0.00133 |       0.09382\n",
      "      0.00213 |       0.00000 |      27.52659 |       0.00133 |       0.09422\n",
      "      0.00138 |       0.00000 |      27.44903 |       0.00138 |       0.09423\n",
      "      0.00121 |       0.00000 |      27.29757 |       0.00124 |       0.09459\n",
      "      0.00081 |       0.00000 |      27.39583 |       0.00128 |       0.09431\n",
      "      0.00095 |       0.00000 |      27.34721 |       0.00128 |       0.09429\n",
      "      0.00137 |       0.00000 |      27.39233 |       0.00140 |       0.09547\n",
      "      0.00194 |       0.00000 |      27.21653 |       0.00140 |       0.09533\n",
      "Evaluating losses...\n",
      "      0.00069 |       0.00000 |      27.16467 |       0.00121 |       0.09459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| EpLenMean       | 82.8         |\n",
      "| EpRewMean       | -81.8        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 7647         |\n",
      "| TimeElapsed     | 2.32e+03     |\n",
      "| TimestepsSoFar  | 737280       |\n",
      "| ev_tdlam_before | 0.886        |\n",
      "| loss_ent        | 0.0945896    |\n",
      "| loss_kl         | 0.0012115105 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0006862979 |\n",
      "| loss_vf_loss    | 27.164673    |\n",
      "----------------------------------\n",
      "********** Iteration 180 ************\n",
      "Eval num_timesteps=737280, episode_reward=-79.10 +/- 15.67\n",
      "Episode length: 80.10 +/- 15.67\n",
      "Eval num_timesteps=737280, episode_reward=-69.30 +/- 4.90\n",
      "Episode length: 70.30 +/- 4.90\n",
      "Eval num_timesteps=737280, episode_reward=-74.40 +/- 6.39\n",
      "Episode length: 75.40 +/- 6.39\n",
      "Eval num_timesteps=737280, episode_reward=-76.70 +/- 13.15\n",
      "Episode length: 77.70 +/- 13.15\n",
      "Eval num_timesteps=737280, episode_reward=-72.20 +/- 8.02\n",
      "Episode length: 73.20 +/- 8.02\n",
      "Eval num_timesteps=737280, episode_reward=-75.00 +/- 14.57\n",
      "Episode length: 76.00 +/- 14.57\n",
      "Eval num_timesteps=737280, episode_reward=-75.10 +/- 8.80\n",
      "Episode length: 76.10 +/- 8.80\n",
      "Eval num_timesteps=737280, episode_reward=-74.60 +/- 11.10\n",
      "Episode length: 75.60 +/- 11.10\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00227 |       0.00000 |      34.53114 |       0.00119 |       0.09351\n",
      "      0.00137 |       0.00000 |      34.27998 |       0.00100 |       0.09317\n",
      "      0.00136 |       0.00000 |      34.37965 |       0.00121 |       0.09353\n",
      "      0.00163 |       0.00000 |      34.10468 |       0.00108 |       0.09359\n",
      "      0.00174 |       0.00000 |      34.26362 |       0.00126 |       0.09364\n",
      "      0.00153 |       0.00000 |      33.97975 |       0.00119 |       0.09366\n",
      "      0.00124 |       0.00000 |      33.99192 |       0.00122 |       0.09314\n",
      "      0.00148 |       0.00000 |      33.99624 |       0.00124 |       0.09408\n",
      "      0.00109 |       0.00000 |      33.82110 |       0.00124 |       0.09415\n",
      "      0.00119 |       0.00000 |      34.03064 |       0.00135 |       0.09381\n",
      "Evaluating losses...\n",
      "      0.00066 |       0.00000 |      33.92102 |       0.00119 |       0.09381\n",
      "-----------------------------------\n",
      "| EpLenMean       | 84.4          |\n",
      "| EpRewMean       | -83.4         |\n",
      "| EpThisIter      | 49            |\n",
      "| EpisodesSoFar   | 7696          |\n",
      "| TimeElapsed     | 2.33e+03      |\n",
      "| TimestepsSoFar  | 741376        |\n",
      "| ev_tdlam_before | 0.86          |\n",
      "| loss_ent        | 0.09380755    |\n",
      "| loss_kl         | 0.001191111   |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00066061283 |\n",
      "| loss_vf_loss    | 33.921017     |\n",
      "-----------------------------------\n",
      "********** Iteration 181 ************\n",
      "Eval num_timesteps=741376, episode_reward=-85.90 +/- 16.88\n",
      "Episode length: 86.90 +/- 16.88\n",
      "Eval num_timesteps=741376, episode_reward=-76.80 +/- 9.30\n",
      "Episode length: 77.80 +/- 9.30\n",
      "Eval num_timesteps=741376, episode_reward=-75.50 +/- 8.21\n",
      "Episode length: 76.50 +/- 8.21\n",
      "Eval num_timesteps=741376, episode_reward=-76.20 +/- 8.76\n",
      "Episode length: 77.20 +/- 8.76\n",
      "Eval num_timesteps=741376, episode_reward=-72.40 +/- 6.41\n",
      "Episode length: 73.40 +/- 6.41\n",
      "Eval num_timesteps=741376, episode_reward=-81.20 +/- 15.66\n",
      "Episode length: 82.20 +/- 15.66\n",
      "Eval num_timesteps=741376, episode_reward=-74.90 +/- 6.30\n",
      "Episode length: 75.90 +/- 6.30\n",
      "Eval num_timesteps=741376, episode_reward=-72.40 +/- 12.75\n",
      "Episode length: 73.40 +/- 12.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00184 |       0.00000 |      42.04276 |       0.00120 |       0.09361\n",
      "      0.00109 |       0.00000 |      41.81008 |       0.00116 |       0.09318\n",
      "      0.00158 |       0.00000 |      41.56630 |       0.00126 |       0.09307\n",
      "      0.00176 |       0.00000 |      41.44945 |       0.00131 |       0.09313\n",
      "      0.00132 |       0.00000 |      41.40711 |       0.00128 |       0.09378\n",
      "      0.00131 |       0.00000 |      41.20695 |       0.00122 |       0.09414\n",
      "      0.00141 |       0.00000 |      41.05283 |       0.00141 |       0.09373\n",
      "      0.00158 |       0.00000 |      41.07305 |       0.00129 |       0.09390\n",
      "      0.00077 |       0.00000 |      40.85516 |       0.00137 |       0.09330\n",
      "      0.00160 |       0.00000 |      40.93115 |       0.00133 |       0.09378\n",
      "Evaluating losses...\n",
      "      0.00089 |       0.00000 |      40.92359 |       0.00126 |       0.09334\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.2         |\n",
      "| EpRewMean       | -85.2        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7742         |\n",
      "| TimeElapsed     | 2.34e+03     |\n",
      "| TimestepsSoFar  | 745472       |\n",
      "| ev_tdlam_before | 0.82         |\n",
      "| loss_ent        | 0.09333997   |\n",
      "| loss_kl         | 0.0012604176 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0008936664 |\n",
      "| loss_vf_loss    | 40.923588    |\n",
      "----------------------------------\n",
      "********** Iteration 182 ************\n",
      "Eval num_timesteps=745472, episode_reward=-70.70 +/- 9.15\n",
      "Episode length: 71.70 +/- 9.15\n",
      "Eval num_timesteps=745472, episode_reward=-111.00 +/- 57.69\n",
      "Episode length: 112.00 +/- 57.69\n",
      "Eval num_timesteps=745472, episode_reward=-74.40 +/- 9.36\n",
      "Episode length: 75.40 +/- 9.36\n",
      "Eval num_timesteps=745472, episode_reward=-85.20 +/- 28.00\n",
      "Episode length: 86.20 +/- 28.00\n",
      "Eval num_timesteps=745472, episode_reward=-70.30 +/- 5.60\n",
      "Episode length: 71.30 +/- 5.60\n",
      "Eval num_timesteps=745472, episode_reward=-71.00 +/- 6.08\n",
      "Episode length: 72.00 +/- 6.08\n",
      "Eval num_timesteps=745472, episode_reward=-75.70 +/- 13.33\n",
      "Episode length: 76.70 +/- 13.33\n",
      "Eval num_timesteps=745472, episode_reward=-77.70 +/- 18.36\n",
      "Episode length: 78.70 +/- 18.36\n",
      "Eval num_timesteps=745472, episode_reward=-81.50 +/- 16.33\n",
      "Episode length: 82.50 +/- 16.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00152 |       0.00000 |      41.18222 |       0.00139 |       0.09028\n",
      "      0.00331 |       0.00000 |      40.55366 |       0.00139 |       0.09118\n",
      "      0.00209 |       0.00000 |      40.08810 |       0.00126 |       0.09028\n",
      "      0.00117 |       0.00000 |      39.90949 |       0.00130 |       0.09062\n",
      "      0.00198 |       0.00000 |      39.67926 |       0.00130 |       0.09092\n",
      "      0.00158 |       0.00000 |      39.65705 |       0.00134 |       0.09045\n",
      "      0.00072 |       0.00000 |      39.55140 |       0.00123 |       0.09081\n",
      "      0.00161 |       0.00000 |      39.47988 |       0.00122 |       0.09110\n",
      "      0.00239 |       0.00000 |      39.29102 |       0.00127 |       0.09026\n",
      "      0.00076 |       0.00000 |      39.49077 |       0.00149 |       0.09049\n",
      "Evaluating losses...\n",
      "      0.00092 |       0.00000 |      39.31411 |       0.00121 |       0.09035\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.9         |\n",
      "| EpRewMean       | -88.9        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 7786         |\n",
      "| TimeElapsed     | 2.35e+03     |\n",
      "| TimestepsSoFar  | 749568       |\n",
      "| ev_tdlam_before | 0.821        |\n",
      "| loss_ent        | 0.09034604   |\n",
      "| loss_kl         | 0.0012092598 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0009178417 |\n",
      "| loss_vf_loss    | 39.31411     |\n",
      "----------------------------------\n",
      "********** Iteration 183 ************\n",
      "Eval num_timesteps=749568, episode_reward=-89.20 +/- 47.84\n",
      "Episode length: 90.20 +/- 47.84\n",
      "Eval num_timesteps=749568, episode_reward=-72.80 +/- 6.54\n",
      "Episode length: 73.80 +/- 6.54\n",
      "Eval num_timesteps=749568, episode_reward=-74.50 +/- 10.93\n",
      "Episode length: 75.50 +/- 10.93\n",
      "Eval num_timesteps=749568, episode_reward=-72.80 +/- 18.94\n",
      "Episode length: 73.80 +/- 18.94\n",
      "Eval num_timesteps=749568, episode_reward=-84.30 +/- 32.11\n",
      "Episode length: 85.30 +/- 32.11\n",
      "Eval num_timesteps=749568, episode_reward=-75.10 +/- 10.68\n",
      "Episode length: 76.10 +/- 10.68\n",
      "Eval num_timesteps=749568, episode_reward=-82.30 +/- 21.01\n",
      "Episode length: 83.30 +/- 21.01\n",
      "Eval num_timesteps=749568, episode_reward=-73.50 +/- 6.18\n",
      "Episode length: 74.50 +/- 6.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00197 |       0.00000 |      37.18736 |       0.00126 |       0.09518\n",
      "      0.00119 |       0.00000 |      36.71167 |       0.00133 |       0.09460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00204 |       0.00000 |      36.61231 |       0.00116 |       0.09439\n",
      "      0.00283 |       0.00000 |      36.30820 |       0.00113 |       0.09467\n",
      "      0.00088 |       0.00000 |      36.12381 |       0.00103 |       0.09429\n",
      "      0.00183 |       0.00000 |      36.04939 |       0.00131 |       0.09457\n",
      "      0.00102 |       0.00000 |      36.02916 |       0.00128 |       0.09475\n",
      "      0.00291 |       0.00000 |      35.80712 |       0.00138 |       0.09511\n",
      "      0.00267 |       0.00000 |      35.69721 |       0.00138 |       0.09435\n",
      "      0.00205 |       0.00000 |      35.57595 |       0.00133 |       0.09536\n",
      "Evaluating losses...\n",
      "      0.00127 |       0.00000 |      35.57999 |       0.00139 |       0.09502\n",
      "----------------------------------\n",
      "| EpLenMean       | 89           |\n",
      "| EpRewMean       | -88          |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 7833         |\n",
      "| TimeElapsed     | 2.36e+03     |\n",
      "| TimestepsSoFar  | 753664       |\n",
      "| ev_tdlam_before | 0.846        |\n",
      "| loss_ent        | 0.0950167    |\n",
      "| loss_kl         | 0.0013880769 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0012718289 |\n",
      "| loss_vf_loss    | 35.57999     |\n",
      "----------------------------------\n",
      "********** Iteration 184 ************\n",
      "Eval num_timesteps=753664, episode_reward=-73.50 +/- 7.79\n",
      "Episode length: 74.50 +/- 7.79\n",
      "Eval num_timesteps=753664, episode_reward=-71.50 +/- 5.97\n",
      "Episode length: 72.50 +/- 5.97\n",
      "Eval num_timesteps=753664, episode_reward=-71.90 +/- 8.44\n",
      "Episode length: 72.90 +/- 8.44\n",
      "Eval num_timesteps=753664, episode_reward=-73.90 +/- 9.58\n",
      "Episode length: 74.90 +/- 9.58\n",
      "Eval num_timesteps=753664, episode_reward=-92.60 +/- 53.59\n",
      "Episode length: 93.60 +/- 53.59\n",
      "Eval num_timesteps=753664, episode_reward=-73.20 +/- 7.48\n",
      "Episode length: 74.20 +/- 7.48\n",
      "Eval num_timesteps=753664, episode_reward=-72.70 +/- 7.44\n",
      "Episode length: 73.70 +/- 7.44\n",
      "Eval num_timesteps=753664, episode_reward=-81.10 +/- 22.67\n",
      "Episode length: 82.10 +/- 22.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00147 |       0.00000 |      46.49147 |       0.00115 |       0.09119\n",
      "      0.00263 |       0.00000 |      46.32048 |       0.00114 |       0.09089\n",
      "      0.00230 |       0.00000 |      46.19929 |       0.00133 |       0.09095\n",
      "      0.00148 |       0.00000 |      46.22411 |       0.00120 |       0.09091\n",
      "      0.00233 |       0.00000 |      45.99792 |       0.00127 |       0.09092\n",
      "      0.00128 |       0.00000 |      45.89825 |       0.00119 |       0.09101\n",
      "      0.00198 |       0.00000 |      45.82664 |       0.00113 |       0.09106\n",
      "      0.00156 |       0.00000 |      45.69879 |       0.00107 |       0.09039\n",
      "      0.00216 |       0.00000 |      45.61689 |       0.00149 |       0.09071\n",
      "      0.00161 |       0.00000 |      45.55220 |       0.00136 |       0.09065\n",
      "Evaluating losses...\n",
      "      0.00146 |       0.00000 |      45.66130 |       0.00128 |       0.09059\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.1         |\n",
      "| EpRewMean       | -88.1        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 7879         |\n",
      "| TimeElapsed     | 2.37e+03     |\n",
      "| TimestepsSoFar  | 757760       |\n",
      "| ev_tdlam_before | 0.801        |\n",
      "| loss_ent        | 0.09058943   |\n",
      "| loss_kl         | 0.0012764869 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.001460088  |\n",
      "| loss_vf_loss    | 45.6613      |\n",
      "----------------------------------\n",
      "********** Iteration 185 ************\n",
      "Eval num_timesteps=757760, episode_reward=-77.30 +/- 14.64\n",
      "Episode length: 78.30 +/- 14.64\n",
      "Eval num_timesteps=757760, episode_reward=-85.80 +/- 17.15\n",
      "Episode length: 86.80 +/- 17.15\n",
      "Eval num_timesteps=757760, episode_reward=-72.80 +/- 5.74\n",
      "Episode length: 73.80 +/- 5.74\n",
      "Eval num_timesteps=757760, episode_reward=-75.50 +/- 17.48\n",
      "Episode length: 76.50 +/- 17.48\n",
      "Eval num_timesteps=757760, episode_reward=-76.50 +/- 9.29\n",
      "Episode length: 77.50 +/- 9.29\n",
      "Eval num_timesteps=757760, episode_reward=-78.60 +/- 10.47\n",
      "Episode length: 79.60 +/- 10.47\n",
      "Eval num_timesteps=757760, episode_reward=-73.30 +/- 7.80\n",
      "Episode length: 74.30 +/- 7.80\n",
      "Eval num_timesteps=757760, episode_reward=-77.00 +/- 8.65\n",
      "Episode length: 78.00 +/- 8.65\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00220 |       0.00000 |      28.52896 |       0.00123 |       0.09357\n",
      "      0.00203 |       0.00000 |      28.17093 |       0.00116 |       0.09295\n",
      "      0.00206 |       0.00000 |      28.03031 |       0.00119 |       0.09329\n",
      "      0.00083 |       0.00000 |      27.93643 |       0.00123 |       0.09287\n",
      "      0.00198 |       0.00000 |      27.82279 |       0.00130 |       0.09284\n",
      "      0.00212 |       0.00000 |      27.61169 |       0.00120 |       0.09267\n",
      "      0.00165 |       0.00000 |      27.67199 |       0.00117 |       0.09283\n",
      "      0.00216 |       0.00000 |      27.60791 |       0.00118 |       0.09259\n",
      "      0.00223 |       0.00000 |      27.61484 |       0.00110 |       0.09334\n",
      "      0.00170 |       0.00000 |      27.65251 |       0.00126 |       0.09358\n",
      "Evaluating losses...\n",
      "      0.00193 |       0.00000 |      27.47581 |       0.00124 |       0.09285\n",
      "----------------------------------\n",
      "| EpLenMean       | 85.8         |\n",
      "| EpRewMean       | -84.8        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 7928         |\n",
      "| TimeElapsed     | 2.38e+03     |\n",
      "| TimestepsSoFar  | 761856       |\n",
      "| ev_tdlam_before | 0.886        |\n",
      "| loss_ent        | 0.09285208   |\n",
      "| loss_kl         | 0.0012399347 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019261038 |\n",
      "| loss_vf_loss    | 27.475811    |\n",
      "----------------------------------\n",
      "********** Iteration 186 ************\n",
      "Eval num_timesteps=761856, episode_reward=-78.30 +/- 8.60\n",
      "Episode length: 79.30 +/- 8.60\n",
      "Eval num_timesteps=761856, episode_reward=-77.90 +/- 11.86\n",
      "Episode length: 78.90 +/- 11.86\n",
      "Eval num_timesteps=761856, episode_reward=-70.90 +/- 7.60\n",
      "Episode length: 71.90 +/- 7.60\n",
      "Eval num_timesteps=761856, episode_reward=-76.20 +/- 10.60\n",
      "Episode length: 77.20 +/- 10.60\n",
      "Eval num_timesteps=761856, episode_reward=-79.90 +/- 21.12\n",
      "Episode length: 80.90 +/- 21.12\n",
      "Eval num_timesteps=761856, episode_reward=-79.90 +/- 10.70\n",
      "Episode length: 80.90 +/- 10.70\n",
      "Eval num_timesteps=761856, episode_reward=-72.00 +/- 2.53\n",
      "Episode length: 73.00 +/- 2.53\n",
      "Eval num_timesteps=761856, episode_reward=-74.20 +/- 9.44\n",
      "Episode length: 75.20 +/- 9.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00214 |       0.00000 |      39.29147 |       0.00129 |       0.09257\n",
      "      0.00223 |       0.00000 |      39.00409 |       0.00120 |       0.09246\n",
      "      0.00128 |       0.00000 |      38.87859 |       0.00118 |       0.09283\n",
      "      0.00123 |       0.00000 |      38.61302 |       0.00134 |       0.09293\n",
      "      0.00188 |       0.00000 |      38.52652 |       0.00133 |       0.09254\n",
      "      0.00179 |       0.00000 |      38.28459 |       0.00122 |       0.09316\n",
      "      0.00119 |       0.00000 |      38.26163 |       0.00132 |       0.09265\n",
      "      0.00103 |       0.00000 |      38.03445 |       0.00126 |       0.09325\n",
      "      0.00162 |       0.00000 |      37.92366 |       0.00142 |       0.09299\n",
      "      0.00210 |       0.00000 |      37.78865 |       0.00143 |       0.09310\n",
      "Evaluating losses...\n",
      "      0.00138 |       0.00000 |      37.67310 |       0.00112 |       0.09383\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.3         |\n",
      "| EpRewMean       | -85.3        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 7973         |\n",
      "| TimeElapsed     | 2.39e+03     |\n",
      "| TimestepsSoFar  | 765952       |\n",
      "| ev_tdlam_before | 0.832        |\n",
      "| loss_ent        | 0.093834825  |\n",
      "| loss_kl         | 0.0011228612 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013822415 |\n",
      "| loss_vf_loss    | 37.6731      |\n",
      "----------------------------------\n",
      "********** Iteration 187 ************\n",
      "Eval num_timesteps=765952, episode_reward=-73.80 +/- 7.04\n",
      "Episode length: 74.80 +/- 7.04\n",
      "Eval num_timesteps=765952, episode_reward=-79.80 +/- 10.77\n",
      "Episode length: 80.80 +/- 10.77\n",
      "Eval num_timesteps=765952, episode_reward=-81.60 +/- 26.90\n",
      "Episode length: 82.60 +/- 26.90\n",
      "Eval num_timesteps=765952, episode_reward=-76.30 +/- 10.34\n",
      "Episode length: 77.30 +/- 10.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=765952, episode_reward=-74.30 +/- 9.68\n",
      "Episode length: 75.30 +/- 9.68\n",
      "Eval num_timesteps=765952, episode_reward=-79.50 +/- 20.84\n",
      "Episode length: 80.50 +/- 20.84\n",
      "Eval num_timesteps=765952, episode_reward=-83.40 +/- 13.10\n",
      "Episode length: 84.40 +/- 13.10\n",
      "Eval num_timesteps=765952, episode_reward=-97.60 +/- 45.25\n",
      "Episode length: 98.60 +/- 45.25\n",
      "Eval num_timesteps=765952, episode_reward=-78.40 +/- 16.23\n",
      "Episode length: 79.40 +/- 16.23\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00170 |       0.00000 |      32.89487 |       0.00127 |       0.09288\n",
      "      0.00183 |       0.00000 |      32.82096 |       0.00125 |       0.09300\n",
      "      0.00292 |       0.00000 |      32.73682 |       0.00134 |       0.09248\n",
      "      0.00146 |       0.00000 |      32.61089 |       0.00114 |       0.09322\n",
      "      0.00148 |       0.00000 |      32.61640 |       0.00128 |       0.09270\n",
      "      0.00124 |       0.00000 |      32.49252 |       0.00129 |       0.09289\n",
      "      0.00168 |       0.00000 |      32.45595 |       0.00116 |       0.09285\n",
      "      0.00139 |       0.00000 |      32.52408 |       0.00138 |       0.09273\n",
      "      0.00190 |       0.00000 |      32.50196 |       0.00136 |       0.09274\n",
      "      0.00197 |       0.00000 |      32.37679 |       0.00145 |       0.09290\n",
      "Evaluating losses...\n",
      "      0.00175 |       0.00000 |      32.44432 |       0.00120 |       0.09235\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.2         |\n",
      "| EpRewMean       | -86.2        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 8021         |\n",
      "| TimeElapsed     | 2.4e+03      |\n",
      "| TimestepsSoFar  | 770048       |\n",
      "| ev_tdlam_before | 0.867        |\n",
      "| loss_ent        | 0.09235399   |\n",
      "| loss_kl         | 0.0011960705 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017493058 |\n",
      "| loss_vf_loss    | 32.444324    |\n",
      "----------------------------------\n",
      "********** Iteration 188 ************\n",
      "Eval num_timesteps=770048, episode_reward=-82.40 +/- 20.78\n",
      "Episode length: 83.40 +/- 20.78\n",
      "Eval num_timesteps=770048, episode_reward=-74.20 +/- 12.68\n",
      "Episode length: 75.20 +/- 12.68\n",
      "Eval num_timesteps=770048, episode_reward=-74.60 +/- 7.03\n",
      "Episode length: 75.60 +/- 7.03\n",
      "Eval num_timesteps=770048, episode_reward=-82.30 +/- 14.84\n",
      "Episode length: 83.30 +/- 14.84\n",
      "Eval num_timesteps=770048, episode_reward=-73.80 +/- 8.12\n",
      "Episode length: 74.80 +/- 8.12\n",
      "Eval num_timesteps=770048, episode_reward=-72.50 +/- 9.01\n",
      "Episode length: 73.50 +/- 9.01\n",
      "Eval num_timesteps=770048, episode_reward=-84.50 +/- 21.91\n",
      "Episode length: 85.50 +/- 21.91\n",
      "Eval num_timesteps=770048, episode_reward=-76.40 +/- 12.81\n",
      "Episode length: 77.40 +/- 12.81\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00156 |       0.00000 |      34.08418 |       0.00111 |       0.09181\n",
      "      0.00229 |       0.00000 |      34.01838 |       0.00137 |       0.09267\n",
      "      0.00163 |       0.00000 |      33.90936 |       0.00121 |       0.09277\n",
      "      0.00177 |       0.00000 |      33.98179 |       0.00130 |       0.09199\n",
      "      0.00157 |       0.00000 |      33.82642 |       0.00118 |       0.09245\n",
      "      0.00165 |       0.00000 |      33.84003 |       0.00126 |       0.09245\n",
      "      0.00147 |       0.00000 |      33.75725 |       0.00120 |       0.09280\n",
      "      0.00197 |       0.00000 |      33.78236 |       0.00124 |       0.09262\n",
      "      0.00143 |       0.00000 |      33.73422 |       0.00146 |       0.09203\n",
      "      0.00193 |       0.00000 |      33.75412 |       0.00132 |       0.09253\n",
      "Evaluating losses...\n",
      "      0.00161 |       0.00000 |      33.65573 |       0.00114 |       0.09292\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.2         |\n",
      "| EpRewMean       | -85.2        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 8068         |\n",
      "| TimeElapsed     | 2.41e+03     |\n",
      "| TimestepsSoFar  | 774144       |\n",
      "| ev_tdlam_before | 0.86         |\n",
      "| loss_ent        | 0.09292113   |\n",
      "| loss_kl         | 0.0011425867 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0016145257 |\n",
      "| loss_vf_loss    | 33.65573     |\n",
      "----------------------------------\n",
      "********** Iteration 189 ************\n",
      "Eval num_timesteps=774144, episode_reward=-70.90 +/- 9.26\n",
      "Episode length: 71.90 +/- 9.26\n",
      "Eval num_timesteps=774144, episode_reward=-84.50 +/- 26.58\n",
      "Episode length: 85.50 +/- 26.58\n",
      "Eval num_timesteps=774144, episode_reward=-75.00 +/- 7.13\n",
      "Episode length: 76.00 +/- 7.13\n",
      "Eval num_timesteps=774144, episode_reward=-85.50 +/- 40.87\n",
      "Episode length: 86.50 +/- 40.87\n",
      "Eval num_timesteps=774144, episode_reward=-80.80 +/- 11.93\n",
      "Episode length: 81.80 +/- 11.93\n",
      "Eval num_timesteps=774144, episode_reward=-68.80 +/- 6.10\n",
      "Episode length: 69.80 +/- 6.10\n",
      "Eval num_timesteps=774144, episode_reward=-76.10 +/- 18.21\n",
      "Episode length: 77.10 +/- 18.21\n",
      "Eval num_timesteps=774144, episode_reward=-82.10 +/- 18.45\n",
      "Episode length: 83.10 +/- 18.45\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00237 |       0.00000 |      47.88877 |       0.00121 |       0.08785\n",
      "      0.00222 |       0.00000 |      47.13634 |       0.00124 |       0.08769\n",
      "      0.00190 |       0.00000 |      46.62309 |       0.00119 |       0.08761\n",
      "      0.00142 |       0.00000 |      46.30083 |       0.00106 |       0.08754\n",
      "      0.00186 |       0.00000 |      46.06700 |       0.00107 |       0.08758\n",
      "      0.00121 |       0.00000 |      45.57636 |       0.00126 |       0.08776\n",
      "      0.00093 |       0.00000 |      45.53518 |       0.00130 |       0.08774\n",
      "      0.00095 |       0.00000 |      45.17838 |       0.00111 |       0.08741\n",
      "      0.00126 |       0.00000 |      44.91190 |       0.00113 |       0.08825\n",
      "      0.00268 |       0.00000 |      44.97611 |       0.00123 |       0.08722\n",
      "Evaluating losses...\n",
      "      0.00131 |       0.00000 |      44.80350 |       0.00126 |       0.08826\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 8112         |\n",
      "| TimeElapsed     | 2.42e+03     |\n",
      "| TimestepsSoFar  | 778240       |\n",
      "| ev_tdlam_before | 0.797        |\n",
      "| loss_ent        | 0.0882593    |\n",
      "| loss_kl         | 0.0012612115 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0013108435 |\n",
      "| loss_vf_loss    | 44.803497    |\n",
      "----------------------------------\n",
      "********** Iteration 190 ************\n",
      "Eval num_timesteps=778240, episode_reward=-74.00 +/- 13.31\n",
      "Episode length: 75.00 +/- 13.31\n",
      "Eval num_timesteps=778240, episode_reward=-72.90 +/- 5.70\n",
      "Episode length: 73.90 +/- 5.70\n",
      "Eval num_timesteps=778240, episode_reward=-74.40 +/- 9.01\n",
      "Episode length: 75.40 +/- 9.01\n",
      "Eval num_timesteps=778240, episode_reward=-80.80 +/- 16.82\n",
      "Episode length: 81.80 +/- 16.82\n",
      "Eval num_timesteps=778240, episode_reward=-75.80 +/- 6.71\n",
      "Episode length: 76.80 +/- 6.71\n",
      "Eval num_timesteps=778240, episode_reward=-84.10 +/- 24.15\n",
      "Episode length: 85.10 +/- 24.15\n",
      "Eval num_timesteps=778240, episode_reward=-112.40 +/- 86.73\n",
      "Episode length: 113.40 +/- 86.73\n",
      "Eval num_timesteps=778240, episode_reward=-82.40 +/- 13.12\n",
      "Episode length: 83.40 +/- 13.12\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00300 |       0.00000 |      31.87110 |       0.00118 |       0.09395\n",
      "      0.00213 |       0.00000 |      31.76890 |       0.00116 |       0.09540\n",
      "      0.00195 |       0.00000 |      31.72045 |       0.00118 |       0.09502\n",
      "      0.00245 |       0.00000 |      31.51069 |       0.00127 |       0.09623\n",
      "      0.00141 |       0.00000 |      31.43488 |       0.00126 |       0.09621\n",
      "      0.00152 |       0.00000 |      31.37723 |       0.00128 |       0.09629\n",
      "      0.00110 |       0.00000 |      31.17220 |       0.00130 |       0.09637\n",
      "      0.00171 |       0.00000 |      31.10354 |       0.00125 |       0.09643\n",
      "      0.00305 |       0.00000 |      31.12465 |       0.00127 |       0.09654\n",
      "      0.00270 |       0.00000 |      31.00474 |       0.00129 |       0.09637\n",
      "Evaluating losses...\n",
      "      0.00169 |       0.00000 |      30.94302 |       0.00130 |       0.09649\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.1         |\n",
      "| EpRewMean       | -90.1        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 8158         |\n",
      "| TimeElapsed     | 2.43e+03     |\n",
      "| TimestepsSoFar  | 782336       |\n",
      "| ev_tdlam_before | 0.868        |\n",
      "| loss_ent        | 0.09649335   |\n",
      "| loss_kl         | 0.0012984065 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0016926508 |\n",
      "| loss_vf_loss    | 30.943022    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 191 ************\n",
      "Eval num_timesteps=782336, episode_reward=-73.30 +/- 10.80\n",
      "Episode length: 74.30 +/- 10.80\n",
      "Eval num_timesteps=782336, episode_reward=-91.10 +/- 37.70\n",
      "Episode length: 92.10 +/- 37.70\n",
      "Eval num_timesteps=782336, episode_reward=-79.20 +/- 10.70\n",
      "Episode length: 80.20 +/- 10.70\n",
      "Eval num_timesteps=782336, episode_reward=-90.30 +/- 37.52\n",
      "Episode length: 91.30 +/- 37.52\n",
      "Eval num_timesteps=782336, episode_reward=-71.30 +/- 7.21\n",
      "Episode length: 72.30 +/- 7.21\n",
      "Eval num_timesteps=782336, episode_reward=-84.00 +/- 18.73\n",
      "Episode length: 85.00 +/- 18.73\n",
      "Eval num_timesteps=782336, episode_reward=-72.40 +/- 6.76\n",
      "Episode length: 73.40 +/- 6.76\n",
      "Eval num_timesteps=782336, episode_reward=-78.80 +/- 14.77\n",
      "Episode length: 79.80 +/- 14.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00186 |       0.00000 |      48.36115 |       0.00115 |       0.09056\n",
      "      0.00182 |       0.00000 |      47.98861 |       0.00130 |       0.09071\n",
      "      0.00082 |       0.00000 |      47.85513 |       0.00131 |       0.09042\n",
      "      0.00123 |       0.00000 |      47.91677 |       0.00114 |       0.09122\n",
      "      0.00097 |       0.00000 |      47.41387 |       0.00120 |       0.09113\n",
      "      0.00070 |       0.00000 |      47.50362 |       0.00117 |       0.09169\n",
      "      0.00165 |       0.00000 |      47.33371 |       0.00137 |       0.09113\n",
      "      0.00118 |       0.00000 |      47.27086 |       0.00134 |       0.09079\n",
      "      0.00197 |       0.00000 |      47.05902 |       0.00142 |       0.09123\n",
      "      0.00055 |       0.00000 |      47.03959 |       0.00117 |       0.09112\n",
      "Evaluating losses...\n",
      "      0.00174 |       0.00000 |      46.85859 |       0.00155 |       0.09077\n",
      "----------------------------------\n",
      "| EpLenMean       | 94.5         |\n",
      "| EpRewMean       | -93.5        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 8200         |\n",
      "| TimeElapsed     | 2.44e+03     |\n",
      "| TimestepsSoFar  | 786432       |\n",
      "| ev_tdlam_before | 0.791        |\n",
      "| loss_ent        | 0.090771675  |\n",
      "| loss_kl         | 0.0015537009 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017384596 |\n",
      "| loss_vf_loss    | 46.858585    |\n",
      "----------------------------------\n",
      "********** Iteration 192 ************\n",
      "Eval num_timesteps=786432, episode_reward=-82.10 +/- 31.44\n",
      "Episode length: 83.10 +/- 31.44\n",
      "Eval num_timesteps=786432, episode_reward=-72.20 +/- 5.78\n",
      "Episode length: 73.20 +/- 5.78\n",
      "Eval num_timesteps=786432, episode_reward=-79.50 +/- 11.65\n",
      "Episode length: 80.50 +/- 11.65\n",
      "Eval num_timesteps=786432, episode_reward=-72.90 +/- 7.98\n",
      "Episode length: 73.90 +/- 7.98\n",
      "Eval num_timesteps=786432, episode_reward=-74.50 +/- 11.36\n",
      "Episode length: 75.50 +/- 11.36\n",
      "Eval num_timesteps=786432, episode_reward=-87.30 +/- 30.75\n",
      "Episode length: 88.30 +/- 30.75\n",
      "Eval num_timesteps=786432, episode_reward=-84.50 +/- 30.86\n",
      "Episode length: 85.50 +/- 30.86\n",
      "Eval num_timesteps=786432, episode_reward=-88.70 +/- 48.89\n",
      "Episode length: 89.70 +/- 48.89\n",
      "Eval num_timesteps=786432, episode_reward=-71.80 +/- 5.71\n",
      "Episode length: 72.80 +/- 5.71\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00210 |       0.00000 |      47.93362 |       0.00114 |       0.09396\n",
      "      0.00123 |       0.00000 |      47.39244 |       0.00112 |       0.09416\n",
      "      0.00148 |       0.00000 |      47.08098 |       0.00108 |       0.09420\n",
      "      0.00094 |       0.00000 |      47.28832 |       0.00123 |       0.09386\n",
      "      0.00158 |       0.00000 |      46.97797 |       0.00114 |       0.09429\n",
      "      0.00107 |       0.00000 |      46.75970 |       0.00121 |       0.09474\n",
      "      0.00078 |       0.00000 |      46.59949 |       0.00111 |       0.09459\n",
      "      0.00144 |       0.00000 |      46.70093 |       0.00115 |       0.09402\n",
      "      0.00142 |       0.00000 |      46.66012 |       0.00131 |       0.09448\n",
      "      0.00054 |       0.00000 |      46.27464 |       0.00119 |       0.09440\n",
      "Evaluating losses...\n",
      "      0.00114 |       0.00000 |      46.34465 |       0.00127 |       0.09491\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.3         |\n",
      "| EpRewMean       | -90.3        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 8246         |\n",
      "| TimeElapsed     | 2.45e+03     |\n",
      "| TimestepsSoFar  | 790528       |\n",
      "| ev_tdlam_before | 0.805        |\n",
      "| loss_ent        | 0.09491146   |\n",
      "| loss_kl         | 0.00127029   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0011351705 |\n",
      "| loss_vf_loss    | 46.344646    |\n",
      "----------------------------------\n",
      "********** Iteration 193 ************\n",
      "Eval num_timesteps=790528, episode_reward=-72.90 +/- 5.49\n",
      "Episode length: 73.90 +/- 5.49\n",
      "Eval num_timesteps=790528, episode_reward=-73.00 +/- 10.62\n",
      "Episode length: 74.00 +/- 10.62\n",
      "Eval num_timesteps=790528, episode_reward=-78.20 +/- 11.63\n",
      "Episode length: 79.20 +/- 11.63\n",
      "Eval num_timesteps=790528, episode_reward=-69.40 +/- 4.96\n",
      "Episode length: 70.40 +/- 4.96\n",
      "Eval num_timesteps=790528, episode_reward=-93.40 +/- 41.89\n",
      "Episode length: 94.40 +/- 41.89\n",
      "Eval num_timesteps=790528, episode_reward=-72.30 +/- 8.37\n",
      "Episode length: 73.30 +/- 8.37\n",
      "Eval num_timesteps=790528, episode_reward=-78.90 +/- 9.85\n",
      "Episode length: 79.90 +/- 9.85\n",
      "Eval num_timesteps=790528, episode_reward=-84.50 +/- 26.82\n",
      "Episode length: 85.50 +/- 26.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00348 |       0.00000 |      49.00271 |       0.00131 |       0.10161\n",
      "      0.00476 |       0.00000 |      48.88310 |       0.00139 |       0.10056\n",
      "      0.00240 |       0.00000 |      48.77674 |       0.00134 |       0.10090\n",
      "      0.00120 |       0.00000 |      48.49869 |       0.00139 |       0.10127\n",
      "      0.00379 |       0.00000 |      48.40044 |       0.00137 |       0.10073\n",
      "      0.00208 |       0.00000 |      48.37473 |       0.00126 |       0.10024\n",
      "      0.00308 |       0.00000 |      48.17971 |       0.00141 |       0.10031\n",
      "      0.00294 |       0.00000 |      48.17946 |       0.00138 |       0.10019\n",
      "      0.00072 |       0.00000 |      48.31811 |       0.00134 |       0.10032\n",
      "      0.00133 |       0.00000 |      48.24778 |       0.00140 |       0.09976\n",
      "Evaluating losses...\n",
      "      0.00192 |       0.00000 |      48.16251 |       0.00148 |       0.10031\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.3         |\n",
      "| EpRewMean       | -92.3        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 8290         |\n",
      "| TimeElapsed     | 2.46e+03     |\n",
      "| TimestepsSoFar  | 794624       |\n",
      "| ev_tdlam_before | 0.795        |\n",
      "| loss_ent        | 0.1003144    |\n",
      "| loss_kl         | 0.0014797164 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019194235 |\n",
      "| loss_vf_loss    | 48.162506    |\n",
      "----------------------------------\n",
      "********** Iteration 194 ************\n",
      "Eval num_timesteps=794624, episode_reward=-76.90 +/- 11.63\n",
      "Episode length: 77.90 +/- 11.63\n",
      "Eval num_timesteps=794624, episode_reward=-73.90 +/- 6.38\n",
      "Episode length: 74.90 +/- 6.38\n",
      "Eval num_timesteps=794624, episode_reward=-83.00 +/- 23.49\n",
      "Episode length: 84.00 +/- 23.49\n",
      "Eval num_timesteps=794624, episode_reward=-73.90 +/- 11.26\n",
      "Episode length: 74.90 +/- 11.26\n",
      "Eval num_timesteps=794624, episode_reward=-75.20 +/- 7.47\n",
      "Episode length: 76.20 +/- 7.47\n",
      "Eval num_timesteps=794624, episode_reward=-90.00 +/- 37.84\n",
      "Episode length: 91.00 +/- 37.84\n",
      "Eval num_timesteps=794624, episode_reward=-71.50 +/- 4.59\n",
      "Episode length: 72.50 +/- 4.59\n",
      "Eval num_timesteps=794624, episode_reward=-74.70 +/- 9.63\n",
      "Episode length: 75.70 +/- 9.63\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00126 |       0.00000 |      26.76218 |       0.00110 |       0.09979\n",
      "      0.00165 |       0.00000 |      25.83079 |       0.00125 |       0.09980\n",
      "      0.00137 |       0.00000 |      25.32142 |       0.00110 |       0.09982\n",
      "      0.00250 |       0.00000 |      24.94688 |       0.00105 |       0.09991\n",
      "      0.00253 |       0.00000 |      24.72772 |       0.00111 |       0.10015\n",
      "      0.00220 |       0.00000 |      24.39832 |       0.00105 |       0.09976\n",
      "      0.00252 |       0.00000 |      24.28430 |       0.00112 |       0.09985\n",
      "      0.00170 |       0.00000 |      24.02349 |       0.00122 |       0.09953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00187 |       0.00000 |      23.94154 |       0.00105 |       0.09975\n",
      "      0.00165 |       0.00000 |      24.00033 |       0.00130 |       0.09968\n",
      "Evaluating losses...\n",
      "      0.00217 |       0.00000 |      23.90842 |       0.00125 |       0.10026\n",
      "----------------------------------\n",
      "| EpLenMean       | 85.6         |\n",
      "| EpRewMean       | -84.6        |\n",
      "| EpThisIter      | 51           |\n",
      "| EpisodesSoFar   | 8341         |\n",
      "| TimeElapsed     | 2.47e+03     |\n",
      "| TimestepsSoFar  | 798720       |\n",
      "| ev_tdlam_before | 0.907        |\n",
      "| loss_ent        | 0.10025588   |\n",
      "| loss_kl         | 0.0012520568 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0021656216 |\n",
      "| loss_vf_loss    | 23.908417    |\n",
      "----------------------------------\n",
      "********** Iteration 195 ************\n",
      "Eval num_timesteps=798720, episode_reward=-91.50 +/- 38.56\n",
      "Episode length: 92.50 +/- 38.56\n",
      "Eval num_timesteps=798720, episode_reward=-86.30 +/- 38.98\n",
      "Episode length: 87.30 +/- 38.98\n",
      "Eval num_timesteps=798720, episode_reward=-78.20 +/- 20.33\n",
      "Episode length: 79.20 +/- 20.33\n",
      "Eval num_timesteps=798720, episode_reward=-74.40 +/- 17.96\n",
      "Episode length: 75.40 +/- 17.96\n",
      "Eval num_timesteps=798720, episode_reward=-76.80 +/- 14.03\n",
      "Episode length: 77.80 +/- 14.03\n",
      "Eval num_timesteps=798720, episode_reward=-71.20 +/- 9.17\n",
      "Episode length: 72.20 +/- 9.17\n",
      "Eval num_timesteps=798720, episode_reward=-81.50 +/- 15.38\n",
      "Episode length: 82.50 +/- 15.38\n",
      "Eval num_timesteps=798720, episode_reward=-78.20 +/- 8.75\n",
      "Episode length: 79.20 +/- 8.75\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00143 |       0.00000 |      43.83653 |       0.00109 |       0.09187\n",
      "      0.00114 |       0.00000 |      41.91719 |       0.00114 |       0.09173\n",
      "      0.00166 |       0.00000 |      41.01770 |       0.00108 |       0.09191\n",
      "      0.00165 |       0.00000 |      40.05971 |       0.00116 |       0.09203\n",
      "      0.00111 |       0.00000 |      39.41761 |       0.00106 |       0.09208\n",
      "      0.00095 |       0.00000 |      38.78720 |       0.00110 |       0.09182\n",
      "      0.00184 |       0.00000 |      38.34614 |       0.00112 |       0.09151\n",
      "      0.00117 |       0.00000 |      37.75651 |       0.00110 |       0.09181\n",
      "      0.00181 |       0.00000 |      37.38287 |       0.00108 |       0.09236\n",
      "      0.00236 |       0.00000 |      37.05908 |       0.00112 |       0.09219\n",
      "Evaluating losses...\n",
      "      0.00177 |       0.00000 |      37.00319 |       0.00116 |       0.09198\n",
      "----------------------------------\n",
      "| EpLenMean       | 87           |\n",
      "| EpRewMean       | -86          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 8385         |\n",
      "| TimeElapsed     | 2.48e+03     |\n",
      "| TimestepsSoFar  | 802816       |\n",
      "| ev_tdlam_before | 0.81         |\n",
      "| loss_ent        | 0.09197801   |\n",
      "| loss_kl         | 0.001157633  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017746101 |\n",
      "| loss_vf_loss    | 37.00319     |\n",
      "----------------------------------\n",
      "********** Iteration 196 ************\n",
      "Eval num_timesteps=802816, episode_reward=-79.80 +/- 20.63\n",
      "Episode length: 80.80 +/- 20.63\n",
      "Eval num_timesteps=802816, episode_reward=-73.20 +/- 8.32\n",
      "Episode length: 74.20 +/- 8.32\n",
      "Eval num_timesteps=802816, episode_reward=-82.90 +/- 15.31\n",
      "Episode length: 83.90 +/- 15.31\n",
      "Eval num_timesteps=802816, episode_reward=-70.50 +/- 5.82\n",
      "Episode length: 71.50 +/- 5.82\n",
      "Eval num_timesteps=802816, episode_reward=-74.90 +/- 6.89\n",
      "Episode length: 75.90 +/- 6.89\n",
      "Eval num_timesteps=802816, episode_reward=-81.20 +/- 22.01\n",
      "Episode length: 82.20 +/- 22.01\n",
      "Eval num_timesteps=802816, episode_reward=-76.30 +/- 14.75\n",
      "Episode length: 77.30 +/- 14.75\n",
      "Eval num_timesteps=802816, episode_reward=-70.40 +/- 5.87\n",
      "Episode length: 71.40 +/- 5.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00160 |       0.00000 |      36.51619 |       0.00119 |       0.10146\n",
      "      0.00218 |       0.00000 |      36.25016 |       0.00128 |       0.10009\n",
      "      0.00263 |       0.00000 |      36.25368 |       0.00125 |       0.10067\n",
      "      0.00229 |       0.00000 |      36.11790 |       0.00135 |       0.10066\n",
      "      0.00134 |       0.00000 |      35.99322 |       0.00141 |       0.10042\n",
      "      0.00198 |       0.00000 |      36.00600 |       0.00116 |       0.10025\n",
      "      0.00175 |       0.00000 |      35.87017 |       0.00114 |       0.10026\n",
      "      0.00272 |       0.00000 |      36.02321 |       0.00114 |       0.10023\n",
      "      0.00132 |       0.00000 |      35.83789 |       0.00119 |       0.09986\n",
      "      0.00232 |       0.00000 |      35.85302 |       0.00125 |       0.10009\n",
      "Evaluating losses...\n",
      "      0.00174 |       0.00000 |      35.64632 |       0.00118 |       0.09937\n",
      "----------------------------------\n",
      "| EpLenMean       | 89.1         |\n",
      "| EpRewMean       | -88.1        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 8433         |\n",
      "| TimeElapsed     | 2.49e+03     |\n",
      "| TimestepsSoFar  | 806912       |\n",
      "| ev_tdlam_before | 0.858        |\n",
      "| loss_ent        | 0.099374406  |\n",
      "| loss_kl         | 0.0011802645 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017361479 |\n",
      "| loss_vf_loss    | 35.646324    |\n",
      "----------------------------------\n",
      "********** Iteration 197 ************\n",
      "Eval num_timesteps=806912, episode_reward=-70.90 +/- 6.41\n",
      "Episode length: 71.90 +/- 6.41\n",
      "Eval num_timesteps=806912, episode_reward=-78.00 +/- 8.38\n",
      "Episode length: 79.00 +/- 8.38\n",
      "Eval num_timesteps=806912, episode_reward=-74.70 +/- 8.40\n",
      "Episode length: 75.70 +/- 8.40\n",
      "Eval num_timesteps=806912, episode_reward=-72.80 +/- 10.42\n",
      "Episode length: 73.80 +/- 10.42\n",
      "Eval num_timesteps=806912, episode_reward=-85.50 +/- 29.28\n",
      "Episode length: 86.50 +/- 29.28\n",
      "Eval num_timesteps=806912, episode_reward=-71.70 +/- 6.03\n",
      "Episode length: 72.70 +/- 6.03\n",
      "Eval num_timesteps=806912, episode_reward=-74.20 +/- 9.01\n",
      "Episode length: 75.20 +/- 9.01\n",
      "Eval num_timesteps=806912, episode_reward=-74.70 +/- 6.91\n",
      "Episode length: 75.70 +/- 6.91\n",
      "Eval num_timesteps=806912, episode_reward=-84.90 +/- 27.74\n",
      "Episode length: 85.90 +/- 27.74\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00187 |       0.00000 |      39.91784 |       0.00118 |       0.09554\n",
      "      0.00170 |       0.00000 |      39.50393 |       0.00137 |       0.09538\n",
      "      0.00149 |       0.00000 |      39.29792 |       0.00134 |       0.09549\n",
      "      0.00166 |       0.00000 |      38.91879 |       0.00115 |       0.09511\n",
      "      0.00281 |       0.00000 |      38.74809 |       0.00136 |       0.09547\n",
      "      0.00132 |       0.00000 |      38.56694 |       0.00126 |       0.09522\n",
      "      0.00094 |       0.00000 |      38.36412 |       0.00129 |       0.09518\n",
      "      0.00127 |       0.00000 |      38.13005 |       0.00117 |       0.09482\n",
      "      0.00109 |       0.00000 |      38.10471 |       0.00112 |       0.09493\n",
      "      0.00211 |       0.00000 |      37.92110 |       0.00126 |       0.09494\n",
      "Evaluating losses...\n",
      "      0.00120 |       0.00000 |      37.78343 |       0.00137 |       0.09424\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.3         |\n",
      "| EpRewMean       | -83.3        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 8480         |\n",
      "| TimeElapsed     | 2.5e+03      |\n",
      "| TimestepsSoFar  | 811008       |\n",
      "| ev_tdlam_before | 0.836        |\n",
      "| loss_ent        | 0.0942395    |\n",
      "| loss_kl         | 0.001368312  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0011999568 |\n",
      "| loss_vf_loss    | 37.78343     |\n",
      "----------------------------------\n",
      "********** Iteration 198 ************\n",
      "Eval num_timesteps=811008, episode_reward=-78.40 +/- 8.75\n",
      "Episode length: 79.40 +/- 8.75\n",
      "Eval num_timesteps=811008, episode_reward=-92.20 +/- 36.32\n",
      "Episode length: 93.20 +/- 36.32\n",
      "Eval num_timesteps=811008, episode_reward=-98.60 +/- 59.61\n",
      "Episode length: 99.60 +/- 59.61\n",
      "Eval num_timesteps=811008, episode_reward=-74.60 +/- 6.90\n",
      "Episode length: 75.60 +/- 6.90\n",
      "Eval num_timesteps=811008, episode_reward=-89.40 +/- 39.93\n",
      "Episode length: 90.40 +/- 39.93\n",
      "Eval num_timesteps=811008, episode_reward=-75.20 +/- 9.95\n",
      "Episode length: 76.20 +/- 9.95\n",
      "Eval num_timesteps=811008, episode_reward=-80.70 +/- 27.64\n",
      "Episode length: 81.70 +/- 27.64\n",
      "Eval num_timesteps=811008, episode_reward=-82.50 +/- 18.58\n",
      "Episode length: 83.50 +/- 18.58\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00169 |       0.00000 |      33.42138 |       0.00108 |       0.09567\n",
      "      0.00118 |       0.00000 |      33.19536 |       0.00111 |       0.09625\n",
      "      0.00090 |       0.00000 |      33.22166 |       0.00118 |       0.09562\n",
      "      0.00164 |       0.00000 |      33.22246 |       0.00117 |       0.09613\n",
      "      0.00150 |       0.00000 |      33.17121 |       0.00123 |       0.09583\n",
      "      0.00128 |       0.00000 |      32.96945 |       0.00125 |       0.09571\n",
      "      0.00180 |       0.00000 |      33.02539 |       0.00128 |       0.09575\n",
      "      0.00185 |       0.00000 |      32.99551 |       0.00131 |       0.09563\n",
      "      0.00151 |       0.00000 |      33.01087 |       0.00124 |       0.09600\n",
      "      0.00219 |       0.00000 |      32.84345 |       0.00140 |       0.09550\n",
      "Evaluating losses...\n",
      "      0.00157 |       0.00000 |      32.85413 |       0.00139 |       0.09587\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.7         |\n",
      "| EpRewMean       | -85.7        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 8527         |\n",
      "| TimeElapsed     | 2.51e+03     |\n",
      "| TimestepsSoFar  | 815104       |\n",
      "| ev_tdlam_before | 0.866        |\n",
      "| loss_ent        | 0.0958715    |\n",
      "| loss_kl         | 0.0013869646 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0015723414 |\n",
      "| loss_vf_loss    | 32.854126    |\n",
      "----------------------------------\n",
      "********** Iteration 199 ************\n",
      "Eval num_timesteps=815104, episode_reward=-76.50 +/- 11.15\n",
      "Episode length: 77.50 +/- 11.15\n",
      "Eval num_timesteps=815104, episode_reward=-84.90 +/- 21.49\n",
      "Episode length: 85.90 +/- 21.49\n",
      "Eval num_timesteps=815104, episode_reward=-87.50 +/- 29.00\n",
      "Episode length: 88.50 +/- 29.00\n",
      "Eval num_timesteps=815104, episode_reward=-74.50 +/- 6.41\n",
      "Episode length: 75.50 +/- 6.41\n",
      "Eval num_timesteps=815104, episode_reward=-72.80 +/- 4.94\n",
      "Episode length: 73.80 +/- 4.94\n",
      "Eval num_timesteps=815104, episode_reward=-74.90 +/- 7.46\n",
      "Episode length: 75.90 +/- 7.46\n",
      "Eval num_timesteps=815104, episode_reward=-75.60 +/- 10.05\n",
      "Episode length: 76.60 +/- 10.05\n",
      "Eval num_timesteps=815104, episode_reward=-77.10 +/- 11.44\n",
      "Episode length: 78.10 +/- 11.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00170 |       0.00000 |      36.23609 |       0.00124 |       0.09477\n",
      "      0.00338 |       0.00000 |      36.04065 |       0.00125 |       0.09490\n",
      "      0.00222 |       0.00000 |      35.79616 |       0.00117 |       0.09546\n",
      "      0.00164 |       0.00000 |      35.46462 |       0.00124 |       0.09543\n",
      "      0.00202 |       0.00000 |      35.61288 |       0.00115 |       0.09520\n",
      "      0.00214 |       0.00000 |      35.47694 |       0.00121 |       0.09560\n",
      "      0.00193 |       0.00000 |      35.38832 |       0.00114 |       0.09539\n",
      "      0.00244 |       0.00000 |      35.34805 |       0.00124 |       0.09591\n",
      "      0.00228 |       0.00000 |      35.25420 |       0.00131 |       0.09480\n",
      "      0.00167 |       0.00000 |      35.00526 |       0.00134 |       0.09580\n",
      "Evaluating losses...\n",
      "      0.00207 |       0.00000 |      35.01980 |       0.00138 |       0.09546\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 8572         |\n",
      "| TimeElapsed     | 2.52e+03     |\n",
      "| TimestepsSoFar  | 819200       |\n",
      "| ev_tdlam_before | 0.85         |\n",
      "| loss_ent        | 0.09545725   |\n",
      "| loss_kl         | 0.0013835964 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0020726575 |\n",
      "| loss_vf_loss    | 35.0198      |\n",
      "----------------------------------\n",
      "********** Iteration 200 ************\n",
      "Eval num_timesteps=819200, episode_reward=-75.40 +/- 15.45\n",
      "Episode length: 76.40 +/- 15.45\n",
      "Eval num_timesteps=819200, episode_reward=-77.80 +/- 8.70\n",
      "Episode length: 78.80 +/- 8.70\n",
      "Eval num_timesteps=819200, episode_reward=-75.30 +/- 7.17\n",
      "Episode length: 76.30 +/- 7.17\n",
      "Eval num_timesteps=819200, episode_reward=-76.20 +/- 14.60\n",
      "Episode length: 77.20 +/- 14.60\n",
      "Eval num_timesteps=819200, episode_reward=-79.00 +/- 25.59\n",
      "Episode length: 80.00 +/- 25.59\n",
      "Eval num_timesteps=819200, episode_reward=-81.00 +/- 17.08\n",
      "Episode length: 82.00 +/- 17.08\n",
      "Eval num_timesteps=819200, episode_reward=-91.70 +/- 45.02\n",
      "Episode length: 92.70 +/- 45.02\n",
      "Eval num_timesteps=819200, episode_reward=-79.50 +/- 29.73\n",
      "Episode length: 80.50 +/- 29.73\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00193 |       0.00000 |      34.32257 |       0.00100 |       0.09356\n",
      "      0.00175 |       0.00000 |      34.17191 |       0.00099 |       0.09340\n",
      "      0.00158 |       0.00000 |      34.05989 |       0.00115 |       0.09344\n",
      "      0.00255 |       0.00000 |      33.92398 |       0.00119 |       0.09383\n",
      "      0.00101 |       0.00000 |      33.90170 |       0.00116 |       0.09446\n",
      "      0.00193 |       0.00000 |      33.74387 |       0.00123 |       0.09415\n",
      "      0.00118 |       0.00000 |      33.67595 |       0.00122 |       0.09393\n",
      "      0.00153 |       0.00000 |      33.65060 |       0.00137 |       0.09415\n",
      "      0.00162 |       0.00000 |      33.68724 |       0.00134 |       0.09435\n",
      "      0.00104 |       0.00000 |      33.49421 |       0.00129 |       0.09401\n",
      "Evaluating losses...\n",
      "      0.00093 |       0.00000 |      33.47931 |       0.00134 |       0.09439\n",
      "-----------------------------------\n",
      "| EpLenMean       | 90.6          |\n",
      "| EpRewMean       | -89.6         |\n",
      "| EpThisIter      | 46            |\n",
      "| EpisodesSoFar   | 8618          |\n",
      "| TimeElapsed     | 2.53e+03      |\n",
      "| TimestepsSoFar  | 823296        |\n",
      "| ev_tdlam_before | 0.857         |\n",
      "| loss_ent        | 0.0943898     |\n",
      "| loss_kl         | 0.0013401993  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.00092505594 |\n",
      "| loss_vf_loss    | 33.47931      |\n",
      "-----------------------------------\n",
      "********** Iteration 201 ************\n",
      "Eval num_timesteps=823296, episode_reward=-79.90 +/- 17.81\n",
      "Episode length: 80.90 +/- 17.81\n",
      "Eval num_timesteps=823296, episode_reward=-86.70 +/- 31.47\n",
      "Episode length: 87.70 +/- 31.47\n",
      "Eval num_timesteps=823296, episode_reward=-100.10 +/- 92.81\n",
      "Episode length: 101.10 +/- 92.81\n",
      "Eval num_timesteps=823296, episode_reward=-83.00 +/- 19.99\n",
      "Episode length: 84.00 +/- 19.99\n",
      "Eval num_timesteps=823296, episode_reward=-79.60 +/- 14.40\n",
      "Episode length: 80.60 +/- 14.40\n",
      "Eval num_timesteps=823296, episode_reward=-85.00 +/- 25.55\n",
      "Episode length: 86.00 +/- 25.55\n",
      "Eval num_timesteps=823296, episode_reward=-86.00 +/- 31.51\n",
      "Episode length: 87.00 +/- 31.51\n",
      "Eval num_timesteps=823296, episode_reward=-73.40 +/- 5.64\n",
      "Episode length: 74.40 +/- 5.64\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00260 |       0.00000 |      41.99676 |       0.00128 |       0.09031\n",
      "      0.00143 |       0.00000 |      41.47447 |       0.00110 |       0.09050\n",
      "      0.00176 |       0.00000 |      41.04736 |       0.00107 |       0.09004\n",
      "      0.00098 |       0.00000 |      40.85619 |       0.00108 |       0.09041\n",
      "      0.00110 |       0.00000 |      40.49007 |       0.00100 |       0.09046\n",
      "      0.00107 |       0.00000 |      40.24760 |       0.00104 |       0.09026\n",
      "      0.00162 |       0.00000 |      40.14719 |       0.00103 |       0.09030\n",
      "      0.00122 |       0.00000 |      39.88650 |       0.00111 |       0.09048\n",
      "      0.00125 |       0.00000 |      39.65972 |       0.00106 |       0.09050\n",
      "      0.00147 |       0.00000 |      39.58498 |       0.00118 |       0.09045\n",
      "Evaluating losses...\n",
      "      0.00069 |       0.00000 |      39.39018 |       0.00116 |       0.09062\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.3         |\n",
      "| EpRewMean       | -89.3        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 8664         |\n",
      "| TimeElapsed     | 2.54e+03     |\n",
      "| TimestepsSoFar  | 827392       |\n",
      "| ev_tdlam_before | 0.825        |\n",
      "| loss_ent        | 0.0906244    |\n",
      "| loss_kl         | 0.0011636877 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.000687866  |\n",
      "| loss_vf_loss    | 39.390182    |\n",
      "----------------------------------\n",
      "********** Iteration 202 ************\n",
      "Eval num_timesteps=827392, episode_reward=-78.10 +/- 15.06\n",
      "Episode length: 79.10 +/- 15.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=827392, episode_reward=-73.40 +/- 6.59\n",
      "Episode length: 74.40 +/- 6.59\n",
      "Eval num_timesteps=827392, episode_reward=-72.30 +/- 4.24\n",
      "Episode length: 73.30 +/- 4.24\n",
      "Eval num_timesteps=827392, episode_reward=-73.80 +/- 6.06\n",
      "Episode length: 74.80 +/- 6.06\n",
      "Eval num_timesteps=827392, episode_reward=-75.10 +/- 7.93\n",
      "Episode length: 76.10 +/- 7.93\n",
      "Eval num_timesteps=827392, episode_reward=-91.00 +/- 36.12\n",
      "Episode length: 92.00 +/- 36.12\n",
      "Eval num_timesteps=827392, episode_reward=-72.30 +/- 10.14\n",
      "Episode length: 73.30 +/- 10.14\n",
      "Eval num_timesteps=827392, episode_reward=-75.20 +/- 10.45\n",
      "Episode length: 76.20 +/- 10.45\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00229 |       0.00000 |      39.45067 |       0.00119 |       0.09732\n",
      "      0.00231 |       0.00000 |      39.01068 |       0.00105 |       0.09731\n",
      "      0.00144 |       0.00000 |      38.87870 |       0.00106 |       0.09658\n",
      "      0.00218 |       0.00000 |      38.85786 |       0.00123 |       0.09774\n",
      "      0.00177 |       0.00000 |      38.73349 |       0.00112 |       0.09803\n",
      "      0.00134 |       0.00000 |      38.51173 |       0.00121 |       0.09752\n",
      "      0.00200 |       0.00000 |      38.50765 |       0.00117 |       0.09796\n",
      "      0.00129 |       0.00000 |      38.40793 |       0.00108 |       0.09743\n",
      "      0.00203 |       0.00000 |      38.45052 |       0.00128 |       0.09811\n",
      "      0.00162 |       0.00000 |      38.37774 |       0.00108 |       0.09751\n",
      "Evaluating losses...\n",
      "      0.00156 |       0.00000 |      38.26941 |       0.00119 |       0.09710\n",
      "----------------------------------\n",
      "| EpLenMean       | 87.8         |\n",
      "| EpRewMean       | -86.8        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 8712         |\n",
      "| TimeElapsed     | 2.55e+03     |\n",
      "| TimestepsSoFar  | 831488       |\n",
      "| ev_tdlam_before | 0.843        |\n",
      "| loss_ent        | 0.097101875  |\n",
      "| loss_kl         | 0.0011885995 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0015610516 |\n",
      "| loss_vf_loss    | 38.269405    |\n",
      "----------------------------------\n",
      "********** Iteration 203 ************\n",
      "Eval num_timesteps=831488, episode_reward=-92.90 +/- 52.35\n",
      "Episode length: 93.90 +/- 52.35\n",
      "Eval num_timesteps=831488, episode_reward=-71.60 +/- 5.16\n",
      "Episode length: 72.60 +/- 5.16\n",
      "Eval num_timesteps=831488, episode_reward=-72.00 +/- 6.65\n",
      "Episode length: 73.00 +/- 6.65\n",
      "Eval num_timesteps=831488, episode_reward=-75.20 +/- 10.85\n",
      "Episode length: 76.20 +/- 10.85\n",
      "Eval num_timesteps=831488, episode_reward=-95.90 +/- 51.83\n",
      "Episode length: 96.90 +/- 51.83\n",
      "Eval num_timesteps=831488, episode_reward=-78.30 +/- 7.81\n",
      "Episode length: 79.30 +/- 7.81\n",
      "Eval num_timesteps=831488, episode_reward=-76.20 +/- 8.13\n",
      "Episode length: 77.20 +/- 8.13\n",
      "Eval num_timesteps=831488, episode_reward=-86.80 +/- 23.89\n",
      "Episode length: 87.80 +/- 23.89\n",
      "Eval num_timesteps=831488, episode_reward=-76.20 +/- 10.13\n",
      "Episode length: 77.20 +/- 10.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00228 |       0.00000 |      31.58637 |       0.00109 |       0.09500\n",
      "      0.00242 |       0.00000 |      31.40344 |       0.00112 |       0.09501\n",
      "      0.00290 |       0.00000 |      31.22733 |       0.00117 |       0.09529\n",
      "      0.00258 |       0.00000 |      31.16355 |       0.00131 |       0.09504\n",
      "      0.00145 |       0.00000 |      31.26315 |       0.00122 |       0.09566\n",
      "      0.00164 |       0.00000 |      31.16288 |       0.00122 |       0.09498\n",
      "      0.00188 |       0.00000 |      31.09898 |       0.00128 |       0.09505\n",
      "      0.00294 |       0.00000 |      31.02029 |       0.00136 |       0.09511\n",
      "      0.00155 |       0.00000 |      30.93514 |       0.00136 |       0.09508\n",
      "      0.00205 |       0.00000 |      31.07482 |       0.00140 |       0.09556\n",
      "Evaluating losses...\n",
      "      0.00177 |       0.00000 |      31.03858 |       0.00130 |       0.09528\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.2         |\n",
      "| EpRewMean       | -85.2        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 8760         |\n",
      "| TimeElapsed     | 2.56e+03     |\n",
      "| TimestepsSoFar  | 835584       |\n",
      "| ev_tdlam_before | 0.874        |\n",
      "| loss_ent        | 0.095281266  |\n",
      "| loss_kl         | 0.0013035913 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017653757 |\n",
      "| loss_vf_loss    | 31.038576    |\n",
      "----------------------------------\n",
      "********** Iteration 204 ************\n",
      "Eval num_timesteps=835584, episode_reward=-85.90 +/- 17.72\n",
      "Episode length: 86.90 +/- 17.72\n",
      "Eval num_timesteps=835584, episode_reward=-75.00 +/- 7.43\n",
      "Episode length: 76.00 +/- 7.43\n",
      "Eval num_timesteps=835584, episode_reward=-76.50 +/- 12.68\n",
      "Episode length: 77.50 +/- 12.68\n",
      "Eval num_timesteps=835584, episode_reward=-72.30 +/- 7.43\n",
      "Episode length: 73.30 +/- 7.43\n",
      "Eval num_timesteps=835584, episode_reward=-76.00 +/- 4.34\n",
      "Episode length: 77.00 +/- 4.34\n",
      "Eval num_timesteps=835584, episode_reward=-71.00 +/- 4.96\n",
      "Episode length: 72.00 +/- 4.96\n",
      "Eval num_timesteps=835584, episode_reward=-69.30 +/- 7.81\n",
      "Episode length: 70.30 +/- 7.81\n",
      "Eval num_timesteps=835584, episode_reward=-75.40 +/- 5.43\n",
      "Episode length: 76.40 +/- 5.43\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00177 |       0.00000 |      25.62845 |       0.00118 |       0.10053\n",
      "      0.00370 |       0.00000 |      25.43345 |       0.00122 |       0.10085\n",
      "      0.00226 |       0.00000 |      25.37866 |       0.00110 |       0.10054\n",
      "      0.00176 |       0.00000 |      25.34250 |       0.00110 |       0.10044\n",
      "      0.00222 |       0.00000 |      25.28605 |       0.00123 |       0.10081\n",
      "      0.00143 |       0.00000 |      25.11047 |       0.00119 |       0.10047\n",
      "      0.00183 |       0.00000 |      25.14283 |       0.00119 |       0.10101\n",
      "      0.00209 |       0.00000 |      25.20426 |       0.00131 |       0.10106\n",
      "      0.00184 |       0.00000 |      25.17798 |       0.00125 |       0.10126\n",
      "      0.00195 |       0.00000 |      25.05169 |       0.00127 |       0.10071\n",
      "Evaluating losses...\n",
      "      0.00259 |       0.00000 |      25.19015 |       0.00118 |       0.10072\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.9         |\n",
      "| EpRewMean       | -83.9        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 8809         |\n",
      "| TimeElapsed     | 2.57e+03     |\n",
      "| TimestepsSoFar  | 839680       |\n",
      "| ev_tdlam_before | 0.9          |\n",
      "| loss_ent        | 0.100719385  |\n",
      "| loss_kl         | 0.0011769799 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002585648  |\n",
      "| loss_vf_loss    | 25.190153    |\n",
      "----------------------------------\n",
      "********** Iteration 205 ************\n",
      "Eval num_timesteps=839680, episode_reward=-73.30 +/- 7.80\n",
      "Episode length: 74.30 +/- 7.80\n",
      "Eval num_timesteps=839680, episode_reward=-94.20 +/- 59.28\n",
      "Episode length: 95.20 +/- 59.28\n",
      "Eval num_timesteps=839680, episode_reward=-81.80 +/- 17.39\n",
      "Episode length: 82.80 +/- 17.39\n",
      "Eval num_timesteps=839680, episode_reward=-79.10 +/- 11.72\n",
      "Episode length: 80.10 +/- 11.72\n",
      "Eval num_timesteps=839680, episode_reward=-68.10 +/- 7.92\n",
      "Episode length: 69.10 +/- 7.92\n",
      "Eval num_timesteps=839680, episode_reward=-80.80 +/- 15.05\n",
      "Episode length: 81.80 +/- 15.05\n",
      "Eval num_timesteps=839680, episode_reward=-75.70 +/- 8.30\n",
      "Episode length: 76.70 +/- 8.30\n",
      "Eval num_timesteps=839680, episode_reward=-78.90 +/- 10.13\n",
      "Episode length: 79.90 +/- 10.13\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00208 |       0.00000 |      27.62736 |       0.00116 |       0.09696\n",
      "      0.00272 |       0.00000 |      27.60118 |       0.00108 |       0.09712\n",
      "      0.00320 |       0.00000 |      27.33779 |       0.00130 |       0.09720\n",
      "      0.00234 |       0.00000 |      27.32912 |       0.00133 |       0.09786\n",
      "      0.00144 |       0.00000 |      27.23068 |       0.00133 |       0.09733\n",
      "      0.00201 |       0.00000 |      27.16636 |       0.00128 |       0.09731\n",
      "      0.00254 |       0.00000 |      27.15473 |       0.00115 |       0.09799\n",
      "      0.00179 |       0.00000 |      27.21168 |       0.00130 |       0.09764\n",
      "      0.00234 |       0.00000 |      27.17880 |       0.00122 |       0.09846\n",
      "      0.00200 |       0.00000 |      27.02691 |       0.00138 |       0.09823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "      0.00195 |       0.00000 |      27.13220 |       0.00118 |       0.09868\n",
      "----------------------------------\n",
      "| EpLenMean       | 83.3         |\n",
      "| EpRewMean       | -82.3        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 8858         |\n",
      "| TimeElapsed     | 2.58e+03     |\n",
      "| TimestepsSoFar  | 843776       |\n",
      "| ev_tdlam_before | 0.89         |\n",
      "| loss_ent        | 0.09867745   |\n",
      "| loss_kl         | 0.0011769077 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019455523 |\n",
      "| loss_vf_loss    | 27.132204    |\n",
      "----------------------------------\n",
      "********** Iteration 206 ************\n",
      "Eval num_timesteps=843776, episode_reward=-74.10 +/- 9.71\n",
      "Episode length: 75.10 +/- 9.71\n",
      "Eval num_timesteps=843776, episode_reward=-76.80 +/- 9.09\n",
      "Episode length: 77.80 +/- 9.09\n",
      "Eval num_timesteps=843776, episode_reward=-79.10 +/- 10.92\n",
      "Episode length: 80.10 +/- 10.92\n",
      "Eval num_timesteps=843776, episode_reward=-74.30 +/- 7.25\n",
      "Episode length: 75.30 +/- 7.25\n",
      "Eval num_timesteps=843776, episode_reward=-79.50 +/- 16.46\n",
      "Episode length: 80.50 +/- 16.46\n",
      "Eval num_timesteps=843776, episode_reward=-75.50 +/- 8.32\n",
      "Episode length: 76.50 +/- 8.32\n",
      "Eval num_timesteps=843776, episode_reward=-78.50 +/- 13.02\n",
      "Episode length: 79.50 +/- 13.02\n",
      "Eval num_timesteps=843776, episode_reward=-93.60 +/- 36.66\n",
      "Episode length: 94.60 +/- 36.66\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00215 |       0.00000 |      33.22140 |       0.00114 |       0.09656\n",
      "      0.00236 |       0.00000 |      33.02535 |       0.00121 |       0.09659\n",
      "      0.00190 |       0.00000 |      33.17587 |       0.00120 |       0.09676\n",
      "      0.00250 |       0.00000 |      32.88966 |       0.00135 |       0.09633\n",
      "      0.00376 |       0.00000 |      33.04934 |       0.00124 |       0.09591\n",
      "      0.00259 |       0.00000 |      33.03039 |       0.00117 |       0.09618\n",
      "      0.00279 |       0.00000 |      33.04096 |       0.00123 |       0.09605\n",
      "      0.00272 |       0.00000 |      32.78260 |       0.00131 |       0.09527\n",
      "      0.00178 |       0.00000 |      32.95250 |       0.00126 |       0.09594\n",
      "      0.00245 |       0.00000 |      32.96602 |       0.00143 |       0.09576\n",
      "Evaluating losses...\n",
      "      0.00328 |       0.00000 |      32.79245 |       0.00133 |       0.09560\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.2         |\n",
      "| EpRewMean       | -83.2        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 8906         |\n",
      "| TimeElapsed     | 2.59e+03     |\n",
      "| TimestepsSoFar  | 847872       |\n",
      "| ev_tdlam_before | 0.864        |\n",
      "| loss_ent        | 0.09560393   |\n",
      "| loss_kl         | 0.0013252455 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.003284255  |\n",
      "| loss_vf_loss    | 32.792454    |\n",
      "----------------------------------\n",
      "********** Iteration 207 ************\n",
      "Eval num_timesteps=847872, episode_reward=-75.10 +/- 9.09\n",
      "Episode length: 76.10 +/- 9.09\n",
      "Eval num_timesteps=847872, episode_reward=-73.00 +/- 11.14\n",
      "Episode length: 74.00 +/- 11.14\n",
      "Eval num_timesteps=847872, episode_reward=-74.40 +/- 9.75\n",
      "Episode length: 75.40 +/- 9.75\n",
      "Eval num_timesteps=847872, episode_reward=-94.30 +/- 56.14\n",
      "Episode length: 95.30 +/- 56.14\n",
      "Eval num_timesteps=847872, episode_reward=-77.30 +/- 12.03\n",
      "Episode length: 78.30 +/- 12.03\n",
      "Eval num_timesteps=847872, episode_reward=-76.10 +/- 6.56\n",
      "Episode length: 77.10 +/- 6.56\n",
      "Eval num_timesteps=847872, episode_reward=-85.30 +/- 26.39\n",
      "Episode length: 86.30 +/- 26.39\n",
      "Eval num_timesteps=847872, episode_reward=-71.40 +/- 7.09\n",
      "Episode length: 72.40 +/- 7.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00139 |       0.00000 |      56.25457 |       0.00099 |       0.08506\n",
      "      0.00272 |       0.00000 |      53.07518 |       0.00106 |       0.08496\n",
      "      0.00213 |       0.00000 |      51.08679 |       0.00107 |       0.08522\n",
      "      0.00230 |       0.00000 |      49.67500 |       0.00122 |       0.08463\n",
      "      0.00145 |       0.00000 |      48.81313 |       0.00104 |       0.08493\n",
      "      0.00213 |       0.00000 |      48.12586 |       0.00110 |       0.08425\n",
      "      0.00229 |       0.00000 |      47.48391 |       0.00116 |       0.08472\n",
      "      0.00194 |       0.00000 |      46.76794 |       0.00104 |       0.08382\n",
      "      0.00265 |       0.00000 |      46.40758 |       0.00114 |       0.08403\n",
      "      0.00224 |       0.00000 |      45.98804 |       0.00111 |       0.08377\n",
      "Evaluating losses...\n",
      "      0.00190 |       0.00000 |      45.72907 |       0.00102 |       0.08397\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.2         |\n",
      "| EpRewMean       | -87.2        |\n",
      "| EpThisIter      | 38           |\n",
      "| EpisodesSoFar   | 8944         |\n",
      "| TimeElapsed     | 2.6e+03      |\n",
      "| TimestepsSoFar  | 851968       |\n",
      "| ev_tdlam_before | 0.746        |\n",
      "| loss_ent        | 0.08397117   |\n",
      "| loss_kl         | 0.0010160544 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019032649 |\n",
      "| loss_vf_loss    | 45.72907     |\n",
      "----------------------------------\n",
      "********** Iteration 208 ************\n",
      "Eval num_timesteps=851968, episode_reward=-76.30 +/- 8.52\n",
      "Episode length: 77.30 +/- 8.52\n",
      "Eval num_timesteps=851968, episode_reward=-92.40 +/- 35.14\n",
      "Episode length: 93.40 +/- 35.14\n",
      "Eval num_timesteps=851968, episode_reward=-72.90 +/- 7.89\n",
      "Episode length: 73.90 +/- 7.89\n",
      "Eval num_timesteps=851968, episode_reward=-78.00 +/- 8.33\n",
      "Episode length: 79.00 +/- 8.33\n",
      "Eval num_timesteps=851968, episode_reward=-76.80 +/- 8.05\n",
      "Episode length: 77.80 +/- 8.05\n",
      "Eval num_timesteps=851968, episode_reward=-98.60 +/- 48.49\n",
      "Episode length: 99.60 +/- 48.49\n",
      "Eval num_timesteps=851968, episode_reward=-74.40 +/- 8.19\n",
      "Episode length: 75.40 +/- 8.19\n",
      "Eval num_timesteps=851968, episode_reward=-83.50 +/- 34.27\n",
      "Episode length: 84.50 +/- 34.27\n",
      "Eval num_timesteps=851968, episode_reward=-76.70 +/- 12.44\n",
      "Episode length: 77.70 +/- 12.44\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00165 |       0.00000 |      37.15672 |       0.00105 |       0.09163\n",
      "      0.00229 |       0.00000 |      36.93494 |       0.00120 |       0.09163\n",
      "      0.00210 |       0.00000 |      37.02928 |       0.00117 |       0.09070\n",
      "      0.00187 |       0.00000 |      36.82580 |       0.00115 |       0.09124\n",
      "      0.00239 |       0.00000 |      36.66899 |       0.00106 |       0.09153\n",
      "      0.00127 |       0.00000 |      36.75577 |       0.00107 |       0.09169\n",
      "      0.00164 |       0.00000 |      36.64905 |       0.00111 |       0.09121\n",
      "      0.00132 |       0.00000 |      36.62063 |       0.00108 |       0.09141\n",
      "      0.00172 |       0.00000 |      36.70486 |       0.00143 |       0.09156\n",
      "      0.00174 |       0.00000 |      36.55184 |       0.00112 |       0.09109\n",
      "Evaluating losses...\n",
      "      0.00178 |       0.00000 |      36.55580 |       0.00110 |       0.09156\n",
      "----------------------------------\n",
      "| EpLenMean       | 95           |\n",
      "| EpRewMean       | -94          |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 8991         |\n",
      "| TimeElapsed     | 2.61e+03     |\n",
      "| TimestepsSoFar  | 856064       |\n",
      "| ev_tdlam_before | 0.85         |\n",
      "| loss_ent        | 0.09156351   |\n",
      "| loss_kl         | 0.001098554  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0017758294 |\n",
      "| loss_vf_loss    | 36.555798    |\n",
      "----------------------------------\n",
      "********** Iteration 209 ************\n",
      "Eval num_timesteps=856064, episode_reward=-71.80 +/- 6.18\n",
      "Episode length: 72.80 +/- 6.18\n",
      "Eval num_timesteps=856064, episode_reward=-73.70 +/- 8.36\n",
      "Episode length: 74.70 +/- 8.36\n",
      "Eval num_timesteps=856064, episode_reward=-86.40 +/- 31.32\n",
      "Episode length: 87.40 +/- 31.32\n",
      "Eval num_timesteps=856064, episode_reward=-96.40 +/- 47.21\n",
      "Episode length: 97.40 +/- 47.21\n",
      "Eval num_timesteps=856064, episode_reward=-105.10 +/- 46.77\n",
      "Episode length: 106.10 +/- 46.77\n",
      "Eval num_timesteps=856064, episode_reward=-80.00 +/- 10.85\n",
      "Episode length: 81.00 +/- 10.85\n",
      "Eval num_timesteps=856064, episode_reward=-97.10 +/- 37.17\n",
      "Episode length: 98.10 +/- 37.17\n",
      "Eval num_timesteps=856064, episode_reward=-81.20 +/- 13.76\n",
      "Episode length: 82.20 +/- 13.76\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00330 |       0.00000 |      36.58040 |       0.00112 |       0.09240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00204 |       0.00000 |      36.30972 |       0.00107 |       0.09230\n",
      "      0.00256 |       0.00000 |      36.06178 |       0.00118 |       0.09250\n",
      "      0.00180 |       0.00000 |      36.08831 |       0.00114 |       0.09253\n",
      "      0.00349 |       0.00000 |      35.94538 |       0.00129 |       0.09271\n",
      "      0.00288 |       0.00000 |      36.11548 |       0.00137 |       0.09306\n",
      "      0.00230 |       0.00000 |      35.88663 |       0.00120 |       0.09245\n",
      "      0.00288 |       0.00000 |      35.81067 |       0.00143 |       0.09259\n",
      "      0.00336 |       0.00000 |      35.55354 |       0.00144 |       0.09257\n",
      "      0.00298 |       0.00000 |      35.58441 |       0.00115 |       0.09265\n",
      "Evaluating losses...\n",
      "      0.00261 |       0.00000 |      35.63899 |       0.00140 |       0.09249\n",
      "----------------------------------\n",
      "| EpLenMean       | 94.5         |\n",
      "| EpRewMean       | -93.5        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 9037         |\n",
      "| TimeElapsed     | 2.63e+03     |\n",
      "| TimestepsSoFar  | 860160       |\n",
      "| ev_tdlam_before | 0.849        |\n",
      "| loss_ent        | 0.09249034   |\n",
      "| loss_kl         | 0.0013968188 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0026134115 |\n",
      "| loss_vf_loss    | 35.638992    |\n",
      "----------------------------------\n",
      "********** Iteration 210 ************\n",
      "Eval num_timesteps=860160, episode_reward=-71.80 +/- 5.67\n",
      "Episode length: 72.80 +/- 5.67\n",
      "Eval num_timesteps=860160, episode_reward=-73.50 +/- 6.44\n",
      "Episode length: 74.50 +/- 6.44\n",
      "Eval num_timesteps=860160, episode_reward=-81.40 +/- 13.19\n",
      "Episode length: 82.40 +/- 13.19\n",
      "Eval num_timesteps=860160, episode_reward=-79.80 +/- 18.11\n",
      "Episode length: 80.80 +/- 18.11\n",
      "Eval num_timesteps=860160, episode_reward=-83.30 +/- 21.47\n",
      "Episode length: 84.30 +/- 21.47\n",
      "Eval num_timesteps=860160, episode_reward=-74.50 +/- 10.07\n",
      "Episode length: 75.50 +/- 10.07\n",
      "Eval num_timesteps=860160, episode_reward=-73.90 +/- 7.09\n",
      "Episode length: 74.90 +/- 7.09\n",
      "Eval num_timesteps=860160, episode_reward=-72.30 +/- 6.33\n",
      "Episode length: 73.30 +/- 6.33\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00246 |       0.00000 |      37.85817 |       0.00104 |       0.09203\n",
      "      0.00286 |       0.00000 |      37.78193 |       0.00108 |       0.09146\n",
      "      0.00188 |       0.00000 |      37.82107 |       0.00112 |       0.09101\n",
      "      0.00238 |       0.00000 |      37.53122 |       0.00109 |       0.09197\n",
      "      0.00266 |       0.00000 |      37.51290 |       0.00110 |       0.09198\n",
      "      0.00247 |       0.00000 |      37.49031 |       0.00125 |       0.09150\n",
      "      0.00232 |       0.00000 |      37.44083 |       0.00128 |       0.09105\n",
      "      0.00281 |       0.00000 |      37.30324 |       0.00129 |       0.09126\n",
      "      0.00169 |       0.00000 |      37.34983 |       0.00115 |       0.09084\n",
      "      0.00190 |       0.00000 |      37.21193 |       0.00121 |       0.09081\n",
      "Evaluating losses...\n",
      "      0.00224 |       0.00000 |      37.16724 |       0.00099 |       0.09094\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.7         |\n",
      "| EpRewMean       | -87.7        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 9084         |\n",
      "| TimeElapsed     | 2.64e+03     |\n",
      "| TimestepsSoFar  | 864256       |\n",
      "| ev_tdlam_before | 0.849        |\n",
      "| loss_ent        | 0.09094473   |\n",
      "| loss_kl         | 0.0009894903 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0022356717 |\n",
      "| loss_vf_loss    | 37.16724     |\n",
      "----------------------------------\n",
      "********** Iteration 211 ************\n",
      "Eval num_timesteps=864256, episode_reward=-78.80 +/- 10.93\n",
      "Episode length: 79.80 +/- 10.93\n",
      "Eval num_timesteps=864256, episode_reward=-75.60 +/- 16.13\n",
      "Episode length: 76.60 +/- 16.13\n",
      "Eval num_timesteps=864256, episode_reward=-77.50 +/- 14.71\n",
      "Episode length: 78.50 +/- 14.71\n",
      "Eval num_timesteps=864256, episode_reward=-83.30 +/- 33.51\n",
      "Episode length: 84.30 +/- 33.51\n",
      "Eval num_timesteps=864256, episode_reward=-81.00 +/- 17.65\n",
      "Episode length: 82.00 +/- 17.65\n",
      "Eval num_timesteps=864256, episode_reward=-81.30 +/- 30.52\n",
      "Episode length: 82.30 +/- 30.52\n",
      "Eval num_timesteps=864256, episode_reward=-80.30 +/- 11.40\n",
      "Episode length: 81.30 +/- 11.40\n",
      "Eval num_timesteps=864256, episode_reward=-75.00 +/- 7.32\n",
      "Episode length: 76.00 +/- 7.32\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00232 |       0.00000 |      59.61427 |       0.00107 |       0.09051\n",
      "      0.00201 |       0.00000 |      59.27863 |       0.00120 |       0.08994\n",
      "      0.00222 |       0.00000 |      58.94265 |       0.00105 |       0.09040\n",
      "      0.00246 |       0.00000 |      58.76076 |       0.00100 |       0.09002\n",
      "      0.00203 |       0.00000 |      58.65201 |       0.00136 |       0.08985\n",
      "      0.00115 |       0.00000 |      58.57574 |       0.00117 |       0.09041\n",
      "      0.00169 |       0.00000 |      58.29750 |       0.00120 |       0.08978\n",
      "      0.00309 |       0.00000 |      58.03638 |       0.00126 |       0.08987\n",
      "      0.00168 |       0.00000 |      58.00330 |       0.00113 |       0.09018\n",
      "      0.00160 |       0.00000 |      58.09125 |       0.00117 |       0.08988\n",
      "Evaluating losses...\n",
      "      0.00227 |       0.00000 |      57.92458 |       0.00121 |       0.08983\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.5         |\n",
      "| EpRewMean       | -90.5        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 9127         |\n",
      "| TimeElapsed     | 2.65e+03     |\n",
      "| TimestepsSoFar  | 868352       |\n",
      "| ev_tdlam_before | 0.738        |\n",
      "| loss_ent        | 0.08983127   |\n",
      "| loss_kl         | 0.0012131853 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0022722895 |\n",
      "| loss_vf_loss    | 57.924583    |\n",
      "----------------------------------\n",
      "********** Iteration 212 ************\n",
      "Eval num_timesteps=868352, episode_reward=-72.80 +/- 11.10\n",
      "Episode length: 73.80 +/- 11.10\n",
      "Eval num_timesteps=868352, episode_reward=-88.40 +/- 30.89\n",
      "Episode length: 89.40 +/- 30.89\n",
      "Eval num_timesteps=868352, episode_reward=-77.00 +/- 13.08\n",
      "Episode length: 78.00 +/- 13.08\n",
      "Eval num_timesteps=868352, episode_reward=-73.80 +/- 11.01\n",
      "Episode length: 74.80 +/- 11.01\n",
      "Eval num_timesteps=868352, episode_reward=-84.60 +/- 17.55\n",
      "Episode length: 85.60 +/- 17.55\n",
      "Eval num_timesteps=868352, episode_reward=-75.00 +/- 7.07\n",
      "Episode length: 76.00 +/- 7.07\n",
      "Eval num_timesteps=868352, episode_reward=-85.20 +/- 22.80\n",
      "Episode length: 86.20 +/- 22.80\n",
      "Eval num_timesteps=868352, episode_reward=-72.40 +/- 6.84\n",
      "Episode length: 73.40 +/- 6.84\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00292 |       0.00000 |      34.21289 |       0.00114 |       0.09663\n",
      "      0.00141 |       0.00000 |      34.18209 |       0.00113 |       0.09644\n",
      "      0.00320 |       0.00000 |      34.15968 |       0.00118 |       0.09638\n",
      "      0.00313 |       0.00000 |      34.04063 |       0.00118 |       0.09641\n",
      "      0.00231 |       0.00000 |      33.89605 |       0.00111 |       0.09703\n",
      "      0.00223 |       0.00000 |      33.95274 |       0.00113 |       0.09657\n",
      "      0.00179 |       0.00000 |      33.85844 |       0.00119 |       0.09660\n",
      "      0.00277 |       0.00000 |      33.85071 |       0.00108 |       0.09651\n",
      "      0.00202 |       0.00000 |      33.79744 |       0.00123 |       0.09672\n",
      "      0.00184 |       0.00000 |      33.80628 |       0.00120 |       0.09659\n",
      "Evaluating losses...\n",
      "      0.00303 |       0.00000 |      33.77839 |       0.00106 |       0.09708\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.4         |\n",
      "| EpRewMean       | -89.4        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 9174         |\n",
      "| TimeElapsed     | 2.66e+03     |\n",
      "| TimestepsSoFar  | 872448       |\n",
      "| ev_tdlam_before | 0.864        |\n",
      "| loss_ent        | 0.09707965   |\n",
      "| loss_kl         | 0.0010558802 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0030340315 |\n",
      "| loss_vf_loss    | 33.778393    |\n",
      "----------------------------------\n",
      "********** Iteration 213 ************\n",
      "Eval num_timesteps=872448, episode_reward=-74.50 +/- 5.00\n",
      "Episode length: 75.50 +/- 5.00\n",
      "Eval num_timesteps=872448, episode_reward=-75.20 +/- 14.80\n",
      "Episode length: 76.20 +/- 14.80\n",
      "Eval num_timesteps=872448, episode_reward=-74.10 +/- 7.96\n",
      "Episode length: 75.10 +/- 7.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=872448, episode_reward=-80.30 +/- 19.14\n",
      "Episode length: 81.30 +/- 19.14\n",
      "Eval num_timesteps=872448, episode_reward=-72.10 +/- 8.77\n",
      "Episode length: 73.10 +/- 8.77\n",
      "Eval num_timesteps=872448, episode_reward=-75.70 +/- 7.31\n",
      "Episode length: 76.70 +/- 7.31\n",
      "Eval num_timesteps=872448, episode_reward=-72.60 +/- 5.78\n",
      "Episode length: 73.60 +/- 5.78\n",
      "Eval num_timesteps=872448, episode_reward=-73.70 +/- 5.81\n",
      "Episode length: 74.70 +/- 5.81\n",
      "Eval num_timesteps=872448, episode_reward=-75.90 +/- 11.22\n",
      "Episode length: 76.90 +/- 11.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00206 |       0.00000 |      46.52679 |       0.00116 |       0.09334\n",
      "      0.00261 |       0.00000 |      46.17283 |       0.00116 |       0.09323\n",
      "      0.00170 |       0.00000 |      45.91167 |       0.00120 |       0.09353\n",
      "      0.00199 |       0.00000 |      45.81216 |       0.00131 |       0.09300\n",
      "      0.00304 |       0.00000 |      45.74964 |       0.00148 |       0.09286\n",
      "      0.00271 |       0.00000 |      45.45434 |       0.00125 |       0.09353\n",
      "      0.00189 |       0.00000 |      45.52679 |       0.00135 |       0.09324\n",
      "      0.00355 |       0.00000 |      45.42953 |       0.00152 |       0.09331\n",
      "      0.00161 |       0.00000 |      45.35947 |       0.00148 |       0.09314\n",
      "      0.00054 |       0.00000 |      45.25053 |       0.00138 |       0.09330\n",
      "Evaluating losses...\n",
      "      0.00235 |       0.00000 |      45.23758 |       0.00138 |       0.09269\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.8         |\n",
      "| EpRewMean       | -89.8        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 9217         |\n",
      "| TimeElapsed     | 2.67e+03     |\n",
      "| TimestepsSoFar  | 876544       |\n",
      "| ev_tdlam_before | 0.801        |\n",
      "| loss_ent        | 0.09269252   |\n",
      "| loss_kl         | 0.0013792235 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0023479369 |\n",
      "| loss_vf_loss    | 45.237583    |\n",
      "----------------------------------\n",
      "********** Iteration 214 ************\n",
      "Eval num_timesteps=876544, episode_reward=-75.60 +/- 9.91\n",
      "Episode length: 76.60 +/- 9.91\n",
      "Eval num_timesteps=876544, episode_reward=-77.40 +/- 22.93\n",
      "Episode length: 78.40 +/- 22.93\n",
      "Eval num_timesteps=876544, episode_reward=-78.80 +/- 9.47\n",
      "Episode length: 79.80 +/- 9.47\n",
      "Eval num_timesteps=876544, episode_reward=-80.40 +/- 18.28\n",
      "Episode length: 81.40 +/- 18.28\n",
      "Eval num_timesteps=876544, episode_reward=-80.30 +/- 24.21\n",
      "Episode length: 81.30 +/- 24.21\n",
      "Eval num_timesteps=876544, episode_reward=-77.30 +/- 10.47\n",
      "Episode length: 78.30 +/- 10.47\n",
      "Eval num_timesteps=876544, episode_reward=-74.80 +/- 13.11\n",
      "Episode length: 75.80 +/- 13.11\n",
      "Eval num_timesteps=876544, episode_reward=-73.90 +/- 6.38\n",
      "Episode length: 74.90 +/- 6.38\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00200 |       0.00000 |      26.77625 |       0.00100 |       0.09589\n",
      "      0.00270 |       0.00000 |      26.33158 |       0.00139 |       0.09568\n",
      "      0.00223 |       0.00000 |      26.05541 |       0.00127 |       0.09526\n",
      "      0.00241 |       0.00000 |      25.79570 |       0.00123 |       0.09586\n",
      "      0.00221 |       0.00000 |      25.46832 |       0.00117 |       0.09565\n",
      "      0.00222 |       0.00000 |      25.44284 |       0.00121 |       0.09582\n",
      "      0.00286 |       0.00000 |      25.33331 |       0.00107 |       0.09637\n",
      "      0.00181 |       0.00000 |      25.06935 |       0.00112 |       0.09627\n",
      "      0.00203 |       0.00000 |      25.18579 |       0.00114 |       0.09617\n",
      "      0.00259 |       0.00000 |      25.05156 |       0.00123 |       0.09593\n",
      "Evaluating losses...\n",
      "      0.00253 |       0.00000 |      24.97635 |       0.00115 |       0.09646\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.5         |\n",
      "| EpRewMean       | -87.5        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 9266         |\n",
      "| TimeElapsed     | 2.68e+03     |\n",
      "| TimestepsSoFar  | 880640       |\n",
      "| ev_tdlam_before | 0.9          |\n",
      "| loss_ent        | 0.096455626  |\n",
      "| loss_kl         | 0.0011452506 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0025310223 |\n",
      "| loss_vf_loss    | 24.976353    |\n",
      "----------------------------------\n",
      "********** Iteration 215 ************\n",
      "Eval num_timesteps=880640, episode_reward=-71.30 +/- 5.08\n",
      "Episode length: 72.30 +/- 5.08\n",
      "Eval num_timesteps=880640, episode_reward=-79.40 +/- 14.42\n",
      "Episode length: 80.40 +/- 14.42\n",
      "Eval num_timesteps=880640, episode_reward=-79.40 +/- 11.48\n",
      "Episode length: 80.40 +/- 11.48\n",
      "Eval num_timesteps=880640, episode_reward=-74.40 +/- 8.20\n",
      "Episode length: 75.40 +/- 8.20\n",
      "Eval num_timesteps=880640, episode_reward=-98.20 +/- 67.17\n",
      "Episode length: 99.20 +/- 67.17\n",
      "Eval num_timesteps=880640, episode_reward=-81.00 +/- 10.96\n",
      "Episode length: 82.00 +/- 10.96\n",
      "Eval num_timesteps=880640, episode_reward=-73.20 +/- 6.98\n",
      "Episode length: 74.20 +/- 6.98\n",
      "Eval num_timesteps=880640, episode_reward=-73.80 +/- 7.52\n",
      "Episode length: 74.80 +/- 7.52\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00334 |       0.00000 |      30.48193 |       0.00121 |       0.09701\n",
      "      0.00336 |       0.00000 |      30.09098 |       0.00120 |       0.09709\n",
      "      0.00236 |       0.00000 |      29.91949 |       0.00117 |       0.09756\n",
      "      0.00299 |       0.00000 |      29.76093 |       0.00116 |       0.09759\n",
      "      0.00255 |       0.00000 |      29.62705 |       0.00106 |       0.09729\n",
      "      0.00207 |       0.00000 |      29.50701 |       0.00116 |       0.09694\n",
      "      0.00226 |       0.00000 |      29.59862 |       0.00122 |       0.09726\n",
      "      0.00332 |       0.00000 |      29.46211 |       0.00123 |       0.09766\n",
      "      0.00231 |       0.00000 |      29.48755 |       0.00112 |       0.09681\n",
      "      0.00264 |       0.00000 |      29.37753 |       0.00134 |       0.09716\n",
      "Evaluating losses...\n",
      "      0.00222 |       0.00000 |      29.38457 |       0.00134 |       0.09751\n",
      "----------------------------------\n",
      "| EpLenMean       | 82.8         |\n",
      "| EpRewMean       | -81.8        |\n",
      "| EpThisIter      | 51           |\n",
      "| EpisodesSoFar   | 9317         |\n",
      "| TimeElapsed     | 2.68e+03     |\n",
      "| TimestepsSoFar  | 884736       |\n",
      "| ev_tdlam_before | 0.887        |\n",
      "| loss_ent        | 0.09750633   |\n",
      "| loss_kl         | 0.0013377047 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002219682  |\n",
      "| loss_vf_loss    | 29.384573    |\n",
      "----------------------------------\n",
      "********** Iteration 216 ************\n",
      "Eval num_timesteps=884736, episode_reward=-82.80 +/- 31.85\n",
      "Episode length: 83.80 +/- 31.85\n",
      "Eval num_timesteps=884736, episode_reward=-74.50 +/- 12.42\n",
      "Episode length: 75.50 +/- 12.42\n",
      "Eval num_timesteps=884736, episode_reward=-79.80 +/- 14.89\n",
      "Episode length: 80.80 +/- 14.89\n",
      "Eval num_timesteps=884736, episode_reward=-76.00 +/- 8.35\n",
      "Episode length: 77.00 +/- 8.35\n",
      "Eval num_timesteps=884736, episode_reward=-81.30 +/- 12.46\n",
      "Episode length: 82.30 +/- 12.46\n",
      "Eval num_timesteps=884736, episode_reward=-70.40 +/- 7.45\n",
      "Episode length: 71.40 +/- 7.45\n",
      "Eval num_timesteps=884736, episode_reward=-72.90 +/- 7.18\n",
      "Episode length: 73.90 +/- 7.18\n",
      "Eval num_timesteps=884736, episode_reward=-75.70 +/- 10.87\n",
      "Episode length: 76.70 +/- 10.87\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00344 |       0.00000 |      35.95102 |       0.00110 |       0.09876\n",
      "      0.00319 |       0.00000 |      35.84996 |       0.00123 |       0.09813\n",
      "      0.00348 |       0.00000 |      35.83905 |       0.00122 |       0.09866\n",
      "      0.00259 |       0.00000 |      35.70903 |       0.00119 |       0.09910\n",
      "      0.00161 |       0.00000 |      35.87502 |       0.00117 |       0.09938\n",
      "      0.00171 |       0.00000 |      35.72204 |       0.00111 |       0.09946\n",
      "      0.00119 |       0.00000 |      35.61429 |       0.00114 |       0.09926\n",
      "      0.00244 |       0.00000 |      35.71306 |       0.00127 |       0.09947\n",
      "      0.00238 |       0.00000 |      35.69224 |       0.00117 |       0.09952\n",
      "      0.00314 |       0.00000 |      35.55397 |       0.00143 |       0.09952\n",
      "Evaluating losses...\n",
      "      0.00191 |       0.00000 |      35.79454 |       0.00133 |       0.09993\n",
      "----------------------------------\n",
      "| EpLenMean       | 83.5         |\n",
      "| EpRewMean       | -82.5        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 9365         |\n",
      "| TimeElapsed     | 2.69e+03     |\n",
      "| TimestepsSoFar  | 888832       |\n",
      "| ev_tdlam_before | 0.855        |\n",
      "| loss_ent        | 0.09993254   |\n",
      "| loss_kl         | 0.0013270135 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0019087625 |\n",
      "| loss_vf_loss    | 35.794537    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 217 ************\n",
      "Eval num_timesteps=888832, episode_reward=-93.10 +/- 44.22\n",
      "Episode length: 94.10 +/- 44.22\n",
      "Eval num_timesteps=888832, episode_reward=-83.80 +/- 13.35\n",
      "Episode length: 84.80 +/- 13.35\n",
      "Eval num_timesteps=888832, episode_reward=-73.60 +/- 6.45\n",
      "Episode length: 74.60 +/- 6.45\n",
      "Eval num_timesteps=888832, episode_reward=-73.30 +/- 8.20\n",
      "Episode length: 74.30 +/- 8.20\n",
      "Eval num_timesteps=888832, episode_reward=-78.10 +/- 8.89\n",
      "Episode length: 79.10 +/- 8.89\n",
      "Eval num_timesteps=888832, episode_reward=-72.50 +/- 8.88\n",
      "Episode length: 73.50 +/- 8.88\n",
      "Eval num_timesteps=888832, episode_reward=-74.30 +/- 8.01\n",
      "Episode length: 75.30 +/- 8.01\n",
      "Eval num_timesteps=888832, episode_reward=-85.60 +/- 37.32\n",
      "Episode length: 86.60 +/- 37.32\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00236 |       0.00000 |      25.62059 |       0.00108 |       0.09845\n",
      "      0.00197 |       0.00000 |      25.58949 |       0.00113 |       0.09809\n",
      "      0.00334 |       0.00000 |      25.54470 |       0.00118 |       0.09817\n",
      "      0.00235 |       0.00000 |      25.55500 |       0.00125 |       0.09683\n",
      "      0.00205 |       0.00000 |      25.55412 |       0.00110 |       0.09763\n",
      "      0.00193 |       0.00000 |      25.50217 |       0.00117 |       0.09811\n",
      "      0.00218 |       0.00000 |      25.49671 |       0.00109 |       0.09781\n",
      "      0.00251 |       0.00000 |      25.40600 |       0.00120 |       0.09781\n",
      "      0.00308 |       0.00000 |      25.23789 |       0.00119 |       0.09768\n",
      "      0.00255 |       0.00000 |      25.38340 |       0.00124 |       0.09766\n",
      "Evaluating losses...\n",
      "      0.00222 |       0.00000 |      25.38687 |       0.00120 |       0.09741\n",
      "----------------------------------\n",
      "| EpLenMean       | 84.4         |\n",
      "| EpRewMean       | -83.4        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 9414         |\n",
      "| TimeElapsed     | 2.7e+03      |\n",
      "| TimestepsSoFar  | 892928       |\n",
      "| ev_tdlam_before | 0.898        |\n",
      "| loss_ent        | 0.097405426  |\n",
      "| loss_kl         | 0.001196213  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0022220104 |\n",
      "| loss_vf_loss    | 25.38687     |\n",
      "----------------------------------\n",
      "********** Iteration 218 ************\n",
      "Eval num_timesteps=892928, episode_reward=-74.30 +/- 9.13\n",
      "Episode length: 75.30 +/- 9.13\n",
      "Eval num_timesteps=892928, episode_reward=-91.90 +/- 39.21\n",
      "Episode length: 92.90 +/- 39.21\n",
      "Eval num_timesteps=892928, episode_reward=-71.40 +/- 8.31\n",
      "Episode length: 72.40 +/- 8.31\n",
      "Eval num_timesteps=892928, episode_reward=-84.90 +/- 22.96\n",
      "Episode length: 85.90 +/- 22.96\n",
      "Eval num_timesteps=892928, episode_reward=-81.60 +/- 16.13\n",
      "Episode length: 82.60 +/- 16.13\n",
      "Eval num_timesteps=892928, episode_reward=-68.60 +/- 5.31\n",
      "Episode length: 69.60 +/- 5.31\n",
      "Eval num_timesteps=892928, episode_reward=-95.20 +/- 48.69\n",
      "Episode length: 96.20 +/- 48.69\n",
      "Eval num_timesteps=892928, episode_reward=-72.90 +/- 9.52\n",
      "Episode length: 73.90 +/- 9.52\n",
      "Eval num_timesteps=892928, episode_reward=-83.20 +/- 17.82\n",
      "Episode length: 84.20 +/- 17.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00245 |       0.00000 |      51.02184 |       0.00096 |       0.09044\n",
      "      0.00223 |       0.00000 |      50.45947 |       0.00107 |       0.09004\n",
      "      0.00187 |       0.00000 |      50.16026 |       0.00100 |       0.09043\n",
      "      0.00243 |       0.00000 |      50.05924 |       0.00122 |       0.08981\n",
      "      0.00216 |       0.00000 |      49.76942 |       0.00102 |       0.09005\n",
      "      0.00271 |       0.00000 |      49.51586 |       0.00115 |       0.08987\n",
      "      0.00164 |       0.00000 |      49.18937 |       0.00108 |       0.09047\n",
      "      0.00191 |       0.00000 |      49.28997 |       0.00113 |       0.08986\n",
      "      0.00211 |       0.00000 |      49.04157 |       0.00101 |       0.09000\n",
      "      0.00162 |       0.00000 |      48.84235 |       0.00132 |       0.08934\n",
      "Evaluating losses...\n",
      "      0.00207 |       0.00000 |      48.80525 |       0.00109 |       0.09028\n",
      "----------------------------------\n",
      "| EpLenMean       | 87           |\n",
      "| EpRewMean       | -86          |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 9458         |\n",
      "| TimeElapsed     | 2.72e+03     |\n",
      "| TimestepsSoFar  | 897024       |\n",
      "| ev_tdlam_before | 0.779        |\n",
      "| loss_ent        | 0.09027523   |\n",
      "| loss_kl         | 0.0010897857 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0020706821 |\n",
      "| loss_vf_loss    | 48.805252    |\n",
      "----------------------------------\n",
      "********** Iteration 219 ************\n",
      "Eval num_timesteps=897024, episode_reward=-74.60 +/- 8.66\n",
      "Episode length: 75.60 +/- 8.66\n",
      "Eval num_timesteps=897024, episode_reward=-81.50 +/- 30.89\n",
      "Episode length: 82.50 +/- 30.89\n",
      "Eval num_timesteps=897024, episode_reward=-82.40 +/- 21.50\n",
      "Episode length: 83.40 +/- 21.50\n",
      "Eval num_timesteps=897024, episode_reward=-70.30 +/- 5.92\n",
      "Episode length: 71.30 +/- 5.92\n",
      "Eval num_timesteps=897024, episode_reward=-76.20 +/- 4.51\n",
      "Episode length: 77.20 +/- 4.51\n",
      "Eval num_timesteps=897024, episode_reward=-81.40 +/- 13.49\n",
      "Episode length: 82.40 +/- 13.49\n",
      "Eval num_timesteps=897024, episode_reward=-69.30 +/- 5.10\n",
      "Episode length: 70.30 +/- 5.10\n",
      "Eval num_timesteps=897024, episode_reward=-87.40 +/- 15.53\n",
      "Episode length: 88.40 +/- 15.53\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00188 |       0.00000 |      47.40294 |       0.00113 |       0.09015\n",
      "      0.00176 |       0.00000 |      47.29086 |       0.00117 |       0.08963\n",
      "      0.00170 |       0.00000 |      46.91177 |       0.00127 |       0.08999\n",
      "      0.00204 |       0.00000 |      46.75464 |       0.00120 |       0.08946\n",
      "      0.00212 |       0.00000 |      46.58341 |       0.00116 |       0.08947\n",
      "      0.00216 |       0.00000 |      46.49844 |       0.00127 |       0.08948\n",
      "      0.00195 |       0.00000 |      46.37148 |       0.00130 |       0.08919\n",
      "      0.00184 |       0.00000 |      46.26310 |       0.00129 |       0.08925\n",
      "      0.00203 |       0.00000 |      46.05628 |       0.00121 |       0.08948\n",
      "      0.00170 |       0.00000 |      46.17020 |       0.00119 |       0.08931\n",
      "Evaluating losses...\n",
      "      0.00235 |       0.00000 |      45.98241 |       0.00130 |       0.08910\n",
      "----------------------------------\n",
      "| EpLenMean       | 91           |\n",
      "| EpRewMean       | -90          |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 9503         |\n",
      "| TimeElapsed     | 2.73e+03     |\n",
      "| TimestepsSoFar  | 901120       |\n",
      "| ev_tdlam_before | 0.796        |\n",
      "| loss_ent        | 0.0890964    |\n",
      "| loss_kl         | 0.0012983211 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0023549525 |\n",
      "| loss_vf_loss    | 45.98241     |\n",
      "----------------------------------\n",
      "********** Iteration 220 ************\n",
      "Eval num_timesteps=901120, episode_reward=-82.00 +/- 26.86\n",
      "Episode length: 83.00 +/- 26.86\n",
      "Eval num_timesteps=901120, episode_reward=-74.00 +/- 9.56\n",
      "Episode length: 75.00 +/- 9.56\n",
      "Eval num_timesteps=901120, episode_reward=-79.20 +/- 7.78\n",
      "Episode length: 80.20 +/- 7.78\n",
      "Eval num_timesteps=901120, episode_reward=-81.20 +/- 18.67\n",
      "Episode length: 82.20 +/- 18.67\n",
      "Eval num_timesteps=901120, episode_reward=-74.50 +/- 10.61\n",
      "Episode length: 75.50 +/- 10.61\n",
      "Eval num_timesteps=901120, episode_reward=-71.60 +/- 5.57\n",
      "Episode length: 72.60 +/- 5.57\n",
      "Eval num_timesteps=901120, episode_reward=-76.00 +/- 11.52\n",
      "Episode length: 77.00 +/- 11.52\n",
      "Eval num_timesteps=901120, episode_reward=-75.70 +/- 11.54\n",
      "Episode length: 76.70 +/- 11.54\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00303 |       0.00000 |      44.06289 |       0.00123 |       0.08929\n",
      "      0.00212 |       0.00000 |      43.61065 |       0.00110 |       0.08959\n",
      "      0.00260 |       0.00000 |      43.27821 |       0.00110 |       0.08974\n",
      "      0.00247 |       0.00000 |      43.20142 |       0.00117 |       0.08932\n",
      "      0.00212 |       0.00000 |      42.85694 |       0.00107 |       0.08907\n",
      "      0.00200 |       0.00000 |      42.70504 |       0.00109 |       0.08889\n",
      "      0.00162 |       0.00000 |      42.40186 |       0.00119 |       0.08870\n",
      "      0.00175 |       0.00000 |      42.37848 |       0.00112 |       0.08865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00238 |       0.00000 |      42.16679 |       0.00106 |       0.08864\n",
      "      0.00231 |       0.00000 |      42.15654 |       0.00127 |       0.08877\n",
      "Evaluating losses...\n",
      "      0.00205 |       0.00000 |      41.88422 |       0.00112 |       0.08904\n",
      "----------------------------------\n",
      "| EpLenMean       | 91           |\n",
      "| EpRewMean       | -90          |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 9546         |\n",
      "| TimeElapsed     | 2.73e+03     |\n",
      "| TimestepsSoFar  | 905216       |\n",
      "| ev_tdlam_before | 0.812        |\n",
      "| loss_ent        | 0.08904206   |\n",
      "| loss_kl         | 0.0011239873 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0020544056 |\n",
      "| loss_vf_loss    | 41.884216    |\n",
      "----------------------------------\n",
      "********** Iteration 221 ************\n",
      "Eval num_timesteps=905216, episode_reward=-73.30 +/- 7.66\n",
      "Episode length: 74.30 +/- 7.66\n",
      "Eval num_timesteps=905216, episode_reward=-70.10 +/- 6.83\n",
      "Episode length: 71.10 +/- 6.83\n",
      "Eval num_timesteps=905216, episode_reward=-92.60 +/- 40.52\n",
      "Episode length: 93.60 +/- 40.52\n",
      "Eval num_timesteps=905216, episode_reward=-80.20 +/- 17.79\n",
      "Episode length: 81.20 +/- 17.79\n",
      "Eval num_timesteps=905216, episode_reward=-73.50 +/- 10.27\n",
      "Episode length: 74.50 +/- 10.27\n",
      "Eval num_timesteps=905216, episode_reward=-76.50 +/- 7.32\n",
      "Episode length: 77.50 +/- 7.32\n",
      "Eval num_timesteps=905216, episode_reward=-76.40 +/- 23.28\n",
      "Episode length: 77.40 +/- 23.28\n",
      "Eval num_timesteps=905216, episode_reward=-79.30 +/- 13.72\n",
      "Episode length: 80.30 +/- 13.72\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00255 |       0.00000 |      30.06154 |       0.00115 |       0.09573\n",
      "      0.00279 |       0.00000 |      30.07206 |       0.00126 |       0.09517\n",
      "      0.00202 |       0.00000 |      30.00793 |       0.00123 |       0.09554\n",
      "      0.00298 |       0.00000 |      29.85064 |       0.00111 |       0.09606\n",
      "      0.00328 |       0.00000 |      29.92562 |       0.00119 |       0.09575\n",
      "      0.00204 |       0.00000 |      29.89264 |       0.00114 |       0.09603\n",
      "      0.00245 |       0.00000 |      29.70661 |       0.00116 |       0.09633\n",
      "      0.00187 |       0.00000 |      29.67350 |       0.00120 |       0.09592\n",
      "      0.00256 |       0.00000 |      29.64249 |       0.00122 |       0.09567\n",
      "      0.00287 |       0.00000 |      29.57278 |       0.00123 |       0.09603\n",
      "Evaluating losses...\n",
      "      0.00180 |       0.00000 |      29.67613 |       0.00113 |       0.09582\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.4         |\n",
      "| EpRewMean       | -89.4        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 9594         |\n",
      "| TimeElapsed     | 2.74e+03     |\n",
      "| TimestepsSoFar  | 909312       |\n",
      "| ev_tdlam_before | 0.882        |\n",
      "| loss_ent        | 0.095818415  |\n",
      "| loss_kl         | 0.0011321928 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0018021783 |\n",
      "| loss_vf_loss    | 29.676128    |\n",
      "----------------------------------\n",
      "********** Iteration 222 ************\n",
      "Eval num_timesteps=909312, episode_reward=-86.30 +/- 23.20\n",
      "Episode length: 87.30 +/- 23.20\n",
      "Eval num_timesteps=909312, episode_reward=-72.50 +/- 7.61\n",
      "Episode length: 73.50 +/- 7.61\n",
      "Eval num_timesteps=909312, episode_reward=-79.20 +/- 11.59\n",
      "Episode length: 80.20 +/- 11.59\n",
      "Eval num_timesteps=909312, episode_reward=-70.20 +/- 9.41\n",
      "Episode length: 71.20 +/- 9.41\n",
      "Eval num_timesteps=909312, episode_reward=-73.20 +/- 5.53\n",
      "Episode length: 74.20 +/- 5.53\n",
      "Eval num_timesteps=909312, episode_reward=-128.70 +/- 127.27\n",
      "Episode length: 129.60 +/- 126.98\n",
      "Eval num_timesteps=909312, episode_reward=-75.10 +/- 8.31\n",
      "Episode length: 76.10 +/- 8.31\n",
      "Eval num_timesteps=909312, episode_reward=-72.00 +/- 7.82\n",
      "Episode length: 73.00 +/- 7.82\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00350 |       0.00000 |      25.86160 |       0.00132 |       0.09907\n",
      "      0.00323 |       0.00000 |      25.76797 |       0.00138 |       0.09826\n",
      "      0.00370 |       0.00000 |      25.64656 |       0.00143 |       0.09854\n",
      "      0.00325 |       0.00000 |      25.58697 |       0.00133 |       0.09871\n",
      "      0.00237 |       0.00000 |      25.50543 |       0.00118 |       0.09845\n",
      "      0.00245 |       0.00000 |      25.62677 |       0.00136 |       0.09876\n",
      "      0.00431 |       0.00000 |      25.44116 |       0.00123 |       0.09800\n",
      "      0.00278 |       0.00000 |      25.45174 |       0.00114 |       0.09797\n",
      "      0.00313 |       0.00000 |      25.49361 |       0.00124 |       0.09830\n",
      "      0.00311 |       0.00000 |      25.58430 |       0.00123 |       0.09818\n",
      "Evaluating losses...\n",
      "      0.00387 |       0.00000 |      25.28461 |       0.00137 |       0.09828\n",
      "----------------------------------\n",
      "| EpLenMean       | 85.7         |\n",
      "| EpRewMean       | -84.7        |\n",
      "| EpThisIter      | 49           |\n",
      "| EpisodesSoFar   | 9643         |\n",
      "| TimeElapsed     | 2.75e+03     |\n",
      "| TimestepsSoFar  | 913408       |\n",
      "| ev_tdlam_before | 0.9          |\n",
      "| loss_ent        | 0.09828485   |\n",
      "| loss_kl         | 0.0013710537 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0038713147 |\n",
      "| loss_vf_loss    | 25.284605    |\n",
      "----------------------------------\n",
      "********** Iteration 223 ************\n",
      "Eval num_timesteps=913408, episode_reward=-81.50 +/- 24.59\n",
      "Episode length: 82.50 +/- 24.59\n",
      "Eval num_timesteps=913408, episode_reward=-68.50 +/- 3.96\n",
      "Episode length: 69.50 +/- 3.96\n",
      "Eval num_timesteps=913408, episode_reward=-84.60 +/- 24.83\n",
      "Episode length: 85.60 +/- 24.83\n",
      "Eval num_timesteps=913408, episode_reward=-70.60 +/- 6.53\n",
      "Episode length: 71.60 +/- 6.53\n",
      "Eval num_timesteps=913408, episode_reward=-75.10 +/- 9.57\n",
      "Episode length: 76.10 +/- 9.57\n",
      "Eval num_timesteps=913408, episode_reward=-71.30 +/- 7.36\n",
      "Episode length: 72.30 +/- 7.36\n",
      "Eval num_timesteps=913408, episode_reward=-82.10 +/- 26.92\n",
      "Episode length: 83.10 +/- 26.92\n",
      "Eval num_timesteps=913408, episode_reward=-69.20 +/- 7.57\n",
      "Episode length: 70.20 +/- 7.57\n",
      "Eval num_timesteps=913408, episode_reward=-76.20 +/- 11.29\n",
      "Episode length: 77.20 +/- 11.29\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00190 |       0.00000 |      32.66287 |       0.00109 |       0.09208\n",
      "      0.00174 |       0.00000 |      32.59885 |       0.00111 |       0.09258\n",
      "      0.00200 |       0.00000 |      32.61592 |       0.00107 |       0.09209\n",
      "      0.00208 |       0.00000 |      32.58766 |       0.00115 |       0.09241\n",
      "      0.00177 |       0.00000 |      32.41149 |       0.00110 |       0.09234\n",
      "      0.00180 |       0.00000 |      32.50824 |       0.00104 |       0.09218\n",
      "      0.00221 |       0.00000 |      32.34658 |       0.00116 |       0.09183\n",
      "      0.00172 |       0.00000 |      32.33926 |       0.00117 |       0.09238\n",
      "      0.00199 |       0.00000 |      32.47617 |       0.00113 |       0.09216\n",
      "      0.00189 |       0.00000 |      32.32714 |       0.00113 |       0.09164\n",
      "Evaluating losses...\n",
      "      0.00213 |       0.00000 |      32.32294 |       0.00110 |       0.09162\n",
      "----------------------------------\n",
      "| EpLenMean       | 85.8         |\n",
      "| EpRewMean       | -84.8        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 9690         |\n",
      "| TimeElapsed     | 2.77e+03     |\n",
      "| TimestepsSoFar  | 917504       |\n",
      "| ev_tdlam_before | 0.867        |\n",
      "| loss_ent        | 0.09162021   |\n",
      "| loss_kl         | 0.0011011374 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.002132052  |\n",
      "| loss_vf_loss    | 32.322937    |\n",
      "----------------------------------\n",
      "********** Iteration 224 ************\n",
      "Eval num_timesteps=917504, episode_reward=-114.40 +/- 128.96\n",
      "Episode length: 115.30 +/- 128.66\n",
      "Eval num_timesteps=917504, episode_reward=-73.50 +/- 11.88\n",
      "Episode length: 74.50 +/- 11.88\n",
      "Eval num_timesteps=917504, episode_reward=-83.10 +/- 12.95\n",
      "Episode length: 84.10 +/- 12.95\n",
      "Eval num_timesteps=917504, episode_reward=-76.50 +/- 8.95\n",
      "Episode length: 77.50 +/- 8.95\n",
      "Eval num_timesteps=917504, episode_reward=-78.20 +/- 10.51\n",
      "Episode length: 79.20 +/- 10.51\n",
      "Eval num_timesteps=917504, episode_reward=-84.00 +/- 31.13\n",
      "Episode length: 85.00 +/- 31.13\n",
      "Eval num_timesteps=917504, episode_reward=-78.00 +/- 23.04\n",
      "Episode length: 79.00 +/- 23.04\n",
      "Eval num_timesteps=917504, episode_reward=-78.50 +/- 13.43\n",
      "Episode length: 79.50 +/- 13.43\n",
      "Optimizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00332 |       0.00000 |      32.41780 |       0.00113 |       0.09326\n",
      "      0.00493 |       0.00000 |      32.08806 |       0.00112 |       0.09327\n",
      "      0.00260 |       0.00000 |      31.94967 |       0.00107 |       0.09334\n",
      "      0.00258 |       0.00000 |      31.70295 |       0.00115 |       0.09321\n",
      "      0.00381 |       0.00000 |      31.56865 |       0.00115 |       0.09307\n",
      "      0.00263 |       0.00000 |      31.53171 |       0.00112 |       0.09331\n",
      "      0.00269 |       0.00000 |      31.28524 |       0.00103 |       0.09337\n",
      "      0.00384 |       0.00000 |      31.20461 |       0.00109 |       0.09360\n",
      "      0.00258 |       0.00000 |      31.19927 |       0.00107 |       0.09318\n",
      "      0.00245 |       0.00000 |      31.03128 |       0.00115 |       0.09330\n",
      "Evaluating losses...\n",
      "      0.00321 |       0.00000 |      31.10853 |       0.00108 |       0.09345\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.2         |\n",
      "| EpRewMean       | -87.2        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 9736         |\n",
      "| TimeElapsed     | 2.78e+03     |\n",
      "| TimestepsSoFar  | 921600       |\n",
      "| ev_tdlam_before | 0.869        |\n",
      "| loss_ent        | 0.09345097   |\n",
      "| loss_kl         | 0.0010810476 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0032104675 |\n",
      "| loss_vf_loss    | 31.108532    |\n",
      "----------------------------------\n",
      "********** Iteration 225 ************\n",
      "Eval num_timesteps=921600, episode_reward=-84.70 +/- 29.27\n",
      "Episode length: 85.70 +/- 29.27\n",
      "Eval num_timesteps=921600, episode_reward=-91.30 +/- 35.58\n",
      "Episode length: 92.30 +/- 35.58\n",
      "Eval num_timesteps=921600, episode_reward=-73.30 +/- 6.77\n",
      "Episode length: 74.30 +/- 6.77\n",
      "Eval num_timesteps=921600, episode_reward=-77.40 +/- 16.67\n",
      "Episode length: 78.40 +/- 16.67\n",
      "Eval num_timesteps=921600, episode_reward=-85.40 +/- 37.74\n",
      "Episode length: 86.40 +/- 37.74\n",
      "Eval num_timesteps=921600, episode_reward=-88.10 +/- 21.64\n",
      "Episode length: 89.10 +/- 21.64\n",
      "Eval num_timesteps=921600, episode_reward=-77.60 +/- 11.00\n",
      "Episode length: 78.60 +/- 11.00\n",
      "Eval num_timesteps=921600, episode_reward=-74.40 +/- 10.22\n",
      "Episode length: 75.40 +/- 10.22\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00202 |       0.00000 |      29.21751 |       0.00104 |       0.09102\n",
      "      0.00375 |       0.00000 |      29.11682 |       0.00106 |       0.09168\n",
      "      0.00270 |       0.00000 |      29.08783 |       0.00106 |       0.09170\n",
      "      0.00272 |       0.00000 |      29.01067 |       0.00122 |       0.09191\n",
      "      0.00133 |       0.00000 |      28.91183 |       0.00101 |       0.09152\n",
      "      0.00234 |       0.00000 |      28.89060 |       0.00111 |       0.09167\n",
      "      0.00328 |       0.00000 |      28.70252 |       0.00109 |       0.09133\n",
      "      0.00282 |       0.00000 |      28.56997 |       0.00101 |       0.09149\n",
      "      0.00293 |       0.00000 |      28.60037 |       0.00118 |       0.09159\n",
      "      0.00199 |       0.00000 |      28.52900 |       0.00097 |       0.09155\n",
      "Evaluating losses...\n",
      "      0.00231 |       0.00000 |      28.63723 |       0.00114 |       0.09148\n",
      "---------------------------------\n",
      "| EpLenMean       | 88.3        |\n",
      "| EpRewMean       | -87.3       |\n",
      "| EpThisIter      | 48          |\n",
      "| EpisodesSoFar   | 9784        |\n",
      "| TimeElapsed     | 2.79e+03    |\n",
      "| TimestepsSoFar  | 925696      |\n",
      "| ev_tdlam_before | 0.884       |\n",
      "| loss_ent        | 0.091480665 |\n",
      "| loss_kl         | 0.001139869 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | 0.0023124   |\n",
      "| loss_vf_loss    | 28.637234   |\n",
      "---------------------------------\n",
      "********** Iteration 226 ************\n",
      "Eval num_timesteps=925696, episode_reward=-91.60 +/- 43.60\n",
      "Episode length: 92.60 +/- 43.60\n",
      "Eval num_timesteps=925696, episode_reward=-74.00 +/- 5.25\n",
      "Episode length: 75.00 +/- 5.25\n",
      "Eval num_timesteps=925696, episode_reward=-69.20 +/- 3.16\n",
      "Episode length: 70.20 +/- 3.16\n",
      "Eval num_timesteps=925696, episode_reward=-80.30 +/- 13.65\n",
      "Episode length: 81.30 +/- 13.65\n",
      "Eval num_timesteps=925696, episode_reward=-74.00 +/- 7.06\n",
      "Episode length: 75.00 +/- 7.06\n",
      "Eval num_timesteps=925696, episode_reward=-79.00 +/- 12.39\n",
      "Episode length: 80.00 +/- 12.39\n",
      "Eval num_timesteps=925696, episode_reward=-73.50 +/- 7.23\n",
      "Episode length: 74.50 +/- 7.23\n",
      "Eval num_timesteps=925696, episode_reward=-81.20 +/- 21.15\n",
      "Episode length: 82.20 +/- 21.15\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00283 |       0.00000 |      30.08285 |       0.00107 |       0.09266\n",
      "      0.00344 |       0.00000 |      30.01530 |       0.00121 |       0.09328\n",
      "      0.00301 |       0.00000 |      30.08223 |       0.00116 |       0.09312\n",
      "      0.00329 |       0.00000 |      29.99040 |       0.00118 |       0.09354\n",
      "      0.00387 |       0.00000 |      29.91474 |       0.00112 |       0.09350\n",
      "      0.00186 |       0.00000 |      29.95791 |       0.00092 |       0.09379\n",
      "      0.00307 |       0.00000 |      29.92162 |       0.00118 |       0.09393\n",
      "      0.00274 |       0.00000 |      29.70404 |       0.00114 |       0.09357\n",
      "      0.00227 |       0.00000 |      29.81303 |       0.00114 |       0.09389\n",
      "      0.00363 |       0.00000 |      29.87298 |       0.00112 |       0.09402\n",
      "Evaluating losses...\n",
      "      0.00203 |       0.00000 |      29.78572 |       0.00100 |       0.09392\n",
      "----------------------------------\n",
      "| EpLenMean       | 85.4         |\n",
      "| EpRewMean       | -84.4        |\n",
      "| EpThisIter      | 48           |\n",
      "| EpisodesSoFar   | 9832         |\n",
      "| TimeElapsed     | 2.8e+03      |\n",
      "| TimestepsSoFar  | 929792       |\n",
      "| ev_tdlam_before | 0.875        |\n",
      "| loss_ent        | 0.093918435  |\n",
      "| loss_kl         | 0.0009970075 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0020285733 |\n",
      "| loss_vf_loss    | 29.785725    |\n",
      "----------------------------------\n",
      "********** Iteration 227 ************\n",
      "Eval num_timesteps=929792, episode_reward=-80.70 +/- 9.43\n",
      "Episode length: 81.70 +/- 9.43\n",
      "Eval num_timesteps=929792, episode_reward=-71.50 +/- 7.10\n",
      "Episode length: 72.50 +/- 7.10\n",
      "Eval num_timesteps=929792, episode_reward=-118.70 +/- 127.46\n",
      "Episode length: 119.60 +/- 127.16\n",
      "Eval num_timesteps=929792, episode_reward=-91.80 +/- 50.27\n",
      "Episode length: 92.80 +/- 50.27\n",
      "Eval num_timesteps=929792, episode_reward=-74.00 +/- 4.05\n",
      "Episode length: 75.00 +/- 4.05\n",
      "Eval num_timesteps=929792, episode_reward=-78.40 +/- 15.23\n",
      "Episode length: 79.40 +/- 15.23\n",
      "Eval num_timesteps=929792, episode_reward=-86.80 +/- 32.67\n",
      "Episode length: 87.80 +/- 32.67\n",
      "Eval num_timesteps=929792, episode_reward=-79.00 +/- 11.74\n",
      "Episode length: 80.00 +/- 11.74\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00188 |       0.00000 |      31.48965 |       0.00123 |       0.09774\n",
      "      0.00257 |       0.00000 |      31.49388 |       0.00117 |       0.09772\n",
      "      0.00263 |       0.00000 |      31.49861 |       0.00129 |       0.09778\n",
      "      0.00200 |       0.00000 |      31.43933 |       0.00125 |       0.09766\n",
      "      0.00303 |       0.00000 |      31.41433 |       0.00132 |       0.09769\n",
      "      0.00280 |       0.00000 |      31.46339 |       0.00118 |       0.09782\n",
      "      0.00304 |       0.00000 |      31.34894 |       0.00119 |       0.09782\n",
      "      0.00226 |       0.00000 |      31.30588 |       0.00132 |       0.09768\n",
      "      0.00144 |       0.00000 |      31.37500 |       0.00123 |       0.09825\n",
      "      0.00296 |       0.00000 |      31.40606 |       0.00125 |       0.09792\n",
      "Evaluating losses...\n",
      "      0.00168 |       0.00000 |      31.37998 |       0.00110 |       0.09866\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.2         |\n",
      "| EpRewMean       | -85.2        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 9879         |\n",
      "| TimeElapsed     | 2.81e+03     |\n",
      "| TimestepsSoFar  | 933888       |\n",
      "| ev_tdlam_before | 0.873        |\n",
      "| loss_ent        | 0.098661676  |\n",
      "| loss_kl         | 0.0011003747 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0016846303 |\n",
      "| loss_vf_loss    | 31.379984    |\n",
      "----------------------------------\n",
      "********** Iteration 228 ************\n",
      "Eval num_timesteps=933888, episode_reward=-104.60 +/- 69.66\n",
      "Episode length: 105.60 +/- 69.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=933888, episode_reward=-75.60 +/- 6.48\n",
      "Episode length: 76.60 +/- 6.48\n",
      "Eval num_timesteps=933888, episode_reward=-74.40 +/- 12.43\n",
      "Episode length: 75.40 +/- 12.43\n",
      "Eval num_timesteps=933888, episode_reward=-77.00 +/- 7.25\n",
      "Episode length: 78.00 +/- 7.25\n",
      "Eval num_timesteps=933888, episode_reward=-77.60 +/- 16.90\n",
      "Episode length: 78.60 +/- 16.90\n",
      "Eval num_timesteps=933888, episode_reward=-70.40 +/- 5.87\n",
      "Episode length: 71.40 +/- 5.87\n",
      "Eval num_timesteps=933888, episode_reward=-72.90 +/- 7.71\n",
      "Episode length: 73.90 +/- 7.71\n",
      "Eval num_timesteps=933888, episode_reward=-73.00 +/- 9.67\n",
      "Episode length: 74.00 +/- 9.67\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00347 |       0.00000 |      40.65913 |       0.00119 |       0.09269\n",
      "      0.00338 |       0.00000 |      40.43990 |       0.00126 |       0.09340\n",
      "      0.00389 |       0.00000 |      40.33883 |       0.00127 |       0.09296\n",
      "      0.00331 |       0.00000 |      40.20588 |       0.00112 |       0.09247\n",
      "      0.00401 |       0.00000 |      39.97136 |       0.00120 |       0.09293\n",
      "      0.00310 |       0.00000 |      39.94611 |       0.00119 |       0.09330\n",
      "      0.00252 |       0.00000 |      39.85508 |       0.00121 |       0.09291\n",
      "      0.00343 |       0.00000 |      39.83372 |       0.00114 |       0.09242\n",
      "      0.00255 |       0.00000 |      39.82045 |       0.00120 |       0.09309\n",
      "      0.00272 |       0.00000 |      39.79588 |       0.00121 |       0.09319\n",
      "Evaluating losses...\n",
      "      0.00229 |       0.00000 |      39.56549 |       0.00116 |       0.09265\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.4         |\n",
      "| EpRewMean       | -89.4        |\n",
      "| EpThisIter      | 44           |\n",
      "| EpisodesSoFar   | 9923         |\n",
      "| TimeElapsed     | 2.82e+03     |\n",
      "| TimestepsSoFar  | 937984       |\n",
      "| ev_tdlam_before | 0.827        |\n",
      "| loss_ent        | 0.092645355  |\n",
      "| loss_kl         | 0.0011640638 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0022859322 |\n",
      "| loss_vf_loss    | 39.565487    |\n",
      "----------------------------------\n",
      "********** Iteration 229 ************\n",
      "Eval num_timesteps=937984, episode_reward=-90.30 +/- 35.31\n",
      "Episode length: 91.30 +/- 35.31\n",
      "Eval num_timesteps=937984, episode_reward=-78.10 +/- 13.69\n",
      "Episode length: 79.10 +/- 13.69\n",
      "Eval num_timesteps=937984, episode_reward=-78.10 +/- 11.93\n",
      "Episode length: 79.10 +/- 11.93\n",
      "Eval num_timesteps=937984, episode_reward=-82.70 +/- 18.87\n",
      "Episode length: 83.70 +/- 18.87\n",
      "Eval num_timesteps=937984, episode_reward=-128.50 +/- 126.86\n",
      "Episode length: 129.40 +/- 126.57\n",
      "Eval num_timesteps=937984, episode_reward=-79.30 +/- 9.38\n",
      "Episode length: 80.30 +/- 9.38\n",
      "Eval num_timesteps=937984, episode_reward=-89.10 +/- 43.21\n",
      "Episode length: 90.10 +/- 43.21\n",
      "Eval num_timesteps=937984, episode_reward=-103.90 +/- 54.70\n",
      "Episode length: 104.90 +/- 54.70\n",
      "Eval num_timesteps=937984, episode_reward=-88.00 +/- 46.53\n",
      "Episode length: 89.00 +/- 46.53\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00284 |       0.00000 |      30.55524 |       0.00125 |       0.09523\n",
      "      0.00256 |       0.00000 |      30.41824 |       0.00124 |       0.09560\n",
      "      0.00196 |       0.00000 |      30.39329 |       0.00113 |       0.09529\n",
      "      0.00308 |       0.00000 |      30.30241 |       0.00113 |       0.09546\n",
      "      0.00236 |       0.00000 |      30.45389 |       0.00099 |       0.09568\n",
      "      0.00235 |       0.00000 |      30.44993 |       0.00128 |       0.09553\n",
      "      0.00242 |       0.00000 |      30.23683 |       0.00119 |       0.09505\n",
      "      0.00213 |       0.00000 |      30.39974 |       0.00111 |       0.09567\n",
      "      0.00262 |       0.00000 |      30.32941 |       0.00119 |       0.09501\n",
      "      0.00324 |       0.00000 |      30.16105 |       0.00124 |       0.09543\n",
      "Evaluating losses...\n",
      "      0.00234 |       0.00000 |      30.24392 |       0.00113 |       0.09503\n",
      "---------------------------------\n",
      "| EpLenMean       | 88.6        |\n",
      "| EpRewMean       | -87.6       |\n",
      "| EpThisIter      | 47          |\n",
      "| EpisodesSoFar   | 9970        |\n",
      "| TimeElapsed     | 2.83e+03    |\n",
      "| TimestepsSoFar  | 942080      |\n",
      "| ev_tdlam_before | 0.878       |\n",
      "| loss_ent        | 0.09502515  |\n",
      "| loss_kl         | 0.00112633  |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | 0.002337237 |\n",
      "| loss_vf_loss    | 30.243916   |\n",
      "---------------------------------\n",
      "********** Iteration 230 ************\n",
      "Eval num_timesteps=942080, episode_reward=-78.00 +/- 10.67\n",
      "Episode length: 79.00 +/- 10.67\n",
      "Eval num_timesteps=942080, episode_reward=-72.70 +/- 5.60\n",
      "Episode length: 73.70 +/- 5.60\n",
      "Eval num_timesteps=942080, episode_reward=-72.90 +/- 4.39\n",
      "Episode length: 73.90 +/- 4.39\n",
      "Eval num_timesteps=942080, episode_reward=-82.70 +/- 21.76\n",
      "Episode length: 83.70 +/- 21.76\n",
      "Eval num_timesteps=942080, episode_reward=-85.20 +/- 13.40\n",
      "Episode length: 86.20 +/- 13.40\n",
      "Eval num_timesteps=942080, episode_reward=-72.90 +/- 9.08\n",
      "Episode length: 73.90 +/- 9.08\n",
      "Eval num_timesteps=942080, episode_reward=-77.80 +/- 8.98\n",
      "Episode length: 78.80 +/- 8.98\n",
      "Eval num_timesteps=942080, episode_reward=-95.70 +/- 39.09\n",
      "Episode length: 96.70 +/- 39.09\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00265 |       0.00000 |      42.12256 |       0.00109 |       0.09069\n",
      "      0.00290 |       0.00000 |      41.84319 |       0.00104 |       0.09136\n",
      "      0.00243 |       0.00000 |      41.76050 |       0.00109 |       0.09147\n",
      "      0.00235 |       0.00000 |      41.79954 |       0.00106 |       0.09097\n",
      "      0.00171 |       0.00000 |      41.43089 |       0.00112 |       0.09118\n",
      "      0.00259 |       0.00000 |      41.26579 |       0.00103 |       0.09135\n",
      "      0.00219 |       0.00000 |      41.28732 |       0.00100 |       0.09113\n",
      "      0.00174 |       0.00000 |      41.09897 |       0.00098 |       0.09151\n",
      "      0.00186 |       0.00000 |      41.18586 |       0.00103 |       0.09099\n",
      "      0.00171 |       0.00000 |      41.03009 |       0.00110 |       0.09113\n",
      "Evaluating losses...\n",
      "      0.00218 |       0.00000 |      40.85326 |       0.00112 |       0.09125\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 10013        |\n",
      "| TimeElapsed     | 2.84e+03     |\n",
      "| TimestepsSoFar  | 946176       |\n",
      "| ev_tdlam_before | 0.826        |\n",
      "| loss_ent        | 0.09125349   |\n",
      "| loss_kl         | 0.0011199927 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0021818941 |\n",
      "| loss_vf_loss    | 40.853264    |\n",
      "----------------------------------\n",
      "********** Iteration 231 ************\n",
      "Eval num_timesteps=946176, episode_reward=-78.10 +/- 17.08\n",
      "Episode length: 79.10 +/- 17.08\n",
      "Eval num_timesteps=946176, episode_reward=-86.60 +/- 42.49\n",
      "Episode length: 87.60 +/- 42.49\n",
      "Eval num_timesteps=946176, episode_reward=-74.90 +/- 7.76\n",
      "Episode length: 75.90 +/- 7.76\n",
      "Eval num_timesteps=946176, episode_reward=-93.50 +/- 47.53\n",
      "Episode length: 94.50 +/- 47.53\n",
      "Eval num_timesteps=946176, episode_reward=-71.10 +/- 7.11\n",
      "Episode length: 72.10 +/- 7.11\n",
      "Eval num_timesteps=946176, episode_reward=-72.80 +/- 6.66\n",
      "Episode length: 73.80 +/- 6.66\n",
      "Eval num_timesteps=946176, episode_reward=-73.70 +/- 9.53\n",
      "Episode length: 74.70 +/- 9.53\n",
      "Eval num_timesteps=946176, episode_reward=-71.60 +/- 8.50\n",
      "Episode length: 72.60 +/- 8.50\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00229 |       0.00000 |      41.40605 |       0.00103 |       0.08603\n",
      "      0.00233 |       0.00000 |      41.06909 |       0.00102 |       0.08622\n",
      "      0.00189 |       0.00000 |      40.97694 |       0.00101 |       0.08643\n",
      "      0.00221 |       0.00000 |      40.89472 |       0.00113 |       0.08667\n",
      "      0.00364 |       0.00000 |      40.85181 |       0.00121 |       0.08605\n",
      "      0.00280 |       0.00000 |      40.87330 |       0.00103 |       0.08658\n",
      "      0.00206 |       0.00000 |      40.65086 |       0.00107 |       0.08618\n",
      "      0.00167 |       0.00000 |      40.60812 |       0.00098 |       0.08642\n",
      "      0.00283 |       0.00000 |      40.56761 |       0.00094 |       0.08637\n",
      "      0.00218 |       0.00000 |      40.48940 |       0.00113 |       0.08615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating losses...\n",
      "      0.00228 |       0.00000 |      40.41426 |       0.00109 |       0.08647\n",
      "----------------------------------\n",
      "| EpLenMean       | 93.8         |\n",
      "| EpRewMean       | -92.8        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 10058        |\n",
      "| TimeElapsed     | 2.85e+03     |\n",
      "| TimestepsSoFar  | 950272       |\n",
      "| ev_tdlam_before | 0.827        |\n",
      "| loss_ent        | 0.08646973   |\n",
      "| loss_kl         | 0.0010907284 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0022791158 |\n",
      "| loss_vf_loss    | 40.41426     |\n",
      "----------------------------------\n",
      "********** Iteration 232 ************\n",
      "Eval num_timesteps=950272, episode_reward=-76.50 +/- 9.01\n",
      "Episode length: 77.50 +/- 9.01\n",
      "Eval num_timesteps=950272, episode_reward=-70.30 +/- 7.66\n",
      "Episode length: 71.30 +/- 7.66\n",
      "Eval num_timesteps=950272, episode_reward=-76.20 +/- 7.96\n",
      "Episode length: 77.20 +/- 7.96\n",
      "Eval num_timesteps=950272, episode_reward=-72.50 +/- 6.20\n",
      "Episode length: 73.50 +/- 6.20\n",
      "Eval num_timesteps=950272, episode_reward=-86.70 +/- 16.17\n",
      "Episode length: 87.70 +/- 16.17\n",
      "Eval num_timesteps=950272, episode_reward=-79.80 +/- 8.60\n",
      "Episode length: 80.80 +/- 8.60\n",
      "Eval num_timesteps=950272, episode_reward=-74.70 +/- 4.56\n",
      "Episode length: 75.70 +/- 4.56\n",
      "Eval num_timesteps=950272, episode_reward=-67.70 +/- 5.80\n",
      "Episode length: 68.70 +/- 5.80\n",
      "New best mean reward!\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00284 |       0.00000 |      32.52922 |       0.00128 |       0.09434\n",
      "      0.00275 |       0.00000 |      32.51241 |       0.00115 |       0.09458\n",
      "      0.00324 |       0.00000 |      32.42495 |       0.00115 |       0.09450\n",
      "      0.00308 |       0.00000 |      32.45123 |       0.00122 |       0.09476\n",
      "      0.00347 |       0.00000 |      32.29689 |       0.00107 |       0.09467\n",
      "      0.00303 |       0.00000 |      32.31430 |       0.00119 |       0.09436\n",
      "      0.00266 |       0.00000 |      32.47311 |       0.00115 |       0.09494\n",
      "      0.00339 |       0.00000 |      32.17496 |       0.00108 |       0.09429\n",
      "      0.00229 |       0.00000 |      32.19322 |       0.00117 |       0.09465\n",
      "      0.00286 |       0.00000 |      32.21719 |       0.00108 |       0.09421\n",
      "Evaluating losses...\n",
      "      0.00310 |       0.00000 |      32.25589 |       0.00122 |       0.09433\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.1         |\n",
      "| EpRewMean       | -89.1        |\n",
      "| EpThisIter      | 47           |\n",
      "| EpisodesSoFar   | 10105        |\n",
      "| TimeElapsed     | 2.86e+03     |\n",
      "| TimestepsSoFar  | 954368       |\n",
      "| ev_tdlam_before | 0.869        |\n",
      "| loss_ent        | 0.09433262   |\n",
      "| loss_kl         | 0.001217605  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0031031587 |\n",
      "| loss_vf_loss    | 32.25589     |\n",
      "----------------------------------\n",
      "********** Iteration 233 ************\n",
      "Eval num_timesteps=954368, episode_reward=-71.30 +/- 7.76\n",
      "Episode length: 72.30 +/- 7.76\n",
      "Eval num_timesteps=954368, episode_reward=-83.90 +/- 22.73\n",
      "Episode length: 84.90 +/- 22.73\n",
      "Eval num_timesteps=954368, episode_reward=-92.70 +/- 38.02\n",
      "Episode length: 93.70 +/- 38.02\n",
      "Eval num_timesteps=954368, episode_reward=-84.60 +/- 24.84\n",
      "Episode length: 85.60 +/- 24.84\n",
      "Eval num_timesteps=954368, episode_reward=-77.40 +/- 8.06\n",
      "Episode length: 78.40 +/- 8.06\n",
      "Eval num_timesteps=954368, episode_reward=-82.00 +/- 16.12\n",
      "Episode length: 83.00 +/- 16.12\n",
      "Eval num_timesteps=954368, episode_reward=-79.70 +/- 9.20\n",
      "Episode length: 80.70 +/- 9.20\n",
      "Eval num_timesteps=954368, episode_reward=-74.80 +/- 7.61\n",
      "Episode length: 75.80 +/- 7.61\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00255 |       0.00000 |      39.44946 |       0.00105 |       0.08902\n",
      "      0.00326 |       0.00000 |      39.37767 |       0.00107 |       0.08898\n",
      "      0.00229 |       0.00000 |      39.24164 |       0.00111 |       0.08930\n",
      "      0.00351 |       0.00000 |      39.30336 |       0.00108 |       0.08935\n",
      "      0.00254 |       0.00000 |      39.28158 |       0.00115 |       0.08875\n",
      "      0.00269 |       0.00000 |      39.22178 |       0.00132 |       0.08902\n",
      "      0.00292 |       0.00000 |      39.27069 |       0.00113 |       0.08922\n",
      "      0.00475 |       0.00000 |      38.97159 |       0.00123 |       0.08874\n",
      "      0.00227 |       0.00000 |      39.17572 |       0.00121 |       0.08932\n",
      "      0.00201 |       0.00000 |      39.10402 |       0.00110 |       0.08916\n",
      "Evaluating losses...\n",
      "      0.00270 |       0.00000 |      39.16062 |       0.00112 |       0.08929\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.4         |\n",
      "| EpRewMean       | -87.4        |\n",
      "| EpThisIter      | 45           |\n",
      "| EpisodesSoFar   | 10150        |\n",
      "| TimeElapsed     | 2.87e+03     |\n",
      "| TimestepsSoFar  | 958464       |\n",
      "| ev_tdlam_before | 0.838        |\n",
      "| loss_ent        | 0.08928984   |\n",
      "| loss_kl         | 0.0011239055 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0027020723 |\n",
      "| loss_vf_loss    | 39.160618    |\n",
      "----------------------------------\n",
      "********** Iteration 234 ************\n",
      "Eval num_timesteps=958464, episode_reward=-71.30 +/- 6.62\n",
      "Episode length: 72.30 +/- 6.62\n",
      "Eval num_timesteps=958464, episode_reward=-73.70 +/- 5.31\n",
      "Episode length: 74.70 +/- 5.31\n",
      "Eval num_timesteps=958464, episode_reward=-74.60 +/- 6.86\n",
      "Episode length: 75.60 +/- 6.86\n",
      "Eval num_timesteps=958464, episode_reward=-79.90 +/- 8.36\n",
      "Episode length: 80.90 +/- 8.36\n",
      "Eval num_timesteps=958464, episode_reward=-71.90 +/- 7.27\n",
      "Episode length: 72.90 +/- 7.27\n",
      "Eval num_timesteps=958464, episode_reward=-72.00 +/- 6.91\n",
      "Episode length: 73.00 +/- 6.91\n",
      "Eval num_timesteps=958464, episode_reward=-84.70 +/- 33.45\n",
      "Episode length: 85.70 +/- 33.45\n",
      "Eval num_timesteps=958464, episode_reward=-75.30 +/- 5.51\n",
      "Episode length: 76.30 +/- 5.51\n",
      "Eval num_timesteps=958464, episode_reward=-79.40 +/- 10.42\n",
      "Episode length: 80.40 +/- 10.42\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00304 |       0.00000 |      48.90442 |       0.00122 |       0.09267\n",
      "      0.00442 |       0.00000 |      48.79292 |       0.00118 |       0.09320\n",
      "      0.00256 |       0.00000 |      48.80248 |       0.00108 |       0.09303\n",
      "      0.00223 |       0.00000 |      48.76453 |       0.00121 |       0.09302\n",
      "      0.00269 |       0.00000 |      48.67879 |       0.00109 |       0.09337\n",
      "      0.00281 |       0.00000 |      48.69329 |       0.00102 |       0.09344\n",
      "      0.00301 |       0.00000 |      48.62591 |       0.00117 |       0.09337\n",
      "      0.00213 |       0.00000 |      48.54733 |       0.00124 |       0.09294\n",
      "      0.00372 |       0.00000 |      48.67171 |       0.00115 |       0.09324\n",
      "      0.00336 |       0.00000 |      48.71911 |       0.00097 |       0.09383\n",
      "Evaluating losses...\n",
      "      0.00313 |       0.00000 |      48.64706 |       0.00121 |       0.09358\n",
      "----------------------------------\n",
      "| EpLenMean       | 91.8         |\n",
      "| EpRewMean       | -90.8        |\n",
      "| EpThisIter      | 43           |\n",
      "| EpisodesSoFar   | 10193        |\n",
      "| TimeElapsed     | 2.88e+03     |\n",
      "| TimestepsSoFar  | 962560       |\n",
      "| ev_tdlam_before | 0.798        |\n",
      "| loss_ent        | 0.09357824   |\n",
      "| loss_kl         | 0.0012066672 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0031304015 |\n",
      "| loss_vf_loss    | 48.647057    |\n",
      "----------------------------------\n",
      "********** Iteration 235 ************\n",
      "Eval num_timesteps=962560, episode_reward=-76.80 +/- 11.57\n",
      "Episode length: 77.80 +/- 11.57\n",
      "Eval num_timesteps=962560, episode_reward=-73.00 +/- 12.55\n",
      "Episode length: 74.00 +/- 12.55\n",
      "Eval num_timesteps=962560, episode_reward=-73.30 +/- 7.96\n",
      "Episode length: 74.30 +/- 7.96\n",
      "Eval num_timesteps=962560, episode_reward=-74.80 +/- 8.77\n",
      "Episode length: 75.80 +/- 8.77\n",
      "Eval num_timesteps=962560, episode_reward=-91.50 +/- 24.25\n",
      "Episode length: 92.50 +/- 24.25\n",
      "Eval num_timesteps=962560, episode_reward=-78.50 +/- 7.85\n",
      "Episode length: 79.50 +/- 7.85\n",
      "Eval num_timesteps=962560, episode_reward=-74.40 +/- 7.55\n",
      "Episode length: 75.40 +/- 7.55\n",
      "Eval num_timesteps=962560, episode_reward=-73.50 +/- 9.19\n",
      "Episode length: 74.50 +/- 9.19\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00364 |       0.00000 |      26.45616 |       0.00128 |       0.09816\n",
      "      0.00406 |       0.00000 |      26.17482 |       0.00123 |       0.09840\n",
      "      0.00337 |       0.00000 |      26.03329 |       0.00099 |       0.09856\n",
      "      0.00368 |       0.00000 |      25.72221 |       0.00114 |       0.09822\n",
      "      0.00437 |       0.00000 |      25.50550 |       0.00132 |       0.09791\n",
      "      0.00313 |       0.00000 |      25.50866 |       0.00121 |       0.09817\n",
      "      0.00241 |       0.00000 |      25.29384 |       0.00096 |       0.09788\n",
      "      0.00273 |       0.00000 |      25.26061 |       0.00109 |       0.09803\n",
      "      0.00406 |       0.00000 |      25.08830 |       0.00128 |       0.09736\n",
      "      0.00275 |       0.00000 |      25.01003 |       0.00116 |       0.09850\n",
      "Evaluating losses...\n",
      "      0.00373 |       0.00000 |      25.06847 |       0.00119 |       0.09816\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.2         |\n",
      "| EpRewMean       | -85.2        |\n",
      "| EpThisIter      | 51           |\n",
      "| EpisodesSoFar   | 10244        |\n",
      "| TimeElapsed     | 2.89e+03     |\n",
      "| TimestepsSoFar  | 966656       |\n",
      "| ev_tdlam_before | 0.907        |\n",
      "| loss_ent        | 0.09815981   |\n",
      "| loss_kl         | 0.0011938119 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0037265983 |\n",
      "| loss_vf_loss    | 25.068468    |\n",
      "----------------------------------\n",
      "********** Iteration 236 ************\n",
      "Eval num_timesteps=966656, episode_reward=-81.10 +/- 16.40\n",
      "Episode length: 82.10 +/- 16.40\n",
      "Eval num_timesteps=966656, episode_reward=-69.50 +/- 4.67\n",
      "Episode length: 70.50 +/- 4.67\n",
      "Eval num_timesteps=966656, episode_reward=-76.70 +/- 16.15\n",
      "Episode length: 77.70 +/- 16.15\n",
      "Eval num_timesteps=966656, episode_reward=-72.40 +/- 8.05\n",
      "Episode length: 73.40 +/- 8.05\n",
      "Eval num_timesteps=966656, episode_reward=-76.60 +/- 5.66\n",
      "Episode length: 77.60 +/- 5.66\n",
      "Eval num_timesteps=966656, episode_reward=-71.60 +/- 12.89\n",
      "Episode length: 72.60 +/- 12.89\n",
      "Eval num_timesteps=966656, episode_reward=-72.30 +/- 5.10\n",
      "Episode length: 73.30 +/- 5.10\n",
      "Eval num_timesteps=966656, episode_reward=-73.60 +/- 7.02\n",
      "Episode length: 74.60 +/- 7.02\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00190 |       0.00000 |      53.74831 |       0.00094 |       0.08560\n",
      "      0.00332 |       0.00000 |      53.55718 |       0.00111 |       0.08556\n",
      "      0.00334 |       0.00000 |      53.35923 |       0.00125 |       0.08536\n",
      "      0.00290 |       0.00000 |      53.05264 |       0.00131 |       0.08600\n",
      "      0.00220 |       0.00000 |      53.09900 |       0.00093 |       0.08524\n",
      "      0.00212 |       0.00000 |      53.00336 |       0.00110 |       0.08569\n",
      "      0.00258 |       0.00000 |      52.92913 |       0.00105 |       0.08521\n",
      "      0.00230 |       0.00000 |      52.96999 |       0.00114 |       0.08571\n",
      "      0.00188 |       0.00000 |      52.69387 |       0.00110 |       0.08520\n",
      "      0.00291 |       0.00000 |      52.57539 |       0.00104 |       0.08531\n",
      "Evaluating losses...\n",
      "      0.00312 |       0.00000 |      52.44220 |       0.00113 |       0.08482\n",
      "----------------------------------\n",
      "| EpLenMean       | 90.3         |\n",
      "| EpRewMean       | -89.3        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 10286        |\n",
      "| TimeElapsed     | 2.9e+03      |\n",
      "| TimestepsSoFar  | 970752       |\n",
      "| ev_tdlam_before | 0.769        |\n",
      "| loss_ent        | 0.08481941   |\n",
      "| loss_kl         | 0.0011298072 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.003124816  |\n",
      "| loss_vf_loss    | 52.442196    |\n",
      "----------------------------------\n",
      "********** Iteration 237 ************\n",
      "Eval num_timesteps=970752, episode_reward=-75.50 +/- 8.31\n",
      "Episode length: 76.50 +/- 8.31\n",
      "Eval num_timesteps=970752, episode_reward=-73.00 +/- 7.51\n",
      "Episode length: 74.00 +/- 7.51\n",
      "Eval num_timesteps=970752, episode_reward=-75.00 +/- 12.24\n",
      "Episode length: 76.00 +/- 12.24\n",
      "Eval num_timesteps=970752, episode_reward=-82.20 +/- 18.10\n",
      "Episode length: 83.20 +/- 18.10\n",
      "Eval num_timesteps=970752, episode_reward=-76.20 +/- 6.69\n",
      "Episode length: 77.20 +/- 6.69\n",
      "Eval num_timesteps=970752, episode_reward=-70.80 +/- 9.59\n",
      "Episode length: 71.80 +/- 9.59\n",
      "Eval num_timesteps=970752, episode_reward=-79.60 +/- 14.73\n",
      "Episode length: 80.60 +/- 14.73\n",
      "Eval num_timesteps=970752, episode_reward=-76.40 +/- 10.12\n",
      "Episode length: 77.40 +/- 10.12\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00297 |       0.00000 |      42.16985 |       0.00108 |       0.09380\n",
      "      0.00341 |       0.00000 |      42.15727 |       0.00111 |       0.09324\n",
      "      0.00320 |       0.00000 |      42.12804 |       0.00117 |       0.09344\n",
      "      0.00268 |       0.00000 |      42.05503 |       0.00112 |       0.09356\n",
      "      0.00216 |       0.00000 |      42.18497 |       0.00109 |       0.09320\n",
      "      0.00208 |       0.00000 |      42.15347 |       0.00111 |       0.09348\n",
      "      0.00285 |       0.00000 |      42.16417 |       0.00122 |       0.09325\n",
      "      0.00422 |       0.00000 |      42.05738 |       0.00109 |       0.09383\n",
      "      0.00247 |       0.00000 |      42.02584 |       0.00123 |       0.09338\n",
      "      0.00235 |       0.00000 |      41.93591 |       0.00122 |       0.09322\n",
      "Evaluating losses...\n",
      "      0.00306 |       0.00000 |      42.07390 |       0.00109 |       0.09308\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.7         |\n",
      "| EpRewMean       | -91.7        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 10332        |\n",
      "| TimeElapsed     | 2.91e+03     |\n",
      "| TimestepsSoFar  | 974848       |\n",
      "| ev_tdlam_before | 0.827        |\n",
      "| loss_ent        | 0.093076915  |\n",
      "| loss_kl         | 0.0010921342 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.003060408  |\n",
      "| loss_vf_loss    | 42.0739      |\n",
      "----------------------------------\n",
      "********** Iteration 238 ************\n",
      "Eval num_timesteps=974848, episode_reward=-82.50 +/- 24.45\n",
      "Episode length: 83.50 +/- 24.45\n",
      "Eval num_timesteps=974848, episode_reward=-87.90 +/- 22.59\n",
      "Episode length: 88.90 +/- 22.59\n",
      "Eval num_timesteps=974848, episode_reward=-67.80 +/- 5.42\n",
      "Episode length: 68.80 +/- 5.42\n",
      "Eval num_timesteps=974848, episode_reward=-77.20 +/- 9.60\n",
      "Episode length: 78.20 +/- 9.60\n",
      "Eval num_timesteps=974848, episode_reward=-76.10 +/- 4.35\n",
      "Episode length: 77.10 +/- 4.35\n",
      "Eval num_timesteps=974848, episode_reward=-87.40 +/- 28.08\n",
      "Episode length: 88.40 +/- 28.08\n",
      "Eval num_timesteps=974848, episode_reward=-73.60 +/- 7.24\n",
      "Episode length: 74.60 +/- 7.24\n",
      "Eval num_timesteps=974848, episode_reward=-82.10 +/- 12.30\n",
      "Episode length: 83.10 +/- 12.30\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00258 |       0.00000 |      33.70116 |       0.00104 |       0.09207\n",
      "      0.00370 |       0.00000 |      33.62990 |       0.00105 |       0.09174\n",
      "      0.00218 |       0.00000 |      33.65472 |       0.00100 |       0.09215\n",
      "      0.00377 |       0.00000 |      33.66162 |       0.00110 |       0.09182\n",
      "      0.00377 |       0.00000 |      33.68848 |       0.00118 |       0.09211\n",
      "      0.00244 |       0.00000 |      33.55433 |       0.00097 |       0.09250\n",
      "      0.00364 |       0.00000 |      33.59893 |       0.00115 |       0.09208\n",
      "      0.00238 |       0.00000 |      33.57626 |       0.00101 |       0.09214\n",
      "      0.00271 |       0.00000 |      33.60530 |       0.00124 |       0.09230\n",
      "      0.00286 |       0.00000 |      33.55299 |       0.00117 |       0.09213\n",
      "Evaluating losses...\n",
      "      0.00308 |       0.00000 |      33.41559 |       0.00108 |       0.09251\n",
      "---------------------------------\n",
      "| EpLenMean       | 89.1        |\n",
      "| EpRewMean       | -88.1       |\n",
      "| EpThisIter      | 47          |\n",
      "| EpisodesSoFar   | 10379       |\n",
      "| TimeElapsed     | 2.92e+03    |\n",
      "| TimestepsSoFar  | 978944      |\n",
      "| ev_tdlam_before | 0.867       |\n",
      "| loss_ent        | 0.09251421  |\n",
      "| loss_kl         | 0.001076517 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | 0.003083319 |\n",
      "| loss_vf_loss    | 33.415592   |\n",
      "---------------------------------\n",
      "********** Iteration 239 ************\n",
      "Eval num_timesteps=978944, episode_reward=-74.80 +/- 5.74\n",
      "Episode length: 75.80 +/- 5.74\n",
      "Eval num_timesteps=978944, episode_reward=-75.80 +/- 7.33\n",
      "Episode length: 76.80 +/- 7.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=978944, episode_reward=-82.90 +/- 27.20\n",
      "Episode length: 83.90 +/- 27.20\n",
      "Eval num_timesteps=978944, episode_reward=-82.60 +/- 11.55\n",
      "Episode length: 83.60 +/- 11.55\n",
      "Eval num_timesteps=978944, episode_reward=-74.30 +/- 7.81\n",
      "Episode length: 75.30 +/- 7.81\n",
      "Eval num_timesteps=978944, episode_reward=-74.90 +/- 9.19\n",
      "Episode length: 75.90 +/- 9.19\n",
      "Eval num_timesteps=978944, episode_reward=-76.60 +/- 10.99\n",
      "Episode length: 77.60 +/- 10.99\n",
      "Eval num_timesteps=978944, episode_reward=-71.00 +/- 9.02\n",
      "Episode length: 72.00 +/- 9.02\n",
      "Eval num_timesteps=978944, episode_reward=-72.20 +/- 5.55\n",
      "Episode length: 73.20 +/- 5.55\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00407 |       0.00000 |      57.07607 |       0.00109 |       0.08172\n",
      "      0.00210 |       0.00000 |      56.82655 |       0.00108 |       0.08109\n",
      "      0.00368 |       0.00000 |      56.66529 |       0.00115 |       0.08137\n",
      "      0.00306 |       0.00000 |      56.29812 |       0.00103 |       0.08129\n",
      "      0.00323 |       0.00000 |      55.83210 |       0.00118 |       0.08089\n",
      "      0.00263 |       0.00000 |      55.72747 |       0.00112 |       0.08098\n",
      "      0.00326 |       0.00000 |      55.65235 |       0.00101 |       0.08106\n",
      "      0.00231 |       0.00000 |      55.33492 |       0.00104 |       0.08159\n",
      "      0.00249 |       0.00000 |      55.24976 |       0.00108 |       0.08112\n",
      "      0.00288 |       0.00000 |      55.10352 |       0.00104 |       0.08119\n",
      "Evaluating losses...\n",
      "      0.00235 |       0.00000 |      54.86226 |       0.00095 |       0.08128\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.7          |\n",
      "| EpRewMean       | -93.7         |\n",
      "| EpThisIter      | 39            |\n",
      "| EpisodesSoFar   | 10418         |\n",
      "| TimeElapsed     | 2.93e+03      |\n",
      "| TimestepsSoFar  | 983040        |\n",
      "| ev_tdlam_before | 0.756         |\n",
      "| loss_ent        | 0.081279956   |\n",
      "| loss_kl         | 0.00095018494 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.0023503974  |\n",
      "| loss_vf_loss    | 54.862263     |\n",
      "-----------------------------------\n",
      "********** Iteration 240 ************\n",
      "Eval num_timesteps=983040, episode_reward=-75.70 +/- 9.10\n",
      "Episode length: 76.70 +/- 9.10\n",
      "Eval num_timesteps=983040, episode_reward=-77.90 +/- 7.31\n",
      "Episode length: 78.90 +/- 7.31\n",
      "Eval num_timesteps=983040, episode_reward=-84.60 +/- 26.97\n",
      "Episode length: 85.60 +/- 26.97\n",
      "Eval num_timesteps=983040, episode_reward=-81.50 +/- 25.03\n",
      "Episode length: 82.50 +/- 25.03\n",
      "Eval num_timesteps=983040, episode_reward=-78.20 +/- 8.70\n",
      "Episode length: 79.20 +/- 8.70\n",
      "Eval num_timesteps=983040, episode_reward=-71.20 +/- 5.21\n",
      "Episode length: 72.20 +/- 5.21\n",
      "Eval num_timesteps=983040, episode_reward=-83.50 +/- 25.94\n",
      "Episode length: 84.50 +/- 25.94\n",
      "Eval num_timesteps=983040, episode_reward=-82.70 +/- 17.77\n",
      "Episode length: 83.70 +/- 17.77\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00242 |       0.00000 |      51.07994 |       0.00090 |       0.08170\n",
      "      0.00190 |       0.00000 |      51.06890 |       0.00109 |       0.08125\n",
      "      0.00224 |       0.00000 |      50.82101 |       0.00103 |       0.08134\n",
      "      0.00230 |       0.00000 |      50.84676 |       0.00092 |       0.08149\n",
      "      0.00288 |       0.00000 |      50.84733 |       0.00097 |       0.08163\n",
      "      0.00219 |       0.00000 |      50.61634 |       0.00091 |       0.08145\n",
      "      0.00225 |       0.00000 |      50.61924 |       0.00095 |       0.08124\n",
      "      0.00196 |       0.00000 |      50.34675 |       0.00091 |       0.08139\n",
      "      0.00172 |       0.00000 |      50.50621 |       0.00093 |       0.08128\n",
      "      0.00308 |       0.00000 |      50.25061 |       0.00105 |       0.08131\n",
      "Evaluating losses...\n",
      "      0.00210 |       0.00000 |      50.59515 |       0.00101 |       0.08141\n",
      "----------------------------------\n",
      "| EpLenMean       | 100          |\n",
      "| EpRewMean       | -99          |\n",
      "| EpThisIter      | 40           |\n",
      "| EpisodesSoFar   | 10458        |\n",
      "| TimeElapsed     | 2.94e+03     |\n",
      "| TimestepsSoFar  | 987136       |\n",
      "| ev_tdlam_before | 0.784        |\n",
      "| loss_ent        | 0.08140902   |\n",
      "| loss_kl         | 0.0010093448 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0020974267 |\n",
      "| loss_vf_loss    | 50.595146    |\n",
      "----------------------------------\n",
      "********** Iteration 241 ************\n",
      "Eval num_timesteps=987136, episode_reward=-71.20 +/- 6.90\n",
      "Episode length: 72.20 +/- 6.90\n",
      "Eval num_timesteps=987136, episode_reward=-76.00 +/- 7.97\n",
      "Episode length: 77.00 +/- 7.97\n",
      "Eval num_timesteps=987136, episode_reward=-88.70 +/- 42.86\n",
      "Episode length: 89.70 +/- 42.86\n",
      "Eval num_timesteps=987136, episode_reward=-73.30 +/- 3.87\n",
      "Episode length: 74.30 +/- 3.87\n",
      "Eval num_timesteps=987136, episode_reward=-75.00 +/- 9.55\n",
      "Episode length: 76.00 +/- 9.55\n",
      "Eval num_timesteps=987136, episode_reward=-72.30 +/- 5.97\n",
      "Episode length: 73.30 +/- 5.97\n",
      "Eval num_timesteps=987136, episode_reward=-76.80 +/- 12.25\n",
      "Episode length: 77.80 +/- 12.25\n",
      "Eval num_timesteps=987136, episode_reward=-80.80 +/- 6.95\n",
      "Episode length: 81.80 +/- 6.95\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00217 |       0.00000 |      29.56425 |       0.00121 |       0.09110\n",
      "      0.00235 |       0.00000 |      29.47715 |       0.00109 |       0.09126\n",
      "      0.00348 |       0.00000 |      29.52046 |       0.00103 |       0.09066\n",
      "      0.00297 |       0.00000 |      29.46629 |       0.00096 |       0.09049\n",
      "      0.00279 |       0.00000 |      29.42065 |       0.00103 |       0.09116\n",
      "      0.00232 |       0.00000 |      29.34003 |       0.00109 |       0.09025\n",
      "      0.00318 |       0.00000 |      29.58164 |       0.00108 |       0.09056\n",
      "      0.00342 |       0.00000 |      29.42024 |       0.00099 |       0.09070\n",
      "      0.00240 |       0.00000 |      29.38573 |       0.00100 |       0.09071\n",
      "      0.00258 |       0.00000 |      29.34905 |       0.00106 |       0.09132\n",
      "Evaluating losses...\n",
      "      0.00348 |       0.00000 |      29.45036 |       0.00097 |       0.09150\n",
      "-----------------------------------\n",
      "| EpLenMean       | 94.9          |\n",
      "| EpRewMean       | -93.9         |\n",
      "| EpThisIter      | 48            |\n",
      "| EpisodesSoFar   | 10506         |\n",
      "| TimeElapsed     | 2.95e+03      |\n",
      "| TimestepsSoFar  | 991232        |\n",
      "| ev_tdlam_before | 0.885         |\n",
      "| loss_ent        | 0.09149577    |\n",
      "| loss_kl         | 0.00096540764 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | 0.0034776744  |\n",
      "| loss_vf_loss    | 29.450356     |\n",
      "-----------------------------------\n",
      "********** Iteration 242 ************\n",
      "Eval num_timesteps=991232, episode_reward=-72.90 +/- 6.64\n",
      "Episode length: 73.90 +/- 6.64\n",
      "Eval num_timesteps=991232, episode_reward=-74.40 +/- 9.45\n",
      "Episode length: 75.40 +/- 9.45\n",
      "Eval num_timesteps=991232, episode_reward=-74.40 +/- 15.19\n",
      "Episode length: 75.40 +/- 15.19\n",
      "Eval num_timesteps=991232, episode_reward=-79.80 +/- 8.72\n",
      "Episode length: 80.80 +/- 8.72\n",
      "Eval num_timesteps=991232, episode_reward=-80.20 +/- 7.95\n",
      "Episode length: 81.20 +/- 7.95\n",
      "Eval num_timesteps=991232, episode_reward=-74.70 +/- 9.13\n",
      "Episode length: 75.70 +/- 9.13\n",
      "Eval num_timesteps=991232, episode_reward=-85.10 +/- 35.62\n",
      "Episode length: 86.10 +/- 35.62\n",
      "Eval num_timesteps=991232, episode_reward=-73.60 +/- 8.79\n",
      "Episode length: 74.60 +/- 8.79\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00329 |       0.00000 |      31.99631 |       0.00114 |       0.09473\n",
      "      0.00406 |       0.00000 |      31.95735 |       0.00120 |       0.09538\n",
      "      0.00457 |       0.00000 |      32.07312 |       0.00127 |       0.09484\n",
      "      0.00223 |       0.00000 |      31.88764 |       0.00107 |       0.09466\n",
      "      0.00372 |       0.00000 |      31.92660 |       0.00113 |       0.09514\n",
      "      0.00333 |       0.00000 |      31.99036 |       0.00108 |       0.09546\n",
      "      0.00308 |       0.00000 |      31.92228 |       0.00120 |       0.09457\n",
      "      0.00333 |       0.00000 |      32.02818 |       0.00113 |       0.09477\n",
      "      0.00404 |       0.00000 |      31.86387 |       0.00104 |       0.09486\n",
      "      0.00252 |       0.00000 |      31.92427 |       0.00098 |       0.09510\n",
      "Evaluating losses...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00320 |       0.00000 |      31.86111 |       0.00117 |       0.09523\n",
      "----------------------------------\n",
      "| EpLenMean       | 86.7         |\n",
      "| EpRewMean       | -85.7        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 10552        |\n",
      "| TimeElapsed     | 2.96e+03     |\n",
      "| TimestepsSoFar  | 995328       |\n",
      "| ev_tdlam_before | 0.873        |\n",
      "| loss_ent        | 0.09522954   |\n",
      "| loss_kl         | 0.0011652657 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0032013634 |\n",
      "| loss_vf_loss    | 31.861115    |\n",
      "----------------------------------\n",
      "********** Iteration 243 ************\n",
      "Eval num_timesteps=995328, episode_reward=-76.50 +/- 10.06\n",
      "Episode length: 77.50 +/- 10.06\n",
      "Eval num_timesteps=995328, episode_reward=-73.30 +/- 10.66\n",
      "Episode length: 74.30 +/- 10.66\n",
      "Eval num_timesteps=995328, episode_reward=-72.60 +/- 6.26\n",
      "Episode length: 73.60 +/- 6.26\n",
      "Eval num_timesteps=995328, episode_reward=-74.90 +/- 8.86\n",
      "Episode length: 75.90 +/- 8.86\n",
      "Eval num_timesteps=995328, episode_reward=-90.00 +/- 34.76\n",
      "Episode length: 91.00 +/- 34.76\n",
      "Eval num_timesteps=995328, episode_reward=-73.40 +/- 7.61\n",
      "Episode length: 74.40 +/- 7.61\n",
      "Eval num_timesteps=995328, episode_reward=-85.70 +/- 27.53\n",
      "Episode length: 86.70 +/- 27.53\n",
      "Eval num_timesteps=995328, episode_reward=-77.90 +/- 7.18\n",
      "Episode length: 78.90 +/- 7.18\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00344 |       0.00000 |      39.14128 |       0.00117 |       0.09190\n",
      "      0.00422 |       0.00000 |      39.19477 |       0.00115 |       0.09205\n",
      "      0.00276 |       0.00000 |      39.07982 |       0.00113 |       0.09189\n",
      "      0.00382 |       0.00000 |      39.21201 |       0.00128 |       0.09215\n",
      "      0.00361 |       0.00000 |      39.12035 |       0.00118 |       0.09218\n",
      "      0.00339 |       0.00000 |      39.17374 |       0.00106 |       0.09189\n",
      "      0.00343 |       0.00000 |      38.93779 |       0.00106 |       0.09231\n",
      "      0.00417 |       0.00000 |      39.30697 |       0.00123 |       0.09242\n",
      "      0.00376 |       0.00000 |      39.13928 |       0.00118 |       0.09190\n",
      "      0.00322 |       0.00000 |      39.16925 |       0.00132 |       0.09145\n",
      "Evaluating losses...\n",
      "      0.00447 |       0.00000 |      39.09831 |       0.00112 |       0.09231\n",
      "----------------------------------\n",
      "| EpLenMean       | 88.8         |\n",
      "| EpRewMean       | -87.8        |\n",
      "| EpThisIter      | 46           |\n",
      "| EpisodesSoFar   | 10598        |\n",
      "| TimeElapsed     | 2.97e+03     |\n",
      "| TimestepsSoFar  | 999424       |\n",
      "| ev_tdlam_before | 0.841        |\n",
      "| loss_ent        | 0.09230931   |\n",
      "| loss_kl         | 0.0011231365 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0044680145 |\n",
      "| loss_vf_loss    | 39.098312    |\n",
      "----------------------------------\n",
      "********** Iteration 244 ************\n",
      "Eval num_timesteps=999424, episode_reward=-72.30 +/- 9.09\n",
      "Episode length: 73.30 +/- 9.09\n",
      "Eval num_timesteps=999424, episode_reward=-75.50 +/- 16.20\n",
      "Episode length: 76.50 +/- 16.20\n",
      "Eval num_timesteps=999424, episode_reward=-91.00 +/- 45.31\n",
      "Episode length: 92.00 +/- 45.31\n",
      "Eval num_timesteps=999424, episode_reward=-85.10 +/- 34.08\n",
      "Episode length: 86.10 +/- 34.08\n",
      "Eval num_timesteps=999424, episode_reward=-76.00 +/- 9.90\n",
      "Episode length: 77.00 +/- 9.90\n",
      "Eval num_timesteps=999424, episode_reward=-75.30 +/- 5.44\n",
      "Episode length: 76.30 +/- 5.44\n",
      "Eval num_timesteps=999424, episode_reward=-85.10 +/- 15.53\n",
      "Episode length: 86.10 +/- 15.53\n",
      "Eval num_timesteps=999424, episode_reward=-69.70 +/- 5.24\n",
      "Episode length: 70.70 +/- 5.24\n",
      "Eval num_timesteps=999424, episode_reward=-83.00 +/- 17.65\n",
      "Episode length: 84.00 +/- 17.65\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00345 |       0.00000 |      56.27845 |       0.00138 |       0.09072\n",
      "      0.00373 |       0.00000 |      56.28173 |       0.00107 |       0.09041\n",
      "      0.00320 |       0.00000 |      56.14001 |       0.00117 |       0.09027\n",
      "      0.00419 |       0.00000 |      56.33330 |       0.00117 |       0.09095\n",
      "      0.00339 |       0.00000 |      56.07658 |       0.00119 |       0.09047\n",
      "      0.00250 |       0.00000 |      56.29254 |       0.00112 |       0.09071\n",
      "      0.00485 |       0.00000 |      56.36076 |       0.00112 |       0.09030\n",
      "      0.00298 |       0.00000 |      56.57084 |       0.00104 |       0.09002\n",
      "      0.00336 |       0.00000 |      56.40602 |       0.00106 |       0.09047\n",
      "      0.00318 |       0.00000 |      56.16403 |       0.00108 |       0.09057\n",
      "Evaluating losses...\n",
      "      0.00605 |       0.00000 |      56.30248 |       0.00106 |       0.09051\n",
      "----------------------------------\n",
      "| EpLenMean       | 92.7         |\n",
      "| EpRewMean       | -91.7        |\n",
      "| EpThisIter      | 42           |\n",
      "| EpisodesSoFar   | 10640        |\n",
      "| TimeElapsed     | 2.98e+03     |\n",
      "| TimestepsSoFar  | 1003520      |\n",
      "| ev_tdlam_before | 0.765        |\n",
      "| loss_ent        | 0.09051315   |\n",
      "| loss_kl         | 0.0010646507 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | 0.0060462467 |\n",
      "| loss_vf_loss    | 56.302483    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "bnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "bnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: -74.812, Std reward: 17.62165304391163\n",
      "BNN - Mean reward: -77.856, Std reward: 19.479303478307433\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    }
   ],
   "source": [
    "model = PPO1.load(os.path.join(LOGDIR, \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"acrobot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EpThisIter</th>\n",
       "      <th>loss_pol_surr</th>\n",
       "      <th>loss_kl</th>\n",
       "      <th>loss_ent</th>\n",
       "      <th>loss_vf_loss</th>\n",
       "      <th>EpisodesSoFar</th>\n",
       "      <th>EpLenMean</th>\n",
       "      <th>TimeElapsed</th>\n",
       "      <th>EpRewMean</th>\n",
       "      <th>ev_tdlam_before</th>\n",
       "      <th>loss_pol_entpen</th>\n",
       "      <th>TimestepsSoFar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.005324</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>1.090146</td>\n",
       "      <td>5.878858</td>\n",
       "      <td>8</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.712777</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>-0.082879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>1.077161</td>\n",
       "      <td>15.449282</td>\n",
       "      <td>16</td>\n",
       "      <td>500.0</td>\n",
       "      <td>47.949600</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>-0.002488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.004511</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>1.081922</td>\n",
       "      <td>24.582613</td>\n",
       "      <td>24</td>\n",
       "      <td>500.0</td>\n",
       "      <td>70.992346</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>-0.019004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.002965</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>34.672935</td>\n",
       "      <td>32</td>\n",
       "      <td>500.0</td>\n",
       "      <td>96.100585</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>-0.003665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>1.076001</td>\n",
       "      <td>45.151184</td>\n",
       "      <td>40</td>\n",
       "      <td>500.0</td>\n",
       "      <td>121.560204</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EpThisIter  loss_pol_surr   loss_kl  loss_ent  loss_vf_loss  EpisodesSoFar  \\\n",
       "0           8      -0.005324  0.008554  1.090146      5.878858              8   \n",
       "1           8      -0.007219  0.009749  1.077161     15.449282             16   \n",
       "2           8      -0.004511  0.007344  1.081922     24.582613             24   \n",
       "3           8      -0.002965  0.004289  1.074305     34.672935             32   \n",
       "4           8      -0.005070  0.006862  1.076001     45.151184             40   \n",
       "\n",
       "   EpLenMean  TimeElapsed  EpRewMean  ev_tdlam_before  loss_pol_entpen  \\\n",
       "0      500.0    19.712777     -500.0        -0.082879              0.0   \n",
       "1      500.0    47.949600     -500.0        -0.002488              0.0   \n",
       "2      500.0    70.992346     -500.0        -0.019004              0.0   \n",
       "3      500.0    96.100585     -500.0        -0.003665              0.0   \n",
       "4      500.0   121.560204     -500.0         0.000535              0.0   \n",
       "\n",
       "   TimestepsSoFar  \n",
       "0            4096  \n",
       "1            8192  \n",
       "2           12288  \n",
       "3           16384  \n",
       "4           20480  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "df = pd.read_csv(LOGDIR+\"/progress.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['loss_kl', 'loss_vf_loss', 'EpRewMean', 'EpThisIter', 'ev_tdlam_before', 'loss_ent']\n",
    "\n",
    "nrow = 2\n",
    "ncol = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol)\n",
    "i = 0\n",
    "for r in range(nrow):\n",
    "    for c in range(ncol):\n",
    "        df[columns[i]].plot(ax=axes[r,c], title=columns[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"bnn_acrobot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EpThisIter</th>\n",
       "      <th>loss_pol_surr</th>\n",
       "      <th>loss_kl</th>\n",
       "      <th>loss_ent</th>\n",
       "      <th>loss_vf_loss</th>\n",
       "      <th>EpisodesSoFar</th>\n",
       "      <th>EpLenMean</th>\n",
       "      <th>TimeElapsed</th>\n",
       "      <th>EpRewMean</th>\n",
       "      <th>ev_tdlam_before</th>\n",
       "      <th>loss_pol_entpen</th>\n",
       "      <th>TimestepsSoFar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.004090</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>1.093306</td>\n",
       "      <td>94.374830</td>\n",
       "      <td>8</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>44.027443</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.002445</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>1.095831</td>\n",
       "      <td>16.246672</td>\n",
       "      <td>16</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>85.937326</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>0.055033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.007889</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>1.081282</td>\n",
       "      <td>24.980585</td>\n",
       "      <td>24</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>126.887491</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>0.240639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>1.056307</td>\n",
       "      <td>38.121216</td>\n",
       "      <td>32</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>169.156316</td>\n",
       "      <td>-496.750000</td>\n",
       "      <td>0.284677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.003610</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>1.024103</td>\n",
       "      <td>37.536053</td>\n",
       "      <td>42</td>\n",
       "      <td>483.142857</td>\n",
       "      <td>209.258473</td>\n",
       "      <td>-482.738095</td>\n",
       "      <td>0.350252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EpThisIter  loss_pol_surr   loss_kl  loss_ent  loss_vf_loss  EpisodesSoFar  \\\n",
       "0           8      -0.004090  0.005345  1.093306     94.374830              8   \n",
       "1           8      -0.002445  0.001788  1.095831     16.246672             16   \n",
       "2           8      -0.007889  0.009487  1.081282     24.980585             24   \n",
       "3           8      -0.006306  0.006660  1.056307     38.121216             32   \n",
       "4          10      -0.003610  0.006880  1.024103     37.536053             42   \n",
       "\n",
       "    EpLenMean  TimeElapsed   EpRewMean  ev_tdlam_before  loss_pol_entpen  \\\n",
       "0  500.000000    44.027443 -500.000000        -0.000174              0.0   \n",
       "1  500.000000    85.937326 -500.000000         0.055033              0.0   \n",
       "2  500.000000   126.887491 -500.000000         0.240639              0.0   \n",
       "3  497.000000   169.156316 -496.750000         0.284677              0.0   \n",
       "4  483.142857   209.258473 -482.738095         0.350252              0.0   \n",
       "\n",
       "   TimestepsSoFar  \n",
       "0            4096  \n",
       "1            8192  \n",
       "2           12288  \n",
       "3           16384  \n",
       "4           20480  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "df = pd.read_csv(LOGDIR+\"/progress.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['loss_kl', 'loss_vf_loss', 'EpRewMean', 'EpThisIter', 'ev_tdlam_before', 'loss_ent']\n",
    "\n",
    "nrow = 2\n",
    "ncol = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol)\n",
    "i = 0\n",
    "for r in range(nrow):\n",
    "    for c in range(ncol):\n",
    "        df[columns[i]].plot(ax=axes[r,c], title=columns[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"bnn_acrobot\"\n",
    "LOGDIR1 = \"acrobot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-78bf1476023d>:46: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/stephen/.local/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/stephen/.local/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/stephen/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:162: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:190: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/stephen/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "bnn = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "dnn = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "bnn.load_parameters(os.path.join(LOGDIR, \"final_model\"))\n",
    "dnn.load_parameters(os.path.join(LOGDIR1, \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 156.935, Std reward: 48.66798511341928\n",
      "BNN - Mean reward: 189.304, Std reward: 18.416991719605022\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 161.503, Std reward: 47.323144348193935\n",
      "BNN - Mean reward: 178.989, Std reward: 32.992982268961384\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import IPython\n",
    "import imageio\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "def record_game(model, env, num_episodes=5, video_filename='video.mp4'):\n",
    "    with imageio.get_writer(video_filename, fps=60) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            while not done:\n",
    "                action, _steps = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            print(\"score:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1000, 1000) to (1008, 1008) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -99.0\n",
      "score: -70.0\n",
      "score: -80.0\n",
      "score: -71.0\n",
      "score: -71.0\n"
     ]
    }
   ],
   "source": [
    "record_game(bnn, env= bnn.get_env(), video_filename=\"bnn_acrobot.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1000, 1000) to (1008, 1008) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -90.0\n",
      "score: -64.0\n",
      "score: -84.0\n",
      "score: -69.0\n",
      "score: -71.0\n"
     ]
    }
   ],
   "source": [
    "record_game(dnn, env= bnn.get_env(), video_filename=\"dnn_acrobot.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99984574, -0.01756392,  0.99954103, -0.03029402, -0.02566405,\n",
       "        0.07983799])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleStates1 = []\n",
    "actionProbs1 = []\n",
    "actionTaken1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "threshold = 0.05# to get a random sample of states \n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "epsilon = np.random.rand()\n",
    "while not done:\n",
    "    action, _steps = model.predict(obs)\n",
    "    epsilon = np.random.rand()\n",
    "    if epsilon < threshold:\n",
    "        sampleStates.append(obs)\n",
    "        for i in range(samples):\n",
    "        x = model.action_probability(obs)\n",
    "        actionProbs.append()\n",
    "        actionTaken.append(action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.92475341,  0.38056686,  0.68388995, -0.72958518,  0.31367297,\n",
       "        -1.16466659]),\n",
       " array([ 0.38541505, -0.92274332,  0.42250234,  0.90636184, -2.29990768,\n",
       "         4.54188642]),\n",
       " array([ 0.09719838, -0.99526503, -0.50891873,  0.86081457,  2.73971728,\n",
       "         0.84946119]),\n",
       " array([ 0.99073933,  0.13577766,  0.2635823 ,  0.96463691,  4.68730119,\n",
       "        -4.93407587])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9798788 , 0.00418431, 0.015937  ], dtype=float32),\n",
       " array([0.00269753, 0.00695524, 0.99034715], dtype=float32),\n",
       " array([0.6481798 , 0.28297842, 0.06884169], dtype=float32),\n",
       " array([9.9926037e-01, 6.7839486e-04, 6.1266510e-05], dtype=float32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionTaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<AcrobotEnv<Acrobot-v1>>>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Action Probability Distribution where action taken is 0'),\n",
       " Text(0, 0.5, 'Probability')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "ax = sns.distplot(actionProbs[i],\n",
    "                 bins = env.action_space.n,\n",
    "                 kde = True,\n",
    "                 color = 'skyblue',\n",
    "                 hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel = f'Action Probability Distribution where action taken is {actionTaken[i]}', ylabel = 'Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00269753, 0.00695524, 0.99034715], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionProbs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = sampleStates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None)]\n"
     ]
    }
   ],
   "source": [
    "something = []\n",
    "for i in range(1000):\n",
    "    something.append(model.predict(sampling, deterministic=False))\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9798788 , 0.00418431, 0.015937  ], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.action_probability(sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (2, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (1, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (2, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None), (0, None)]\n"
     ]
    }
   ],
   "source": [
    "something2 = []\n",
    "for i in range(1000):\n",
    "    something.append(m2.predict(sampling, deterministic=False))\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9659492 , 0.00875428, 0.02529657], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.action_probability(sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
