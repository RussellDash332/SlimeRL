{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseFlipout(layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseFlipout(pi_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseFlipout(vf_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import ActorCriticPolicy, nature_cnn\n",
    "\n",
    "class FeedForwardPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy\n",
    "        (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor\n",
    "        documentation for details).\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, net_arch=None,\n",
    "                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            if feature_extraction == \"cnn\":\n",
    "                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "            elif feature_extraction == \"bnn\":\n",
    "                pi_latent, vf_latent = bnn_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "            else:\n",
    "                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import zip_longest\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.distributions import make_proba_dist_type, CategoricalProbabilityDistribution, \\\n",
    "    MultiCategoricalProbabilityDistribution, DiagGaussianProbabilityDistribution, BernoulliProbabilityDistribution\n",
    "from stable_baselines.common.input import observation_input\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a Bayesian neural net (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to dnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"dnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7feef4951b90>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.01267 |       0.00000 |      88.47886 |       0.00464 |       0.68853\n",
      "     -0.01930 |       0.00000 |      75.37668 |       0.00829 |       0.68489\n",
      "     -0.02108 |       0.00000 |      48.22500 |       0.00851 |       0.68467\n",
      "     -0.02384 |       0.00000 |      24.29067 |       0.00906 |       0.68411\n",
      "     -0.02655 |       0.00000 |      16.54516 |       0.01067 |       0.68253\n",
      "     -0.02866 |       0.00000 |      14.83322 |       0.01187 |       0.68134\n",
      "     -0.02999 |       0.00000 |      13.54036 |       0.01312 |       0.68011\n",
      "     -0.03081 |       0.00000 |      12.16950 |       0.01363 |       0.67961\n",
      "     -0.03152 |       0.00000 |      11.04709 |       0.01363 |       0.67961\n",
      "     -0.03157 |       0.00000 |      10.27760 |       0.01436 |       0.67889\n",
      "Evaluating losses...\n",
      "     -0.03215 |       0.00000 |       9.98238 |       0.01322 |       0.68001\n",
      "----------------------------------\n",
      "| EpLenMean       | 23           |\n",
      "| EpRewMean       | 23           |\n",
      "| EpThisIter      | 180          |\n",
      "| EpisodesSoFar   | 180          |\n",
      "| TimeElapsed     | 3.47         |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | -0.000186    |\n",
      "| loss_ent        | 0.68000704   |\n",
      "| loss_kl         | 0.0132185705 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.032153107 |\n",
      "| loss_vf_loss    | 9.982385     |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.01070 |       0.00000 |      53.99833 |       0.00290 |       0.66845\n",
      "     -0.01583 |       0.00000 |      39.11604 |       0.00314 |       0.66623\n",
      "     -0.01733 |       0.00000 |      35.76246 |       0.00364 |       0.66488\n",
      "     -0.01809 |       0.00000 |      33.85520 |       0.00414 |       0.66357\n",
      "     -0.01853 |       0.00000 |      32.40782 |       0.00447 |       0.66282\n",
      "     -0.01884 |       0.00000 |      31.26832 |       0.00492 |       0.66176\n",
      "     -0.01945 |       0.00000 |      30.35987 |       0.00532 |       0.66086\n",
      "     -0.01972 |       0.00000 |      29.57225 |       0.00553 |       0.66044\n",
      "     -0.01991 |       0.00000 |      28.86039 |       0.00556 |       0.66047\n",
      "     -0.02022 |       0.00000 |      28.19552 |       0.00578 |       0.65997\n",
      "Evaluating losses...\n",
      "     -0.02032 |       0.00000 |      27.84371 |       0.00515 |       0.66153\n",
      "----------------------------------\n",
      "| EpLenMean       | 32.7         |\n",
      "| EpRewMean       | 32.7         |\n",
      "| EpThisIter      | 121          |\n",
      "| EpisodesSoFar   | 301          |\n",
      "| TimeElapsed     | 6.6          |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | 0.232        |\n",
      "| loss_ent        | 0.661527     |\n",
      "| loss_kl         | 0.0051491354 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.020316206 |\n",
      "| loss_vf_loss    | 27.84371     |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00385 |       0.00000 |      62.29148 |       0.00034 |       0.65665\n",
      "     -0.00503 |       0.00000 |      57.76650 |       0.00031 |       0.65653\n",
      "     -0.00529 |       0.00000 |      55.50388 |       0.00029 |       0.65672\n",
      "     -0.00549 |       0.00000 |      54.15552 |       0.00030 |       0.65662\n",
      "     -0.00561 |       0.00000 |      53.19578 |       0.00033 |       0.65628\n",
      "     -0.00570 |       0.00000 |      52.42418 |       0.00033 |       0.65627\n",
      "     -0.00570 |       0.00000 |      51.77416 |       0.00037 |       0.65595\n",
      "     -0.00579 |       0.00000 |      51.19623 |       0.00038 |       0.65585\n",
      "     -0.00583 |       0.00000 |      50.66356 |       0.00038 |       0.65584\n",
      "     -0.00587 |       0.00000 |      50.18942 |       0.00040 |       0.65569\n",
      "Evaluating losses...\n",
      "     -0.00594 |       0.00000 |      49.93943 |       0.00039 |       0.65581\n",
      "-----------------------------------\n",
      "| EpLenMean       | 37            |\n",
      "| EpRewMean       | 37            |\n",
      "| EpThisIter      | 112           |\n",
      "| EpisodesSoFar   | 413           |\n",
      "| TimeElapsed     | 9.67          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | 0.329         |\n",
      "| loss_ent        | 0.6558063     |\n",
      "| loss_kl         | 0.0003933222  |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0059421784 |\n",
      "| loss_vf_loss    | 49.939426     |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "dnn = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "dnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "dnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"bnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7feef62a5a50>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00125 |       0.00000 |      82.73497 |       0.00013 |       0.69300\n",
      "     -0.00472 |       0.00000 |      80.08224 |       0.00089 |       0.69222\n",
      "     -0.00983 |       0.00000 |      77.05745 |       0.00332 |       0.68978\n",
      "     -0.01346 |       0.00000 |      73.53763 |       0.00652 |       0.68662\n",
      "     -0.01497 |       0.00000 |      69.46871 |       0.00794 |       0.68523\n",
      "     -0.01594 |       0.00000 |      64.90005 |       0.00861 |       0.68457\n",
      "     -0.01714 |       0.00000 |      59.87792 |       0.00909 |       0.68409\n",
      "     -0.01861 |       0.00000 |      54.60877 |       0.00950 |       0.68369\n",
      "     -0.01983 |       0.00000 |      49.28902 |       0.00972 |       0.68347\n",
      "     -0.02144 |       0.00000 |      44.15229 |       0.01051 |       0.68269\n",
      "Evaluating losses...\n",
      "     -0.02173 |       0.00000 |      41.64319 |       0.01130 |       0.68191\n",
      "----------------------------------\n",
      "| EpLenMean       | 21.3         |\n",
      "| EpRewMean       | 21.3         |\n",
      "| EpThisIter      | 193          |\n",
      "| EpisodesSoFar   | 193          |\n",
      "| TimeElapsed     | 3.84         |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | 0.00141      |\n",
      "| loss_ent        | 0.6819109    |\n",
      "| loss_kl         | 0.011302933  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.021725958 |\n",
      "| loss_vf_loss    | 41.64319     |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00241 |       0.00000 |      97.09811 |       0.00046 |       0.68158\n",
      "     -0.00686 |       0.00000 |      89.71896 |       0.00158 |       0.67606\n",
      "     -0.00805 |       0.00000 |      83.15277 |       0.00209 |       0.67418\n",
      "     -0.00862 |       0.00000 |      77.12335 |       0.00231 |       0.67361\n",
      "     -0.00921 |       0.00000 |      71.55171 |       0.00234 |       0.67355\n",
      "     -0.01013 |       0.00000 |      66.49274 |       0.00235 |       0.67373\n",
      "     -0.01077 |       0.00000 |      61.76869 |       0.00246 |       0.67342\n",
      "     -0.01111 |       0.00000 |      57.39679 |       0.00260 |       0.67311\n",
      "     -0.01156 |       0.00000 |      53.42760 |       0.00267 |       0.67298\n",
      "     -0.01181 |       0.00000 |      49.74719 |       0.00279 |       0.67258\n",
      "Evaluating losses...\n",
      "     -0.01210 |       0.00000 |      48.02717 |       0.00248 |       0.67357\n",
      "----------------------------------\n",
      "| EpLenMean       | 33.6         |\n",
      "| EpRewMean       | 33.6         |\n",
      "| EpThisIter      | 130          |\n",
      "| EpisodesSoFar   | 323          |\n",
      "| TimeElapsed     | 7.34         |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | -0.0549      |\n",
      "| loss_ent        | 0.6735701    |\n",
      "| loss_kl         | 0.0024790375 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.012096519 |\n",
      "| loss_vf_loss    | 48.027172    |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     6.13e-05 |       0.00000 |      88.62669 |       0.00015 |       0.67365\n",
      "     -0.00114 |       0.00000 |      86.44815 |       0.00023 |       0.67183\n",
      "     -0.00102 |       0.00000 |      84.44508 |       0.00028 |       0.67109\n",
      "     -0.00125 |       0.00000 |      82.49588 |       0.00034 |       0.67057\n",
      "     -0.00152 |       0.00000 |      80.66502 |       0.00030 |       0.67086\n",
      "     -0.00176 |       0.00000 |      78.91126 |       0.00032 |       0.67074\n",
      "     -0.00171 |       0.00000 |      77.24454 |       0.00034 |       0.67064\n",
      "     -0.00160 |       0.00000 |      75.63367 |       0.00033 |       0.67075\n",
      "     -0.00143 |       0.00000 |      74.09881 |       0.00035 |       0.67055\n",
      "     -0.00228 |       0.00000 |      72.54749 |       0.00035 |       0.67062\n",
      "Evaluating losses...\n",
      "     -0.00197 |       0.00000 |      71.76958 |       0.00035 |       0.67055\n",
      "-----------------------------------\n",
      "| EpLenMean       | 33.5          |\n",
      "| EpRewMean       | 33.5          |\n",
      "| EpThisIter      | 121           |\n",
      "| EpisodesSoFar   | 444           |\n",
      "| TimeElapsed     | 10.9          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | -0.0155       |\n",
      "| loss_ent        | 0.67055213    |\n",
      "| loss_kl         | 0.00035242434 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0019701375 |\n",
      "| loss_vf_loss    | 71.76958      |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "bnn = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "bnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "bnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 159.528, Std reward: 49.16516262558276\n",
      "BNN - Mean reward: 179.651, Std reward: 32.289893140114295\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 156.935, Std reward: 48.66798511341928\n",
      "BNN - Mean reward: 189.304, Std reward: 18.416991719605022\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN - Mean reward: 161.503, Std reward: 47.323144348193935\n",
      "BNN - Mean reward: 178.989, Std reward: 32.992982268961384\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
