{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "\n",
    "from mpi4py import MPI\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import bench, logger, PPO1\n",
    "from stable_baselines.common.callbacks import EvalCallback, BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v2.version' from '/Users/stephen/anaconda3/envs/slime-rl/lib/python3.7/site-packages/tensorflow/_api/v2/version/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMESTEPS = int(2e8)\n",
    "SEED = 831\n",
    "EVAL_FREQ = 200000\n",
    "EVAL_EPISODES = 1000\n",
    "LOGDIR = \"ppo1_mpi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed):\n",
    "    env = gym.make(\"SlimeVolley-v0\")\n",
    "    env.seed(seed)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train PPO1 model for slime volleyball, in MPI multiprocessing. Tested for 96 CPUs.\n",
    "    \"\"\"\n",
    "    rank = MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "    if rank == 0:\n",
    "        logger.configure(folder=LOGDIR)\n",
    "\n",
    "    else:\n",
    "        logger.configure(format_strs=[])\n",
    "    workerseed = SEED + 10000 * MPI.COMM_WORLD.Get_rank()\n",
    "    set_global_seeds(workerseed)\n",
    "    env = make_env(workerseed)\n",
    "\n",
    "    env = bench.Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))\n",
    "    env.seed(workerseed)\n",
    "\n",
    "    model = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "               optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear',\n",
    "               verbose=1)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "    model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "    env.close()\n",
    "    del env\n",
    "    if rank == 0:\n",
    "        model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Script for Self-Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "from shutil import copyfile # keep track of generations\n",
    "\n",
    "# Settings\n",
    "SEED = 17\n",
    "NUM_TIMESTEPS = int(1e9)\n",
    "EVAL_FREQ = int(1e5)\n",
    "EVAL_EPISODES = int(1e2)\n",
    "BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n",
    "\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "\n",
    "LOGDIR = \"ppo1_selfplay\"\n",
    "\n",
    "class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "  # wrapper over the normal single player env, but loads the best self play model\n",
    "    def __init__(self):\n",
    "        super(SlimeVolleySelfPlayEnv, self).__init__()\n",
    "        self.policy = self\n",
    "        self.best_model = None\n",
    "        self.best_model_filename = None\n",
    "    def predict(self, obs): # the policy\n",
    "        if self.best_model is None:\n",
    "            return self.action_space.sample() # return a random action\n",
    "        else:\n",
    "            action, _ = self.best_model.predict(obs)\n",
    "            return action\n",
    "    def reset(self):\n",
    "        # load model if it's there\n",
    "        modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n",
    "        modellist.sort()\n",
    "        if len(modellist) > 0:\n",
    "            filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n",
    "            if filename != self.best_model_filename:\n",
    "                print(\"loading model: \", filename)\n",
    "                self.best_model_filename = filename\n",
    "                if self.best_model is not None:\n",
    "                    del self.best_model\n",
    "                self.best_model = PPO1.load(filename, env=self)\n",
    "        return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "LOGDIR1 = \"adversarial/ppo1\"\n",
    "LOGDIR2 = \"adversarial/ppo2\"\n",
    "class SlimeVolleyAdversarialEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "    def __init__(self):\n",
    "        super(SlimeVolleyAdversarialEnv, self).__init__()\n",
    "        self.policy = self\n",
    "        self.best_model1 = None\n",
    "        self.best_model1_filename = None\n",
    "        self.best_model2 = None\n",
    "        self.best_model2_filename = None\n",
    "    def predict(self, obs): #policy\n",
    "        if self.best_model1 is None:\n",
    "            return self.action_space.sample(), self.action_space.sample()\n",
    "        else:\n",
    "            action1, _ = self.best_model1.predict(obs)\n",
    "            action2, _ = self.best_model2.predict(obs)\n",
    "            return action1, action2\n",
    "    \n",
    "    def reset(self):\n",
    "        # loads a model if it is there \n",
    "        modellist1 = [f for f in os.listdir(LOGDIR1) if f.startswith(\"history\")]\n",
    "        modellist2 = [f for f in os.listdir(LOGDIR2) if f.startswith(\"history\")]\n",
    "        modellist1.sort()\n",
    "        modellist2.sort()\n",
    "        if len(modellist1) > 0:\n",
    "            filename1 = os.path.join(LOGDIR1, modellist1[-1]) # the latest best model\n",
    "            if filename1 != self.best_model1_filename:\n",
    "                print(\"loading model: \", filename1)\n",
    "                self.best_model1_filename = filename1\n",
    "                if self.best_model is not None:\n",
    "                    del self.best_model1\n",
    "                self.best_model1 = PPO1.load(filename1, env=self)\n",
    "        if len(modellist2) > 0:\n",
    "            filename2 = os.path.join(LOGDIR2, modellist2[-1]) # the latest best model\n",
    "            if filename2 != self.best_model1_filename:\n",
    "                print(\"loading model: \", filename2)\n",
    "                self.best_model2_filename = filename2\n",
    "                if self.best_model2 is not None:\n",
    "                    del self.best_model2\n",
    "                self.best_model12 = PPO1.load(filename1, env=self)\n",
    "        return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_THRESHOLD1 = 0.5\n",
    "BEST_THRESHOLD2 = 0.5\n",
    "class AdversarialCallback(EvalCallback):\n",
    "    # hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "    # after saving model, resets the best score to be BEST_THRESHOLD\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AdversarialCallback, self).__init__(*args, **kwargs)\n",
    "        self.best_mean_reward1 = BEST_THRESHOLD1\n",
    "        self.best_mean_reward2 = BEST_THRESHOLD2\n",
    "        self.generation = 0\n",
    "    def _on_step(self)-> bool:\n",
    "        result = super(SelfPlayCallback, self)._on_step()\n",
    "        if result and (self.best_mean_reward1 > BEST_THRESHOLD1 or self.best_mean_reward2 > BEST_THRESHOLD2):\n",
    "            self.generation += 1\n",
    "            if self.best_mean_reward1 > BEST_THRESHOLD1:\n",
    "                print(\"MODEL1: mean_reward achieved:\", self.best_mean_reward1)\n",
    "                print(\"MODEL1: new best model, bumping up generation to\", self.generation)\n",
    "                source_file = os.path.join(LOGDIR1, \"best_model.zip\")\n",
    "                backup_file = os.path.join(LOGDIR1, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "                copyfile(source_file, backup_file)\n",
    "                self.best_mean_reward1 = BEST_THRESHOLD1\n",
    "            if self.best_mean_reward2 > BEST_THRESHOLD2:\n",
    "                print(\"MODEL2: mean_reward achieved:\", self.best_mean_reward2)\n",
    "                print(\"MODEL2: new best model, bumping up generation to\", self.generation)\n",
    "                source_file = os.path.join(LOGDIR2, \"best_model.zip\")\n",
    "                backup_file = os.path.join(LOGDIR2, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "                copyfile(source_file, backup_file)\n",
    "                self.best_mean_reward2 = BEST_THRESHOLD2\n",
    "        return result\n",
    "\n",
    "class SelfPlayCallback(EvalCallback):\n",
    "    # hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "    # after saving model, resets the best score to be BEST_THRESHOLD\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SelfPlayCallback, self).__init__(*args, **kwargs)\n",
    "        self.best_mean_reward = BEST_THRESHOLD\n",
    "        self.generation = 0\n",
    "    def _on_step(self) -> bool:\n",
    "        result = super(SelfPlayCallback, self)._on_step()\n",
    "        if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "            self.generation += 1\n",
    "            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n",
    "            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "            copyfile(source_file, backup_file)\n",
    "            self.best_mean_reward = BEST_THRESHOLD\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rollout(env, policy):\n",
    "#     \"\"\" play one agent vs the other in modified gym-style loop. \"\"\"\n",
    "#     obs = env.reset()\n",
    "\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "\n",
    "#         action, _states = policy.predict(obs)\n",
    "#         obs, reward, done, _ = env.step(action)\n",
    "\n",
    "#         total_reward += reward\n",
    "\n",
    "#         if RENDER_MODE:\n",
    "#             env.render()\n",
    "\n",
    "#     return total_reward\n",
    "def rollout(env, policy1, policy2):\n",
    "    \"\"\" play one agent vs the other in modified gym-style loop. \"\"\"\n",
    "    obs1 = env.reset()\n",
    "    obs2 = obs1 # same observation for other agent\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action1, _ = policy1.predict(obs1)\n",
    "        action2, _ = policy2.predict(obs2)\n",
    "        \n",
    "        obs1, reward, done, _ = env.step(action1, action2)\n",
    "\n",
    "        obs2 = info['otherObs']\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if RENDER_MODE:\n",
    "            env.render()\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # train selfplay agent\n",
    "    logger.configure(folder=LOGDIR)\n",
    "\n",
    "    env = SlimeVolleySelfPlayEnv()\n",
    "    env.seed(SEED)\n",
    "\n",
    "    # take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "    model = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                   optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=1)\n",
    "\n",
    "    eval_callback = SelfPlayCallback(env,\n",
    "        best_model_save_path=LOGDIR,\n",
    "        log_path=LOGDIR,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=EVAL_EPISODES,\n",
    "        deterministic=False)\n",
    "\n",
    "    model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "    model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "    env.close()\n",
    "def train():\n",
    "    # train selfplay agent\n",
    "    logger.configure(folder=LOGDIR)\n",
    "\n",
    "#     env = SlimeVolleySelfPlayEnv()\n",
    "    env = SlimeVolleyAdversarialEnv()\n",
    "    env.seed(SEED)\n",
    "\n",
    "    # take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "    model1 = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                   optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=1)\n",
    "\n",
    "    eval_callback = AdversarialCallback(env,\n",
    "        best_model_save_path1=LOGDIR1,\n",
    "        best_model_save_path2=LOGDIR2,\n",
    "        log_path=LOGDIR,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=EVAL_EPISODES,\n",
    "        deterministic=False)\n",
    "\n",
    "    model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "    model.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ppo1_selfplay\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'best_model_save_path1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c7a5764cf04c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVAL_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVAL_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         deterministic=False)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TIMESTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5f7d80aab11c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# after saving model, resets the best score to be BEST_THRESHOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdversarialCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mean_reward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBEST_THRESHOLD1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mean_reward2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBEST_THRESHOLD2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'best_model_save_path1'"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
