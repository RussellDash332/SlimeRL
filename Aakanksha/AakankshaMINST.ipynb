{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVOLUTIONAL NEURAL NETWORKS (BNN COMES LATER)\n",
    "\n",
    "#Import MINST data\n",
    "\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff92ef25890>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image_index = 7777\n",
    "\n",
    "#The label\n",
    "print(y_train[image_index])\n",
    "\n",
    "#The image\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We also need to know the shape of the dataset to channel it to the convolutional\n",
    "# neural network\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "#We may experiment with any number for the first Dense layer;\n",
    "#however, the final Dense layer must have 10 neurons since we\n",
    "#have 10 number classes (0, 1, 2, …, 9). You may always experiment\n",
    "#with kernel size, pool size, activation functions, dropout rate,\n",
    "#and number of neurons in the first Dense layer to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.2074 - acc: 0.9378\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0860 - acc: 0.9736\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0580 - acc: 0.9820\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0443 - acc: 0.9859\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.0356 - acc: 0.9879\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 203us/sample - loss: 0.0289 - acc: 0.9902\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.0249 - acc: 0.9916\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0222 - acc: 0.9927\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0203 - acc: 0.9929\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0173 - acc: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff932e5df90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With the above code, we created an non-optimized empty CNN. \n",
    "#Now it is time to set an optimizer with a given loss function \n",
    "#which uses a metric. Then, we can fit the model by using our \n",
    "#train data.\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10)\n",
    "\n",
    "#You can also set batch_size for the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0642 - acc: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06416861503745228, 0.985]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAANfElEQVR4nO3db6xU9Z3H8c9HthViq8Jyc0MoLt0GJaSxtBnJJiWNpllEEoP1gYEHDaumlweagCFRYqMlMfgv25I+MI23SgqmC2nSGnlA3LqkCUGT6mhYRbCVVUwhCEPQlMYoQr/74B7MLd45c5k58we/71dyMzPnO+eebw587pmZ35zzc0QIwBffJf1uAEBvEHYgCcIOJEHYgSQIO5DEP/VyYzNnzoy5c+f2cpNAKocOHdKJEyc8Ua2jsNteKunnkqZIeioiHi17/ty5c1Wv1zvZJIAStVqtaa3tl/G2p0h6QtJNkhZIWml7Qbu/D0B3dfKefZGkgxHxTkSclrRd0vJq2gJQtU7CPlvSX8Y9Plws+we2R2zXbdcbjUYHmwPQia5/Gh8RoxFRi4ja0NBQtzcHoIlOwn5E0pxxj79WLAMwgDoJ+yuS5tn+uu0vS1ohaUc1bQGoWttDbxFxxvbdkv5bY0NvmyPizco6A1CpjsbZI2KnpJ0V9QKgi/i6LJAEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJjqZstn1I0ilJZyWdiYhaFU0BqF5HYS/cEBEnKvg9ALqIl/FAEp2GPST93vartkcmeoLtEdt12/VGo9Hh5gC0q9OwL46I70i6SdJdtr93/hMiYjQiahFRGxoa6nBzANrVUdgj4khxe1zSs5IWVdEUgOq1HXbbl9n+6rn7kpZI2ldVYwCq1cmn8cOSnrV97vf8V0Q8X0lXACrXdtgj4h1J36qwFwBdxNAbkARhB5Ig7EAShB1IgrADSVRxIgwG2NmzZ0vrt99+e2n9mWeeKa0XQ69tufzyy0vrDzzwQGl93bp1bW87I47sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wD4IMPPiitP/bYY22v//zz5WcdHz58uLTeahz90ksvLa0/8sgjTWt33HFH6brXXnttaX3FihWl9dmzZ5fWs+HIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+AObNm1dabzUO302rV68urT/00EOl9ZkzZ7a97eHh4dJ6q3Pt169f3/a2v4g4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyz98DJkyc7qndybfZOPfHEE6X1Sy7heHGxaPkvZXuz7eO2941bNsP2C7bfLm6nd7dNAJ2azJ/lX0laet6y9ZJ2RcQ8SbuKxwAGWMuwR8RuSee/zlwuaUtxf4ukW6ptC0DV2n3DNRwRR4v770tq+iVm2yO267brjUajzc0B6FTHn65EREiKkvpoRNQiojY0NNTp5gC0qd2wH7M9S5KK2+PVtQSgG9oN+w5Jq4r7qyQ9V007ALql5Ti77W2Srpc00/ZhST+R9Kik39i+U9J7km7rZpMXu7Vr1/a7haZazc/ezXH0M2fOlNZbncfPZ0AXpmXYI2Jlk9L3K+4FQBfx9ScgCcIOJEHYgSQIO5AEYQeS4BTXHjhw4EBpferUqaX1Wq1WWt+zZ88F93TOxo0b2163Uy+++GJp/eDBg6X13bt3V9nOFx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2Hmh1mui9995bWr/vvvtK69dcc03T2pEjR0rXffDBB0vr06d378LBo6OjpfVWl9DmMtYXhr0FJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4APvroo9L6tGnTSuv79u1rWmt1GeunnnqqtD424U9z/ZxOemRkpG/bvhhxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJNxqHLVKtVot6vV6z7Y3KG644YbS+rvvvltab3Xd+bJx+Fb/vvv37y+ttzqfffv27aX1hx9+uGmt1ZTMrXz66ael9Yznu9dqNdXr9Qm//NByb9jebPu47X3jlm2wfcT23uJnWZUNA6jeZP70/UrS0gmWb4qIhcXPzmrbAlC1lmGPiN2STvagFwBd1Mmbmrttv168zG/6xs72iO267Xqj0ehgcwA60W7YfyHpG5IWSjoq6afNnhgRoxFRi4ja0NBQm5sD0Km2wh4RxyLibET8XdIvJS2qti0AVWsr7LZnjXv4A0nNz7EEMBBans9ue5uk6yXNtH1Y0k8kXW97oaSQdEjS6u61ePF78sknS+vz588vra9eXb57y66/3mru93vuuae0/vLLL5fWT506VVrvpozj6J1oGfaIWDnB4qe70AuALuJPI5AEYQeSIOxAEoQdSIKwA0lwKekeuPrqq0vrrYa/Nm3aVFrfubP5eUg33nhj6bqthtZOnz5dWm/1rchly5qfELlt27bSdW+99dbSOi4MR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9gHw+OOPl9bXrFlTWi87hfbDDz8sXbfVlM2LFy8urV955ZWl9bfeeqtpbevWraXrLl060XVO0S6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsA2DKlCml9auuuqq0vnHjxirbqdRLL73UtNZqOuklS5ZU3U5qHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2dFVJ06c6HcLKLQ8stueY/sPtvfbftP2mmL5DNsv2H67uJ3e/XYBtGsyL+PPSFoXEQsk/Zuku2wvkLRe0q6ImCdpV/EYwIBqGfaIOBoRrxX3T0k6IGm2pOWSthRP2yLpli71CKACF/QBne25kr4t6Y+ShiPiaFF6X9Jwk3VGbNdt1xuNRie9AujApMNu+yuSfitpbUT8dXwtxs5omPCshogYjYhaRNRaTQIIoHsmFXbbX9JY0H8dEb8rFh+zPauoz5J0vDstAqhCy6E325b0tKQDEfGzcaUdklZJerS4fa4rHeILa9q0aaX1qVOn9qiTHCYzzv5dST+U9IbtvcWy+zUW8t/YvlPSe5Ju60qHACrRMuwRsUeSm5S/X207ALqFr8sCSRB2IAnCDiRB2IEkCDuQBKe4oiMff/xxaX3Dhg1NazfffHPpuldccUU7LaEJjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7OiqscshTGzBggU97AQc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZ0ZFPPvmk3y1gkjiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASk5mffY6krZKGJYWk0Yj4ue0Nkn4kqVE89f6I2NmtRjGY9u/f3/a61113XYWdoJXJfKnmjKR1EfGa7a9KetX2C0VtU0T8Z/faA1CVyczPflTS0eL+KdsHJM3udmMAqnVB79ltz5X0bUl/LBbdbft125ttT2+yzojtuu16o9GY6CkAemDSYbf9FUm/lbQ2Iv4q6ReSviFpocaO/D+daL2IGI2IWkTUhoaGOu8YQFsmFXbbX9JY0H8dEb+TpIg4FhFnI+Lvkn4paVH32gTQqZZh99jlQZ+WdCAifjZu+axxT/uBpH3VtwegKpP5NP67kn4o6Q3be4tl90taaXuhxobjDkla3YX+MOCmT5/wo5rPzJgxo2lt8eLFVbeDEpP5NH6PpIku/s2YOnAR4Rt0QBKEHUiCsANJEHYgCcIOJEHYgSS4lDQ6Mn/+/NI650MMDo7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5CEI6J3G7Mbkt4bt2impBM9a+DCDGpvg9qXRG/tqrK3f4mICa//1tOwf27jdj0ian1roMSg9jaofUn01q5e9cbLeCAJwg4k0e+wj/Z5+2UGtbdB7Uuit3b1pLe+vmcH0Dv9PrID6BHCDiTRl7DbXmr7T7YP2l7fjx6asX3I9hu299qu97mXzbaP2943btkM2y/Yfru4Lb9we29722D7SLHv9tpe1qfe5tj+g+39tt+0vaZY3td9V9JXT/Zbz9+z254i6c+S/l3SYUmvSFoZEe1P9F0h24ck1SKi71/AsP09SX+TtDUivlkse1zSyYh4tPhDOT0i7huQ3jZI+lu/p/EuZiuaNX6acUm3SPoP9XHflfR1m3qw3/pxZF8k6WBEvBMRpyVtl7S8D30MvIjYLenkeYuXS9pS3N+isf8sPdekt4EQEUcj4rXi/ilJ56YZ7+u+K+mrJ/oR9tmS/jLu8WEN1nzvIen3tl+1PdLvZiYwHBFHi/vvSxruZzMTaDmNdy+dN834wOy7dqY/7xQf0H3e4oj4jqSbJN1VvFwdSDH2HmyQxk4nNY13r0wwzfhn+rnv2p3+vFP9CPsRSXPGPf5asWwgRMSR4va4pGc1eFNRHzs3g25xe7zP/XxmkKbxnmiacQ3Avuvn9Of9CPsrkubZ/rrtL0taIWlHH/r4HNuXFR+cyPZlkpZo8Kai3iFpVXF/laTn+tjLPxiUabybTTOuPu+7vk9/HhE9/5G0TGOfyP+fpB/3o4cmff2rpP8tft7sd2+StmnsZd2nGvts405J/yxpl6S3Jf2PpBkD1Nszkt6Q9LrGgjWrT70t1thL9Ncl7S1+lvV735X01ZP9xtdlgST4gA5IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvh/3cEPVpNqH1oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#An example prediction\n",
    "image_index = 4444\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/9\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2589 - acc: 0.9256\n",
      "Epoch 2/9\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1166 - acc: 0.96520s - loss: 0.1187 - ac\n",
      "Epoch 3/9\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0790 - acc: 0.9761\n",
      "Epoch 4/9\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0605 - acc: 0.98090s - loss: 0.0596 - acc: \n",
      "Epoch 5/9\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0454 - acc: 0.9861\n",
      "Epoch 6/9\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0362 - acc: 0.9888\n",
      "Epoch 7/9\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0292 - acc: 0.99091s\n",
      "Epoch 8/9\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0245 - acc: 0.9919\n",
      "Epoch 9/9\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0204 - acc: 0.9938\n",
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.0813 - acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08129265486844524, 0.9765]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the keras model\n",
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape=input_shape))\n",
    "model2.add(Dense(128,activation=tf.nn.relu))\n",
    "model2.add(Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "#compile\n",
    "model2.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model2.fit(x=x_train,y=y_train, epochs=9)\n",
    "\n",
    "#Evaluating the model\n",
    "model2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#An example prediction\n",
    "image_index = 4152\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred2 = model2.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred2.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAYESIAN NEURAL NETWORKS\n",
    "\n",
    "Instead of point estimate of weights, BNN approximates the distribution of weights, commonly a Gaussian/normal distribution with two hyperparameters (mean and standard deviation), based on prior information and data.\n",
    "\n",
    "BNN can be integrated into any neural network models, but here I’m interested in its application on convolutional neural networks (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load libriaries and functions.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define helper functions.\n",
    "scaler = StandardScaler()\n",
    "detector = IsolationForest(n_estimators=1000, behaviour=\"deprecated\", contamination=\"auto\", random_state=0)\n",
    "neg_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 16.2701 - acc: 0.2336s - loss: 16.4453 - acc\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 12.5530 - acc: 0.2125\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 11.7336 - acc: 0.1413\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 11.6498 - acc: 0.0703s - loss: 11.7630 - acc: \n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 9.2071 - acc: 0.0938\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 11.8653 - acc: 0.1157\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 10.0002 - acc: 0.0921\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 9.0200 - acc: 0.0671\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 16.9758 - acc: 0.0575\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 10.9622 - acc: 0.0864\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 7.9787 - acc: 0.15290s - loss: 8.0521 - acc: \n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 9.6333 - acc: 0.1528\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 9.7081 - acc: 0.0573\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 9.4669 - acc: 0.0622\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 10.4550 - acc: 0.0537\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 7.4759 - acc: 0.1272\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 8.6890 - acc: 0.1129\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 6.5759 - acc: 0.1124\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 5.6928 - acc: 0.13140s - loss: 5.7635 -\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 8.1020 - acc: 0.1129\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                21980     \n",
      "_________________________________________________________________\n",
      "distribution_weights (Dense) (None, 65)                1885      \n",
      "_________________________________________________________________\n",
      "output (MultivariateNormalTr ((None, 10), (None, 10))  0         \n",
      "=================================================================\n",
      "Total params: 23,865\n",
      "Trainable params: 23,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "n_epochs = 20\n",
    "n_samples = x_train.shape[0]\n",
    "n_batches = 10\n",
    "batch_size = np.floor(n_samples/n_batches)\n",
    "buffer_size = n_samples\n",
    "\n",
    "# Define prior for regularization.\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(10, dtype=tf.float64), scale=1.0), reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "# Define model instance.\n",
    "\n",
    "model3 = tfk.Sequential([\n",
    "tfk.layers.Flatten(input_shape=input_shape, name=\"input\"),\n",
    "tfk.layers.Dense(28, activation=\"relu\", name=\"dense_1\"),\n",
    "tfk.layers.Dense(tfp.layers.MultivariateNormalTriL.params_size(\n",
    "10), activation=None, name=\"distribution_weights\"),\n",
    "tfp.layers.MultivariateNormalTriL(10, activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior, weight=1/n_batches), name=\"output\")\n",
    "], name=\"model\")\n",
    "\n",
    "\n",
    "\n",
    "# Compile model.\n",
    "model3.compile(optimizer=\"adam\", loss=neg_log_likelihood, metrics=['accuracy'])\n",
    "\n",
    "# Run training session.\n",
    "model3.fit(x=x_train, y=y_train, epochs=n_epochs, verbose=True)\n",
    "\n",
    "# Describe model.\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 72us/sample - loss: 7.6495 - acc: 0.0782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.649450236987868, 0.0782]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "model3.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not convert a MultivariateNormalTriL into a Tensor or Operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-4effb1ea12fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4329\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;31m# sample_weight_mode value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    564\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2177\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3676\u001b[0m                'backend') % key\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3678\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mGraphExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0;31m# dependencies in call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3330\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3331\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3332\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(control_inputs)\u001b[0m\n\u001b[1;32m   5252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(self, control_inputs)\u001b[0m\n\u001b[1;32m   4686\u001b[0m           (hasattr(c, \"_handle\") and hasattr(c, \"op\"))):\n\u001b[1;32m   4687\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4688\u001b[0;31m       \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4689\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4690\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3606\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3607\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3609\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3694\u001b[0m       \u001b[0;31m# We give up!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" %\n\u001b[0;32m-> 3696\u001b[0;31m                       (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   3697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a MultivariateNormalTriL into a Tensor or Operation."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#An example prediction\n",
    "image_index = 4329\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred3 = model3.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred3.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“slime-rl”",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
