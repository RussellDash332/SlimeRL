{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives tl;dr\n",
    "def bnn_extractor(flat_observations, net_arch, act_fun):\n",
    "    \"\"\"\n",
    "    Constructs an variational layer that receives observations as an input and outputs a latent representation for the policy and\n",
    "    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "    structure:\n",
    "    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "       If the number of ints is zero, there will be no shared layers.\n",
    "    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "    would be specified as [128, 128].\n",
    "    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n",
    "    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n",
    "        See above for details on its formatting.\n",
    "    :param act_fun: (tf function) The activation function to use for the networks.\n",
    "    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n",
    "        If all layers are shared, then ``latent_policy == latent_value``\n",
    "    \"\"\"\n",
    "    \n",
    "    latent = flat_observations\n",
    "    policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "    value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p)\n",
    "\n",
    "    # Iterate through the shared layers and build the shared parts of the network\n",
    "    for idx, layer in enumerate(net_arch):\n",
    "        if isinstance(layer, int):  # Check that this is a shared layer\n",
    "            layer_size = layer\n",
    "#             latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "            latent = act_fun(tfp.layers.DenseFlipout(layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "        else:\n",
    "            assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "            if 'pi' in layer:\n",
    "                assert isinstance(layer['pi'], list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                policy_only_layers = layer['pi']\n",
    "\n",
    "            if 'vf' in layer:\n",
    "                assert isinstance(layer['vf'], list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                value_only_layers = layer['vf']\n",
    "            break  # From here on the network splits up in policy and value network\n",
    "\n",
    "    # Build the non-shared part of the network\n",
    "    latent_policy = latent\n",
    "    latent_value = latent\n",
    "    for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n",
    "        if pi_layer_size is not None:\n",
    "            assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "#             latent_policy = act_fun(linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_policy = act_fun(tfp.layers.DenseFlipout(pi_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "        if vf_layer_size is not None:\n",
    "            assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "#             latent_value = act_fun(linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "            latent_value = act_fun(tfp.layers.DenseFlipout(vf_layer_size, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn)(latent))\n",
    "\n",
    "    return latent_policy, latent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import ActorCriticPolicy, nature_cnn\n",
    "\n",
    "#something like an evaluation of the method used\n",
    "class FeedForwardPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a feed forward neural network.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy\n",
    "        (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor\n",
    "        documentation for details).\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, net_arch=None,\n",
    "                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n",
    "        super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "        \n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            if feature_extraction == \"cnn\":\n",
    "                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\n",
    "            elif feature_extraction == \"bnn\":\n",
    "                pi_latent, vf_latent = bnn_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "            else:\n",
    "                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import zip_longest\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.distributions import make_proba_dist_type, CategoricalProbabilityDistribution, \\\n",
    "    MultiCategoricalProbabilityDistribution, DiagGaussianProbabilityDistribution, BernoulliProbabilityDistribution\n",
    "from stable_baselines.common.input import observation_input\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnnPolicy(FeedForwardPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using a Bayesian neural net (2 layers of 64)\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):\n",
    "        super(BnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"bnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to dnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"dnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:162: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:190: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13b810650>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00974 |       0.00000 |      86.01871 |       0.00416 |       0.68911\n",
      "     -0.01660 |       0.00000 |      72.62526 |       0.00789 |       0.68547\n",
      "     -0.01867 |       0.00000 |      45.76273 |       0.00835 |       0.68501\n",
      "     -0.02130 |       0.00000 |      22.93425 |       0.00919 |       0.68418\n",
      "     -0.02393 |       0.00000 |      15.95167 |       0.01034 |       0.68305\n",
      "     -0.02520 |       0.00000 |      14.53490 |       0.01165 |       0.68177\n",
      "     -0.02676 |       0.00000 |      13.44226 |       0.01212 |       0.68130\n",
      "     -0.02778 |       0.00000 |      12.26323 |       0.01270 |       0.68073\n",
      "     -0.02821 |       0.00000 |      11.25233 |       0.01310 |       0.68034\n",
      "     -0.02869 |       0.00000 |      10.50572 |       0.01317 |       0.68027\n",
      "Evaluating losses...\n",
      "     -0.02855 |       0.00000 |      10.21981 |       0.01142 |       0.68197\n",
      "----------------------------------\n",
      "| EpLenMean       | 21.8         |\n",
      "| EpRewMean       | 21.8         |\n",
      "| EpThisIter      | 183          |\n",
      "| EpisodesSoFar   | 183          |\n",
      "| TimeElapsed     | 4            |\n",
      "| TimestepsSoFar  | 4096         |\n",
      "| ev_tdlam_before | -0.00181     |\n",
      "| loss_ent        | 0.6819691    |\n",
      "| loss_kl         | 0.01141722   |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.028546477 |\n",
      "| loss_vf_loss    | 10.219812    |\n",
      "----------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.01263 |       0.00000 |      53.30185 |       0.00297 |       0.66999\n",
      "     -0.01789 |       0.00000 |      38.41278 |       0.00327 |       0.66786\n",
      "     -0.01937 |       0.00000 |      35.28123 |       0.00367 |       0.66670\n",
      "     -0.01993 |       0.00000 |      33.46874 |       0.00417 |       0.66546\n",
      "     -0.02035 |       0.00000 |      32.15681 |       0.00437 |       0.66504\n",
      "     -0.02071 |       0.00000 |      31.14059 |       0.00468 |       0.66431\n",
      "     -0.02099 |       0.00000 |      30.33042 |       0.00471 |       0.66435\n",
      "     -0.02128 |       0.00000 |      29.65064 |       0.00486 |       0.66404\n",
      "     -0.02137 |       0.00000 |      29.03898 |       0.00503 |       0.66373\n",
      "     -0.02160 |       0.00000 |      28.46129 |       0.00521 |       0.66333\n",
      "Evaluating losses...\n",
      "     -0.02178 |       0.00000 |      28.16967 |       0.00562 |       0.66239\n",
      "----------------------------------\n",
      "| EpLenMean       | 31.3         |\n",
      "| EpRewMean       | 31.3         |\n",
      "| EpThisIter      | 125          |\n",
      "| EpisodesSoFar   | 308          |\n",
      "| TimeElapsed     | 7.81         |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | 0.211        |\n",
      "| loss_ent        | 0.6623884    |\n",
      "| loss_kl         | 0.005618421  |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.021778602 |\n",
      "| loss_vf_loss    | 28.16967     |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00381 |       0.00000 |      82.99218 |       0.00029 |       0.65965\n",
      "     -0.00485 |       0.00000 |      72.45718 |       0.00030 |       0.65937\n",
      "     -0.00520 |       0.00000 |      67.47987 |       0.00031 |       0.65914\n",
      "     -0.00543 |       0.00000 |      64.58725 |       0.00032 |       0.65912\n",
      "     -0.00550 |       0.00000 |      62.61908 |       0.00032 |       0.65900\n",
      "     -0.00564 |       0.00000 |      61.14854 |       0.00034 |       0.65900\n",
      "     -0.00569 |       0.00000 |      59.93436 |       0.00035 |       0.65868\n",
      "     -0.00575 |       0.00000 |      58.87246 |       0.00037 |       0.65858\n",
      "     -0.00576 |       0.00000 |      57.92395 |       0.00039 |       0.65840\n",
      "     -0.00585 |       0.00000 |      57.03346 |       0.00040 |       0.65827\n",
      "Evaluating losses...\n",
      "     -0.00591 |       0.00000 |      56.59664 |       0.00038 |       0.65851\n",
      "-----------------------------------\n",
      "| EpLenMean       | 46.1          |\n",
      "| EpRewMean       | 46.1          |\n",
      "| EpThisIter      | 85            |\n",
      "| EpisodesSoFar   | 393           |\n",
      "| TimeElapsed     | 11.3          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | 0.244         |\n",
      "| loss_ent        | 0.658507      |\n",
      "| loss_kl         | 0.00038115424 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0059081875 |\n",
      "| loss_vf_loss    | 56.59664      |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "dnn = PPO1(MlpPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "dnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "dnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to bnn_cartpole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[722]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TIMESTEPS = int(1e4)\n",
    "SEED = 722\n",
    "EVAL_FREQ = 250000\n",
    "EVAL_EPISODES = 10  # was 1000\n",
    "\n",
    "LOGDIR = \"bnn_cartpole\" # moved to zoo afterwards.\n",
    "logger.configure(folder=LOGDIR)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-26d750599950>:47: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:162: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py:190: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/hema/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "********** Iteration 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/try-bayesian/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v0>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1039b3dd0>\n",
      "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00148 |       0.00000 |      95.33205 |       0.00016 |       0.69298\n",
      "     -0.00584 |       0.00000 |      92.44428 |       0.00108 |       0.69207\n",
      "     -0.01160 |       0.00000 |      89.07041 |       0.00414 |       0.68904\n",
      "     -0.01473 |       0.00000 |      84.93721 |       0.00732 |       0.68593\n",
      "     -0.01566 |       0.00000 |      80.09994 |       0.00860 |       0.68469\n",
      "     -0.01641 |       0.00000 |      74.67774 |       0.00837 |       0.68490\n",
      "     -0.01734 |       0.00000 |      68.80899 |       0.00852 |       0.68475\n",
      "     -0.01873 |       0.00000 |      62.70371 |       0.00899 |       0.68429\n",
      "     -0.01968 |       0.00000 |      56.63303 |       0.00940 |       0.68388\n",
      "     -0.02112 |       0.00000 |      50.76483 |       0.00978 |       0.68350\n",
      "Evaluating losses...\n",
      "     -0.02155 |       0.00000 |      47.86052 |       0.00897 |       0.68429\n",
      "---------------------------------\n",
      "| EpLenMean       | 22.7        |\n",
      "| EpRewMean       | 22.7        |\n",
      "| EpThisIter      | 172         |\n",
      "| EpisodesSoFar   | 172         |\n",
      "| TimeElapsed     | 4.45        |\n",
      "| TimestepsSoFar  | 4096        |\n",
      "| ev_tdlam_before | -0.00437    |\n",
      "| loss_ent        | 0.68429095  |\n",
      "| loss_kl         | 0.008967493 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | -0.02154512 |\n",
      "| loss_vf_loss    | 47.860516   |\n",
      "---------------------------------\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00292 |       0.00000 |      84.53751 |       0.00052 |       0.68168\n",
      "     -0.00709 |       0.00000 |      78.28397 |       0.00195 |       0.67568\n",
      "     -0.00753 |       0.00000 |      72.57352 |       0.00238 |       0.67448\n",
      "     -0.00850 |       0.00000 |      67.38642 |       0.00230 |       0.67483\n",
      "     -0.00919 |       0.00000 |      62.62259 |       0.00260 |       0.67390\n",
      "     -0.00934 |       0.00000 |      58.25915 |       0.00250 |       0.67446\n",
      "     -0.00987 |       0.00000 |      54.21346 |       0.00269 |       0.67388\n",
      "     -0.01046 |       0.00000 |      50.53906 |       0.00271 |       0.67394\n",
      "     -0.01087 |       0.00000 |      47.15131 |       0.00268 |       0.67410\n",
      "     -0.01102 |       0.00000 |      44.09626 |       0.00298 |       0.67327\n",
      "Evaluating losses...\n",
      "     -0.01100 |       0.00000 |      42.62811 |       0.00282 |       0.67388\n",
      "----------------------------------\n",
      "| EpLenMean       | 26.1         |\n",
      "| EpRewMean       | 26.1         |\n",
      "| EpThisIter      | 149          |\n",
      "| EpisodesSoFar   | 321          |\n",
      "| TimeElapsed     | 8.03         |\n",
      "| TimestepsSoFar  | 8192         |\n",
      "| ev_tdlam_before | -0.0902      |\n",
      "| loss_ent        | 0.67387533   |\n",
      "| loss_kl         | 0.0028233912 |\n",
      "| loss_pol_entpen | 0.0          |\n",
      "| loss_pol_surr   | -0.011000194 |\n",
      "| loss_vf_loss    | 42.62811     |\n",
      "----------------------------------\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00017 |       0.00000 |      86.56606 |       0.00018 |       0.67532\n",
      "     -0.00141 |       0.00000 |      84.20171 |       0.00029 |       0.67327\n",
      "     -0.00084 |       0.00000 |      82.01009 |       0.00035 |       0.67258\n",
      "     -0.00077 |       0.00000 |      79.96583 |       0.00040 |       0.67219\n",
      "     -0.00134 |       0.00000 |      78.04777 |       0.00040 |       0.67200\n",
      "     -0.00163 |       0.00000 |      76.16700 |       0.00035 |       0.67254\n",
      "     -0.00155 |       0.00000 |      74.46637 |       0.00035 |       0.67246\n",
      "     -0.00136 |       0.00000 |      72.78864 |       0.00039 |       0.67204\n",
      "     -0.00160 |       0.00000 |      71.23609 |       0.00039 |       0.67216\n",
      "     -0.00154 |       0.00000 |      69.65989 |       0.00038 |       0.67229\n",
      "Evaluating losses...\n",
      "     -0.00180 |       0.00000 |      68.94234 |       0.00037 |       0.67222\n",
      "-----------------------------------\n",
      "| EpLenMean       | 32.3          |\n",
      "| EpRewMean       | 32.3          |\n",
      "| EpThisIter      | 122           |\n",
      "| EpisodesSoFar   | 443           |\n",
      "| TimeElapsed     | 11.5          |\n",
      "| TimestepsSoFar  | 12288         |\n",
      "| ev_tdlam_before | -0.00475      |\n",
      "| loss_ent        | 0.6722202     |\n",
      "| loss_kl         | 0.00037007534 |\n",
      "| loss_pol_entpen | 0.0           |\n",
      "| loss_pol_surr   | -0.0018045818 |\n",
      "| loss_vf_loss    | 68.942345     |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# take mujoco hyperparams (but doubled timesteps_per_actorbatch to cover more steps.)\n",
    "bnn = PPO1(BnnPolicy, env, timesteps_per_actorbatch=4096, clip_param=0.2, entcoeff=0.0, optim_epochs=10,\n",
    "                 optim_stepsize=3e-4, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=LOGDIR, log_path=LOGDIR, eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "bnn.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "bnn.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn score: 49.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    if random.random() < 0.3:\n",
    "        obs = np.random.rand(4,)\n",
    "    action, _states = dnn.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "env.close()\n",
    "print(\"dnn score:\", total_reward)\n",
    "#exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnn score: 18.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    if random.random() < 0.3:\n",
    "        obs = np.random.rand(4,)\n",
    "    action, _states = bnn.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "env.close()\n",
    "print(\"bnn score:\", total_reward)\n",
    "#exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f57ae409907f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dnn' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=100)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=100)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(dnn, dnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"DNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(bnn, bnn.get_env(), n_eval_episodes=1000)\n",
    "print(f\"BNN - Mean reward: {mean_reward}, Std reward: {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (try-bayesian)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
