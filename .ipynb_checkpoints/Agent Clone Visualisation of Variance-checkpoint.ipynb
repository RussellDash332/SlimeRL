{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Port of Neural Slime Volleyball to Python Gym Environment\n",
    "David Ha (2020)\n",
    "Original version:\n",
    "https://otoro.net/slimevolley\n",
    "https://blog.otoro.net/2015/03/28/neural-slime-volleyball/\n",
    "https://github.com/hardmaru/neuralslimevolley\n",
    "No dependencies apart from Numpy and Gym\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "import cv2 # installed with gym anyways\n",
    "from collections import deque\n",
    "\n",
    "np.set_printoptions(threshold=20, precision=3, suppress=True, linewidth=200)\n",
    "\n",
    "# game settings:\n",
    "\n",
    "RENDER_MODE = True\n",
    "\n",
    "REF_W = 24*2\n",
    "REF_H = REF_W\n",
    "REF_U = 1.5 # ground height\n",
    "REF_WALL_WIDTH = 1.0 # wall width\n",
    "REF_WALL_HEIGHT = 3.5\n",
    "PLAYER_SPEED_X = 10*1.75\n",
    "PLAYER_SPEED_Y = 10*1.35\n",
    "MAX_BALL_SPEED = 15*1.5\n",
    "TIMESTEP = 1/30.\n",
    "NUDGE = 0.1\n",
    "FRICTION = 1.0 # 1 means no FRICTION, less means FRICTION\n",
    "INIT_DELAY_FRAMES = 30\n",
    "GRAVITY = -9.8*2*1.5\n",
    "\n",
    "MAXLIVES = 5 # game ends when one agent loses this many games\n",
    "\n",
    "WINDOW_WIDTH = 1200\n",
    "WINDOW_HEIGHT = 500\n",
    "\n",
    "FACTOR = WINDOW_WIDTH / REF_W\n",
    "\n",
    "# if set to true, renders using cv2 directly on numpy array\n",
    "# (otherwise uses pyglet / opengl -> much smoother for human player)\n",
    "PIXEL_MODE = False\n",
    "PIXEL_SCALE = 4 # first render at multiple of Pixel Obs resolution, then downscale. Looks better.\n",
    "\n",
    "PIXEL_WIDTH = 84*2*1\n",
    "PIXEL_HEIGHT = 84*1\n",
    "\n",
    "def setNightColors():\n",
    "  ### night time color:\n",
    "  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n",
    "  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n",
    "  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n",
    "  BALL_COLOR = (217, 79, 0)\n",
    "  AGENT_LEFT_COLOR = (35, 93, 188)\n",
    "  AGENT_RIGHT_COLOR = (255, 236, 0)\n",
    "  PIXEL_AGENT_LEFT_COLOR = (255, 191, 0) # AMBER\n",
    "  PIXEL_AGENT_RIGHT_COLOR = (255, 191, 0) # AMBER\n",
    "  \n",
    "  BACKGROUND_COLOR = (11, 16, 19)\n",
    "  FENCE_COLOR = (102, 56, 35)\n",
    "  COIN_COLOR = FENCE_COLOR\n",
    "  GROUND_COLOR = (116, 114, 117)\n",
    "\n",
    "def setDayColors():\n",
    "  ### day time color:\n",
    "  ### note: do not use day time colors for pixel-obs training.\n",
    "  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n",
    "  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n",
    "  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n",
    "  global PIXEL_SCALE, PIXEL_WIDTH, PIXEL_HEIGHT\n",
    "  PIXEL_SCALE = int(4*1.0)\n",
    "  PIXEL_WIDTH = int(84*2*1.0)\n",
    "  PIXEL_HEIGHT = int(84*1.0)\n",
    "  BALL_COLOR = (255, 200, 20)\n",
    "  AGENT_LEFT_COLOR = (240, 75, 0)\n",
    "  AGENT_RIGHT_COLOR = (0, 150, 255)\n",
    "  PIXEL_AGENT_LEFT_COLOR = (240, 75, 0)\n",
    "  PIXEL_AGENT_RIGHT_COLOR = (0, 150, 255)\n",
    "  \n",
    "  BACKGROUND_COLOR = (255, 255, 255)\n",
    "  FENCE_COLOR = (240, 210, 130)\n",
    "  COIN_COLOR = FENCE_COLOR\n",
    "  GROUND_COLOR = (128, 227, 153)\n",
    "\n",
    "setNightColors()\n",
    "\n",
    "# by default, don't load rendering (since we want to use it in headless cloud machines)\n",
    "rendering = None\n",
    "def checkRendering():\n",
    "  global rendering\n",
    "  if rendering is None:\n",
    "    from gym.envs.classic_control import rendering as rendering\n",
    "\n",
    "def setPixelObsMode():\n",
    "  \"\"\"\n",
    "  used for experimental pixel-observation mode\n",
    "  note: new dim's chosen to be PIXEL_SCALE (2x) as Pixel Obs dims (will be downsampled)\n",
    "  also, both agent colors are identical, to potentially facilitate multiagent\n",
    "  \"\"\"\n",
    "  global WINDOW_WIDTH, WINDOW_HEIGHT, FACTOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR, PIXEL_MODE\n",
    "  PIXEL_MODE = True\n",
    "  WINDOW_WIDTH = PIXEL_WIDTH * PIXEL_SCALE\n",
    "  WINDOW_HEIGHT = PIXEL_HEIGHT * PIXEL_SCALE\n",
    "  FACTOR = WINDOW_WIDTH / REF_W\n",
    "  AGENT_LEFT_COLOR = PIXEL_AGENT_LEFT_COLOR\n",
    "  AGENT_RIGHT_COLOR = PIXEL_AGENT_RIGHT_COLOR\n",
    "\n",
    "def upsize_image(img):\n",
    "  return cv2.resize(img, (PIXEL_WIDTH * PIXEL_SCALE, PIXEL_HEIGHT * PIXEL_SCALE), interpolation=cv2.INTER_NEAREST)\n",
    "def downsize_image(img):\n",
    "  return cv2.resize(img, (PIXEL_WIDTH, PIXEL_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# conversion from space to pixels (allows us to render to diff resolutions)\n",
    "def toX(x):\n",
    "  return (x+REF_W/2)*FACTOR\n",
    "def toP(x):\n",
    "  return (x)*FACTOR\n",
    "def toY(y):\n",
    "  return y*FACTOR\n",
    "\n",
    "class DelayScreen:\n",
    "  \"\"\" initially the ball is held still for INIT_DELAY_FRAMES(30) frames \"\"\"\n",
    "  def __init__(self, life=INIT_DELAY_FRAMES):\n",
    "    self.life = 0\n",
    "    self.reset(life)\n",
    "  def reset(self, life=INIT_DELAY_FRAMES):\n",
    "    self.life = life\n",
    "  def status(self):\n",
    "    if (self.life == 0):\n",
    "      return True\n",
    "    self.life -= 1\n",
    "    return False\n",
    "\n",
    "def make_half_circle(radius=10, res=20, filled=True):\n",
    "  \"\"\" helper function for pyglet renderer\"\"\"\n",
    "  points = []\n",
    "  for i in range(res+1):\n",
    "    ang = math.pi-math.pi*i / res\n",
    "    points.append((math.cos(ang)*radius, math.sin(ang)*radius))\n",
    "  if filled:\n",
    "    return rendering.FilledPolygon(points)\n",
    "  else:\n",
    "    return rendering.PolyLine(points, True)\n",
    "\n",
    "def _add_attrs(geom, color):\n",
    "  \"\"\" help scale the colors from 0-255 to 0.0-1.0 (pyglet renderer) \"\"\"\n",
    "  r = color[0]\n",
    "  g = color[1]\n",
    "  b = color[2]\n",
    "  geom.set_color(r/255., g/255., b/255.)\n",
    "\n",
    "def create_canvas(canvas, c):\n",
    "  if PIXEL_MODE:\n",
    "    result = np.ones((WINDOW_HEIGHT, WINDOW_WIDTH, 3), dtype=np.uint8)\n",
    "    for channel in range(3):\n",
    "      result[:, :, channel] *= c[channel]\n",
    "    return result\n",
    "  else:\n",
    "    rect(canvas, 0, 0, WINDOW_WIDTH, -WINDOW_HEIGHT, color=BACKGROUND_COLOR)\n",
    "    return canvas\n",
    "\n",
    "def rect(canvas, x, y, width, height, color):\n",
    "  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n",
    "  if PIXEL_MODE:\n",
    "    canvas = cv2.rectangle(canvas, (round(x), round(WINDOW_HEIGHT-y)),\n",
    "      (round(x+width), round(WINDOW_HEIGHT-y+height)),\n",
    "      color, thickness=-1, lineType=cv2.LINE_AA)\n",
    "    return canvas\n",
    "  else:\n",
    "    box = rendering.make_polygon([(0,0), (0,-height), (width, -height), (width,0)])\n",
    "    trans = rendering.Transform()\n",
    "    trans.set_translation(x, y)\n",
    "    _add_attrs(box, color)\n",
    "    box.add_attr(trans)\n",
    "    canvas.add_onetime(box)\n",
    "    return canvas\n",
    "\n",
    "def half_circle(canvas, x, y, r, color):\n",
    "  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n",
    "  if PIXEL_MODE:\n",
    "    return cv2.ellipse(canvas, (round(x), WINDOW_HEIGHT-round(y)),\n",
    "      (round(r), round(r)), 0, 0, -180, color, thickness=-1, lineType=cv2.LINE_AA)\n",
    "  else:\n",
    "    geom = make_half_circle(r)\n",
    "    # TODO: Transparency\n",
    "#     def render1_fn(self):\n",
    "#         if   len(self.v) == 4 : glBegin(GL_QUADS)\n",
    "#         elif len(self.v)  > 4 : glBegin(GL_POLYGON)\n",
    "#         else: glBegin(GL_TRIANGLES)\n",
    "#         glEnable(GL_BLEND)\n",
    "#         glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\n",
    "#         for p in self.v:\n",
    "#             glVertex3f(p[0], p[1],0)  # draw each vertex\n",
    "#         glEnd()\n",
    "#     setattr(geom, \"render1\", render1_fn)\n",
    "    trans = rendering.Transform()\n",
    "    trans.set_translation(x, y)\n",
    "    _add_attrs(geom, color)\n",
    "    geom.add_attr(trans)\n",
    "    canvas.add_onetime(geom)\n",
    "    return canvas\n",
    "\n",
    "def circle(canvas, x, y, r, color):\n",
    "  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n",
    "  if PIXEL_MODE:\n",
    "    return cv2.circle(canvas, (round(x), round(WINDOW_HEIGHT-y)), round(r),\n",
    "      color, thickness=-1, lineType=cv2.LINE_AA)\n",
    "  else:\n",
    "    geom = rendering.make_circle(r, res=40)\n",
    "    trans = rendering.Transform()\n",
    "    trans.set_translation(x, y)\n",
    "    _add_attrs(geom, color)\n",
    "    geom.add_attr(trans)\n",
    "    canvas.add_onetime(geom)\n",
    "    return canvas\n",
    "\n",
    "class Particle:\n",
    "  \"\"\" used for the ball, and also for the round stub above the fence \"\"\"\n",
    "  def __init__(self, x, y, vx, vy, r, c):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    self.prev_x = self.x\n",
    "    self.prev_y = self.y\n",
    "    self.vx = vx\n",
    "    self.vy = vy\n",
    "    self.r = r\n",
    "    self.c = c\n",
    "  def display(self, canvas):\n",
    "    return circle(canvas, toX(self.x), toY(self.y), toP(self.r), color=self.c)\n",
    "  def move(self):\n",
    "    self.prev_x = self.x\n",
    "    self.prev_y = self.y\n",
    "    self.x += self.vx * TIMESTEP\n",
    "    self.y += self.vy * TIMESTEP\n",
    "  def applyAcceleration(self, ax, ay):\n",
    "    self.vx += ax * TIMESTEP\n",
    "    self.vy += ay * TIMESTEP\n",
    "  def checkEdges(self):\n",
    "    if (self.x<=(self.r-REF_W/2)):\n",
    "      self.vx *= -FRICTION\n",
    "      self.x = self.r-REF_W/2+NUDGE*TIMESTEP\n",
    "\n",
    "    if (self.x >= (REF_W/2-self.r)):\n",
    "      self.vx *= -FRICTION;\n",
    "      self.x = REF_W/2-self.r-NUDGE*TIMESTEP\n",
    "\n",
    "    if (self.y<=(self.r+REF_U)):\n",
    "      self.vy *= -FRICTION\n",
    "      self.y = self.r+REF_U+NUDGE*TIMESTEP\n",
    "      if (self.x <= 0):\n",
    "        return -1\n",
    "      else:\n",
    "        return 1\n",
    "    if (self.y >= (REF_H-self.r)):\n",
    "      self.vy *= -FRICTION\n",
    "      self.y = REF_H-self.r-NUDGE*TIMESTEP\n",
    "    # fence:\n",
    "    if ((self.x <= (REF_WALL_WIDTH/2+self.r)) and (self.prev_x > (REF_WALL_WIDTH/2+self.r)) and (self.y <= REF_WALL_HEIGHT)):\n",
    "      self.vx *= -FRICTION\n",
    "      self.x = REF_WALL_WIDTH/2+self.r+NUDGE*TIMESTEP\n",
    "\n",
    "    if ((self.x >= (-REF_WALL_WIDTH/2-self.r)) and (self.prev_x < (-REF_WALL_WIDTH/2-self.r)) and (self.y <= REF_WALL_HEIGHT)):\n",
    "      self.vx *= -FRICTION\n",
    "      self.x = -REF_WALL_WIDTH/2-self.r-NUDGE*TIMESTEP\n",
    "    return 0;\n",
    "  def getDist2(self, p): # returns distance squared from p\n",
    "    dy = p.y - self.y\n",
    "    dx = p.x - self.x\n",
    "    return (dx*dx+dy*dy)\n",
    "  def isColliding(self, p): # returns true if it is colliding w/ p\n",
    "    r = self.r+p.r\n",
    "    return (r*r > self.getDist2(p)) # if distance is less than total radius, then colliding.\n",
    "  def bounce(self, p): # bounce two balls that have collided (this and that)\n",
    "    abx = self.x-p.x\n",
    "    aby = self.y-p.y\n",
    "    abd = math.sqrt(abx*abx+aby*aby)\n",
    "    abx /= abd # normalize\n",
    "    aby /= abd\n",
    "    nx = abx # reuse calculation\n",
    "    ny = aby\n",
    "    abx *= NUDGE\n",
    "    aby *= NUDGE\n",
    "    while(self.isColliding(p)):\n",
    "      self.x += abx\n",
    "      self.y += aby\n",
    "    ux = self.vx - p.vx\n",
    "    uy = self.vy - p.vy\n",
    "    un = ux*nx + uy*ny\n",
    "    unx = nx*(un*2.) # added factor of 2\n",
    "    uny = ny*(un*2.) # added factor of 2\n",
    "    ux -= unx\n",
    "    uy -= uny\n",
    "    self.vx = ux + p.vx\n",
    "    self.vy = uy + p.vy\n",
    "  def limitSpeed(self, minSpeed, maxSpeed):\n",
    "    mag2 = self.vx*self.vx+self.vy*self.vy;\n",
    "    if (mag2 > (maxSpeed*maxSpeed) ):\n",
    "      mag = math.sqrt(mag2)\n",
    "      self.vx /= mag\n",
    "      self.vy /= mag\n",
    "      self.vx *= maxSpeed\n",
    "      self.vy *= maxSpeed\n",
    "\n",
    "    if (mag2 < (minSpeed*minSpeed) ):\n",
    "      mag = math.sqrt(mag2)\n",
    "      self.vx /= mag\n",
    "      self.vy /= mag\n",
    "      self.vx *= minSpeed\n",
    "      self.vy *= minSpeed\n",
    "\n",
    "class Wall:\n",
    "  \"\"\" used for the fence, and also the ground \"\"\"\n",
    "  def __init__(self, x, y, w, h, c):\n",
    "    self.x = x;\n",
    "    self.y = y;\n",
    "    self.w = w;\n",
    "    self.h = h;\n",
    "    self.c = c\n",
    "  def display(self, canvas):\n",
    "    return rect(canvas, toX(self.x-self.w/2), toY(self.y+self.h/2), toP(self.w), toP(self.h), color=self.c)\n",
    "\n",
    "class RelativeState:\n",
    "  \"\"\"\n",
    "  keeps track of the obs.\n",
    "  Note: the observation is from the perspective of the agent.\n",
    "  an agent playing either side of the fence must see obs the same way\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    # agent\n",
    "    self.x = 0\n",
    "    self.y = 0\n",
    "    self.vx = 0\n",
    "    self.vy = 0\n",
    "    # ball\n",
    "    self.bx = 0\n",
    "    self.by = 0\n",
    "    self.bvx = 0\n",
    "    self.bvy = 0\n",
    "    # opponent\n",
    "    self.ox = 0\n",
    "    self.oy = 0\n",
    "    self.ovx = 0\n",
    "    self.ovy = 0\n",
    "  def getObservation(self):\n",
    "    result = [self.x, self.y, self.vx, self.vy,\n",
    "              self.bx, self.by, self.bvx, self.bvy,\n",
    "              self.ox, self.oy, self.ovx, self.ovy]\n",
    "    scaleFactor = 10.0  # scale inputs to be in the order of magnitude of 10 for neural network.\n",
    "    result = np.array(result) / scaleFactor\n",
    "    return result\n",
    "  def copy(self):\n",
    "    rs = RelativeState()\n",
    "    rs.x = self.x\n",
    "    rs.y = self.y\n",
    "    rs.vx = self.vx\n",
    "    rs.vy = self.vy\n",
    "    rs.bx = self.bx\n",
    "    rs.by = self.by\n",
    "    rs.bvx = self.bvx\n",
    "    rs.bvy = self.bvy\n",
    "    rs.ox = self.ox\n",
    "    rs.oy = self.oy\n",
    "    rs.ovx = self.ovx\n",
    "    rs.ovy = self.ovy\n",
    "    return rs\n",
    "\n",
    "class Agent:\n",
    "  \"\"\" keeps track of the agent in the game. note this is not the policy network \"\"\"\n",
    "  def __init__(self, dir, x, y, c):\n",
    "    self.dir = dir # -1 means left, 1 means right player for symmetry.\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    self.r = 1.5\n",
    "    self.c = c\n",
    "    self.vx = 0\n",
    "    self.vy = 0\n",
    "    self.desired_vx = 0\n",
    "    self.desired_vy = 0\n",
    "    self.state = RelativeState()\n",
    "    self.emotion = \"happy\"; # hehe...\n",
    "    self.life = MAXLIVES\n",
    "  def lives(self):\n",
    "    return self.life\n",
    "  def setAction(self, action):\n",
    "    forward = False\n",
    "    backward = False\n",
    "    jump = False\n",
    "    if action[0] > 0:\n",
    "      forward = True\n",
    "    if action[1] > 0:\n",
    "      backward = True\n",
    "    if action[2] > 0:\n",
    "      jump = True\n",
    "    self.desired_vx = 0\n",
    "    self.desired_vy = 0\n",
    "    if (forward and (not backward)):\n",
    "      self.desired_vx = -PLAYER_SPEED_X\n",
    "    if (backward and (not forward)):\n",
    "      self.desired_vx = PLAYER_SPEED_X\n",
    "    if jump:\n",
    "      self.desired_vy = PLAYER_SPEED_Y\n",
    "  def move(self):\n",
    "    self.x += self.vx * TIMESTEP\n",
    "    self.y += self.vy * TIMESTEP\n",
    "  def step(self):\n",
    "    self.x += self.vx * TIMESTEP\n",
    "    self.y += self.vy * TIMESTEP\n",
    "  def update(self):\n",
    "    self.vy += GRAVITY * TIMESTEP\n",
    "\n",
    "    if (self.y <= REF_U + NUDGE*TIMESTEP):\n",
    "      self.vy = self.desired_vy\n",
    "\n",
    "    self.vx = self.desired_vx*self.dir\n",
    "\n",
    "    self.move()\n",
    "\n",
    "    if (self.y <= REF_U):\n",
    "      self.y = REF_U;\n",
    "      self.vy = 0;\n",
    "\n",
    "    # stay in their own half:\n",
    "    if (self.x*self.dir <= (REF_WALL_WIDTH/2+self.r) ):\n",
    "      self.vx = 0;\n",
    "      self.x = self.dir*(REF_WALL_WIDTH/2+self.r)\n",
    "\n",
    "    if (self.x*self.dir >= (REF_W/2-self.r) ):\n",
    "      self.vx = 0;\n",
    "      self.x = self.dir*(REF_W/2-self.r)\n",
    "  def updateState(self, ball, opponent):\n",
    "    \"\"\" normalized to side, appears different for each agent's perspective\"\"\"\n",
    "    # agent's self\n",
    "    self.state.x = self.x*self.dir\n",
    "    self.state.y = self.y\n",
    "    self.state.vx = self.vx*self.dir\n",
    "    self.state.vy = self.vy\n",
    "    # ball\n",
    "    self.state.bx = ball.x*self.dir\n",
    "    self.state.by = ball.y\n",
    "    self.state.bvx = ball.vx*self.dir\n",
    "    self.state.bvy = ball.vy\n",
    "    # opponent\n",
    "    self.state.ox = opponent.x*(-self.dir)\n",
    "    self.state.oy = opponent.y\n",
    "    self.state.ovx = opponent.vx*(-self.dir)\n",
    "    self.state.ovy = opponent.vy\n",
    "  def getObservation(self):\n",
    "    return self.state.getObservation()\n",
    "\n",
    "  def display(self, canvas, bx, by):\n",
    "    x = self.x\n",
    "    y = self.y\n",
    "    r = self.r\n",
    "\n",
    "    angle = math.pi * 60 / 180\n",
    "    if self.dir == 1:\n",
    "      angle = math.pi * 120 / 180\n",
    "    eyeX = 0\n",
    "    eyeY = 0\n",
    "\n",
    "    canvas = half_circle(canvas, toX(x), toY(y), toP(r), color=self.c)\n",
    "\n",
    "    # track ball with eyes (replace with observed info later):\n",
    "    c = math.cos(angle)\n",
    "    s = math.sin(angle)\n",
    "    ballX = bx-(x+(0.6)*r*c);\n",
    "    ballY = by-(y+(0.6)*r*s);\n",
    "\n",
    "    if (self.emotion == \"sad\"):\n",
    "      ballX = -self.dir\n",
    "      ballY = -3\n",
    "\n",
    "    dist = math.sqrt(ballX*ballX+ballY*ballY)\n",
    "    eyeX = ballX/dist\n",
    "    eyeY = ballY/dist\n",
    "\n",
    "    canvas = circle(canvas, toX(x+(0.6)*r*c), toY(y+(0.6)*r*s), toP(r)*0.3, color=(255, 255, 255))\n",
    "    canvas = circle(canvas, toX(x+(0.6)*r*c+eyeX*0.15*r), toY(y+(0.6)*r*s+eyeY*0.15*r), toP(r)*0.1, color=(0, 0, 0))\n",
    "\n",
    "    # draw coins (lives) left\n",
    "    for i in range(1, self.life):\n",
    "      canvas = circle(canvas, toX(self.dir*(REF_W/2+0.5-i*2.)), WINDOW_HEIGHT-toY(1.5), toP(0.5), color=COIN_COLOR)\n",
    "\n",
    "    return canvas\n",
    "  def clone(self):\n",
    "    agent_clone = Agent(self.dir, self.x, self.y, self.c)\n",
    "    agent_clone.r = self.r\n",
    "    agent_clone.vx = self.vx\n",
    "    agent_clone.vy = self.vy\n",
    "    agent_clone.desired_vx = self.desired_vx\n",
    "    agent_clone.state = self.state.copy()\n",
    "    agent_clone.emotion = self.emotion\n",
    "    agent_clone.life = self.life\n",
    "    return agent_clone\n",
    "\n",
    "class BaselinePolicy:\n",
    "  \"\"\" Tiny RNN policy with only 120 parameters of otoro.net/slimevolley agent \"\"\"\n",
    "  def __init__(self):\n",
    "    self.nGameInput = 8 # 8 states for agent\n",
    "    self.nGameOutput = 3 # 3 buttons (forward, backward, jump)\n",
    "    self.nRecurrentState = 4 # extra recurrent states for feedback.\n",
    "\n",
    "    self.nOutput = self.nGameOutput+self.nRecurrentState\n",
    "    self.nInput = self.nGameInput+self.nOutput\n",
    "    \n",
    "    # store current inputs and outputs\n",
    "    self.inputState = np.zeros(self.nInput)\n",
    "    self.outputState = np.zeros(self.nOutput)\n",
    "    self.prevOutputState = np.zeros(self.nOutput)\n",
    "\n",
    "    \"\"\"See training details: https://blog.otoro.net/2015/03/28/neural-slime-volleyball/ \"\"\"\n",
    "    self.weight = np.array(\n",
    "      [7.5719, 4.4285, 2.2716, -0.3598, -7.8189, -2.5422, -3.2034, 0.3935, 1.2202, -0.49, -0.0316, 0.5221, 0.7026, 0.4179, -2.1689,\n",
    "       1.646, -13.3639, 1.5151, 1.1175, -5.3561, 5.0442, 0.8451, 0.3987, -2.9501, -3.7811, -5.8994, 6.4167, 2.5014, 7.338, -2.9887,\n",
    "       2.4586, 13.4191, 2.7395, -3.9708, 1.6548, -2.7554, -1.5345, -6.4708, 9.2426, -0.7392, 0.4452, 1.8828, -2.6277, -10.851, -3.2353,\n",
    "       -4.4653, -3.1153, -1.3707, 7.318, 16.0902, 1.4686, 7.0391, 1.7765, -1.155, 2.6697, -8.8877, 1.1958, -3.2839, -5.4425, 1.6809,\n",
    "       7.6812, -2.4732, 1.738, 0.3781, 0.8718, 2.5886, 1.6911, 1.2953, -9.0052, -4.6038, -6.7447, -2.5528, 0.4391, -4.9278, -3.6695,\n",
    "       -4.8673, -1.6035, 1.5011, -5.6124, 4.9747, 1.8998, 3.0359, 6.2983, -4.8568, -2.1888, -4.1143, -3.9874, -0.0459, 4.7134, 2.8952,\n",
    "       -9.3627, -4.685, 0.3601, -1.3699, 9.7294, 11.5596, 0.1918, 3.0783, 0.0329, -0.1362, -0.1188, -0.7579, 0.3278, -0.977, -0.9377])\n",
    "\n",
    "    self.bias = np.array([2.2935,-2.0353,-1.7786,5.4567,-3.6368,3.4996,-0.0685])\n",
    "\n",
    "    # unflatten weight, convert it into 7x15 matrix.\n",
    "    self.weight = self.weight.reshape(self.nGameOutput+self.nRecurrentState,\n",
    "      self.nGameInput+self.nGameOutput+self.nRecurrentState)\n",
    "  def reset(self):\n",
    "    self.inputState = np.zeros(self.nInput)\n",
    "    self.outputState = np.zeros(self.nOutput)\n",
    "    self.prevOutputState = np.zeros(self.nOutput)\n",
    "  def _forward(self):\n",
    "    self.prevOutputState = self.outputState\n",
    "    self.outputState = np.tanh(np.dot(self.weight, self.inputState)+self.bias)\n",
    "  def _setInputState(self, obs):\n",
    "    # obs is: (op is opponent). obs is also from perspective of the agent (x values negated for other agent)\n",
    "    [x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy, op_x, op_y, op_vx, op_vy] = obs\n",
    "    self.inputState[0:self.nGameInput] = np.array([x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy])\n",
    "    self.inputState[self.nGameInput:] = self.outputState\n",
    "  def _getAction(self):\n",
    "    forward = 0\n",
    "    backward = 0\n",
    "    jump = 0\n",
    "    if (self.outputState[0] > 0.75):\n",
    "      forward = 1\n",
    "    if (self.outputState[1] > 0.75):\n",
    "      backward = 1\n",
    "    if (self.outputState[2] > 0.75):\n",
    "      jump = 1\n",
    "    return [forward, backward, jump]\n",
    "  def predict(self, obs):\n",
    "    \"\"\" take obs, update rnn state, return action \"\"\"\n",
    "    self._setInputState(obs)\n",
    "    self._forward()\n",
    "    return self._getAction()\n",
    "\n",
    "class Game:\n",
    "  \"\"\"\n",
    "  the main slime volley game.\n",
    "  can be used in various settings, such as ai vs ai, ai vs human, human vs human\n",
    "  \"\"\"\n",
    "  def __init__(self, np_random=np.random):\n",
    "    self.ball = None\n",
    "    self.ground = None\n",
    "    self.fence = None\n",
    "    self.fenceStub = None\n",
    "    self.agent_left = None\n",
    "    self.agent_right = None\n",
    "    self.agent_clones = None\n",
    "    self.n_clones = None\n",
    "    self.delayScreen = None\n",
    "    self.np_random = np_random\n",
    "    self.reset()\n",
    "  def createClones(self, n=3):\n",
    "    self.n_clones = n\n",
    "    self.agent_clones = [self.agent_right.clone() for _ in range(self.n_clones)]\n",
    "  def updateClones(self, ball, opponent):\n",
    "    for clone in self.agent_clones:\n",
    "      clone.updateState(ball, opponent)\n",
    "  def reset(self):\n",
    "    self.ground = Wall(0, 0.75, REF_W, REF_U, c=GROUND_COLOR)\n",
    "    self.fence = Wall(0, 0.75 + REF_WALL_HEIGHT/2, REF_WALL_WIDTH, (REF_WALL_HEIGHT-1.5), c=FENCE_COLOR)\n",
    "    self.fenceStub = Particle(0, REF_WALL_HEIGHT, 0, 0, REF_WALL_WIDTH/2, c=FENCE_COLOR);\n",
    "    ball_vx = self.np_random.uniform(low=-20, high=20)\n",
    "    ball_vy = self.np_random.uniform(low=10, high=25)\n",
    "    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n",
    "    self.agent_left = Agent(-1, -REF_W/4, 1.5, c=AGENT_LEFT_COLOR)\n",
    "    self.agent_right = Agent(1, REF_W/4, 1.5, c=AGENT_RIGHT_COLOR)\n",
    "    self.agent_left.updateState(self.ball, self.agent_right)\n",
    "    self.agent_right.updateState(self.ball, self.agent_left)\n",
    "    if self.agent_clones is not None:\n",
    "        self.createClones(n=self.n_clones)\n",
    "        self.updateClones(self.ball, self.agent_left)\n",
    "    self.delayScreen = DelayScreen()\n",
    "  def newMatch(self):\n",
    "    ball_vx = self.np_random.uniform(low=-20, high=20)\n",
    "    ball_vy = self.np_random.uniform(low=10, high=25)\n",
    "    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n",
    "    self.delayScreen.reset()\n",
    "  def step(self):\n",
    "    \"\"\" main game loop \"\"\"\n",
    "\n",
    "    self.betweenGameControl()\n",
    "    self.agent_left.update()\n",
    "    self.agent_right.update()\n",
    "    for clone in self.agent_clones:\n",
    "        clone.update()\n",
    "\n",
    "    if self.delayScreen.status():\n",
    "      self.ball.applyAcceleration(0, GRAVITY)\n",
    "      self.ball.limitSpeed(0, MAX_BALL_SPEED)\n",
    "      self.ball.move()\n",
    "\n",
    "    if (self.ball.isColliding(self.agent_left)):\n",
    "      self.ball.bounce(self.agent_left)\n",
    "    if (self.ball.isColliding(self.agent_right)):\n",
    "      self.ball.bounce(self.agent_right)\n",
    "      if self.agent_clones is not None:\n",
    "        self.createClones(n=self.n_clones)\n",
    "    if (self.ball.isColliding(self.fenceStub)):\n",
    "      self.ball.bounce(self.fenceStub)\n",
    "\n",
    "    # negated, since we want reward to be from the persepctive of right agent being trained.\n",
    "    result = -self.ball.checkEdges()\n",
    "\n",
    "    if (result != 0):\n",
    "      self.newMatch() # not reset, but after a point is scored\n",
    "      if result < 0: # baseline agent won\n",
    "        self.agent_left.emotion = \"happy\"\n",
    "        self.agent_right.emotion = \"sad\"\n",
    "        self.agent_right.life -= 1\n",
    "      else:\n",
    "        self.agent_left.emotion = \"sad\"\n",
    "        self.agent_right.emotion = \"happy\"\n",
    "        self.agent_left.life -= 1\n",
    "      return result\n",
    "\n",
    "    # update internal states (the last thing to do)\n",
    "    self.agent_left.updateState(self.ball, self.agent_right)\n",
    "    self.agent_right.updateState(self.ball, self.agent_left)\n",
    "    if self.agent_clones is not None:\n",
    "      self.updateClones(self.ball, self.agent_left)\n",
    "\n",
    "    return result\n",
    "  def display(self, canvas):\n",
    "    # background color\n",
    "    # if PIXEL_MODE is True, canvas is an RGB array.\n",
    "    # if PIXEL_MODE is False, canvas is viewer object\n",
    "    canvas = create_canvas(canvas, c=BACKGROUND_COLOR)\n",
    "    canvas = self.fence.display(canvas)\n",
    "    canvas = self.fenceStub.display(canvas)\n",
    "    canvas = self.agent_left.display(canvas, self.ball.x, self.ball.y)\n",
    "    canvas = self.agent_right.display(canvas, self.ball.x, self.ball.y)\n",
    "    if self.agent_clones is not None:\n",
    "      for clone in self.agent_clones:\n",
    "        canvas = clone.display(canvas, self.ball.x, self.ball.y)\n",
    "    canvas = self.ball.display(canvas)\n",
    "    canvas = self.ground.display(canvas)\n",
    "    return canvas\n",
    "  def betweenGameControl(self):\n",
    "    agent = [self.agent_left, self.agent_right]\n",
    "    if (self.delayScreen.life > 0):\n",
    "      pass\n",
    "      '''\n",
    "      for i in range(2):\n",
    "        if (agent[i].emotion == \"sad\"):\n",
    "          agent[i].setAction([0, 0, 0]) # nothing\n",
    "      '''\n",
    "    else:\n",
    "      agent[0].emotion = \"happy\"\n",
    "      agent[1].emotion = \"happy\"\n",
    "\n",
    "class SlimeVolleyEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Gym wrapper for Slime Volley game.\n",
    "  By default, the agent you are training controls the right agent\n",
    "  on the right. The agent on the left is controlled by the baseline\n",
    "  RNN policy.\n",
    "  Game ends when an agent loses 5 matches (or at t=3000 timesteps).\n",
    "  Note: Optional mode for MARL experiments, like self-play which\n",
    "  deviates from Gym env. Can be enabled via supplying optional action\n",
    "  to override the default baseline agent's policy:\n",
    "  obs1, reward, done, info = env.step(action1, action2)\n",
    "  the next obs for the right agent is returned in the optional\n",
    "  fourth item from the step() method.\n",
    "  reward is in the perspective of the right agent so the reward\n",
    "  for the left agent is the negative of this number.\n",
    "  \"\"\"\n",
    "  metadata = {\n",
    "    'render.modes': ['human', 'rgb_array', 'state'],\n",
    "    'video.frames_per_second' : 50\n",
    "  }\n",
    "\n",
    "  # for compatibility with typical atari wrappers\n",
    "  atari_action_meaning = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"FIRE\",\n",
    "    2: \"UP\",\n",
    "    3: \"RIGHT\",\n",
    "    4: \"LEFT\",\n",
    "    5: \"DOWN\",\n",
    "    6: \"UPRIGHT\",\n",
    "    7: \"UPLEFT\",\n",
    "    8: \"DOWNRIGHT\",\n",
    "    9: \"DOWNLEFT\",\n",
    "    10: \"UPFIRE\",\n",
    "    11: \"RIGHTFIRE\",\n",
    "    12: \"LEFTFIRE\",\n",
    "    13: \"DOWNFIRE\",\n",
    "    14: \"UPRIGHTFIRE\",\n",
    "    15: \"UPLEFTFIRE\",\n",
    "    16: \"DOWNRIGHTFIRE\",\n",
    "    17: \"DOWNLEFTFIRE\",\n",
    "  }\n",
    "  atari_action_set = {\n",
    "    0, # NOOP\n",
    "    4, # LEFT\n",
    "    7, # UPLEFT\n",
    "    2, # UP\n",
    "    6, # UPRIGHT\n",
    "    3, # RIGHT\n",
    "  }\n",
    "\n",
    "  action_table = [[0, 0, 0], # NOOP\n",
    "                  [1, 0, 0], # LEFT (forward)\n",
    "                  [1, 0, 1], # UPLEFT (forward jump)\n",
    "                  [0, 0, 1], # UP (jump)\n",
    "                  [0, 1, 1], # UPRIGHT (backward jump)\n",
    "                  [0, 1, 0]] # RIGHT (backward)\n",
    "\n",
    "  from_pixels = False\n",
    "  atari_mode = False\n",
    "  survival_bonus = False # Depreciated: augment reward, easier to train\n",
    "  multiagent = True # optional args anyways\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Reward modes:\n",
    "    net score = right agent wins minus left agent wins\n",
    "    0: returns net score (basic reward)\n",
    "    1: returns 0.01 x number of timesteps (max 3000) (survival reward)\n",
    "    2: sum of basic reward and survival reward\n",
    "    0 is suitable for evaluation, while 1 and 2 may be good for training\n",
    "    Setting multiagent to True puts in info (4th thing returned in stop)\n",
    "    the otherObs, the observation for the other agent. See multiagent.py\n",
    "    Setting self.from_pixels to True makes the observation with multiples\n",
    "    of 84, since usual atari wrappers downsample to 84x84\n",
    "    \"\"\"\n",
    "\n",
    "    self.t = 0\n",
    "    self.t_limit = 3000\n",
    "\n",
    "    #self.action_space = spaces.Box(0, 1.0, shape=(3,))\n",
    "    if self.atari_mode:\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "    else:\n",
    "      self.action_space = spaces.MultiBinary(3)\n",
    "\n",
    "    if self.from_pixels:\n",
    "      setPixelObsMode()\n",
    "      self.observation_space = spaces.Box(low=0, high=255,\n",
    "        shape=(PIXEL_HEIGHT, PIXEL_WIDTH, 3), dtype=np.uint8)\n",
    "    else:\n",
    "      high = np.array([np.finfo(np.float32).max] * 12)\n",
    "      self.observation_space = spaces.Box(-high, high)\n",
    "    self.canvas = None\n",
    "    self.previous_rgbarray = None\n",
    "\n",
    "    self.game = Game()\n",
    "    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n",
    "\n",
    "    self.policy = BaselinePolicy() # the “bad guy”\n",
    "\n",
    "    self.viewer = None\n",
    "\n",
    "    # another avenue to override the built-in AI's action, going past many env wraps:\n",
    "    self.otherAction = None\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    self.game = Game(np_random=self.np_random)\n",
    "    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n",
    "    return [seed]\n",
    "\n",
    "  def getObs(self):\n",
    "    if self.from_pixels:\n",
    "      obs = self.render(mode='state')\n",
    "      self.canvas = obs\n",
    "    else:\n",
    "      obs = self.game.agent_right.getObservation()\n",
    "    return obs\n",
    "\n",
    "  def discreteToBox(self, n):\n",
    "    # convert discrete action n into the actual triplet action\n",
    "    if isinstance(n, (list, tuple, np.ndarray)): # original input for some reason, just leave it:\n",
    "      if len(n) == 3:\n",
    "        return n\n",
    "    assert (int(n) == n) and (n >= 0) and (n < 6)\n",
    "    return self.action_table[n]\n",
    "\n",
    "  def step(self, action, otherAction=None, cloneActions=None):\n",
    "    \"\"\"\n",
    "    baseAction is only used if multiagent mode is True\n",
    "    note: although the action space is multi-binary, float vectors\n",
    "    are fine (refer to setAction() to see how they get interpreted)\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    self.t += 1\n",
    "\n",
    "    if self.otherAction is not None:\n",
    "      otherAction = self.otherAction\n",
    "\n",
    "    if self.atari_mode:\n",
    "      action = self.discreteToBox(action)\n",
    "      if otherAction is not None:\n",
    "        otherAction = self.discreteToBox(otherAction)\n",
    "\n",
    "    if otherAction is None: # override baseline policy\n",
    "      obs = self.game.agent_left.getObservation()\n",
    "      otherAction = self.policy.predict(obs)\n",
    "    self.game.agent_left.setAction(otherAction)\n",
    "    self.game.agent_right.setAction(action) # external agent is agent_right\n",
    "    \n",
    "    if cloneActions is not None:\n",
    "        if self.game.agent_clones is None:\n",
    "            self.game.createClones(n=len(cloneActions))\n",
    "            self.game.updateClones(self.game.ball, self.game.agent_left)\n",
    "        else:\n",
    "            for clone, cloneAction in zip(self.game.agent_clones, cloneActions):\n",
    "                clone.setAction(cloneAction)\n",
    "\n",
    "    reward = self.game.step()\n",
    "\n",
    "    obs = self.getObs()\n",
    "\n",
    "    if self.t >= self.t_limit:\n",
    "      done = True\n",
    "\n",
    "    if self.game.agent_left.life <= 0 or self.game.agent_right.life <= 0:\n",
    "      done = True\n",
    "\n",
    "    otherObs = None\n",
    "    if self.multiagent:\n",
    "      if self.from_pixels:\n",
    "        otherObs = cv2.flip(obs, 1) # horizontal flip\n",
    "      else:\n",
    "        otherObs = self.game.agent_left.getObservation()\n",
    "        \n",
    "    if cloneActions is not None:\n",
    "        cloneObs = [clone.getObservation() for clone in self.game.agent_clones]\n",
    "\n",
    "    info = {\n",
    "      'ale.lives': self.game.agent_right.lives(),\n",
    "      'ale.otherLives': self.game.agent_left.lives(),\n",
    "      'otherObs': otherObs,\n",
    "      'state': self.game.agent_right.getObservation(),\n",
    "      'otherState': self.game.agent_left.getObservation(),\n",
    "      'cloneObs': cloneObs,\n",
    "    }\n",
    "\n",
    "    if self.survival_bonus:\n",
    "      return obs, reward+0.01, done, info\n",
    "    return obs, reward, done, info\n",
    "\n",
    "  def init_game_state(self):\n",
    "    self.t = 0\n",
    "    self.game.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.init_game_state()\n",
    "    return self.getObs()\n",
    "\n",
    "  def checkViewer(self):\n",
    "    # for opengl viewer\n",
    "    if self.viewer is None:\n",
    "      checkRendering()\n",
    "      self.viewer = rendering.SimpleImageViewer(maxwidth=2160) # macbook pro resolution\n",
    "\n",
    "  def render(self, mode='human', close=False):\n",
    "\n",
    "    if PIXEL_MODE:\n",
    "      if self.canvas is not None: # already rendered\n",
    "        rgb_array = self.canvas\n",
    "        self.canvas = None\n",
    "        if mode == 'rgb_array' or mode == 'human':\n",
    "          self.checkViewer()\n",
    "          larger_canvas = upsize_image(rgb_array)\n",
    "          self.viewer.imshow(larger_canvas)\n",
    "          if (mode=='rgb_array'):\n",
    "            return larger_canvas\n",
    "          else:\n",
    "            return\n",
    "\n",
    "      self.canvas = self.game.display(self.canvas)\n",
    "      # scale down to original res (looks better than rendering directly to lower res)\n",
    "      self.canvas = downsize_image(self.canvas)\n",
    "\n",
    "      if mode=='state':\n",
    "        return np.copy(self.canvas)\n",
    "\n",
    "      # upsampling w/ nearest interp method gives a retro \"pixel\" effect look\n",
    "      larger_canvas = upsize_image(self.canvas)\n",
    "      self.checkViewer()\n",
    "      self.viewer.imshow(larger_canvas)\n",
    "      if (mode=='rgb_array'):\n",
    "        return larger_canvas\n",
    "\n",
    "    else: # pyglet renderer\n",
    "      if self.viewer is None:\n",
    "        checkRendering()\n",
    "        self.viewer = rendering.Viewer(WINDOW_WIDTH, WINDOW_HEIGHT)\n",
    "\n",
    "      self.game.display(self.viewer)\n",
    "      return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "  def close(self):\n",
    "    if self.viewer:\n",
    "      self.viewer.close()\n",
    "    \n",
    "  def get_action_meanings(self):\n",
    "    return [self.atari_action_meaning[i] for i in self.atari_action_set]\n",
    "\n",
    "class SlimeVolleyPixelEnv(SlimeVolleyEnv):\n",
    "  from_pixels = True\n",
    "\n",
    "class SlimeVolleyAtariEnv(SlimeVolleyEnv):\n",
    "  from_pixels = True\n",
    "  atari_mode = True\n",
    "\n",
    "class SlimeVolleySurvivalAtariEnv(SlimeVolleyEnv):\n",
    "  from_pixels = True\n",
    "  atari_mode = True\n",
    "  survival_bonus = True\n",
    "\n",
    "class SurvivalRewardEnv(gym.RewardWrapper):\n",
    "  def __init__(self, env):\n",
    "    \"\"\"\n",
    "    adds 0.01 to the reward for every timestep agent survives\n",
    "    :param env: (Gym Environment) the environment\n",
    "    \"\"\"\n",
    "    gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "  def reward(self, reward):\n",
    "    \"\"\"\n",
    "    adds that extra survival bonus for living a bit longer!\n",
    "    :param reward: (float)\n",
    "    \"\"\"\n",
    "    return reward + 0.01\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "  def __init__(self, env, n_frames):\n",
    "    \"\"\"Stack n_frames last frames.\n",
    "    (don't use lazy frames)\n",
    "    modified from:\n",
    "    stable_baselines.common.atari_wrappers\n",
    "    :param env: (Gym Environment) the environment\n",
    "    :param n_frames: (int) the number of frames to stack\n",
    "    \"\"\"\n",
    "    gym.Wrapper.__init__(self, env)\n",
    "    self.n_frames = n_frames\n",
    "    self.frames = deque([], maxlen=n_frames)\n",
    "    shp = env.observation_space.shape\n",
    "    self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * n_frames),\n",
    "                                        dtype=env.observation_space.dtype)\n",
    "\n",
    "  def reset(self):\n",
    "    obs = self.env.reset()\n",
    "    for _ in range(self.n_frames):\n",
    "        self.frames.append(obs)\n",
    "    return self._get_ob()\n",
    "\n",
    "  def step(self, action):\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    self.frames.append(obs)\n",
    "    return self._get_ob(), reward, done, info\n",
    "\n",
    "  def _get_ob(self):\n",
    "    assert len(self.frames) == self.n_frames\n",
    "    return np.concatenate(list(self.frames), axis=2)\n",
    "\n",
    "#####################\n",
    "# helper functions: #\n",
    "#####################\n",
    "\n",
    "def multiagent_rollout(env, policy_right, policy_left, render_mode=False):\n",
    "  \"\"\"\n",
    "  play one agent vs the other in modified gym-style loop.\n",
    "  important: returns the score from perspective of policy_right.\n",
    "  \"\"\"\n",
    "  obs_right = env.reset()\n",
    "  obs_left = obs_right # same observation at the very beginning for the other agent\n",
    "\n",
    "  done = False\n",
    "  total_reward = 0\n",
    "  t = 0\n",
    "\n",
    "  while not done:\n",
    "\n",
    "    action_right = policy_right.predict(obs_right)\n",
    "    action_left = policy_left.predict(obs_left)\n",
    "\n",
    "    # uses a 2nd (optional) parameter for step to put in the other action\n",
    "    # and returns the other observation in the 4th optional \"info\" param in gym's step()\n",
    "    obs_right, reward, done, info = env.step(action_right, action_left)\n",
    "    obs_left = info['otherObs']\n",
    "\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "\n",
    "    if render_mode:\n",
    "      env.render()\n",
    "\n",
    "  return total_reward, t\n",
    "\n",
    "def render_atari(obs):\n",
    "  \"\"\"\n",
    "  Helper function that takes in a processed obs (84,84,4)\n",
    "  Useful for visualizing what an Atari agent actually *sees*\n",
    "  Outputs in Atari visual format (Top: resized to orig dimensions, buttom: 4 frames)\n",
    "  \"\"\"\n",
    "  tempObs = []\n",
    "  obs = np.copy(obs)\n",
    "  for i in range(4):\n",
    "    if i == 3:\n",
    "      latest = np.copy(obs[:, :, i])\n",
    "    if i > 0: # insert vertical lines\n",
    "      obs[:, 0, i] = 141\n",
    "    tempObs.append(obs[:, :, i])\n",
    "  latest = np.expand_dims(latest, axis=2)\n",
    "  latest = np.concatenate([latest*255.0] * 3, axis=2).astype(np.uint8)\n",
    "  latest = cv2.resize(latest, (84 * 8, 84 * 4), interpolation=cv2.INTER_NEAREST)\n",
    "  tempObs = np.concatenate(tempObs, axis=1)\n",
    "  tempObs = np.expand_dims(tempObs, axis=2)\n",
    "  tempObs = np.concatenate([tempObs*255.0] * 3, axis=2).astype(np.uint8)\n",
    "  tempObs = cv2.resize(tempObs, (84 * 8, 84 * 2), interpolation=cv2.INTER_NEAREST)\n",
    "  return np.concatenate([latest, tempObs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Reg envs for gym #\n",
    "####################\n",
    "\n",
    "register(\n",
    "    id='SlimeVolley-v0',\n",
    "    entry_point='slimevolleygym.slimevolley:SlimeVolleyEnv'\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='SlimeVolleyPixel-v0',\n",
    "    entry_point='slimevolleygym.slimevolley:SlimeVolleyPixelEnv'\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='SlimeVolleyNoFrameskip-v0',\n",
    "    entry_point='slimevolleygym.slimevolley:SlimeVolleyAtariEnv'\n",
    ")\n",
    "\n",
    "register(\n",
    "\n",
    "    id='SlimeVolleySurvivalNoFrameskip-v0',\n",
    "    entry_point='slimevolleygym.slimevolley:SlimeVolleySurvivalAtariEnv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# RENDER_MODE = True\n",
    "# N_CLONES = 5\n",
    "# policy = BaselinePolicy() # defaults to use RNN Baseline for player\n",
    "\n",
    "# env = SlimeVolleyEnv()\n",
    "# obs = env.reset()\n",
    "# cloneObs = [obs] * N_CLONES\n",
    "\n",
    "# steps = 0\n",
    "# total_reward = 0\n",
    "# action = np.array([0, 0, 0])\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "\n",
    "#     action = policy.predict(obs)\n",
    "#     cloneActions = [policy.predict(cloneOb) for cloneOb in cloneObs]\n",
    "#     obs, reward, done, info = env.step(action, cloneActions=cloneActions)\n",
    "#     cloneObs = info['cloneObs']\n",
    "\n",
    "#     total_reward += reward\n",
    "\n",
    "#     if RENDER_MODE:\n",
    "#         env.render()\n",
    "#         sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import IPython\n",
    "import imageio\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "def record_game(model, env, num_episodes=5, N_CLONES=5, video_filename='video.mp4'):\n",
    "    with imageio.get_writer(video_filename, fps=60) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            cloneObs = [obs] * N_CLONES\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            while not done:\n",
    "                action, _steps = model.predict(obs)\n",
    "                cloneActions = [model.predict(cloneOb)[0] for cloneOb in cloneObs]\n",
    "                obs, reward, done, info = env.step(action, cloneActions=cloneActions)\n",
    "                cloneObs = info['cloneObs']\n",
    "                total_reward += reward\n",
    "                video.append_data(env.render('rgb_array'))\n",
    "\n",
    "            print(\"score:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Slime-RL\\Jet\\model.py:131: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_probability\\python\\layers\\util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\distributions.py:465: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\ppo1\\pposgd_simple.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\ppo1\\pposgd_simple.py:162: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\notes\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\stable_baselines\\ppo1\\pposgd_simple.py:190: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "LOGDIR = \"exp/self/ppo-bnn-mujoco\"\n",
    "model = PPO1.load(os.path.join(LOGDIR, \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notes\\AppData\\Roaming\\Python\\Python37\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 1200) to (512, 1200) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fda4837cfeff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSlimeVolleyEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mvideo_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[0membed_mp4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b75a789084c5>\u001b[0m in \u001b[0;36mrecord_game\u001b[1;34m(model, env, num_episodes, N_CLONES, video_filename)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mcloneObs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cloneObs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-dfb11a5e36e1>\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender1\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# draw each vertex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mglEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_circle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck_glend\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\slime-rl\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: If AttributeError: 'ImageData' object has no attribute 'data' exists\n",
    "# Do pip install pyglet==1.3.2.\n",
    "\n",
    "video_filename = os.path.join(LOGDIR, \"viz_clone.mp4\")\n",
    "record_game(\n",
    "    model=model,\n",
    "    env=SlimeVolleyEnv(),\n",
    "    num_episodes=5,\n",
    "    video_filename=video_filename\n",
    ")\n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0247bb3e6f8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSlimeVolleyEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvideo_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[0membed_mp4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6b942e4edbf5>\u001b[0m in \u001b[0;36mrecord_game\u001b[1;34m(model, env, num_episodes, N_CLONES, video_filename)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mcloneActions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloneOb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcloneOb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcloneObs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloneActions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcloneActions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "video_filename = 'baseline-var_viz.mp4'\n",
    "record_game(\n",
    "    model=BaselinePolicy(),\n",
    "    env=SlimeVolleyEnv(),\n",
    "    num_episodes=5,\n",
    "    video_filename=video_filename\n",
    ")\n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slime-rl)",
   "language": "python",
   "name": "slime-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
